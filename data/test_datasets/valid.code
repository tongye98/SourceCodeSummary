def set Level ( level = 0 ) : Logger . level = level
def list imdbs ( ) : return sets . keys ( )
def get New Repository ( ) : return Lash Repository ( ) 
def get user config ( config file =  None  , default config =  False  ) : if default config : return copy . copy ( DEFAULT CONFIG ) if ( config file and ( config file is not USER CONFIG PATH ) ) : return get config ( config file ) try : env config file = os . environ [ u'COOKIECUTTER CONFIG' ] except  Key  Error  : if os . path . exists ( USER CONFIG PATH ) : return get config ( USER CONFIG PATH ) else : return copy . copy ( DEFAULT CONFIG ) else : return get config ( env config file ) 
def delete ( uuid ) : timings = [ ] errors = [ ] for index in range ( 0 , constants . NUM SAMPLES ) : start = time . time ( ) try : memcache . delete ( ( uuid + str ( index ) ) ) total time = ( time . time ( ) - start ) except  Exception  as exception : logging . exception ( exception ) errors . append ( str ( exception ) ) total time = 0 timings . append ( ( total time * constants . SECONDS TO MILLI ) ) return ( timings , errors ) 
def cloud init ( name , vm  =  None  , ** kwargs ) : init interface = cloud init interface ( name , vm  , ** kwargs ) name = init interface . pop ( 'name' , name ) return init ( name , ** init interface ) 
@ register . function @ jinja2 . contextfunction def get doc path ( context , path , extension ) : lang = getattr ( context [ 'request' ] , 'LANG' , 'en-US' ) if ( lang in settings . AMO LANGUAGES ) : try : localized file path = ( '%s/%s.%s' % ( path , lang , extension ) ) with open ( localized file path ) : return localized file path except IO Error  : return ( '%s/en-US.%s' % ( path , extension ) ) 
def load model ( ) : print ' Loading   dictionary...' with open ( path to dictionary , 'rb' ) as f : worddict = pkl . load ( f ) print ' Creating   inverted  dictionary...' word idict = dict ( ) for ( kk , vv ) in worddict . iteritems ( ) : word idict [ vv ] = kk word idict [ 0 ] = '<eos>' word idict [ 1 ] = 'UNK' print ' Loading   model  options...' with open ( ( '%s.pkl' % path to model ) , 'rb' ) as f : options = pkl . load ( f ) print ' Loading   model  parameters...' params = init params ( options ) params = load params ( path to model , params ) tparams = init tparams ( params ) trng =  Random  Streams  ( 1234 ) ( f init , f next ) = build sampler ( tparams , options , trng ) dec = dict ( ) dec [ 'options' ] = options dec [ 'trng' ] = trng dec [ 'worddict' ] = worddict dec [ 'word idict' ] = word idict dec [ 'tparams' ] = tparams dec [ 'f init' ] = f init dec [ 'f next' ] = f next return dec 
def fib ( num ) : num = int ( num ) if ( num < 0 ) : raise  Value  Error  ( ' Negative   number  is  not  allowed!' ) start = time . time ( ) if ( num < 2 ) : return ( num , ( time . time ( ) - start ) ) return ( (  fib ( ( num - 1 ) ) +  fib ( ( num - 2 ) ) ) , ( time . time ( ) - start ) ) 
def  Get  Args  ( ) : parser = argparse .  Argument  Parser  ( description = ' Process   args  for  retrieving  all  the   Virtual    Machines ' ) parser . add argument ( '-s' , '--host' , required =  True  , action = 'store' , help = ' Remote   host  to  connect  to' ) parser . add argument ( '-o' , '--port' , type = int , default = 443 , action = 'store' , help = ' Port   to  connect  on' ) parser . add argument ( '-u' , '--user' , required =  True  , action = 'store' , help = ' User   name  to  use  when  connecting  to  host' ) parser . add argument ( '-p' , '--password' , required =  False  , action = 'store' , help = ' Password   to  use  when  connecting  to  host' ) ( parser . add argument ( '-d' , '--datacenter' , required =  True  , help = 'name  of  the  datacenter' ) , ) parser . add argument ( '-f' , '--folder' , required =  True  , help = 'name  of  the  folder' ) args = parser . parse args ( ) return args 
def pick channels evoked ( orig , include = [ ] , exclude = 'bads' ) : if ( ( len ( include ) == 0 ) and ( len ( exclude ) == 0 ) ) : return orig exclude =  check excludes includes ( exclude , info = orig . info , allow bads =  True  ) sel = pick channels ( orig . info [ 'ch names' ] , include = include , exclude = exclude ) if ( len ( sel ) == 0 ) : raise  Value  Error  ( ' Warning   :   No   channels  match  the  selection.' ) res = deepcopy ( orig ) res . info = pick info ( res . info , sel ) res . data = res . data [ sel , : ] return res 
def campaign keyword ( ) : return s3 rest controller ( ) 
def rec keep fields ( rec , names ) : if cbook . is string like ( names ) : names = names . split ( u',' ) arrays = [ ] for name in names : arrays . append ( rec [ name ] ) return np . rec . fromarrays ( arrays , names = names ) 
def fixup paths ( path ) : try : import google google .   path   . append ( '{0}/google' . format ( path ) ) except  Import  Error  : pass sys . path . insert ( 0 , path ) 
def id or nid ( song dict ) : return ( song dict . get ( u'id' ) or song dict [ u'nid' ] ) 
@ snippet def topic create ( client , to delete ) : TOPIC NAME = ( 'topic create-%d' % (  millis ( ) , ) ) topic = client . topic ( TOPIC NAME ) topic . create ( ) to delete . append ( topic ) 
def ldap2py ( val ) : return utf8 decode ( val ) 
def get topic name ( prefix , table , operation , host =  None  ) : if host : return ( '%s-%s-%s.%s' % ( prefix , table , operation , host ) ) return ( '%s-%s-%s' % ( prefix , table , operation ) ) 
def is table ( doctype ) : def get tables ( ) : return db . sql list ( u'select  name  from  tab Doc  Type   where  istable=1' ) tables = cache ( ) . get value ( u'is table' , get tables ) return ( doctype in tables ) 
def text Diff  ( a , b ) : out = [ ] ( a , b ) = ( html2list ( a ) , html2list ( b ) ) s = difflib .  Sequence  Matcher  (  None  , a , b ) for e in s . get opcodes ( ) : if ( e [ 0 ] == 'replace' ) : out . append ( ( ( ( ( '<del>' + '' . join ( a [ e [ 1 ] : e [ 2 ] ] ) ) + '</del><ins>' ) + '' . join ( b [ e [ 3 ] : e [ 4 ] ] ) ) + '</ins>' ) ) elif ( e [ 0 ] == 'delete' ) : out . append ( ( ( '<del  >' + '' . join ( a [ e [ 1 ] : e [ 2 ] ] ) ) + '</del>' ) ) elif ( e [ 0 ] == 'insert' ) : out . append ( ( ( '<ins  >' + '' . join ( b [ e [ 3 ] : e [ 4 ] ] ) ) + '</ins>' ) ) elif ( e [ 0 ] == 'equal' ) : out . append ( '' . join ( b [ e [ 3 ] : e [ 4 ] ] ) ) else : raise ( ( " Um ,  something's  broken.  I  didn't  expect  a  '" +   ` e [ 0 ] ` ) + "'." ) return '' . join ( out ) 
def tokenize input ( input sent , vocab ) : input tok = [ ] for sent in input sent : text int = [ ( ( - 1 ) if ( t not in vocab ) else vocab [ t ] ) for t in tokenize ( sent ) ] input tok . append ( np . array ( text int ) ) return np . array ( input tok ) 
def  message pb to mapping ( message pb ) : return { 'message Id ' : message pb . message id , 'data' : message pb . data , 'attributes' : message pb . attributes , 'publish Time ' :  pb timestamp to rfc3339 ( message pb . publish time ) } 
def plot evoked ( evoked , picks =  None  , exclude = 'bads' , unit =  True  , show =  True  , ylim =  None  , xlim = 'tight' , proj =  False  , hline =  None  , units =  None  , scalings =  None  , titles =  None  , axes =  None  , gfp =  False  , window title =  None  , spatial colors =  False  , zorder = 'unsorted' , selectable =  True  ) : return  plot evoked ( evoked = evoked , picks = picks , exclude = exclude , unit = unit , show = show , ylim = ylim , proj = proj , xlim = xlim , hline = hline , units = units , scalings = scalings , titles = titles , axes = axes , plot type = 'butterfly' , gfp = gfp , window title = window title , spatial colors = spatial colors , zorder = zorder , selectable = selectable ) 
def  get nonstar args ( func ) : return func . func code . co varnames [ : func . func code . co argcount ] 
def  clear search indexes storage ( search index path ) : if os . path . lexists ( search index path ) : try : os . remove ( search index path ) except OS Error  as e : logging . warning ( ' Failed   to  remove  search  indexes  file  %r:  %s' , search index path , e ) 
def  make text block ( name , content , content type =  None  ) : if ( content type == 'xhtml' ) : return ( u'<%s  type="xhtml"><div  xmlns="%s">%s</div></%s>\n' % ( name , XHTML NAMESPACE , content , name ) ) if ( not content type ) : return ( u'<%s>%s</%s>\n' % ( name , escape ( content ) , name ) ) return ( u'<%s  type="%s">%s</%s>\n' % ( name , content type , escape ( content ) , name ) ) 
def expand ip block ( block ) : addresses = [ ] ip r = block . base ip . split ( '.' ) last quad = int ( ip r [ 3 ] ) address root = ( '%s.%s.%s.' % ( ip r [ 0 ] , ip r [ 1 ] , ip r [ 2 ] ) ) for i in range ( int ( block . size ) ) : addresses . append ( ( address root + str ( ( last quad + i ) ) ) ) return addresses 
def check ( name , port =  None  , ** kwargs ) : host = name ret = { 'name' : name , 'result' :  True  , 'changes' : { } , 'comment' : '' } if ( 'test' not in kwargs ) : kwargs [ 'test' ] =   opts   . get ( 'test' ,  False  ) if kwargs [ 'test' ] : ret [ 'result' ] =  True  ret [ 'comment' ] = ' The   connection  will  be  tested' else : results =   salt   [ 'network.connect' ] ( host , port , ** kwargs ) ret [ 'result' ] = results [ 'result' ] ret [ 'comment' ] = results [ 'comment' ] return ret 
def get publisher ( url , namespace = 'ceilometer.publisher' ) : parse result = urlparse . urlparse ( url ) loaded driver = driver .  Driver  Manager  ( namespace , parse result . scheme ) return loaded driver . driver ( parse result ) 
def increment index ( builder , val ) : one = val . type ( 1 ) return builder . add ( val , one , flags = [ 'nsw' ] ) 
def calc gc content ( sequence ) : d = { } for nt in [ 'A' , 'T' , 'G' , 'C' ] : d [ nt ] = ( sequence . count ( nt ) + sequence . count ( nt . lower ( ) ) ) gc = ( d . get ( 'G' , 0 ) + d . get ( 'C' , 0 ) ) if ( gc == 0 ) : return 0 return ( ( gc * 1.0 ) / ( ( d [ 'A' ] + d [ 'T' ] ) + gc ) ) 
def load vi open in editor bindings ( ) : registry =  Registry  ( ) navigation mode = (  Vi  Mode  ( ) &  Vi  Navigation  Mode  ( ) ) registry . add binding ( u'v' ) ( get by name ( u'edit-and-execute-command' ) ) return registry 
def short float fmt ( x ) : return u'{0:f}' . format ( x ) . rstrip ( u'0' ) . rstrip ( u'.' ) 
def set trace ( frame =  None  ) :  Terminal  Pdb  ( ) . set trace ( ( frame or sys .  getframe ( ) . f back ) ) 
def  downscale ( images , K ) : arr = np . zeros ( [ K , K , 3 , 3 ] ) arr [ : , : , 0 , 0 ] = ( 1.0 / ( K * K ) ) arr [ : , : , 1 , 1 ] = ( 1.0 / ( K * K ) ) arr [ : , : , 2 , 2 ] = ( 1.0 / ( K * K ) ) dowscale weight = tf . constant ( arr , dtype = tf . float32 ) downscaled = tf . nn . conv2d ( images , dowscale weight , strides = [ 1 , K , K , 1 ] , padding = 'SAME' ) return downscaled 
def include config ( include , orig path , verbose , exit on config errors =  False  ) : if ( not include ) : return { } if ( orig path is  None  ) : return { } if isinstance ( include , str ) : include = [ include ] configuration = { } for path in include : path = os . path . expanduser ( path ) if ( not os . path . isabs ( path ) ) : path = os . path . join ( os . path . dirname ( orig path ) , path ) if ( len ( glob . glob ( path ) ) == 0 ) : if verbose : log . warning ( ' Warning   parsing  configuration  file:  "include"  path/glob  \'{0}\'  matches  no  files' . format ( path ) ) for fn  in sorted ( glob . glob ( path ) ) : log . debug ( " Including   configuration  from  '{0}'" . format ( fn  ) ) try : opts =  read conf file ( fn  ) except salt . exceptions .  Salt  Configuration  Error  as error : log . error ( error ) if exit on config errors : sys . exit ( salt . defaults . exitcodes . EX GENERIC ) else : opts = { } include = opts . get ( 'include' , [ ] ) if include : opts . update ( include config ( include , fn  , verbose ) ) salt . utils . dictupdate . update ( configuration , opts ,  True  ,  True  ) return configuration 
def test inequalities symbol name same ( ) : A = ( x , y , S ( 0 ) , ( S ( 1 ) / 3 ) , pi , oo , ( - oo ) ) for a in A : for b in A : assert (  Gt  ( a , b ) == ( a > b ) ) assert (  Lt  ( a , b ) == ( a < b ) ) assert (  Ge  ( a , b ) == ( a >= b ) ) assert (  Le  ( a , b ) == ( a <= b ) ) for b in ( y , S ( 0 ) , ( S ( 1 ) / 3 ) , pi , oo , ( - oo ) ) : assert (  Gt  ( x , b , evaluate =  False  ) == ( x > b ) ) assert (  Lt  ( x , b , evaluate =  False  ) == ( x < b ) ) assert (  Ge  ( x , b , evaluate =  False  ) == ( x >= b ) ) assert (  Le  ( x , b , evaluate =  False  ) == ( x <= b ) ) for b in ( y , S ( 0 ) , ( S ( 1 ) / 3 ) , pi , oo , ( - oo ) ) : assert (  Gt  ( b , x , evaluate =  False  ) == ( b > x ) ) assert (  Lt  ( b , x , evaluate =  False  ) == ( b < x ) ) assert (  Ge  ( b , x , evaluate =  False  ) == ( b >= x ) ) assert (  Le  ( b , x , evaluate =  False  ) == ( b <= x ) ) 
def truncate words ( s , num , end text = '...' ) : s = force unicode ( s ) length = int ( num ) words = s . split ( ) if ( len ( words ) > length ) : words = words [ : length ] if ( not words [ ( - 1 ) ] . endswith ( end text ) ) : words . append ( end text ) return u'  ' . join ( words ) 
def test compute proj eog ( ) : raw = read raw fif ( raw fname ) . crop ( 0 , 10 ) raw . load data ( ) for average in [  False  ,  True  ] : n projs init = len ( raw . info [ 'projs' ] ) ( projs , events ) = compute proj eog ( raw , n mag = 2 , n grad = 2 , n eeg = 2 , bads = [ 'MEG  2443' ] , average = average , avg ref =  True  , no proj =  False  , l freq =  None  , h freq =  None  , reject =  None  , tmax = dur use , filter length = 6000 ) assert true ( ( len ( projs ) == ( 7 + n projs init ) ) ) assert true ( ( np . abs ( ( events . shape [ 0 ] - np . sum ( np . less ( eog times , dur use ) ) ) ) <= 1 ) ) ssp eog = [ proj for proj in projs if proj [ 'desc' ] . startswith ( 'EOG' ) ] ssp eog = [ proj for proj in ssp eog if ( 'PCA-01' in proj [ 'desc' ] ) ] ( thresh eeg , thresh axial , thresh planar ) = ( 0.9 , 0.3 , 0.1 ) for proj in ssp eog : if ( 'planar' in proj [ 'desc' ] ) : assert true ( ( proj [ 'explained var' ] > thresh planar ) ) elif ( 'axial' in proj [ 'desc' ] ) : assert true ( ( proj [ 'explained var' ] > thresh axial ) ) elif ( 'eeg' in proj [ 'desc' ] ) : assert true ( ( proj [ 'explained var' ] > thresh eeg ) ) with warnings . catch warnings ( record =  True  ) as w : warnings . simplefilter ( 'always' ) ( projs , events ) = compute proj eog ( raw , n mag = 2 , n grad = 2 , n eeg = 2 , average = average , bads = [ ] , avg ref =  True  , no proj =  False  , l freq =  None  , h freq =  None  , tmax = dur use ) assert true ( ( len ( w ) >= 1 ) ) assert equal ( projs ,  None  ) 
def unique labels ( list of labels ) : list of labels = [ idx for ( idx , labels ) in enumerate ( list of labels ) ] return list of labels 
def asrun ( pl , ascript ) : return run cmd ( pl , [ u'osascript' , u'-' ] , ascript ) 
def target internal dependencies ( target ) : for dep in target . dependencies : if isinstance ( dep ,  Jarable  ) : ( yield dep ) else : for childdep in target internal dependencies ( dep ) : ( yield childdep ) 
def   Fixed  Sizer  ( value size ) : def  Specific  Sizer  ( field number , is repeated , is packed ) : tag size =   Tag  Size  ( field number ) if is packed : local  Varint  Size  =   Varint  Size  def  Packed  Field  Size  ( value ) : result = ( len ( value ) * value size ) return ( ( result + local  Varint  Size  ( result ) ) + tag size ) return  Packed  Field  Size  elif is repeated : element size = ( value size + tag size ) def  Repeated  Field  Size  ( value ) : return ( len ( value ) * element size ) return  Repeated  Field  Size  else : field size = ( value size + tag size ) def  Field  Size  ( value ) : return field size return  Field  Size  return  Specific  Sizer  
def setup axes1 ( fig , rect ) : tr =  Affine 2D ( ) . scale ( 2 , 1 ) . rotate deg ( 30 ) grid helper = floating axes .  Grid  Helper  Curve  Linear  ( tr , extremes = ( ( - 0.5 ) , 3.5 , 0 , 4 ) ) ax1 = floating axes .  Floating  Subplot  ( fig , rect , grid helper = grid helper ) fig . add subplot ( ax1 ) aux ax = ax1 . get aux axes ( tr ) grid helper . grid finder . grid locator1 .  nbins = 4 grid helper . grid finder . grid locator2 .  nbins = 4 return ( ax1 , aux ax ) 
def action ( fun =  None  , cloudmap =  None  , names =  None  , provider =  None  , instance =  None  , ** kwargs ) : client =  get client ( ) info = client . action ( fun , cloudmap , names , provider , instance , kwargs ) return info 
def hash 160 to address ( h160 , addrtype = 0 ) : if ( ( h160 is  None  ) or ( len ( h160 ) is not 20 ) ) : return  None  vh160 = ( chr ( addrtype ) + h160 ) h =  Hash  ( vh160 ) addr = ( vh160 + h [ 0 : 4 ] ) return b58encode ( addr ) 
def get projects with bugs ( ) : projects = mysite . search . models .  Project  . objects . annotate ( bug count =  Count  ( 'bug' ) ) . filter ( bug count  gt = 0 ) . order by ( u'display name' ) return projects 
def DEFINE list ( name , default , help , flag values = FLAGS , ** args ) : parser =  List  Parser  ( ) serializer =  List  Serializer  ( ',' ) DEFINE ( parser , name , default , help , flag values , serializer , ** args ) 
@ never cache @ csrf exempt @ psa ( '{0}:complete' . format ( NAMESPACE ) ) def complete ( request , backend , * args , ** kwargs ) : return do complete ( request . backend ,  do login , request . user , redirect name = REDIRECT FIELD NAME , * args , ** kwargs ) 
def in6 getnsmac ( a ) : a = struct . unpack ( '16B' , a ) [ ( - 4 ) : ] mac = '33:33:' mac += ':' . join ( map ( ( lambda x : ( '%.2x' % x ) ) , a ) ) return mac 
def group to gid ( group ) : if ( group is  None  ) : return '' try : if isinstance ( group , int ) : return group return grp . getgrnam ( group ) . gr gid except  Key  Error  : return '' 
def user has permission ( user db , permission type ) : if ( not cfg . CONF . rbac . enable ) : return  True  resolver = resolvers . get resolver for permission type ( permission type = permission type ) result = resolver . user has permission ( user db = user db , permission type = permission type ) return result 
def  resume Hash DB Values  ( ) : kb . abs File  Paths  = ( hashDB Retrieve  ( HASHDB KEYS . KB ABS FILE PATHS ,  True  ) or kb . abs File  Paths  ) kb . brute . tables = ( hashDB Retrieve  ( HASHDB KEYS . KB BRUTE TABLES ,  True  ) or kb . brute . tables ) kb . brute . columns = ( hashDB Retrieve  ( HASHDB KEYS . KB BRUTE COLUMNS ,  True  ) or kb . brute . columns ) kb . chars = ( hashDB Retrieve  ( HASHDB KEYS . KB CHARS ,  True  ) or kb . chars ) kb . dynamic Markings  = ( hashDB Retrieve  ( HASHDB KEYS . KB DYNAMIC MARKINGS ,  True  ) or kb . dynamic Markings  ) kb . xp Cmdshell  Available  = ( hashDB Retrieve  ( HASHDB KEYS . KB XP CMDSHELL AVAILABLE ) or kb . xp Cmdshell  Available  ) kb . error Chunk  Length  = hashDB Retrieve  ( HASHDB KEYS . KB ERROR CHUNK LENGTH ) if ( kb . error Chunk  Length  and kb . error Chunk  Length  . isdigit ( ) ) : kb . error Chunk  Length  = int ( kb . error Chunk  Length  ) else : kb . error Chunk  Length  =  None  conf . tmp Path  = ( conf . tmp Path  or hashDB Retrieve  ( HASHDB KEYS . CONF TMP PATH ) ) for injection in ( hashDB Retrieve  ( HASHDB KEYS . KB INJECTIONS ,  True  ) or [ ] ) : if ( isinstance ( injection ,  Injection  Dict  ) and ( injection . place in conf . param Dict  ) and ( injection . parameter in conf . param Dict  [ injection . place ] ) ) : if ( ( not conf . tech ) or intersect ( conf . tech , injection . data . keys ( ) ) ) : if intersect ( conf . tech , injection . data . keys ( ) ) : injection . data = dict ( filter ( ( lambda ( key , item ) : ( key in conf . tech ) ) , injection . data . items ( ) ) ) if ( injection not in kb . injections ) : kb . injections . append ( injection )  resumeDBMS ( )  resumeOS ( ) 
def  get old license titles ( migrate engine ) : titles = { } select licenses = 'SELECT  id,  name  FROM  license;' q = migrate engine . execute ( select licenses ) for ( id , title ) in q : titles [ id ] = title return titles 
def test censure on rectangular images ( ) : rect image = np . random . rand ( 300 , 200 ) square image = np . random . rand ( 200 , 200 ) CENSURE ( ) . detect ( square image ) CENSURE ( ) . detect ( rect image ) 
@ require POST @ login required def reply ( request , forum slug , thread id ) : forum = get object or 404 (  Forum  , slug = forum slug ) user = request . user if ( not forum . allows posting by ( user ) ) : if forum . allows viewing by ( user ) : raise  Permission  Denied  else : raise  Http 404 form =  Reply  Form  ( request . POST ) post preview =  None  if form . is valid ( ) : thread = get object or 404 (  Thread  , pk = thread id , forum = forum ) if ( not thread . is locked ) : reply  = form . save ( commit =  False  ) reply  . thread = thread reply  . author = request . user if ( 'preview' in request . POST ) : post preview = reply  post preview . author post count = reply  . author . post set . count ( ) elif ( not is ratelimited ( request , 'forum-post' , '15/d' ) ) : reply  . save ( ) statsd . incr ( 'forums.reply' ) if  Setting  . get for user ( request . user , 'forums watch after reply' ) :  New  Post  Event  . notify ( request . user , thread )  New  Post  Event  ( reply  ) . fire ( exclude = reply  . author ) return  Http  Response  Redirect  ( thread . get last post url ( ) ) return posts ( request , forum slug , thread id , form , post preview , is reply =  True  ) 
def common dtype ( cols ) : try : return metadata . common dtype ( cols ) except metadata .  Merge  Conflict  Error  as err : tme =  Table  Merge  Error  ( u' Columns   have  incompatible  types  {0}' . format ( err .  incompat types ) ) tme .  incompat types = err .  incompat types raise tme 
def test digit ( arr ) : for element in arr : if ( not element . isdigit ( ) ) : print " Error :  '{}'  is  not  numeric." . format ( element ) exit ( 1 ) 
def make external id ( product ) : domain = getattr ( settings , 'DOMAIN' ,  None  ) if ( not domain ) : domain = 'marketplace-dev' external id = domain . split ( '.' ) [ 0 ] return '{0}:{1}' . format ( external id , product . pk ) 
def new figure manager ( num , * args , ** kwargs ) :  Figure  Class  = kwargs . pop ( ' Figure  Class ' ,  Figure  ) this Fig  =  Figure  Class  ( * args , ** kwargs ) canvas =  Figure  Canvas  Template  ( this Fig  ) manager =  Figure  Manager  Template  ( canvas , num ) return manager 
def  newer ( a , b ) : if ( not os . path . exists ( a ) ) : return  False  if ( not os . path . exists ( b ) ) : return  True  return ( os . path . getmtime ( a ) >= os . path . getmtime ( b ) ) 
def xor ( aa , bb ) : result = bytearray ( ) for ( a , b ) in zip ( bytearray ( aa ) , bytearray ( bb ) ) : result . append ( ( a ^ b ) ) return result 
def  compile func ( body ) : body = u'def  {0}():\n        {1}' . format ( FUNC NAME , body . replace ( '\n' , '\n        ' ) ) code = compile ( body , 'inline' , 'exec' ) env = { } eval ( code , env ) return env [ FUNC NAME ] 
@ contextfunction def object tree path ( context , object , skipself =  False  ) : response format = 'html' if ( 'response format' in context ) : response format = context [ 'response format' ] path = object . get tree path ( skipself ) return  Markup  ( render to string ( 'core/tags/object tree path' , { 'path' : path , 'skipself' : skipself } , response format = response format ) ) 
def start ( name , quiet =  False  , path =  None  ) : data =  do names ( name , 'start' , path = path ) if ( data and ( not quiet ) ) :   jid event   . fire event ( { 'data' : data , 'outputter' : 'lxc start' } , 'progress' ) return data 
def cgsnapshot destroy ( context , cgsnapshot id ) : return IMPL . cgsnapshot destroy ( context , cgsnapshot id ) 
def skip Unless GIS Lookup  ( * gis lookups ) : def decorator ( test func ) : @ wraps ( test func ) def skip wrapper ( * args , ** kwargs ) : if any ( ( ( key not in connection . ops . gis operators ) for key in gis lookups ) ) : raise unittest .  Skip  Test  ( ( " Database   doesn't  support  all  the  lookups:  %s" % ',  ' . join ( gis lookups ) ) ) return test func ( * args , ** kwargs ) return skip wrapper return decorator 
def  parse range header ( range header ) : if ( not range header ) : return (  None  ,  None  ) try : ( range type , ranges ) = range header . split ( '=' , 1 ) if ( range type != 'bytes' ) : return (  None  ,  None  ) ranges = ranges . lstrip ( ) if ( ',' in ranges ) : return (  None  ,  None  ) end =  None  if ranges . startswith ( '-' ) : start = int ( ranges ) if ( start == 0 ) : return (  None  ,  None  ) else : split range = ranges . split ( '-' , 1 ) start = int ( split range [ 0 ] ) if ( ( len ( split range ) == 2 ) and split range [ 1 ] . strip ( ) ) : end = ( int ( split range [ 1 ] ) + 1 ) if ( start > end ) : return (  None  ,  None  ) return ( start , end ) except  Value  Error  : return (  None  ,  None  ) 
def writeestr ( dst , edict ) : os . unlink ( dst . as pathname ( ) )  Res  . F Sp  Create  Res  File  ( dst , 'RSED' , 'rsrc' , sm All  Scripts  ) output =  Res  . F Sp  Open  Res  File  ( dst , WRITE )  Res  .  Use  Res  File  ( output ) for num in edict . keys ( ) : res =  Res  .  Resource  (  Pstring  ( edict [ num ] [ 0 ] ) ) res .  Add  Resource  ( ' Estr ' , num , '' ) res .  Write  Resource  ( )  Res  .  Close  Res  File  ( output ) 
def complete year spans ( spans ) : spans . sort ( key = ( lambda x : x [ 'from' ] ) ) for ( x , y ) in pairwise ( spans ) : if ( 'to' not in x ) : x [ 'to' ] = ( y [ 'from' ] - 1 ) if ( spans and ( 'to' not in spans [ ( - 1 ) ] ) ) : spans [ ( - 1 ) ] [ 'to' ] = datetime . now ( ) . year 
@ with setup ( step runner environ ) def test failing behave as step doesnt pass ( ) : runnable step =  Step  . from string ( ' Given   I  have  a  step  which  calls  the  "other  step  fails"  step  with  behave as' ) try : runnable step . run (  True  ) except : pass assert false ( runnable step . passed ) 
def get New  Derivation  ( element Node  , prefix , side Length  ) : return  Bevel  Derivation  ( element Node  , prefix , side Length  ) 
def  hashed key ( key ) : return md5 ( key . encode ( u'utf-8' ) ) . hexdigest ( ) 
def make links absolute ( root , base url ) : def link repl ( href ) : return urljoin ( base url , href ) rewrite links ( root , link repl ) 
def unrepr ( s ) : if ( not s ) : return s if ( sys . version info < ( 3 , 0 ) ) : b =   Builder 2 ( ) else : b =   Builder 3 ( ) obj = b . astnode ( s ) return b . build ( obj ) 
def getframeinfo ( frame , context = 1 ) : if istraceback ( frame ) : lineno = frame . tb lineno frame = frame . tb frame else : lineno = frame . f lineno if ( not isframe ( frame ) ) : raise  Type  Error  ( 'arg  is  not  a  frame  or  traceback  object' ) filename = ( getsourcefile ( frame ) or getfile ( frame ) ) if ( context > 0 ) : start = ( ( lineno - 1 ) - ( context // 2 ) ) try : ( lines , lnum ) = findsource ( frame ) except IO Error  : lines = index =  None  else : start = max ( start , 1 ) start = max ( 0 , min ( start , ( len ( lines ) - context ) ) ) lines = lines [ start : ( start + context ) ] index = ( ( lineno - 1 ) - start ) else : lines = index =  None  return  Traceback  ( filename , lineno , frame . f code . co name , lines , index ) 
def call ( * args , ** kwargs ) : return  Popen  ( * args , ** kwargs ) . wait ( ) 
def validate gantt ( df ) : if ( pd and isinstance ( df , pd . core . frame .  Data  Frame  ) ) : for key in REQUIRED GANTT KEYS : if ( key not in df ) : raise exceptions .  Plotly  Error  ( ' The   columns  in  your  dataframe  must  include  the  following  keys:  {0}' . format ( ',  ' . join ( REQUIRED GANTT KEYS ) ) ) num of rows = len ( df . index ) chart = [ ] for index in range ( num of rows ) : task dict = { } for key in df : task dict [ key ] = df . ix [ index ] [ key ] chart . append ( task dict ) return chart if ( not isinstance ( df , list ) ) : raise exceptions .  Plotly  Error  ( ' You   must  input  either  a  dataframe  or  a  list  of  dictionaries.' ) if ( len ( df ) <= 0 ) : raise exceptions .  Plotly  Error  ( ' Your   list  is  empty.   It   must  contain  at  least  one  dictionary.' ) if ( not isinstance ( df [ 0 ] , dict ) ) : raise exceptions .  Plotly  Error  ( ' Your   list  must  only  include  dictionaries.' ) return df 
def  solve lambert ( f , symbol , gens ) : ( nrhs , lhs ) = f . as independent ( symbol , as  Add  =  True  ) rhs = ( - nrhs ) lamcheck = [ tmp for tmp in gens if ( ( tmp . func in [ exp , log ] ) or ( tmp . is  Pow  and ( symbol in tmp . exp . free symbols ) ) ) ] if ( not lamcheck ) : raise  Not  Implemented  Error  ( ) if lhs . is  Mul  : lhs = expand log ( log ( lhs ) ) rhs = log ( rhs ) lhs = factor ( lhs , deep =  True  ) r =  Dummy  ( ) ( i , lhs ) =  invert ( ( lhs - r ) , symbol ) rhs = i . xreplace ( { r : rhs } ) soln = [ ] if ( not soln ) : mainlog =  mostfunc ( lhs , log , symbol ) if mainlog : if ( lhs . is  Mul  and ( rhs != 0 ) ) : soln =  lambert ( ( log ( lhs ) - log ( rhs ) ) , symbol ) elif lhs . is  Add  : other = lhs . subs ( mainlog , 0 ) if ( other and ( not other . is  Add  ) and [ tmp for tmp in other . atoms (  Pow  ) if ( symbol in tmp . free symbols ) ] ) : if ( not rhs ) : diff = ( log ( other ) - log ( ( other - lhs ) ) ) else : diff = ( log ( ( lhs - other ) ) - log ( ( rhs - other ) ) ) soln =  lambert ( expand log ( diff ) , symbol ) else : soln =  lambert ( ( lhs - rhs ) , symbol ) if ( not soln ) : mainexp =  mostfunc ( lhs , exp , symbol ) if mainexp : lhs = collect ( lhs , mainexp ) if ( lhs . is  Mul  and ( rhs != 0 ) ) : soln =  lambert ( expand log ( ( log ( lhs ) - log ( rhs ) ) ) , symbol ) elif lhs . is  Add  : other = lhs . subs ( mainexp , 0 ) mainterm = ( lhs - other ) rhs = ( rhs - other ) if ( mainterm . could extract minus sign ( ) and rhs . could extract minus sign ( ) ) : mainterm *= ( - 1 ) rhs *= ( - 1 ) diff = ( log ( mainterm ) - log ( rhs ) ) soln =  lambert ( expand log ( diff ) , symbol ) if ( not soln ) : mainpow =  mostfunc ( lhs ,  Pow  , symbol ) if ( mainpow and ( symbol in mainpow . exp . free symbols ) ) : lhs = collect ( lhs , mainpow ) if ( lhs . is  Mul  and ( rhs != 0 ) ) : soln =  lambert ( expand log ( ( log ( lhs ) - log ( rhs ) ) ) , symbol ) elif lhs . is  Add  : other = lhs . subs ( mainpow , 0 ) mainterm = ( lhs - other ) rhs = ( rhs - other ) diff = ( log ( mainterm ) - log ( rhs ) ) soln =  lambert ( expand log ( diff ) , symbol ) if ( not soln ) : raise  Not  Implemented  Error  ( ( '%s  does  not  appear  to  have  a  solution  in  terms  of   Lambert W' % f ) ) return list ( ordered ( soln ) ) 
def dump exception ( typ , val , tb ) : tbtext = '' . join ( traceback . format exception ( typ , val , tb ) ) ( sys . last type , sys . last value , sys . last traceback ) = ( typ , val , tb ) try : pickled exc = pickle . dumps ( ( typ , val , tbtext ) , protocol = PICKLE PROTOCOL ) except pickle .  Pickling  Error  as ex : newval =  Nested  Exception  ( ( 'pickling  error  %s\nexception  type:  %r\nexception  object:  %s' % ( ex , typ , val ) ) ) pickled exc = pickle . dumps ( (  Nested  Exception  , newval , tbtext ) , protocol = PICKLE PROTOCOL ) return pickled exc 
def get default role ( request ) : global DEFAULT ROLE default = getattr ( settings , 'OPENSTACK KEYSTONE DEFAULT ROLE' ,  None  ) if ( default and ( DEFAULT ROLE is  None  ) ) : try : roles = keystoneclient ( request , admin =  True  ) . roles . list ( ) except  Exception  : roles = [ ] exceptions . handle ( request ) for role in roles : if ( ( role . id == default ) or ( role . name == default ) ) : DEFAULT ROLE = role break return DEFAULT ROLE 
def   Make  Sync  Search  Service  Call  ( call , request , response , deadline ) :   Validate  Deadline  ( deadline ) logging . warning ( '  Make  Sync  Search  Service  Call   is  deprecated;  please  use  API.' ) try : if ( deadline is  None  ) : apiproxy stub map .  Make  Sync  Call  ( 'search' , call , request , response ) else : rpc = apiproxy stub map .  User RPC ( 'search' , deadline = deadline ) rpc . make call ( call , request , response ) rpc . wait ( ) rpc . check success ( ) except apiproxy errors .  Application  Error  as e : raise   To  Search  Error  ( e ) 
def convert background to jpg ( background url ) : file path = get path of temp url ( background url ) im =  Image  . open ( file path ) out im = file path . replace ( 'png' , 'jpg' ) bg =  Image  . new ( 'RGB' , im . size , ( 255 , 255 , 255 ) ) bg . paste ( im , ( 0 , 0 ) , im ) bg . save ( out im , quality = 55 ) return out im 
def get unpack formats ( ) : formats = [ ( name , info [ 0 ] , info [ 3 ] ) for ( name , info ) in  UNPACK FORMATS . items ( ) ] formats . sort ( ) return formats 
def  linux bin exists ( binary ) : for search cmd in ( 'which' , 'type  -ap' ) : try : return (   salt   [ 'cmd.retcode' ] ( '{0}  {1}' . format ( search cmd , binary ) ) == 0 ) except salt . exceptions .  Command  Execution  Error  : pass try : return ( len (   salt   [ 'cmd.run all' ] ( 'whereis  -b  {0}' . format ( binary ) ) [ 'stdout' ] . split ( ) ) > 1 ) except salt . exceptions .  Command  Execution  Error  : return  False  
def test reshape configure ( backend default ) : bsz = backend default . bsz reshape 0 =  Reshape  ( ( 0 , 10 , 10 ) ) reshape 0 . configure ( ( 10 , 2 , 50 ) ) assert ( reshape 0 . out shape == ( 10 , 10 , 10 ) ) reshape 1 =  Reshape  ( ( 10 , 25 , ( - 1 ) ) ) reshape 1 . configure ( ( 10 , 2 , 50 ) ) assert ( reshape 1 . out shape == ( 10 , 25 , 4 ) ) reshape 2 =  Reshape  ( ( 5 , ( - 1 ) ) ) reshape 2 . configure ( ( 10 , 2 , 25 ) ) assert ( reshape 2 . out shape == ( 5 , 100 ) ) assert ( reshape 2 . out shape t == ( 5 , ( 100 * bsz ) ) ) reshape 3 =  Reshape  ( ( 5 , ( - 1 ) , 5 ) ) reshape 3 . configure ( ( 10 , 25 ) ) assert ( reshape 3 . out shape == ( 5 , 10 , 5 ) ) 
def threaded reactor ( ) : global  twisted thread try : from twisted . internet import reactor except  Import  Error  : return (  None  ,  None  ) if ( not  twisted thread ) : from twisted . python import threadable from threading import  Thread   twisted thread =  Thread  ( target = ( lambda : reactor . run ( install Signal  Handlers  =  False  ) ) )  twisted thread . set Daemon  (  True  )  twisted thread . start ( ) return ( reactor ,  twisted thread ) 
def  reg dir ( ) : return os . path . join (   opts   [ 'cachedir' ] , 'thorium' ) 
def optimize ( node , environment ) : optimizer =  Optimizer  ( environment ) return optimizer . visit ( node ) 
def status show ( context , data dict ) : return { 'site title' : config . get ( 'ckan.site title' ) , 'site description' : config . get ( 'ckan.site description' ) , 'site url' : config . get ( 'ckan.site url' ) , 'ckan version' : ckan .   version   , 'error emails to' : config . get ( 'email to' ) , 'locale default' : config . get ( 'ckan.locale default' ) , 'extensions' : config . get ( 'ckan.plugins' ) . split ( ) } 
def setup logging ( config ) : cli fmt = '%(message)s' file fmt = '%(asctime)s:%(levelname)s:%(name)s:%(message)s' logfile = 'letsencrypt.log' if config . quiet : level = constants . QUIET LOGGING LEVEL else : level = ( ( - config . verbose count ) * 10 ) ( file handler , log file path ) = setup log file handler ( config , logfile = logfile , fmt = file fmt ) cli handler =  cli log handler ( level , cli fmt ) root logger = logging . get Logger  ( ) root logger . set Level  ( logging . DEBUG ) root logger . add Handler  ( cli handler ) root logger . add Handler  ( file handler ) logger . debug ( ' Root   logging  level  set  at  %d' , level ) logger . info ( ' Saving   debug  log  to  %s' , log file path ) 
def is aa ( residue , standard =  False  ) : if ( not isinstance ( residue , basestring ) ) : residue = residue . get resname ( ) residue = residue . upper ( ) if standard : return ( residue in d3 to index ) else : return ( residue in SCOP Data  . protein letters 3to1 ) 
def  create playlist ( context , name , tracks ) : uri schemes = set ( [ urllib . parse . urlparse ( t . uri ) . scheme for t in tracks ] ) for scheme in uri schemes : new playlist = context . core . playlists . create ( name , scheme ) . get ( ) if ( new playlist is  None  ) : logger . debug ( u" Backend   for  scheme  %s  can't  create  playlists" , scheme ) continue new playlist = new playlist . replace ( tracks = tracks ) saved playlist = context . core . playlists . save ( new playlist ) . get ( ) if ( saved playlist is not  None  ) : return else : continue default scheme = context . dispatcher . config [ u'mpd' ] [ u'default playlist scheme' ] new playlist = context . core . playlists . create ( name , default scheme ) . get ( ) if ( new playlist is  None  ) : logger . warning ( u"MPD's  default  backend  can't  create  playlists" ) raise exceptions .  Mpd  Failed  To  Save  Playlist  ( default scheme ) new playlist = new playlist . replace ( tracks = tracks ) saved playlist = context . core . playlists . save ( new playlist ) . get ( ) if ( saved playlist is  None  ) : uri scheme = urllib . parse . urlparse ( new playlist . uri ) . scheme raise exceptions .  Mpd  Failed  To  Save  Playlist  ( uri scheme ) 
def get Voronoi  Loop  By  Points  ( inside , loop , outsides ) : for outside in outsides : loop = get Voronoi  Loop  By  Point  ( inside , loop , outside ) return loop 
@ library . filter def collapse linebreaks ( text ) : return  whitespace then break . sub ( '\r\n' , text ) 
def  sss ( l1 , l2 , l3 ) : c1 =  Circle  ( ( 0 , 0 ) , l3 ) c2 =  Circle  ( ( l1 , 0 ) , l2 ) inter = [ a for a in c1 . intersection ( c2 ) if a . y . is nonnegative ] if ( not inter ) : return  None  pt = inter [ 0 ] return  Triangle  ( ( 0 , 0 ) , ( l1 , 0 ) , pt ) 
def standard normal ( size =  None  , dtype = float ) : return normal ( size = size , dtype = dtype ) 
def create or update trigger type db ( trigger type ) : assert isinstance ( trigger type , dict ) trigger type api =  Trigger  Type API ( ** trigger type ) trigger type api . validate ( ) trigger type api =  Trigger  Type API . to model ( trigger type api ) ref =  Resource  Reference  . to string reference ( name = trigger type api . name , pack = trigger type api . pack ) existing trigger type db = get trigger type db ( ref ) if existing trigger type db : is update =  True  else : is update =  False  if is update : trigger type api . id = existing trigger type db . id trigger type db =  Trigger  Type  . add or update ( trigger type api ) extra = { 'trigger type db' : trigger type db } if is update : LOG . audit ( ( ' Trigger  Type   updated.   Trigger  Type .id=%s' % trigger type db . id ) , extra = extra ) else : LOG . audit ( ( ' Trigger  Type   created.   Trigger  Type .id=%s' % trigger type db . id ) , extra = extra ) return trigger type db 
def lookup lastlines ( lastlines dirpath , path ) : underscored = path . replace ( '/' , ' ' ) try : lastlines file = open ( os . path . join ( lastlines dirpath , underscored ) ) except ( OS Error  , IO Error  ) : return lastlines = lastlines file . read ( ) lastlines file . close ( ) os . remove ( lastlines file . name ) if ( not lastlines ) : return try : target file = open ( path ) except ( OS Error  , IO Error  ) : return target data = target file . read ( ) target file . close ( ) loc = target data . rfind ( lastlines ) if ( loc == ( - 1 ) ) : return reverse lineno = target data . count ( '\n' , ( loc + len ( lastlines ) ) ) return reverse lineno 
def set Level  ( level = 0 ) : I Logger  . level = level 
def inverse sine transform ( F , k , x , ** hints ) : return  Inverse  Sine  Transform  ( F , k , x ) . doit ( ** hints ) 
def convert uptime hours ( sys uptime ) : return ( ( int ( sys uptime ) / 100.0 ) / 3600.0 ) 
def  Begin  Threads  Simple  Marshal  ( num Threads  , cookie ) : ret = [ ] for i in range ( num Threads  ) : h Event  = win32event .  Create  Event  (  None  , 0 , 0 ,  None  ) thread . start new (  Test  Interp  In  Thread  , ( h Event  , cookie ) ) ret . append ( h Event  ) return ret 
def parse interval ( interval string ) : regexp = u'^\\d+  (second|minute|hour|day|week)s?$' if ( not re . match ( regexp , interval string ) ) : raise  Value  Error  ( u"should  be  in  format  'x  (seconds|minutes|hours|days|weeks)'" ) return parse timedelta ( interval string ) 
def change process owner ( uid , gid ) : try : os . setgid ( gid ) os . setuid ( uid ) except  Exception  as exc : error =  Daemon OS Environment  Error  ( ( ' Unable   to  change  file  creation  mask  (%(exc)s)' % vars ( ) ) ) raise error 
def list bucket files ( bucket name , prefix , max keys = 1000 ) : scope = 'https://www.googleapis.com/auth/devstorage.read only' url = ( 'https://%s.commondatastorage.googleapis.com/?' % bucket name ) query = [ ( 'max-keys' , max keys ) ] if prefix : query . append ( ( 'prefix' , prefix ) ) url += urllib . urlencode ( query ) ( auth token ,   ) = app identity . get access token ( scope ) result = urlfetch . fetch ( url , method = urlfetch . GET , headers = { ' Authorization ' : ( 'O Auth   %s' % auth token ) , 'x-goog-api-version' : '2' } ) if ( result and ( result . status code == 200 ) ) : doc = xml . dom . minidom . parse String  ( result . content ) return [ node . child Nodes  [ 0 ] . data for node in doc . get Elements  By  Tag  Name  ( ' Key ' ) ] raise  Backup  Validation  Exception  ( ' Request   to   Google    Cloud    Storage   failed' ) 
def runmodule ( name = '  main  ' , ** kw ) : main ( default Test  = name , ** kw ) 
def plot histograms ( ax , prng , nb samples = 10000 ) : params = ( ( 10 , 10 ) , ( 4 , 12 ) , ( 50 , 12 ) , ( 6 , 55 ) ) for ( a , b ) in params : values = prng . beta ( a , b , size = nb samples ) ax . hist ( values , histtype = 'stepfilled' , bins = 30 , alpha = 0.8 , normed =  True  ) ax . annotate ( ' Annotation ' , xy = ( 0.25 , 4.25 ) , xycoords = 'data' , xytext = ( 0.9 , 0.9 ) , textcoords = 'axes  fraction' , va = 'top' , ha = 'right' , bbox = dict ( boxstyle = 'round' , alpha = 0.2 ) , arrowprops = dict ( arrowstyle = '->' , connectionstyle = 'angle,angleA=-95,angleB=35,rad=10' ) ) return ax 
def best match ( supported , header ) : parsed header = [ parse media range ( r ) for r in header . split ( ',' ) ] weighted matches = [ ( quality parsed ( mime type , parsed header ) , mime type ) for mime type in supported ] weighted matches . sort ( ) return ( ( weighted matches [ ( - 1 ) ] [ 0 ] and weighted matches [ ( - 1 ) ] [ 1 ] ) or '' ) 
def scan multilang ( tokens , module elem ) : tokenizer = perl lexer .  Perl  Multi  Lang  Lexer  ( tokens ) parser = perl parser .  Parser  ( tokenizer , lang = ' Perl HTML' , provide full docs = g Provide  Full  Docs  ) parser . module Name  = '' parser . parse ( ) parse tree = parser . produce CIX  No  Header  ( module elem ) csl tokens = tokenizer . get csl tokens ( ) return ( csl tokens , tokenizer . has perl code ( ) ) 
def can eliminate tool dependency ( metadata dict , name , dependency type , version ) : td dict = metadata dict . get ( 'tool dependencies' , { } ) for ( td key , td val ) in td dict . items ( ) : if ( td key == 'set environment' ) : for td in td val : n = td . get ( 'name' ,  None  ) t = td . get ( 'type' ,  None  ) if ( ( n == name ) and ( t == dependency type ) ) : return  False  else : n = td val . get ( 'name' ,  None  ) t = td val . get ( 'type' ,  None  ) v = td val . get ( 'version' ,  None  ) if ( ( n == name ) and ( t == dependency type ) and ( v == version ) ) : return  False  return  True  
def ramp ( x =  None  , v min = 0 , v max = 1 , name =  None  ) : return tf . clip by value ( x , clip value min = v min , clip value max = v max , name = name ) 
def fill diagonal offset ( a , val , offset ) : return fill diagonal offset  ( a , val , offset ) 
def parse ref ( container , refspec ) : refspec = to bytes ( refspec ) possible refs = [ refspec , ( 'refs/' + refspec ) , ( 'refs/tags/' + refspec ) , ( 'refs/heads/' + refspec ) , ( 'refs/remotes/' + refspec ) , ( ( 'refs/remotes/' + refspec ) + '/HEAD' ) ] for ref in possible refs : if ( ref in container ) : return ref else : raise  Key  Error  ( refspec ) 
def reset ignored ( ) : cmd = [ 'softwareupdate' , '--reset-ignored' ] salt . utils . mac utils . execute return success ( cmd ) return ( list ignored ( ) == [ ] ) 
def  set Results  File  ( ) : if ( not conf . multiple Targets  ) : return if ( not conf . resultsFP ) : conf . results Filename  = os . path . join ( paths . SQLMAP OUTPUT PATH , time . strftime ( RESULTS FILE FORMAT ) . lower ( ) ) try : conf . resultsFP = open File  ( conf . results Filename  , 'w+' , UNICODE ENCODING , buffering = 0 ) except ( OS Error  , IO Error  ) as ex : try : warn Msg  = ( "unable  to  create  results  file  '%s'  ('%s').  " % ( conf . results Filename  , get Unicode  ( ex ) ) ) conf . results Filename  = tempfile . mkstemp ( prefix = MKSTEMP PREFIX . RESULTS , suffix = '.csv' ) [ 1 ] conf . resultsFP = open File  ( conf . results Filename  , 'w+' , UNICODE ENCODING , buffering = 0 ) warn Msg  += ( " Using   temporary  file  '%s'  instead" % conf . results Filename  ) logger . warn ( warn Msg  ) except IO Error  as   : err Msg  = ( "unable  to  write  to  the  temporary  directory  ('%s').  " %   ) err Msg  += ' Please   make  sure  that  your  disk  is  not  full  and  ' err Msg  += 'that  you  have  sufficient  write  permissions  to  ' err Msg  += 'create  temporary  files  and/or  directories' raise  Sqlmap  System  Exception  ( err Msg  ) conf . resultsFP . writelines ( ( ' Target   URL, Place , Parameter , Technique (s), Note (s)%s' % os . linesep ) ) logger . info ( ( "using  '%s'  as  the  CSV  results  file  in  multiple  targets  mode" % conf . results Filename  ) ) 
def global settings ( ) :  xml = '<!--   Sample   file  for   Get    Global   command  -->\n                            <RIBCL  VERSION="2.0">\n                                  <LOGIN  USER LOGIN="x"  PASSWORD="x">\n                                      <RIB INFO  MODE="read">\n                                          <GET GLOBAL SETTINGS  />\n                                      </RIB INFO>\n                                  </LOGIN>\n                              </RIBCL>' return   execute cmd ( ' Global   Settings ' ,  xml ) 
def top ( topfn , test =  None  , ** kwargs ) :   pillar   . update ( kwargs . get ( 'pillar' , { } ) ) st kwargs =   salt   . kwargs   opts   [ 'grains' ] =   grains   if salt . utils . test mode ( test = test , ** kwargs ) :   opts   [ 'test' ] =  True  else :   opts   [ 'test' ] =   opts   . get ( 'test' ,  None  ) st  = salt . client . ssh . state . SSH High  State  (   opts   ,   pillar   ,   salt   ,   context   [ 'fileclient' ] ) st  . opts [ 'state top' ] = os . path . join ( 'salt://' , topfn ) chunks = st  . compile low chunks ( ) file refs = salt . client . ssh . state . lowstate file refs ( chunks ,  merge extra filerefs ( kwargs . get ( 'extra filerefs' , '' ) ,   opts   . get ( 'extra filerefs' , '' ) ) ) trans tar = salt . client . ssh . state . prep trans tar (   opts   ,   context   [ 'fileclient' ] , chunks , file refs ,   pillar   , st kwargs [ 'id ' ] ) trans tar sum = salt . utils . get hash ( trans tar ,   opts   [ 'hash type' ] ) cmd = 'state.pkg  {0}/salt state.tgz  test={1}  pkg sum={2}  hash type={3}' . format (   opts   [ 'thin dir' ] , test , trans tar sum ,   opts   [ 'hash type' ] ) single = salt . client . ssh .  Single  (   opts   , cmd , fsclient =   context   [ 'fileclient' ] , minion opts =   salt   . minion opts , ** st kwargs ) single . shell . send ( trans tar , '{0}/salt state.tgz' . format (   opts   [ 'thin dir' ] ) ) ( stdout , stderr ,   ) = single . cmd block ( ) try : os . remove ( trans tar ) except ( OS Error  , IO Error  ) : pass try : return json . loads ( stdout , object hook = salt . utils . decode dict ) except  Exception  as e : log . error ( 'JSON   Render   failed  for:  {0}\n{1}' . format ( stdout , stderr ) ) log . error ( str ( e ) ) return stdout 
def list apps ( ) : if ( ( not request . vars . username ) or ( not request . vars . password ) ) : raise HTTP ( 400 ) client =  Server  Proxy  ( ( 'https://%(username)s:%(password)s@%(username)s.pythonanywhere.com/admin/webservices/call/jsonrpc' % request . vars ) ) regex = re . compile ( '^\\w+$' ) local = [ f for f in os . listdir ( apath ( r = request ) ) if regex . match ( f ) ] try : pythonanywhere = client . list apps ( ) except  Protocol  Error  as error : raise HTTP ( error . errcode ) return response . json ( { 'local' : local , 'pythonanywhere' : pythonanywhere } ) 
def object hash ( x , h =  None  ) : if ( h is  None  ) : h = hashlib . md5 ( ) if hasattr ( x , 'keys' ) : keys =  sort keys ( x ) for key in keys : object hash ( key , h ) object hash ( x [ key ] , h ) elif isinstance ( x , bytes ) : h . update ( x ) elif isinstance ( x , ( string types , float , int , type (  None  ) ) ) : h . update ( str ( type ( x ) ) . encode ( 'utf-8' ) ) h . update ( str ( x ) . encode ( 'utf-8' ) ) elif isinstance ( x , np . ndarray ) : x = np . asarray ( x ) h . update ( str ( x . shape ) . encode ( 'utf-8' ) ) h . update ( str ( x . dtype ) . encode ( 'utf-8' ) ) h . update ( x . tostring ( ) ) elif hasattr ( x , '  len  ' ) : h . update ( str ( type ( x ) ) . encode ( 'utf-8' ) ) for xx in x : object hash ( xx , h ) else : raise  Runtime  Error  ( ( 'unsupported  type:  %s  (%s)' % ( type ( x ) , x ) ) ) return int ( h . hexdigest ( ) , 16 ) 
def  deserialize blobs ( artifact type , blobs from db , artifact properties ) : for ( blob name , blob value ) in six . iteritems ( blobs from db ) : if ( not blob value ) : continue if isinstance ( artifact type . metadata . attributes . blobs . get ( blob name ) , declarative .  List  Attribute  Definition  ) : val = [ ] for v in blob value : b = definitions .  Blob  ( size = v [ 'size' ] , locations = v [ 'locations' ] , checksum = v [ 'checksum' ] , item key = v [ 'item key' ] ) val . append ( b ) elif ( len ( blob value ) == 1 ) : val = definitions .  Blob  ( size = blob value [ 0 ] [ 'size' ] , locations = blob value [ 0 ] [ 'locations' ] , checksum = blob value [ 0 ] [ 'checksum' ] , item key = blob value [ 0 ] [ 'item key' ] ) else : raise exception .  Invalid  Artifact  Property  Value  ( message =   ( ' Blob   %(name)s  may  not  have  multiple  values' ) , name = blob name ) artifact properties [ blob name ] = val 
def solve ( a , b , sym pos =  False  ) : if sym pos : ( l , u ) =  cholesky ( a ) else : ( p , l , u ) = lu ( a ) b = p . T . dot ( b ) uy = solve triangular ( l , b , lower =  True  ) return solve triangular ( u , uy ) 
def  bgp dispatcher ( payload ) : cls = conf . raw layer if ( payload is  None  ) : cls =  get cls ( 'BGP Header ' , conf . raw layer ) elif ( ( len ( payload ) >=  BGP HEADER SIZE ) and ( payload [ : 16 ] ==  BGP HEADER MARKER ) ) : message type = struct . unpack ( '!B' , payload [ 18 ] ) [ 0 ] if ( message type == 4 ) : cls =  get cls ( 'BGP Keep  Alive ' ) else : cls =  get cls ( 'BGP Header ' ) return cls 
def set node schedulability ( facts ) : if ( 'node' in facts ) : if ( 'schedulable' not in facts [ 'node' ] ) : if ( 'master' in facts ) : facts [ 'node' ] [ 'schedulable' ] =  False  else : facts [ 'node' ] [ 'schedulable' ] =  True  return facts 
def migrate location credentials ( migrate engine , to quoted ) : meta = sqlalchemy . schema .  Meta  Data  ( ) meta . bind = migrate engine images table = sqlalchemy .  Table  ( 'images' , meta , autoload =  True  ) images = list ( images table . select ( images table . c . location . startswith ( 'swift' ) ) . execute ( ) ) for image in images : fixed uri = fix uri credentials ( image [ 'location' ] , to quoted ) images table . update ( ) . where ( ( images table . c . id == image [ 'id' ] ) ) . values ( location = fixed uri ) . execute ( ) 
def  valid code ( seed , drift = 0 ) : return totp ( key = seed , t = ( int ( time . time ( ) ) + ( drift * 30 ) ) ) 
def set global options ( options ) : global  global options  global options = dict ( options ) 
def solow jacobian ( t , k , g , n , s , alpha , delta ) : jac = ( ( ( s * alpha ) * ( k ** ( alpha - 1 ) ) ) - ( ( g + n ) + delta ) ) return jac 
def stats aggregate ( ) : return s3 rest controller ( ) 
def render constants ( ) : generate file ( 'constant enums.pxi' , cython enums , pjoin ( root , 'zmq' , 'backend' , 'cython' ) ) generate file ( 'constants.pxi' , constants pyx , pjoin ( root , 'zmq' , 'backend' , 'cython' ) ) generate file ( 'zmq constants.h' , ifndefs , pjoin ( root , 'zmq' , 'utils' ) ) 
def  defragment mountpoint ( mountpoint ) : out =   salt   [ 'cmd.run all' ] ( 'btrfs  filesystem  defragment  -f  {0}' . format ( mountpoint ) ) return { 'mount point' : mountpoint , 'passed' : ( not out [ 'stderr' ] ) , 'log' : ( out [ 'stderr' ] or  False  ) , 'range' :  False  } 
def  write proxy conf ( proxyfile ) : msg = ' Invalid   value  for  proxy  file  provided!,   Supplied   value  =  {0}' . format ( proxyfile ) log . trace ( ' Salt    Proxy    Module :  write  proxy  conf' ) if proxyfile : log . debug ( ' Writing   proxy  conf  file' ) with salt . utils . fopen ( proxyfile , 'w' ) as proxy conf : proxy conf . write ( 'master  =  {0}' . format (   grains   [ 'master' ] ) ) msg = ' Wrote   proxy  file  {0}' . format ( proxyfile ) log . debug ( msg ) return msg 
def file in same dir ( ref file , desired file ) : return os . path . join ( * ( split path ( ref file ) [ : ( - 1 ) ] + [ desired file ] ) ) 
def libc ver ( executable = sys . executable , lib = '' , version = '' , chunksize = 2048 ) : if ( executable == sys . executable ) : return ( 'glibc' , '2.6' ) if hasattr ( os . path , 'realpath' ) : executable = os . path . realpath ( executable ) f = open ( executable , 'rb' ) binary = f . read ( chunksize ) pos = 0 while 1 : m =  libc search . search ( binary , pos ) if ( not m ) : binary = f . read ( chunksize ) if ( not binary ) : break pos = 0 continue ( libcinit , glibc , glibcversion , so , threads , soversion ) = m . groups ( ) if ( libcinit and ( not lib ) ) : lib = 'libc' elif glibc : if ( lib != 'glibc' ) : lib = 'glibc' version = glibcversion elif ( glibcversion > version ) : version = glibcversion elif so : if ( lib != 'glibc' ) : lib = 'libc' if ( soversion and ( soversion > version ) ) : version = soversion if ( threads and ( version [ ( - len ( threads ) ) : ] != threads ) ) : version = ( version + threads ) pos = m . end ( ) f . close ( ) return ( lib , version ) 
def  get pool results ( * args , ** kwargs ) : import hashlib tgt = args [ 0 ] cmd = args [ 1 ] ret = { } sort = kwargs . pop ( 'survey sort' , 'down' ) direction = ( sort != 'up' ) tgt type = kwargs . pop ( 'tgt type' , 'compound' ) if ( tgt type not in [ 'compound' , 'pcre' ] ) : tgt type = 'compound' kwargs passthru = dict ( ( ( k , kwargs [ k ] ) for k in kwargs . iterkeys ( ) if ( not k . startswith ( ' ' ) ) ) ) client = salt . client . get local client (   opts   [ 'conf file' ] ) try : minions = client . cmd ( tgt , cmd , args [ 2 : ] , timeout =   opts   [ 'timeout' ] , tgt type = tgt type , kwarg = kwargs passthru ) except  Salt  Client  Error  as client error : print ( client error ) return ret for minion in sorted ( minions ) : digest = hashlib . sha256 ( str ( minions [ minion ] ) ) . hexdigest ( ) if ( digest not in ret ) : ret [ digest ] = { } ret [ digest ] [ 'pool' ] = [ ] ret [ digest ] [ 'result' ] = str ( minions [ minion ] ) ret [ digest ] [ 'pool' ] . append ( minion ) sorted ret = [ ] for k in sorted ( ret , key = ( lambda k : len ( ret [ k ] [ 'pool' ] ) ) , reverse = direction ) : sorted ret . append ( ret [ k ] ) return sorted ret 
def get Unpacked  Loops  ( loops ) : if ( len ( loops ) == 1 ) : first Loop  = loops [ 0 ] if ( first Loop  .   class   == list ) : return first Loop  return loops 
def p assignment operator ( t ) : pass 
def run ( verbosity = 1 ) :  Text  Test  Runner  ( verbosity = verbosity ) . run ( suite ( ) ) 
def get input stream ( environ , safe fallback =  True  ) : stream = environ [ 'wsgi.input' ] content length = get content length ( environ ) if environ . get ( 'wsgi.input terminated' ) : return stream if ( content length is  None  ) : return ( ( safe fallback and  empty stream ) or stream ) return  Limited  Stream  ( stream , content length ) 
def add Lists  To  Craft  Type  Repository  ( file Name  Help  , repository ) : settings . add Lists  To  Repository  ( file Name  Help  , get Profile  Directory  , repository ) dots Minus  One  = ( file Name  Help  . count ( '.' ) - 1 ) x = 0 x Addition  = 400 for step in xrange ( dots Minus  One  ) : x += x Addition  x Addition  /= 2 repository . window Position  . value = ( '%s+0' % x ) 
def strip files ( files , argv max = ( 256 * 1024 ) ) : tostrip = [ ( fn , flipwritable ( fn ) ) for fn in files ] while tostrip : cmd = list ( STRIPCMD ) flips = [ ] pathlen = sum ( [ ( len ( s ) + 1 ) for s in cmd ] ) while ( pathlen < argv max ) : if ( not tostrip ) : break ( added , flip ) = tostrip . pop ( ) pathlen += ( len ( added ) + 1 ) cmd . append ( added ) flips . append ( ( added , flip ) ) else : cmd . pop ( ) tostrip . append ( flips . pop ( ) ) os . spawnv ( os . P WAIT , cmd [ 0 ] , cmd ) for args in flips : flipwritable ( * args ) 
def  create transport endpoint ( reactor , endpoint config ) : if I Stream  Client  Endpoint  . provided By  ( endpoint config ) : endpoint = I Stream  Client  Endpoint  ( endpoint config ) elif ( endpoint config [ 'type' ] == 'tcp' ) : version = int ( endpoint config . get ( 'version' , 4 ) ) host = str ( endpoint config [ 'host' ] ) port = int ( endpoint config [ 'port' ] ) timeout = int ( endpoint config . get ( 'timeout' , 10 ) ) tls = endpoint config . get ( 'tls' ,  None  ) if tls : if ( not  TLS ) : raise  Runtime  Error  ( 'TLS  configured  in  transport,  but  TLS  support  is  not  installed  (eg   Open SSL?)' ) if I Open SSL Client  Connection  Creator  . provided By  ( tls ) : context = I Open SSL Client  Connection  Creator  ( tls ) elif isinstance ( tls ,  Certificate  Options  ) : context = tls elif ( tls is  True  ) : context = options For  Client TLS ( host ) else : raise  Runtime  Error  ( 'unknown  type  {}  for  "tls"  configuration  in  transport' . format ( type ( tls ) ) ) if ( version == 4 ) : endpoint = SSL4 Client  Endpoint  ( reactor , host , port , context , timeout = timeout ) elif ( version == 6 ) : raise  Runtime  Error  ( 'TLS  on  I Pv 6  not  implemented' ) else : assert  False  , 'should  not  arrive  here' elif ( version == 4 ) : endpoint = TCP4 Client  Endpoint  ( reactor , host , port , timeout = timeout ) elif ( version == 6 ) : try : from twisted . internet . endpoints import TCP6 Client  Endpoint  except  Import  Error  : raise  Runtime  Error  ( 'I Pv 6  is  not  supported  (please  upgrade   Twisted )' ) endpoint = TCP6 Client  Endpoint  ( reactor , host , port , timeout = timeout ) else : assert  False  , 'should  not  arrive  here' elif ( endpoint config [ 'type' ] == 'unix' ) : path = endpoint config [ 'path' ] timeout = int ( endpoint config . get ( 'timeout' , 10 ) ) endpoint = UNIX Client  Endpoint  ( reactor , path , timeout = timeout ) else : assert  False  , 'should  not  arrive  here' return endpoint 
def  get service endpoint ( context , svc , region =  None  , public =  True  ) : region =  safe region ( region ) context = ( context or identity ) url type = {  True  : 'public' ,  False  : 'private' } [ public ] svc obj = context . services . get ( svc ) if ( not svc obj ) : return  None  ep = svc obj . endpoints . get ( region , { } ) . get ( url type ) if ( not ep ) : ep = svc obj . endpoints . get ( 'ALL' , { } ) . get ( url type ) return ep 
def  Connect  No SSL ( host = 'localhost' , port = 443 , user = 'root' , pwd = '' , service = 'hostd' , adapter = 'SOAP' , namespace =  None  , path = '/sdk' , version =  None  , key File  =  None  , cert File  =  None  , thumbprint =  None  , b64token =  None  , mechanism = 'userpass' ) : if hasattr ( ssl , ' create unverified context' ) : ssl Context  = ssl .  create unverified context ( ) else : ssl Context  =  None  return  Connect  ( host = host , port = port , user = user , pwd = pwd , service = service , adapter = adapter , namespace = namespace , path = path , version = version , key File  = key File  , cert File  = cert File  , thumbprint = thumbprint , ssl Context  = ssl Context  , b64token = b64token , mechanism = mechanism ) 
def win handle is a console ( handle ) : from ctypes import byref , POINTER , windll , WINFUNCTYPE from ctypes . wintypes import BOOL , DWORD , HANDLE FILE TYPE CHAR = 2 FILE TYPE REMOTE = 32768 INVALID HANDLE VALUE = DWORD ( ( - 1 ) ) . value  Get  Console  Mode  = WINFUNCTYPE ( BOOL , HANDLE , POINTER ( DWORD ) ) ( ( ' Get  Console  Mode ' , windll . kernel32 ) )  Get  File  Type  = WINFUNCTYPE ( DWORD , DWORD ) ( ( ' Get  File  Type ' , windll . kernel32 ) ) if ( ( handle == INVALID HANDLE VALUE ) or ( handle is  None  ) ) : return  False  return ( ( (  Get  File  Type  ( handle ) & ( ~ FILE TYPE REMOTE ) ) == FILE TYPE CHAR ) and  Get  Console  Mode  ( handle , byref ( DWORD ( ) ) ) ) 
@ testing . requires testing data def test dipole fitting ctf ( ) : raw ctf = read raw ctf ( fname ctf ) . set eeg reference ( ) events = make fixed length events ( raw ctf , 1 ) evoked =  Epochs  ( raw ctf , events , 1 , 0 , 0 , baseline =  None  ) . average ( ) cov = make ad hoc cov ( evoked . info ) sphere = make sphere model ( ( 0.0 , 0.0 , 0.0 ) ) fit dipole ( evoked , cov , sphere ) 
def pull ( ) : print green ( ( '%s:   Upgrading   code' % env . host ) ) with cd ( '/home/web2py/applications/eden/' ) : try : print green ( ( '%s:   Upgrading   to  version  %i' % ( env . host , env . revno ) ) ) run ( ( 'bzr  pull  -r  %i' % env . revno ) , pty =  True  ) except : if ( not env . tested ) : print green ( ( '%s:   Upgrading   to  current   Trunk ' % env . host ) ) run ( 'bzr  pull' , pty =  True  ) else : print green ( ( '%s:   Getting   the  version  to  update  to' % env . host ) ) run ( ( 'scp  -i  /root/.ssh/sahana release  %s@%s:/home/web2py/applications/eden/VERSION  /root/VERSION test' % ( env . user , test host ) ) , pty =  True  ) version = run ( 'cat  /root/VERSION test' , pty =  True  ) env . revno = int ( version . split ( '  ' ) [ 0 ] . replace ( 'r' , '' ) ) print green ( ( '%s:   Upgrading   to  version  %i' % ( env . host , env . revno ) ) ) run ( ( 'bzr  pull  -r  %i' % env . revno ) , pty =  True  ) 
def  Connect  ( * args , ** kwargs ) : from  My SQ Ldb  . connections import  Connection  return  Connection  ( * args , ** kwargs ) 
def  get rcvar ( name , jail =  None  ) : if ( not available ( name , jail ) ) : log . error ( ' Service   {0}  not  found' . format ( name ) ) return  False  cmd = '{0}  {1}  rcvar' . format (  cmd ( jail ) , name ) for line in   salt   [ 'cmd.run stdout' ] ( cmd , python shell =  False  ) . splitlines ( ) : if ( ' enable="' not in line ) : continue ( rcvar ,   ) = line . split ( '=' , 1 ) return rcvar return  None  
def parse file ( descriptor file , descriptor type =  None  , validate =  False  , document handler =  Document  Handler  . ENTRIES , ** kwargs ) : handler =  None  if isinstance ( descriptor file , ( bytes , str type ) ) : if stem . util . system . is tarfile ( descriptor file ) : handler =  parse file for tar path else : handler =  parse file for path elif isinstance ( descriptor file , tarfile .  Tar  File  ) : handler =  parse file for tarfile if handler : for desc in handler ( descriptor file , descriptor type , validate , document handler , ** kwargs ) : ( yield desc ) return initial position = descriptor file . tell ( ) first line = stem . util . str tools .  to unicode ( descriptor file . readline ( ) . strip ( ) ) metrics header match = re . match ( '^@type  (\\S+)  (\\d+).(\\d+)$' , first line ) if ( not metrics header match ) : descriptor file . seek ( initial position ) descriptor path = getattr ( descriptor file , 'name' ,  None  ) filename = ( '<undefined>' if ( descriptor path is  None  ) else os . path . basename ( descriptor file . name ) ) file parser =  None  if ( descriptor type is not  None  ) : descriptor type match = re . match ( '^(\\S+)  (\\d+).(\\d+)$' , descriptor type ) if descriptor type match : ( desc type , major version , minor version ) = descriptor type match . groups ( ) file parser = ( lambda f :  parse metrics file ( desc type , int ( major version ) , int ( minor version ) , f , validate , document handler , ** kwargs ) ) else : raise  Value  Error  ( " The   descriptor type  must  be  of  the  form  '<type>  <major version>.<minor version>'" ) elif metrics header match : ( desc type , major version , minor version ) = metrics header match . groups ( ) file parser = ( lambda f :  parse metrics file ( desc type , int ( major version ) , int ( minor version ) , f , validate , document handler , ** kwargs ) ) elif ( ( filename == 'cached-descriptors' ) or ( filename == 'cached-descriptors.new' ) ) : file parser = ( lambda f : stem . descriptor . server descriptor .  parse file ( f , validate = validate , ** kwargs ) ) elif ( ( filename == 'cached-extrainfo' ) or ( filename == 'cached-extrainfo.new' ) ) : file parser = ( lambda f : stem . descriptor . extrainfo descriptor .  parse file ( f , validate = validate , ** kwargs ) ) elif ( ( filename == 'cached-microdescs' ) or ( filename == 'cached-microdescs.new' ) ) : file parser = ( lambda f : stem . descriptor . microdescriptor .  parse file ( f , validate = validate , ** kwargs ) ) elif ( filename == 'cached-consensus' ) : file parser = ( lambda f : stem . descriptor . networkstatus .  parse file ( f , validate = validate , document handler = document handler , ** kwargs ) ) elif ( filename == 'cached-microdesc-consensus' ) : file parser = ( lambda f : stem . descriptor . networkstatus .  parse file ( f , is microdescriptor =  True  , validate = validate , document handler = document handler , ** kwargs ) ) if file parser : for desc in file parser ( descriptor file ) : if ( descriptor path is not  None  ) : desc .  set path ( os . path . abspath ( descriptor path ) ) ( yield desc ) return raise  Type  Error  ( ( " Unable   to  determine  the  descriptor's  type.  filename:  '%s',  first  line:  '%s'" % ( filename , first line ) ) ) 
def popup text ( title , output , par widget ) : h2 = ( ( par widget . height * 3 ) // 4 ) w2 = ( ( par widget . width * 6 ) // 7 ) x = ( ( ( par widget . width - w2 ) // 2 ) - 1 ) y = ( ( ( par widget . height - h2 ) // 2 ) - 1 ) borders = curses . newwin ( ( h2 + 2 ) , ( w2 + 2 ) , ( par widget . y + y ) , ( par widget . x + x ) ) borders . border ( ) borders . addstr ( 0 , ( ( w2 - len ( title ) ) // 2 ) , ( '  %s  ' % title ) ) borders . refresh ( ) w =  Window  ( ) w . screen = borders w . widgets . append (  Listbox  ( ( ( par widget . x + x ) + 1 ) , ( ( par widget . y + y ) + 1 ) , w2 , h2 , output ) ) ret = w . start view ( w . screen ) return ( ret , ( w . widgets [ 0 ] . win y + w . widgets [ 0 ] . cursor y ) ) 
def flush ( bank , key =  None  ) : if ( key is  None  ) : c key = bank else : c key = '{0}/{1}' . format ( bank , key ) try : return api . kv . delete ( c key , recurse = ( key is  None  ) ) except  Exception  as exc : raise  Salt  Cache  Error  ( ' There   was  an  error  removing  the  key,  {0}:  {1}' . format ( c key , exc ) ) 
def merge percentiles ( finalq , qs , vals ,  Ns  , interpolation = 'lower' ) : if isinstance ( finalq ,  Iterator  ) : finalq = list ( finalq ) finalq = np . array ( finalq ) qs = list ( map ( list , qs ) ) vals = list ( vals )  Ns  = list (  Ns  ) L = list ( zip ( * [ ( q , val , N ) for ( q , val , N ) in zip ( qs , vals ,  Ns  ) if N ] ) ) if ( not L ) : raise  Value  Error  ( ' No   non-trivial  arrays  found' ) ( qs , vals ,  Ns  ) = L if ( str ( vals [ 0 ] . dtype ) == 'category' ) : result = merge percentiles ( finalq , qs , [ v . codes for v in vals ] ,  Ns  , interpolation ) import pandas as pd return pd .  Categorical  . from codes ( result , vals [ 0 ] . categories , vals [ 0 ] . ordered ) if ( not np . issubdtype ( vals [ 0 ] . dtype , np . number ) ) : interpolation = 'nearest' if ( ( len ( vals ) != len ( qs ) ) or ( len (  Ns  ) != len ( qs ) ) ) : raise  Value  Error  ( 'qs,  vals,  and   Ns   parameters  must  be  the  same  length' ) counts = [ ] for ( q , N ) in zip ( qs ,  Ns  ) : count = np . empty ( len ( q ) ) count [ 1 : ] = np . diff ( q ) count [ 0 ] = q [ 0 ] count *= N counts . append ( count ) combined vals counts = merge sorted ( * map ( zip , vals , counts ) ) ( combined vals , combined counts ) = zip ( * combined vals counts ) combined vals = np . array ( combined vals ) combined counts = np . array ( combined counts ) combined q = np . cumsum ( combined counts ) desired q = ( finalq * sum (  Ns  ) ) if ( interpolation == 'linear' ) : rv = np . interp ( desired q , combined q , combined vals ) else : left = np . searchsorted ( combined q , desired q , side = 'left' ) right = ( np . searchsorted ( combined q , desired q , side = 'right' ) - 1 ) np . minimum ( left , ( len ( combined vals ) - 1 ) , left ) lower = np . minimum ( left , right ) upper = np . maximum ( left , right ) if ( interpolation == 'lower' ) : rv = combined vals [ lower ] elif ( interpolation == 'higher' ) : rv = combined vals [ upper ] elif ( interpolation == 'midpoint' ) : rv = ( 0.5 * ( combined vals [ lower ] + combined vals [ upper ] ) ) elif ( interpolation == 'nearest' ) : lower residual = np . abs ( ( combined q [ lower ] - desired q ) ) upper residual = np . abs ( ( combined q [ upper ] - desired q ) ) mask = ( lower residual > upper residual ) index = lower index [ mask ] = upper [ mask ] rv = combined vals [ index ] else : raise  Value  Error  ( "interpolation  can  only  be  'linear',  'lower',  'higher',  'midpoint',  or  'nearest'" ) return rv 
def human Readable  Mask  ( mask ) : s = [ ] for ( k , v ) in  FLAG TO HUMAN : if ( k & mask ) : s . append ( v ) return s 
def job runner ( job ) : redirect output ( job output file ( job ) ) log ( ( " Running   in  wrapper  mode  for  '%s'\n" % job . id ) )  Experiment  Grid  . job running ( job . expt dir , job . id ) job . start t = int ( time . time ( ) ) job . status = 'running' save job ( job ) success =  False  start time = time . time ( ) try : if ( job . language == MATLAB ) : run matlab job ( job ) elif ( job . language == PYTHON ) : run python job ( job ) elif ( job . language == SHELL ) : run torch job ( job ) elif ( job . language == MCR ) : run mcr job ( job ) else : raise  Exception  ( ' That   function  type  has  not  been  implemented.' ) success =  True  except : log ( ( '-' * 40 ) ) log ( ' Problem   running  the  job:' ) log ( sys . exc info ( ) ) log ( traceback . print exc ( limit = 1000 ) ) log ( ( '-' * 40 ) ) end time = time . time ( ) duration = ( end time - start time ) job file = job file for ( job ) job = load job ( job file ) log ( ' Job   file  reloaded.' ) if ( not job .  Has  Field  ( 'value' ) ) : log ( ' Could   not  find  value  in  output  file.' ) success =  False  if success : log ( ( ' Completed   successfully  in  %0.2f  seconds.  [%f]' % ( duration , job . value ) ) )  Experiment  Grid  . job complete ( job . expt dir , job . id , job . value , duration ) job . status = 'complete' else : log ( ( ' Job   failed  in  %0.2f  seconds.' % duration ) )  Experiment  Grid  . job broken ( job . expt dir , job . id ) job . status = 'broken' job . end t = int ( time . time ( ) ) job . duration = duration save job ( job ) 
def resource id from record tuple ( record ) : return record [ 0 ] [ 'resource id' ] 
def object dict ( objects , by ref =  False  , error message =  None  , error dict =  None  ) : if by ref : items = ( ( obj . ref , obj ) for obj in objects ) else : items = ( ( obj . name , obj ) for obj in objects ) ordered =  Ordered  Dict  ( ) for ( key , value ) in items : if ( key in ordered ) : error message = ( error message or ' Duplicate   key  {key}' ) error dict = ( error dict or { } ) raise  Model  Error  ( error message . format ( key = key , ** error dict ) ) ordered [ key ] = value return ordered 
def add Observer  ( section , name , target , attr =  None  , dtype = str , callback =  None  , default =  None  ) : observers = config . observers . setdefault ( ( section . lower ( ) , name . lower ( ) ) , { } ) if ( not attr ) : tokens = name . lower ( ) . split ( ) attr = ( tokens [ 0 ] + '' . join ( ( t . title ( ) for t in tokens [ 1 : ] ) ) ) log . debug ( ' Subscribing   %s.%s' , target , attr ) attr = intern ( attr ) targetref = weakref . ref ( target ) observers . setdefault ( ( targetref , attr ) , callback ) val =  get Property  ( section , name , dtype , default ) setattr ( target , attr , val ) if callback : callback ( val ) 
def iter fields ( node ) : for field in node .  fields : try : ( yield ( field , getattr ( node , field ) ) ) except  Attribute  Error  : pass 
def validipaddr ( address ) : try : octets = address . split ( '.' ) if ( len ( octets ) != 4 ) : return  False  for x in octets : if ( not ( 0 <= int ( x ) <= 255 ) ) : return  False  except  Value  Error  : return  False  return  True  
def line search wolfe2 ( f , myfprime , xk , pk , gfk =  None  , old fval =  None  , old old fval =  None  , args = ( ) , c1 = 0.0001 , c2 = 0.9 , amax = 50 ) : fc = [ 0 ] gc = [ 0 ] gval = [  None  ] def phi ( alpha ) : fc [ 0 ] += 1 return f ( ( xk + ( alpha * pk ) ) , * args ) if isinstance ( myfprime , tuple ) : def derphi ( alpha ) : fc [ 0 ] += ( len ( xk ) + 1 ) eps = myfprime [ 1 ] fprime = myfprime [ 0 ] newargs = ( ( f , eps ) + args ) gval [ 0 ] = fprime ( ( xk + ( alpha * pk ) ) , * newargs ) return np . dot ( gval [ 0 ] , pk ) else : fprime = myfprime def derphi ( alpha ) : gc [ 0 ] += 1 gval [ 0 ] = fprime ( ( xk + ( alpha * pk ) ) , * args ) return np . dot ( gval [ 0 ] , pk ) if ( gfk is  None  ) : gfk = fprime ( xk , * args ) derphi0 = np . dot ( gfk , pk ) ( alpha star , phi star , old fval , derphi star ) = scalar search wolfe2 ( phi , derphi , old fval , old old fval , derphi0 , c1 , c2 , amax ) if ( derphi star is  None  ) : warn ( ' The   line  search  algorithm  did  not  converge' ,  Line  Search  Warning  ) else : derphi star = gval [ 0 ] return ( alpha star , fc [ 0 ] , gc [ 0 ] , phi star , old fval , derphi star ) 
def get default currency ( ) : return settings . OSCAR DEFAULT CURRENCY 
def no spm ( ) : if ( ( u'NIPYPE NO MATLAB' in os . environ ) or (  Info  . version ( ) is  None  ) ) : return  True  else : return  False  
def send sms ( profile , body , to , from  ) : ret = { } ret [ 'message' ] = { } ret [ 'message' ] [ 'sid' ] =  None  client =  get twilio ( profile ) try : message = client . sms . messages . create ( body = body , to = to , from  = from  ) except  Twilio  Rest  Exception  as exc : ret [ ' error' ] = { } ret [ ' error' ] [ 'code' ] = exc . code ret [ ' error' ] [ 'msg' ] = exc . msg ret [ ' error' ] [ 'status' ] = exc . status log . debug ( ' Could   not  send  sms.   Error :  {0}' . format ( ret ) ) return ret ret [ 'message' ] = { } ret [ 'message' ] [ 'sid' ] = message . sid ret [ 'message' ] [ 'price' ] = message . price ret [ 'message' ] [ 'price unit' ] = message . price unit ret [ 'message' ] [ 'status' ] = message . status ret [ 'message' ] [ 'num segments' ] = message . num segments ret [ 'message' ] [ 'body' ] = message . body ret [ 'message' ] [ 'date sent' ] = str ( message . date sent ) ret [ 'message' ] [ 'date created' ] = str ( message . date created ) log . info ( ret ) return ret 
def  Temporary  File  ( mode = 'w+b' , bufsize = ( - 1 ) , suffix = '' , prefix = template , dir =  None  ) : return  String IO ( ) 
def err ( msg , die =  None  ) : sys . stderr . write ( ( msg + u'\n' ) ) if die : sys . exit ( ( die if ( type ( die ) is int ) else 1 ) ) 
@ contextmanager def random seed ( seed ) : state = random . getstate ( ) random . seed ( seed ) try : ( yield ) finally : random . setstate ( state ) 
def instrumented test render ( self , context ) : dispatcher . send ( signal = signals . template rendered , sender = self , template = self , context = context ) return self . nodelist . render ( context ) 
def enable inheritance ( path , object Type  , clear =  False  ) : dc = dacl Constants  ( ) object Type  = dc . get Object  Type  Bit  ( object Type  ) path = dc . process Path  ( path , object Type  ) return  set dacl inheritance ( path , object Type  ,  True  ,  None  , clear ) 
def test Simple  ( data = ' Hello   <b> World </b><br/><img  src="img/test.jpg"/>' , dest = 'test.pdf' ) : pdf = pisa .  Create PDF ( c String IO .  String IO ( data ) , file ( dest , 'wb' ) ) if pdf . err : dump Errors  ( pdf ) else : pisa . start Viewer  ( dest ) 
def partition data ( X , y ) : (  Xs  , ys ) = shuffle array ( X , y ) mapping = { } for i in xrange ( len ( y ) ) : yi = ys [ i ] try : mapping [ yi ] . append ( i ) except  Key  Error  : mapping [ yi ] = [ i ] (  Xtrain  , ytrain ) = ( [ ] , [ ] ) (  Xtest  , ytest ) = ( [ ] , [ ] ) for ( key , indices ) in mapping . iteritems ( ) :  Xtrain  . extend ( [  Xs  [ i ] for i in indices [ : 1 ] ] ) ytrain . extend ( [ ys [ i ] for i in indices [ : 1 ] ] )  Xtest  . extend ( [  Xs  [ i ] for i in indices [ 1 : 20 ] ] ) ytest . extend ( [ ys [ i ] for i in indices [ 1 : 20 ] ] ) return (  Xtrain  , ytrain ,  Xtest  , ytest ) 
def get info extractor ( ie name ) : return globals ( ) [ ( ie name + u'IE' ) ] 
def  is known retryable ( exception ) : if isinstance ( exception , API Error  ) : if ( exception . response . status code == INTERNAL SERVER ERROR ) : error text = exception . response . text return any ( ( ( known in error text ) for known in [ u' Unknown   device' , u'no  such  device' ] ) ) if isinstance ( exception ,  Connection  Error  ) : if ( ( len ( exception . args ) > 0 ) and isinstance ( exception . args [ 0 ] ,  Protocol  Error  ) ) : if ( ( len ( exception . args [ 0 ] . args ) > 1 ) and isinstance ( exception . args [ 0 ] . args [ 1 ] , socket error ) ) : return ( exception . args [ 0 ] . args [ 1 ] . errno in { ECONNREFUSED } ) return  False  
def list imdbs ( ) : return   sets . keys ( ) 
def  ascii encode ( inarray , out =  None  ) : out dtype = np . dtype ( ( 'S{0}' . format ( ( inarray . dtype . itemsize // 4 ) ) , inarray . dtype . shape ) ) if ( out is not  None  ) : out = out . view ( out dtype ) op dtypes = [ inarray . dtype , out dtype ] op flags = [ [ 'readonly' ] , [ 'writeonly' , 'allocate' ] ] it = np . nditer ( [ inarray , out ] , op dtypes = op dtypes , op flags = op flags , flags = [ 'zerosize ok' ] ) try : for ( initem , outitem ) in it : outitem [ ... ] = initem . item ( ) . encode ( 'ascii' ) except  Unicode  Encode  Error  as exc : index = np . unravel index ( it . iterindex , inarray . shape ) raise   Unicode  Array  Encode  Error  ( * ( exc . args + ( index , ) ) ) return it . operands [ 1 ] 
def test exclude columns ( ) : table =  Unordered  Table  ( [ ] , exclude = u'i' ) assert ( table . columns . names ( ) == [ u'alpha' , u'beta' ] ) class  Partial  Table  (  Unordered  Table  , ) : class  Meta  : exclude = ( u'alpha' , ) table =  Partial  Table  ( [ ] ) assert ( table . columns . names ( ) == [ u'i' , u'beta' ] ) class  Addon  Table  (  Partial  Table  , ) : added = tables .  Column  ( ) table =  Addon  Table  ( [ ] ) assert ( table . columns . names ( ) == [ u'i' , u'beta' , u'added' ] ) class  Exclude  Table  (  Unordered  Table  , ) : added = tables .  Column  ( ) class  Meta  : exclude = ( u'beta' , ) table =  Exclude  Table  ( [ ] ) assert ( table . columns . names ( ) == [ u'i' , u'alpha' , u'added' ] ) 
def search ( arr , item ) : arr = mergesort ( arr ) first = 0 last = ( len ( arr ) - 1 ) found =  False  while ( ( first <= last ) and ( not found ) ) : midpoint = ( ( first + last ) // 2 ) if ( arr [ midpoint ] == item ) : found =  True  elif ( item < arr [ midpoint ] ) : last = ( midpoint - 1 ) else : first = ( midpoint + 1 ) return found 
@ ratelimit ( ) @ requires auth ( 'item' ) @ pre event def put ( resource , payload =  None  , ** lookup ) : return put internal ( resource , payload , concurrency check =  True  , skip validation =  False  , ** lookup ) 
@ register canonicalize @ register specialize @ gof . local optimizer ( [  Subtensor  ,  Advanced  Subtensor 1 ] ) def local useless subtensor ( node ) : if ( not hasattr ( node , 'fgraph' ) ) : return if ( not hasattr ( node . fgraph , 'shape feature' ) ) : return shape of = node . fgraph . shape feature . shape of if isinstance ( node . op ,  Subtensor  ) : cdata = node . op . get constant idx ( node . inputs , allow partial =  True  , only process constants =  True  ) for ( pos , idx ) in enumerate ( cdata ) : if ( not isinstance ( idx , slice ) ) : return  False  if ( ( idx . start is not  None  ) and ( idx . start != 0 ) ) : return  False  if ( ( idx . step is not  None  ) and ( idx . step != 1 ) ) : return  False  for ( pos , idx ) in enumerate ( cdata ) : length pos = shape of [ node . inputs [ 0 ] ] [ pos ] if isinstance ( idx . stop , ( integer types , numpy . integer ) ) : length pos data = sys . maxsize try : length pos data = get scalar constant value ( length pos , only process constants =  True  ) except  Not  Scalar  Constant  Error  : pass if ( idx . stop < length pos data ) : return  False  elif isinstance ( idx . stop , gof .  Variable  ) : length pos shape i = idx . stop if ( length pos shape i . owner and isinstance ( length pos shape i . owner . op , T .  Scalar  From  Tensor  ) ) : length pos shape i = length pos shape i . owner . inputs [ 0 ] elif ( length pos . owner and isinstance ( length pos . owner . op , T .  Tensor  From  Scalar  ) ) : length pos = length pos . owner . inputs [ 0 ] else : return  False  assert ( str ( length pos . type . dtype ) == 'int64' ) assert ( str ( length pos shape i . type . dtype ) in [ 'int8' , 'int16' , 'int32' , 'int64' ] ) if ( length pos shape i != length pos ) : return  False  elif ( idx . stop is  None  ) : pass else : return  False  elif isinstance ( node . op ,  Advanced  Subtensor 1 ) : try : length = get scalar constant value ( shape of [ node . inputs [ 0 ] ] [ 0 ] , only process constants =  True  ) except  Not  Scalar  Constant  Error  : return  False  idx = node . inputs [ 1 ] if isinstance ( idx , T .  Constant  ) : idx = idx . value if ( len ( idx ) != length ) : return  False  if numpy . any ( ( idx != numpy . arange ( length ) ) ) : return  False  elif ( ( idx . owner is not  None  ) and isinstance ( idx . owner . op , T . A Range  ) ) : try : ( start , stop , step ) = map ( ( lambda x : get scalar constant value ( x , only process constants =  True  ) ) , idx . owner . inputs ) except  Not  Scalar  Constant  Error  : return  False  if ( start != 0 ) : return  False  if ( stop != length ) : return  False  if ( step != 1 ) : return  False  else : return  False  else : return  False  return [ node . inputs [ 0 ] ] 
def arp cache ( attrs =  None  , where =  None  ) : return  osquery cmd ( table = 'arp cache' , attrs = attrs , where = where ) 
def  getfinder ( ) : global  finder talker if ( not  finder talker ) :  finder talker =  Finder  .  Finder  ( )  finder talker . send flags = ( (  finder talker . send flags |  Apple  Events  . kAE Can  Interact  ) |  Apple  Events  . kAE Can  Switch  Layer  ) return  finder talker 
def trim ( string ) : lines = string . expandtabs ( ) . splitlines ( ) lines = ( list ( map ( str . lstrip , lines [ : 1 ] ) ) + left trim lines ( lines [ 1 : ] ) ) return '\n' . join ( trim leading lines ( trim trailing lines ( lines ) ) ) 
def init ( ) : if qtutils . version check ( '5.3.0' ) : default ciphers = Q Ssl  Socket  . default Ciphers  ( ) log . init . debug ( ' Default    Qt   ciphers:  {}' . format ( ',  ' . join ( ( c . name ( ) for c in default ciphers ) ) ) ) else : default ciphers = Q Ssl  Socket  . supported Ciphers  ( ) log . init . debug ( ' Supported    Qt   ciphers:  {}' . format ( ',  ' . join ( ( c . name ( ) for c in default ciphers ) ) ) ) good ciphers = [ ] bad ciphers = [ ] for cipher in default ciphers : if  is secure cipher ( cipher ) : good ciphers . append ( cipher ) else : bad ciphers . append ( cipher ) log . init . debug ( ' Disabling   bad  ciphers:  {}' . format ( ',  ' . join ( ( c . name ( ) for c in bad ciphers ) ) ) ) Q Ssl  Socket  . set Default  Ciphers  ( good ciphers ) 
def list nodes select ( call =  None  ) : return salt . utils . cloud . list nodes select ( list nodes full ( 'function' ) ,   opts   [ 'query.selection' ] , call ) 
def capture signals ( ) : return  Capture  Signals  ( ALL SIGNALS ) 
def scale by constant ( builder , val , factor ) : return builder . mul ( val ,  Constant  . int ( TIMEDELTA64 , factor ) ) 
def service get all bmc by host ( context , host ) : return IMPL . service get all bmc by host ( context , host ) 
def libvlc media new path ( p instance , path ) : f = (   Cfunctions  . get ( 'libvlc media new path' ,  None  ) or   Cfunction  ( 'libvlc media new path' , ( ( 1 , ) , ( 1 , ) ) , class result (  Media  ) , ctypes . c void p ,  Instance  , ctypes . c char p ) ) return f ( p instance , path ) 
def p abstract declarator 2 ( t ) : pass 
def fail with changes ( name ) : ret = { 'name' : name , 'changes' : { } , 'result' :  False  , 'comment' : ' Failure !' } ret [ 'changes' ] = { 'testing' : { 'old' : ' Unchanged ' , 'new' : ' Something   pretended  to  change' } } if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = " If   we  weren't  testing,  this  would  be  failed  with  changes" return ret 
def  coerced Unicode  ( s ) : if isinstance ( s , bytes ) : if  PY3 : raise  Type  Error  ( ( ' Expected   str  not  %r  (bytes)' % ( s , ) ) ) else : return s . decode ( 'ascii' ) else : return s 
def reboot ( zone , single =  False  , altinit =  None  , smf options =  None  ) : ret = { 'status' :  True  } boot options = '' if single : boot options = '-s  {0}' . format ( boot options ) if altinit : boot options = '-i  {0}  {1}' . format ( altinit , boot options ) if smf options : boot options = '-m  {0}  {1}' . format ( smf options , boot options ) if ( boot options != '' ) : boot options = '  --  {0}' . format ( boot options . strip ( ) ) res =   salt   [ 'cmd.run all' ] ( 'zoneadm  {zone}  reboot{boot opts}' . format ( zone = ( '-u  {0}' . format ( zone ) if  is uuid ( zone ) else '-z  {0}' . format ( zone ) ) , boot opts = boot options ) ) ret [ 'status' ] = ( res [ 'retcode' ] == 0 ) ret [ 'message' ] = ( res [ 'stdout' ] if ret [ 'status' ] else res [ 'stderr' ] ) ret [ 'message' ] = ret [ 'message' ] . replace ( 'zoneadm:  ' , '' ) if ( ret [ 'message' ] == '' ) : del ret [ 'message' ] return ret 
def get Supported  Key  Exchanges  ( ) : from cryptography . hazmat . backends import default backend from cryptography . hazmat . primitives . asymmetric import ec from twisted . conch . ssh . keys import  curve Table  backend = default backend ( ) kex Algorithms  =  kex Algorithms  . copy ( ) for key Algorithm  in list ( kex Algorithms  ) : if key Algorithm  . startswith ( 'ecdh' ) : key Algorithm  Dsa  = key Algorithm  . replace ( 'ecdh' , 'ecdsa' ) supported = backend . elliptic curve exchange algorithm supported ( ec . ECDH ( ) ,  curve Table  [ key Algorithm  Dsa  ] ) if ( not supported ) : kex Algorithms  . pop ( key Algorithm  ) return sorted ( kex Algorithms  , key = ( lambda kex Algorithm  : kex Algorithms  [ kex Algorithm  ] . preference ) ) 
def check fixed len bcs dups ( header , mapping data , errors ) : header field to check = ' Barcode  Sequence ' try : check ix = header . index ( header field to check ) except  Value  Error  : return errors barcodes = [ ] correction = 1 for curr data in mapping data : barcodes . append ( upper ( curr data [ check ix ] ) ) dups = duplicates indices ( barcodes ) for curr dup in dups : for curr loc in dups [ curr dup ] : errors . append ( ( ' Duplicate   barcode  %s  found. DCTB %d,%d' % ( curr dup , ( curr loc + correction ) , check ix ) ) ) return errors 
def pop ( string , encoding =  None  ) : try : ( dlen , rest ) = string . split ( ':' , 1 ) dlen = int ( dlen ) except  Value  Error  : raise  Value  Error  ( 'not  a  tnetstring:  missing  or  invalid  length  prefix' ) try : ( data , type , remain ) = ( rest [ : dlen ] , rest [ dlen ] , rest [ ( dlen + 1 ) : ] ) except  Index  Error  : raise  Value  Error  ( 'not  a  tnetstring:  invalid  length  prefix' ) if ( type == ',' ) : if ( encoding is not  None  ) : return ( data . decode ( encoding ) , remain ) return ( data , remain ) if ( type == '#' ) : try : return ( int ( data ) , remain ) except  Value  Error  : raise  Value  Error  ( 'not  a  tnetstring:  invalid  integer  literal' ) if ( type == '^' ) : try : return ( float ( data ) , remain ) except  Value  Error  : raise  Value  Error  ( 'not  a  tnetstring:  invalid  float  literal' ) if ( type == '!' ) : if ( data == 'true' ) : return (  True  , remain ) elif ( data == 'false' ) : return (  False  , remain ) else : raise  Value  Error  ( 'not  a  tnetstring:  invalid  boolean  literal' ) if ( type == '~' ) : if data : raise  Value  Error  ( 'not  a  tnetstring:  invalid  null  literal' ) return (  None  , remain ) if ( type == ']' ) : l = [ ] while data : ( item , data ) = pop ( data , encoding ) l . append ( item ) return ( l , remain ) if ( type == '}' ) : d = { } while data : ( key , data ) = pop ( data , encoding ) ( val , data ) = pop ( data , encoding ) d [ key ] = val return ( d , remain ) raise  Value  Error  ( 'unknown  type  tag' ) 
def is python file ( filename ) : if filename . endswith ( u'.py' ) : return  True  try : with open with encoding ( filename ) as f : first line = f . readlines ( 1 ) [ 0 ] except ( IO Error  ,  Index  Error  ) : return  False  if ( not PYTHON SHEBANG REGEX . match ( first line ) ) : return  False  return  True  
def  do lin field coeff ( bem rr , tris , tn , ta , rmags , cosmags , ws , bins ) : coeff = np . zeros ( ( ( bins [ ( - 1 ) ] + 1 ) , len ( bem rr ) ) ) for ( tri , tri nn , tri area ) in zip ( tris , tn , ta ) : tri rr = bem rr [ tri ] zz = [ ] for trr in tri rr : diff = ( rmags - trr ) dl = np . sum ( ( diff * diff ) , axis = 1 ) c = fast cross 3d ( diff , tri nn [ np . newaxis , : ] ) x = ( ( tri area * np . sum ( ( c * cosmags ) , axis = 1 ) ) / ( ( 3.0 * dl ) * np . sqrt ( dl ) ) ) zz += [ np . bincount ( bins , weights = ( x * ws ) , minlength = ( bins [ ( - 1 ) ] + 1 ) ) ] coeff [ : , tri ] += np . array ( zz ) . T return coeff 
def textinfo from path ( path , encoding =  None  , follow symlinks =  False  , quick determine lang =  False  ) : return  Text  Info  . init from path ( path , encoding = encoding , follow symlinks = follow symlinks , quick determine lang = quick determine lang ) 
def convert serializable ( records ) : for r in records : r . msg = r . get Message  ( ) r . args = ( ) 
@ not implemented for ( 'undirected' ) def out degree centrality ( G ) : centrality = { } s = ( 1.0 / ( len ( G ) - 1.0 ) ) centrality = { n : ( d * s ) for ( n , d ) in G . out degree ( ) } return centrality 
def as url ( scheme , host =  None  , port =  None  , user =  None  , password =  None  , path =  None  , query =  None  , sanitize =  False  , mask = u'**' ) : parts = [ u'{0}://' . format ( scheme ) ] if ( user or password ) : if user : parts . append ( safequote ( user ) ) if password : if sanitize : parts . extend ( ( [ u':' , mask ] if mask else [ u':' ] ) ) else : parts . extend ( [ u':' , safequote ( password ) ] ) parts . append ( u'@' ) parts . append ( ( safequote ( host ) if host else u'' ) ) if port : parts . extend ( [ u':' , port ] ) parts . extend ( [ u'/' , path ] ) return u'' . join ( ( str ( part ) for part in parts if part ) ) 
def package finder ( argv ) : try : command =  Install  Command  ( ) except  Type  Error  : from pip . baseparser import create main parser command =  Install  Command  ( create main parser ( ) ) ( options ,   ) = loads ( dumps ( command . parser ) ) . parse args ( argv ) possible options = [ 'find links' , FORMAT CONTROL ARG , 'allow external' , 'allow unverified' , 'allow all external' , ( 'allow all prereleases' , 'pre' ) , 'process dependency links' ] kwargs = { } for option in possible options : ( kw , attr ) = ( option if isinstance ( option , tuple ) else ( option , option ) ) value = getattr ( options , attr , MARKER ) if ( value is not MARKER ) : kwargs [ kw ] = value index urls = ( [ options . index url ] + options . extra index urls ) if options . no index : index urls = [ ] index urls += getattr ( options , 'mirrors' , [ ] ) if hasattr ( command , ' build session' ) : kwargs [ 'session' ] = command .  build session ( options ) return  Package  Finder  ( index urls = index urls , ** kwargs ) 
def get view id list ( user , site , check global =  True  , use cache =  True  ) : page ids =  get page ids for action ( user = user , site = site , action = 'view page' , check global = check global , use cache = use cache ) return page ids 
@ transaction task def send first edit email ( revision pk ) : revision =  Revision  . objects . get ( pk = revision pk ) ( user , doc ) = ( revision . creator , revision . document ) subject = ( u'[MDN]  [%(loc)s]  %(user)s  made  their  first  edit,  to:  %(doc)s' % { 'loc' : doc . locale , 'user' : user . username , 'doc' : doc . title } ) message = render to string ( 'wiki/email/edited.ltxt' , context dict ( revision ) ) doc url = absolutify ( doc . get absolute url ( ) ) email =  Email  Message  ( subject , message , settings . DEFAULT FROM EMAIL , to = [ config . EMAIL LIST SPAM WATCH ] , headers = { 'X- Kuma - Document - Url ' : doc url , 'X- Kuma - Editor - Username ' : user . username } ) email . send ( ) 
def rewrite links ( input html , callback , ** kwargs ) : return lxml rewrite links ( ( u'<div>%s</div>' % input html ) , callback , ** kwargs ) 
def get object jsonschema ( question , required fields , is reviewer ) : object jsonschema = { 'type' : 'object' , 'additional Properties ' :  False  , 'properties' : { } } required = [ ] properties = question . get ( 'properties' ) if properties : for property in properties : if ( property . get ( 'required' ,  False  ) and required fields ) : required . append ( property [ 'id' ] ) values = extract question values ( property , required fields , is reviewer ) object jsonschema [ 'properties' ] [ property [ 'id' ] ] = { 'type' : 'object' , 'additional Properties ' :  False  , 'properties' : values } if required fields : object jsonschema [ 'properties' ] [ property [ 'id' ] ] [ 'required' ] = [ 'value' ] if ( required fields and is required ( question ) ) : object jsonschema [ 'required' ] = required return object jsonschema 
def vn free ar ( call =  None  , kwargs =  None  ) : if ( call != 'function' ) : raise  Salt  Cloud  System  Exit  ( ' The   vn free ar  function  must  be  called  with  -f  or  --function.' ) if ( kwargs is  None  ) : kwargs = { } vn id = kwargs . get ( 'vn id' ,  None  ) vn name = kwargs . get ( 'vn name' ,  None  ) ar id = kwargs . get ( 'ar id' ,  None  ) if ( ar id is  None  ) : raise  Salt  Cloud  System  Exit  ( " The   vn free ar  function  requires  an  'rn id'  to  be  provided." ) if vn id : if vn name : log . warning ( " Both   the  'vn id'  and  'vn name'  arguments  were  provided.  'vn id'  will  take  precedence." ) elif vn name : vn id = get vn id ( kwargs = { 'name' : vn name } ) else : raise  Salt  Cloud  System  Exit  ( " The   vn free ar  function  requires  a  'vn id'  or  a  'vn name'  to  be  provided." ) ( server , user , password ) =  get xml rpc ( ) auth = ':' . join ( [ user , password ] ) response = server . one . vn . free ar ( auth , int ( vn id ) , int ( ar id ) ) data = { 'action' : 'vn.free ar' , 'ar freed' : response [ 0 ] , 'resource id' : response [ 1 ] , 'error code' : response [ 2 ] } return data 
def test passthrough context ( ) : form =  Sample  Form  ( ) form . helper =  Form  Helper  ( ) form . helper . template = u'custom form template with context.html' c = { u'prefix' : u'foo' , u'suffix' : u'bar' } html = render crispy form ( form , helper = form . helper , context = c ) assert ( u' Got   prefix:  foo' in html ) assert ( u' Got   suffix:  bar' in html ) 
def setup fog ( ) : gl Enable  ( GL FOG ) gl Fogfv  ( GL FOG COLOR , ( G Lfloat  * 4 ) ( 0.5 , 0.69 , 1.0 , 1 ) ) gl Hint  ( GL FOG HINT , GL DONT CARE ) gl Fogi  ( GL FOG MODE , GL LINEAR ) gl Fogf  ( GL FOG START , 20.0 ) gl Fogf  ( GL FOG END , 60.0 ) 
def polynomial multiply mod ( m1 , m2 , polymod , p ) : prod = ( ( ( len ( m1 ) + len ( m2 ) ) - 1 ) * [ 0 ] ) for i in range ( len ( m1 ) ) : for j in range ( len ( m2 ) ) : prod [ ( i + j ) ] = ( ( prod [ ( i + j ) ] + ( m1 [ i ] * m2 [ j ] ) ) % p ) return polynomial reduce mod ( prod , polymod , p ) 
def build Sliced  Network  ( ) : N =  Feed  Forward  Network  ( 'sliced' ) a =  Linear  Layer  ( 2 , name = 'a' ) b =  Linear  Layer  ( 2 , name = 'b' ) N . add Input  Module  ( a ) N . add Output  Module  ( b ) N . add Connection  (  Full  Connection  ( a , b , in Slice  To  = 1 , out Slice  From  = 1 ) ) N . add Connection  (  Full  Connection  ( a , b , in Slice  From  = 1 , out Slice  To  = 1 ) ) N . sort Modules  ( ) return N 
def py hamilton filter ( initial probabilities , regime transition , conditional likelihoods ) : k regimes = len ( initial probabilities ) nobs = conditional likelihoods . shape [ ( - 1 ) ] order = ( conditional likelihoods . ndim - 2 ) dtype = conditional likelihoods . dtype filtered marginal probabilities = np . zeros ( ( k regimes , nobs ) , dtype = dtype ) predicted joint probabilities = np . zeros ( ( ( ( k regimes , ) * ( order + 1 ) ) + ( nobs , ) ) , dtype = dtype ) joint likelihoods = np . zeros ( ( nobs , ) , dtype ) filtered joint probabilities = np . zeros ( ( ( ( k regimes , ) * ( order + 1 ) ) + ( ( nobs + 1 ) , ) ) , dtype = dtype ) filtered marginal probabilities [ : , 0 ] = initial probabilities tmp = np . copy ( initial probabilities ) shape = ( k regimes , k regimes ) for i in range ( order ) : tmp = ( np . reshape ( regime transition [ ... , i ] , ( shape + ( ( 1 , ) * i ) ) ) * tmp ) filtered joint probabilities [ ... , 0 ] = tmp shape = ( k regimes , k regimes ) shape += ( ( 1 , ) * ( order - 1 ) ) shape += ( regime transition . shape [ ( - 1 ) ] , ) regime transition = np . reshape ( regime transition , shape ) if ( regime transition . shape [ ( - 1 ) ] > 1 ) : regime transition = regime transition [ ... , order : ] transition t = 0 for t in range ( nobs ) : if ( regime transition . shape [ ( - 1 ) ] > 1 ) : transition t = t predicted joint probabilities [ ... , t ] = ( regime transition [ ... , transition t ] * filtered joint probabilities [ ... , t ] . sum ( axis = ( - 1 ) ) ) tmp = ( conditional likelihoods [ ... , t ] * predicted joint probabilities [ ... , t ] ) joint likelihoods [ t ] = np . sum ( tmp ) filtered joint probabilities [ ... , ( t + 1 ) ] = ( tmp / joint likelihoods [ t ] ) filtered marginal probabilities = filtered joint probabilities [ ... , 1 : ] for i in range ( 1 , ( filtered marginal probabilities . ndim - 1 ) ) : filtered marginal probabilities = np . sum ( filtered marginal probabilities , axis = ( - 2 ) ) return ( filtered marginal probabilities , predicted joint probabilities , joint likelihoods , filtered joint probabilities [ ... , 1 : ] ) 
def  find function ( name , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) for funcs in salt . utils . boto3 . paged call ( conn . list functions ) : for func in funcs [ ' Functions ' ] : if ( func [ ' Function  Name ' ] == name ) : return func return  None  
def factorint ( n , limit =  None  , use trial =  True  , use rho =  True  , use pm1 =  True  , verbose =  False  , visual =  None  ) : factordict = { } if ( visual and ( not isinstance ( n ,  Mul  ) ) and ( not isinstance ( n , dict ) ) ) : factordict = factorint ( n , limit = limit , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose , visual =  False  ) elif isinstance ( n ,  Mul  ) : factordict = dict ( [ ( int ( k ) , int ( v ) ) for ( k , v ) in list ( n . as powers dict ( ) . items ( ) ) ] ) elif isinstance ( n , dict ) : factordict = n if ( factordict and ( isinstance ( n ,  Mul  ) or isinstance ( n , dict ) ) ) : for k in list ( factordict . keys ( ) ) : if isprime ( k ) : continue e = factordict . pop ( k ) d = factorint ( k , limit = limit , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose , visual =  False  ) for ( k , v ) in d . items ( ) : if ( k in factordict ) : factordict [ k ] += ( v * e ) else : factordict [ k ] = ( v * e ) if ( visual or ( ( type ( n ) is dict ) and ( visual is not  True  ) and ( visual is not  False  ) ) ) : if ( factordict == { } ) : return S .  One  if ( ( - 1 ) in factordict ) : factordict . pop ( ( - 1 ) ) args = [ S .  Negative  One  ] else : args = [ ] args . extend ( [  Pow  ( evaluate =  False  , * i ) for i in sorted ( factordict . items ( ) ) ] ) return  Mul  ( evaluate =  False  , * args ) elif ( isinstance ( n , dict ) or isinstance ( n ,  Mul  ) ) : return factordict assert ( use trial or use rho or use pm1 ) n = as int ( n ) if limit : limit = int ( limit ) if ( n < 0 ) : factors = factorint ( ( - n ) , limit = limit , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose , visual =  False  ) factors [ ( - 1 ) ] = 1 return factors if ( limit and ( limit < 2 ) ) : if ( n == 1 ) : return { } return { n : 1 } elif ( n < 10 ) : return [ { 0 : 1 } , { } , { 2 : 1 } , { 3 : 1 } , { 2 : 2 } , { 5 : 1 } , { 2 : 1 , 3 : 1 } , { 7 : 1 } , { 2 : 3 } , { 3 : 2 } ] [ n ] factors = { } if verbose : sn = str ( n ) if ( len ( sn ) > 50 ) : print ( ( ( ( ' Factoring   %s' % sn [ : 5 ] ) + ( '..(%i  other  digits)..' % ( len ( sn ) - 10 ) ) ) + sn [ ( - 5 ) : ] ) ) else : print ( ' Factoring ' , n ) if use trial : small = ( 2 ** 15 ) fail max = 600 small = min ( small , ( limit or small ) ) if verbose : print ( ( trial int msg % ( 2 , small , fail max ) ) ) ( n , next p ) =  factorint small ( factors , n , small , fail max ) else : next p = 2 if ( factors and verbose ) : for k in sorted ( factors ) : print ( ( factor msg % ( k , factors [ k ] ) ) ) if ( next p == 0 ) : if ( n > 1 ) : factors [ int ( n ) ] = 1 if verbose : print ( complete msg ) return factors try : if ( limit and ( next p > limit ) ) : if verbose : print ( ' Exceeded   limit:' , limit )  check termination ( factors , n , limit , use trial , use rho , use pm1 , verbose ) if ( n > 1 ) : factors [ int ( n ) ] = 1 return factors else : sqrt n = integer nthroot ( n , 2 ) [ 0 ] a = ( sqrt n + 1 ) a2 = ( a ** 2 ) b2 = ( a2 - n ) for i in range ( 3 ) : ( b , fermat ) = integer nthroot ( b2 , 2 ) if fermat : break b2 += ( ( 2 * a ) + 1 ) a += 1 if fermat : if verbose : print ( fermat msg ) if limit : limit -= 1 for r in [ ( a - b ) , ( a + b ) ] : facs = factorint ( r , limit = limit , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose ) factors . update ( facs ) raise  Stop  Iteration   check termination ( factors , n , limit , use trial , use rho , use pm1 , verbose ) except  Stop  Iteration  : if verbose : print ( complete msg ) return factors ( low , high ) = ( next p , ( 2 * next p ) ) limit = ( limit or sqrt n ) limit += 1 while 1 : try : high  = high if ( limit < high  ) : high  = limit if use trial : if verbose : print ( ( trial msg % ( low , high  ) ) ) ps = sieve . primerange ( low , high  ) ( n , found trial ) =  trial ( factors , n , ps , verbose ) if found trial :  check termination ( factors , n , limit , use trial , use rho , use pm1 , verbose ) else : found trial =  False  if ( high > limit ) : if verbose : print ( ' Exceeded   limit:' , limit ) if ( n > 1 ) : factors [ int ( n ) ] = 1 raise  Stop  Iteration  if ( not found trial ) : if ( use pm1 or use rho ) : high root = max ( int ( math . log ( ( high  ** 0.7 ) ) ) , low , 3 ) if use pm1 : if verbose : print ( ( pm1 msg % ( high root , high  ) ) ) c = pollard pm1 ( n , B = high root , seed = high  ) if c : ps = factorint ( c , limit = ( limit - 1 ) , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose ) ( n ,   ) =  trial ( factors , n , ps , verbose =  False  )  check termination ( factors , n , limit , use trial , use rho , use pm1 , verbose ) if use rho : max steps = high root if verbose : print ( ( rho msg % ( 1 , max steps , high  ) ) ) c = pollard rho ( n , retries = 1 , max steps = max steps , seed = high  ) if c : ps = factorint ( c , limit = ( limit - 1 ) , use trial = use trial , use rho = use rho , use pm1 = use pm1 , verbose = verbose ) ( n ,   ) =  trial ( factors , n , ps , verbose =  False  )  check termination ( factors , n , limit , use trial , use rho , use pm1 , verbose ) except  Stop  Iteration  : if verbose : print ( complete msg ) return factors ( low , high ) = ( high , ( high * 2 ) ) 
def create image metadata ( data ) : disk format = data [ 'disk format' ] if ( disk format in ( 'ami' , 'aki' , 'ari' ) ) : container format = disk format elif ( disk format == 'docker' ) : disk format = 'raw' container format = 'docker' else : container format = 'bare' meta = { 'protected' : data [ 'protected' ] , 'disk format' : disk format , 'container format' : container format , 'min disk' : ( data [ 'minimum disk' ] or 0 ) , 'min ram' : ( data [ 'minimum ram' ] or 0 ) , 'name' : data [ 'name' ] } is public = data . get ( 'is public' , data . get ( 'public' ,  False  ) ) properties = { } if data . get ( 'description' ) : properties [ 'description' ] = data [ 'description' ] if data . get ( 'kernel' ) : properties [ 'kernel id' ] = data [ 'kernel' ] if data . get ( 'ramdisk' ) : properties [ 'ramdisk id' ] = data [ 'ramdisk' ] if data . get ( 'architecture' ) : properties [ 'architecture' ] = data [ 'architecture' ] if ( api . glance . VERSIONS . active < 2 ) : meta . update ( { 'is public' : is public , 'properties' : properties } ) else : meta [ 'visibility' ] = ( 'public' if is public else 'private' ) meta . update ( properties ) return meta 
def get service module name ( service model ) : name = service model . metadata . get ( 'service Abbreviation ' , service model . metadata . get ( 'service Full  Name ' , service model . service name ) ) name = name . replace ( ' Amazon ' , '' ) name = name . replace ( 'AWS' , '' ) name = re . sub ( '\\W+' , '' , name ) return name 
def s one set ( topics ) : s one set = [ ] for top words in topics : s one set t = [ ] for w prime in top words : s one set t . append ( ( w prime , top words ) ) s one set . append ( s one set t ) return s one set 
def DEFINE float ( name , default , help ) : CONFIG .  Add  Option  ( type info .  Float  ( name = name , default = default , description = help ) ) 
def assert Raises  (  exception ,  callable =  None  , * args , ** kwargs ) : manager =   Assert  Raises  Contextmanager  ( exception =  exception ) if (  callable is not  None  ) : with manager :  callable ( * args , ** kwargs ) else : return manager 
def renew hook ( config , domains , lineage path ) : if config . renew hook : if ( not config . dry run ) : os . environ [ 'RENEWED DOMAINS' ] = '  ' . join ( domains ) os . environ [ 'RENEWED LINEAGE' ] = lineage path logger . info ( ' Running   renew-hook  command:  %s' , config . renew hook )  run hook ( config . renew hook ) else : logger . warning ( ' Dry   run:  skipping  renewal  hook  command:  %s' , config . renew hook ) 
def hypsum ( expr , n , start , prec ) : from sympy import  Float  , hypersimp , lambdify if ( prec == float ( 'inf' ) ) : raise  Not  Implemented  Error  ( 'does  not  support  inf  prec' ) if start : expr = expr . subs ( n , ( n + start ) ) hs = hypersimp ( expr , n ) if ( hs is  None  ) : raise  Not  Implemented  Error  ( 'a  hypergeometric  series  is  required' ) ( num , den ) = hs . as numer denom ( ) func1 = lambdify ( n , num ) func2 = lambdify ( n , den ) ( h , g , p ) = check convergence ( num , den , n ) if ( h < 0 ) : raise  Value  Error  ( ( ' Sum   diverges  like  (n!)^%i' % ( - h ) ) ) term = expr . subs ( n , 0 ) if ( not term . is  Rational  ) : raise  Not  Implemented  Error  ( ' Non   rational  term  functionality  is  not  implemented.' ) if ( ( h > 0 ) or ( ( h == 0 ) and ( abs ( g ) > 1 ) ) ) : term = ( ( MPZ ( term . p ) << prec ) // term . q ) s = term k = 1 while ( abs ( term ) > 5 ) : term *= MPZ ( func1 ( ( k - 1 ) ) ) term //= MPZ ( func2 ( ( k - 1 ) ) ) s += term k += 1 return from man exp ( s , ( - prec ) ) else : alt = ( g < 0 ) if ( abs ( g ) < 1 ) : raise  Value  Error  ( ( ' Sum   diverges  like  (%i)^n' % abs ( ( 1 / g ) ) ) ) if ( ( p < 1 ) or ( ( p == 1 ) and ( not alt ) ) ) : raise  Value  Error  ( ( ' Sum   diverges  like  n^%i' % ( - p ) ) ) vold =  None  ndig = prec to dps ( prec ) while  True  : prec2 = ( 4 * prec ) term0 = ( ( MPZ ( term . p ) << prec2 ) // term . q ) def summand ( k ,  term = [ term0 ] ) : if k : k = int ( k )  term [ 0 ] *= MPZ ( func1 ( ( k - 1 ) ) )  term [ 0 ] //= MPZ ( func2 ( ( k - 1 ) ) ) return make mpf ( from man exp (  term [ 0 ] , ( - prec2 ) ) ) with workprec ( prec ) : v = nsum ( summand , [ 0 , mpmath inf ] , method = 'richardson' ) vf =  Float  ( v , ndig ) if ( ( vold is not  None  ) and ( vold == vf ) ) : break prec += prec vold = vf return v .  mpf  
def parse acl ( * args , ** kwargs ) : version = kwargs . pop ( 'version' ,  None  ) if ( version in ( 1 ,  None  ) ) : return parse acl v1 ( * args ) elif ( version == 2 ) : return parse acl v2 ( * args , ** kwargs ) else : raise  Value  Error  ( ( ' Unknown   ACL  version:  parse acl(%r,  %r)' % ( args , kwargs ) ) ) 
def copyfileobj ( fsrc , fdst , length = ( 16 * 1024 ) ) : while 1 : buf = fsrc . read ( length ) if ( not buf ) : break fdst . write ( buf ) 
@ verbose def load data ( condition = 'visual' , data format = 'raw' , data type = 'experimental' , path =  None  , force update =  False  , update path =  None  , verbose =  None  ) : if ( not ( condition . lower ( ) in valid conditions ) ) : raise  Value  Error  ( ( ' Unknown   condition  "%s"' % condition ) ) if ( data format not in valid data formats ) : raise  Value  Error  ( ( ' Unknown   data format  "%s"' % data format ) ) if ( data type not in valid data types ) : raise  Value  Error  ( ( ' Unknown   data type  "%s"' % data type ) ) urls = url match ( condition , data format , data type ) data paths = list ( ) for url in urls : data paths . extend ( data path ( url , path , force update , update path ) ) return data paths 
def is Line  Intersecting  Inside X Segment  ( begin Complex  , end Complex  , segment First X , segment Second X , y ) : x Intersection  = getX Intersection  If  Exists  ( begin Complex  , end Complex  , y ) if ( x Intersection  ==  None  ) : return  False  if ( x Intersection  < min ( segment First X , segment Second X ) ) : return  False  return ( x Intersection  <= max ( segment First X , segment Second X ) ) 
def get breadcrumbs ( query , path ) : breadcrumbs = [ ] dir query = '' if path : for item in path . split ( os . sep ) : dir query = os . path . join ( dir query , item ) breadcrumbs . append ( [ item , dir query ] ) return breadcrumbs 
def catalog ( ) : global  default t = getattr (  active , 'value' ,  None  ) if ( t is not  None  ) : return t if (  default is  None  ) : from django . conf import settings  default = translation ( settings . LANGUAGE CODE ) return  default 
@ treeio login required @ handle response format def field add ( request , response format = 'html' ) : if ( not request . user . profile . is admin ( 'treeio.infrastructure' ) ) : return user denied ( request , message = " You   don't  have  administrator  access  to  the   Infrastructure   module" , response format = response format ) if request . POST : if ( 'cancel' not in request . POST ) : field =  Item  Field  ( ) form =  Item  Field  Form  ( request . POST , instance = field ) if form . is valid ( ) : field = form . save ( request ) field . set user from request ( request ) return  Http  Response  Redirect  ( reverse ( 'infrastructure field view' , args = [ field . id ] ) ) else : return  Http  Response  Redirect  ( reverse ( 'infrastructure settings view' ) ) else : form =  Item  Field  Form  ( ) context =  get default context ( request ) context . update ( { 'form' : form } ) return render to response ( 'infrastructure/field add' , context , context instance =  Request  Context  ( request ) , response format = response format ) 
def lowercase value ( value ) : if isinstance ( value , six . string types ) : result = value . lower ( ) elif isinstance ( value , ( list , tuple ) ) : result = [ str ( item ) . lower ( ) for item in value ] elif isinstance ( value , dict ) : result = { } for ( key , value ) in six . iteritems ( value ) : result [ key . lower ( ) ] = str ( value ) . lower ( ) else : result = value return result 
def editor test ( ) : from spyder . utils . qthelpers import qapplication app = qapplication ( ) dialog =  Collections  Editor  ( ) dialog . setup ( get test data ( ) ) dialog . show ( ) app . exec  ( ) 
def copy image ( module , ec2 ) : tags = module . params . get ( 'tags' ) params = { ' Source  Region ' : module . params . get ( 'source region' ) , ' Source  Image  Id ' : module . params . get ( 'source image id' ) , ' Name ' : module . params . get ( 'name' ) , ' Description ' : module . params . get ( 'description' ) , ' Encrypted ' : module . params . get ( 'encrypted' ) } if module . params . get ( 'kms key id' ) : params [ ' Kms  Key  Id ' ] = module . params . get ( 'kms key id' ) try : image id = ec2 . copy image ( ** params ) [ ' Image  Id ' ] if module . params . get ( 'wait' ) : ec2 . get waiter ( 'image available' ) . wait (  Image  Ids  = [ image id ] ) if module . params . get ( 'tags' ) : ec2 . create tags (  Resources  = [ image id ] ,  Tags  = [ { ' Key ' : k , ' Value ' : v } for ( k , v ) in module . params . get ( 'tags' ) . items ( ) ] ) module . exit json ( changed =  True  , image id = image id ) except  Waiter  Error  as we : module . fail json ( msg = ( ' An   error  occured  waiting  for  the  image  to  become  available.  (%s)' % we . reason ) ) except  Client  Error  as ce : module . fail json ( msg = ce . message ) except  No  Credentials  Error  : module . fail json ( msg = ' Unable   to  authenticate,  AWS  credentials  are  invalid.' ) except  Exception  as e : module . fail json ( msg = ( ' Unhandled   exception.  (%s)' % str ( e ) ) ) 
def generate ssh key ( request ) : try : subprocess . check output ( [ u'ssh-keygen' , u'-q' , u'-N' , u'' , u'-C' , u' Weblate ' , u'-t' , u'rsa' , u'-f' , ssh file ( RSA KEY ) ] , stderr = subprocess . STDOUT , env = get clean env ( ) ) messages . success ( request ,   ( u' Created   new  SSH  key.' ) ) except ( subprocess .  Called  Process  Error  , OS Error  ) as exc : messages . error ( request , (   ( u' Failed   to  generate  key:  %s' ) % getattr ( exc , u'output' , str ( exc ) ) ) ) 
def configure codelab igor jenkins ( ) :  Yaml  Bindings  . update yml source ( '/opt/spinnaker/config/spinnaker-local.yml' , { 'jenkins' : { 'default Master ' : { 'name' : ' Codelab  Jenkins ' , 'base Url ' : 'http://localhost:9090' , 'username' : 'admin' , 'password' : 'admin' } } , 'igor' : { 'enabled' : 'true' } } ) 
def sessions ( app , url = 'http://localhost:8080/manager' , timeout = 180 ) : return  simple cmd ( 'sessions' , app , url , timeout = timeout ) 
def skip With  Client  If  ( condition , reason ) : def client pass ( * args , ** kwargs ) : pass def skipdec ( obj ) : retval = unittest . skip ( reason ) ( obj ) if ( not isinstance ( obj , type ) ) : retval . client skip = ( lambda f : client pass ) return retval def noskipdec ( obj ) : if ( not ( isinstance ( obj , type ) or hasattr ( obj , 'client skip' ) ) ) : obj . client skip = ( lambda f : f ) return obj return ( skipdec if condition else noskipdec ) 
def read plain ( file obj , type  , count ) : if ( count == 0 ) : return [ ] conv = DECODE PLAIN [ type  ] return conv ( file obj , count ) 
def maybe ref ( ref ) : if ( not isinstance ( ref , str ) ) : return ref return ref to obj ( ref ) 
def  Local  Context  ( function ) : @ functools . wraps ( function ) def setter ( * a , ** kw ) : if ( not kw ) : return function ( * a ) with context . local ( ** { k : kw . pop ( k ) for ( k , v ) in kw . items ( ) if isinstance ( getattr (  Context  Type  , k ,  None  ) , property ) } ) : return function ( * a , ** kw ) return setter 
def get Right  Strip  Minus  Split  ( line String  ) : old Line  String  Length  = ( - 1 ) while ( old Line  String  Length  < len ( line String  ) ) : old Line  String  Length  = len ( line String  ) line String  = line String  . replace ( '-  ' , '-' ) return line String  . split ( ) 
def setup ( app ) : app . add role ( 'rfc' , rfclink ) return 
def get module ( module name , version =  None  ) : if ( not is installed ( module name , version ) ) : return  None  return   import   ( module name , fromlist = [ module name . rpartition ( '.' ) [ ( - 1 ) ] ] ) 
def get entry point abs path ( pack =  None  , entry point =  None  ) : if ( not entry point ) : return  None  if os . path . isabs ( entry point ) : pack base path = get pack base path ( pack name = pack ) common prefix = os . path . commonprefix ( [ pack base path , entry point ] ) if ( common prefix != pack base path ) : raise  Value  Error  ( ( ' Entry   point  file  "%s"  is  located  outside  of  the  pack  directory' % entry point ) ) return entry point entry point abs path = get pack resource file abs path ( pack ref = pack , resource type = 'action' , file path = entry point ) return entry point abs path 
def fix case ( name ) : for ( s , r ) in LOCALE NORMALIZATION . items ( ) : name = name . replace ( s , r ) return name 
def  matches ( event , token values ) : if ( event . user id is not  None  ) : for attribute name in [ 'user id' , 'trustor id' , 'trustee id' ] : if ( event . user id == token values [ attribute name ] ) : break else : return  False  if ( event . domain id is not  None  ) : for attribute name in [ 'identity domain id' , 'assignment domain id' ] : if ( event . domain id == token values [ attribute name ] ) : break else : return  False  if ( event . domain scope id is not  None  ) : if ( event . domain scope id != token values [ 'assignment domain id' ] ) : return  False  attribute names = [ 'project id' , 'expires at' , 'trust id' , 'consumer id' , 'access token id' , 'audit id' , 'audit chain id' ] for attribute name in attribute names : if ( getattr ( event , attribute name ) is not  None  ) : if ( getattr ( event , attribute name ) != token values [ attribute name ] ) : return  False  if ( event . role id is not  None  ) : roles = token values [ 'roles' ] for role in roles : if ( event . role id == role ) : break else : return  False  if ( token values [ 'issued at' ] > event . issued before ) : return  False  return  True  
def sanitize address ( addr , encoding ) : if ( not isinstance ( addr , tuple ) ) : addr = parseaddr ( force text ( addr ) ) ( nm , addr ) = addr ( localpart , domain ) = (  None  ,  None  ) nm =  Header  ( nm , encoding ) . encode ( ) try : addr . encode ( 'ascii' ) except  Unicode  Encode  Error  : ( localpart , domain ) = split addr ( addr , encoding ) if ( localpart and domain ) : address =  Address  ( nm , username = localpart , domain = domain ) return str ( address ) try : address =  Address  ( nm , addr spec = addr ) except (  Invalid  Header  Defect  ,  Non ASCII Local  Part  Defect  ) : ( localpart , domain ) = split addr ( addr , encoding ) address =  Address  ( nm , username = localpart , domain = domain ) return str ( address ) 
def denormalize column names ( parsed data ) : cols = parsed data . columns . tolist ( ) base columns = defaultdict ( list ) for col in cols : if ( '.' in col ) : base columns [ col ] . append ( col . split ( '.' ) [ ( - 1 ) ] ) rename = { } for ( col , new cols ) in iteritems ( base columns ) : if ( len ( new cols ) == 1 ) : rename [ col ] = new cols [ 0 ] if ( len ( list ( rename . keys ( ) ) ) > 0 ) : return parsed data . rename ( columns = rename ) else : return parsed data 
def  Prune  Unwanted  Targets  ( targets , flat list , dependency nodes , root targets , data ) : qualified root targets = [ ] for target in root targets : target = target . strip ( ) qualified targets = gyp . common .  Find  Qualified  Targets  ( target , flat list ) if ( not qualified targets ) : raise  Gyp  Error  ( ( ' Could   not  find  target  %s' % target ) ) qualified root targets . extend ( qualified targets ) wanted targets = { } for target in qualified root targets : wanted targets [ target ] = targets [ target ] for dependency in dependency nodes [ target ] .  Deep  Dependencies  ( ) : wanted targets [ dependency ] = targets [ dependency ] wanted flat list = [ t for t in flat list if ( t in wanted targets ) ] for build file in data [ 'target build files' ] : if ( not ( 'targets' in data [ build file ] ) ) : continue new targets = [ ] for target in data [ build file ] [ 'targets' ] : qualified name = gyp . common .  Qualified  Target  ( build file , target [ 'target name' ] , target [ 'toolset' ] ) if ( qualified name in wanted targets ) : new targets . append ( target ) data [ build file ] [ 'targets' ] = new targets return ( wanted targets , wanted flat list ) 
def   Connect  ( region ) : ec2 region =  None  for r in regions ( ) : if ( r . name == region ) : ec2 region = r break assert ( ec2 region is not  None  ) , ( '"%s"  not  in  the  list  of  ec2  regions:  %s' % ( region , ',  ' . join ( k Valid  Regions  ) ) ) return boto . connect ec2 ( region = ec2 region ) 
def get refs ( addon , branch =  None  , sha =  None  , connection =  None  ) : connection = ( connection or  Git  Hub  Client  ( external account = addon . external account ) ) if ( sha and ( not branch ) ) : raise HTTP Error  ( http . BAD REQUEST ) if ( not branch ) : repo = connection . repo ( addon . user , addon . repo ) if ( repo is  None  ) : return (  None  ,  None  ,  None  ) branch = repo . default branch registered branches = ( [  Branch  . from json ( b ) for b in addon . registration data . get ( 'branches' , [ ] ) ] if addon . owner . is registration else [ ] ) registered branch names = [ each . name for each in registered branches ] if ( registered branches and ( branch not in registered branch names ) ) : raise HTTP Error  ( http . BAD REQUEST ) branches = ( registered branches or connection . branches ( addon . user , addon . repo ) ) for each in branches : if ( branch == each . name ) : sha = each . commit . sha break return ( branch , sha , branches ) 
def present ( name , ip ) : ret = { 'name' : name , 'changes' : { } , 'result' :  None  , 'comment' : '' } if ( not isinstance ( ip , list ) ) : ip = [ ip ] comments = [ ] for  ip in ip : if   salt   [ 'hosts.has pair' ] (  ip , name ) : ret [ 'result' ] =  True  comments . append ( ' Host   {0}  ({1})  already  present' . format ( name ,  ip ) ) elif   opts   [ 'test' ] : comments . append ( ' Host   {0}  ({1})  needs  to  be  added/updated' . format ( name ,  ip ) ) elif ( salt . utils . validate . net . ipv4 addr (  ip ) or salt . utils . validate . net . ipv6 addr (  ip ) ) : if   salt   [ 'hosts.add host' ] (  ip , name ) : ret [ 'changes' ] = { 'host' : name } ret [ 'result' ] =  True  comments . append ( ' Added   host  {0}  ({1})' . format ( name ,  ip ) ) else : ret [ 'result' ] =  False  comments . append ( ' Failed   to  set  host' ) else : ret [ 'result' ] =  False  comments . append ( ' Invalid   IP   Address   for  {0}  ({1})' . format ( name ,  ip ) ) ret [ 'comment' ] = '\n' . join ( comments ) return ret 
def  create temporary ( path ) : return  create carefully ( ( '%s.%s.%s.%s' % ( path , int ( time . time ( ) ) , socket . gethostname ( ) , os . getpid ( ) ) ) ) 
def get Crafted  Text  ( file Name  , text = '' , lift Repository  =  None  ) : return get Crafted  Text  From  Text  ( archive . get Text  If  Empty  ( file Name  , text ) , lift Repository  ) 
def server power ( status , host =  None  , admin username =  None  , admin password =  None  , module =  None  ) : return   execute cmd ( 'serveraction  {0}' . format ( status ) , host = host , admin username = admin username , admin password = admin password , module = module ) 
def cummingsify ( poem ) : def success ( ) : return poem . lower ( ) def gibberish ( ) : raise  Gibberish  Error  ( ) def bug ( ) : raise  Value  Error  ( ) return random . choice ( [ success , gibberish , bug ] ) ( ) 
def set url facts if unset ( facts ) : if ( 'master' in facts ) : hostname = facts [ 'common' ] [ 'hostname' ] cluster hostname = facts [ 'master' ] . get ( 'cluster hostname' ) cluster public hostname = facts [ 'master' ] . get ( 'cluster public hostname' ) public hostname = facts [ 'common' ] [ 'public hostname' ] api hostname = ( cluster hostname if cluster hostname else hostname ) api public hostname = ( cluster public hostname if cluster public hostname else public hostname ) console path = facts [ 'master' ] [ 'console path' ] etcd hosts = facts [ 'master' ] [ 'etcd hosts' ] use ssl = dict ( api = facts [ 'master' ] [ 'api use ssl' ] , public api = facts [ 'master' ] [ 'api use ssl' ] , loopback api = facts [ 'master' ] [ 'api use ssl' ] , console = facts [ 'master' ] [ 'console use ssl' ] , public console = facts [ 'master' ] [ 'console use ssl' ] , etcd = facts [ 'master' ] [ 'etcd use ssl' ] ) ports = dict ( api = facts [ 'master' ] [ 'api port' ] , public api = facts [ 'master' ] [ 'api port' ] , loopback api = facts [ 'master' ] [ 'api port' ] , console = facts [ 'master' ] [ 'console port' ] , public console = facts [ 'master' ] [ 'console port' ] , etcd = facts [ 'master' ] [ 'etcd port' ] ) etcd urls = [ ] if ( etcd hosts != '' ) : facts [ 'master' ] [ 'etcd port' ] = ports [ 'etcd' ] facts [ 'master' ] [ 'embedded etcd' ] =  False  for host in etcd hosts : etcd urls . append ( format url ( use ssl [ 'etcd' ] , host , ports [ 'etcd' ] ) ) else : etcd urls = [ format url ( use ssl [ 'etcd' ] , hostname , ports [ 'etcd' ] ) ] facts [ 'master' ] . setdefault ( 'etcd urls' , etcd urls ) prefix hosts = [ ( 'api' , api hostname ) , ( 'public api' , api public hostname ) , ( 'loopback api' , hostname ) ] for ( prefix , host ) in prefix hosts : facts [ 'master' ] . setdefault ( ( prefix + ' url' ) , format url ( use ssl [ prefix ] , host , ports [ prefix ] ) ) r lhn = '{0}:{1}' . format ( hostname , ports [ 'api' ] ) . replace ( '.' , '-' ) r lhu = 'system:openshift-master/{0}:{1}' . format ( api hostname , ports [ 'api' ] ) . replace ( '.' , '-' ) facts [ 'master' ] . setdefault ( 'loopback cluster name' , r lhn ) facts [ 'master' ] . setdefault ( 'loopback context name' , 'default/{0}/system:openshift-master' . format ( r lhn ) ) facts [ 'master' ] . setdefault ( 'loopback user' , r lhu ) prefix hosts = [ ( 'console' , api hostname ) , ( 'public console' , api public hostname ) ] for ( prefix , host ) in prefix hosts : facts [ 'master' ] . setdefault ( ( prefix + ' url' ) , format url ( use ssl [ prefix ] , host , ports [ prefix ] , console path ) ) return facts 
def edge load centrality ( G , cutoff =  False  ) : betweenness = { } for ( u , v ) in G . edges ( ) : betweenness [ ( u , v ) ] = 0.0 betweenness [ ( v , u ) ] = 0.0 for source in G : ubetween =  edge betweenness ( G , source , cutoff = cutoff ) for ( e , ubetweenv ) in ubetween . items ( ) : betweenness [ e ] += ubetweenv return betweenness 
def demo ( choice =  None  , print times =  True  , print grammar =  False  , print trees =  True  , trace = 2 , sent = u'I  saw   John   with  a  dog  with  my  cookie' , numparses = 5 ) : import sys , time from nltk import nonterminals ,  Production  , CFG grammar = demo grammar ( ) if print grammar : print ( u'*   Grammar ' ) print ( grammar ) print ( u'*   Sentence :' ) print ( sent ) tokens = sent . split ( ) print ( tokens ) print ( ) if ( choice is  None  ) : print ( u'    1:   Top -down  chart  parser' ) print ( u'    2:   Bottom -up  chart  parser' ) print ( u'    3:   Bottom -up  left-corner  chart  parser' ) print ( u'    4:   Left -corner  chart  parser  with  bottom-up  filter' ) print ( u'    5:   Stepping   chart  parser  (alternating  top-down  &  bottom-up)' ) print ( u'    6:   All   parsers' ) print ( u'\n Which   parser  (1-6)?  ' , end = u'  ' ) choice = sys . stdin . readline ( ) . strip ( ) print ( ) choice = str ( choice ) if ( choice not in u'123456' ) : print ( u' Bad   parser  number' ) return times = { } strategies = { u'1' : ( u' Top -down' , TD STRATEGY ) , u'2' : ( u' Bottom -up' , BU STRATEGY ) , u'3' : ( u' Bottom -up  left-corner' , BU LC STRATEGY ) , u'4' : ( u' Filtered   left-corner' , LC STRATEGY ) } choices = [ ] if ( choice in strategies ) : choices = [ choice ] if ( choice == u'6' ) : choices = u'1234' for strategy in choices : print ( ( u'*   Strategy :  ' + strategies [ strategy ] [ 0 ] ) ) print ( ) cp =  Chart  Parser  ( grammar , strategies [ strategy ] [ 1 ] , trace = trace ) t = time . time ( ) chart = cp . chart parse ( tokens ) parses = list ( chart . parses ( grammar . start ( ) ) ) times [ strategies [ strategy ] [ 0 ] ] = ( time . time ( ) - t ) print ( u' Nr   edges  in  chart:' , len ( chart . edges ( ) ) ) if numparses : assert ( len ( parses ) == numparses ) , u' Not   all  parses  found' if print trees : for tree in parses : print ( tree ) else : print ( u' Nr   trees:' , len ( parses ) ) print ( ) if ( choice in u'56' ) : print ( u'*   Strategy :   Stepping   (top-down  vs  bottom-up)' ) print ( ) t = time . time ( ) cp =  Stepping  Chart  Parser  ( grammar , trace = trace ) cp . initialize ( tokens ) for i in range ( 5 ) : print ( u'***  SWITCH  TO  TOP  DOWN' ) cp . set strategy ( TD STRATEGY ) for ( j , e ) in enumerate ( cp . step ( ) ) : if ( ( j > 20 ) or ( e is  None  ) ) : break print ( u'***  SWITCH  TO  BOTTOM  UP' ) cp . set strategy ( BU STRATEGY ) for ( j , e ) in enumerate ( cp . step ( ) ) : if ( ( j > 20 ) or ( e is  None  ) ) : break times [ u' Stepping ' ] = ( time . time ( ) - t ) print ( u' Nr   edges  in  chart:' , len ( cp . chart ( ) . edges ( ) ) ) if numparses : assert ( len ( list ( cp . parses ( ) ) ) == numparses ) , u' Not   all  parses  found' if print trees : for tree in cp . parses ( ) : print ( tree ) else : print ( u' Nr   trees:' , len ( list ( cp . parses ( ) ) ) ) print ( ) if ( not ( print times and times ) ) : return print ( u'*   Parsing   times' ) print ( ) maxlen = max ( ( len ( key ) for key in times ) ) format = ( ( u'%' + repr ( maxlen ) ) + u's  parser:  %6.3fsec' ) times items = times . items ( ) for ( parser , t ) in sorted ( times items , key = ( lambda a : a [ 1 ] ) ) : print ( ( format % ( parser , t ) ) ) 
def user password not empty ( key , data , errors , context ) : if ( ( data . get ( ( 'password hash' , ) , missing ) is not missing ) and authz . is sysadmin ( context . get ( 'user' ) ) ) : return if ( ( not ( ( 'password1' , ) in data ) ) and ( not ( ( 'password2' , ) in data ) ) ) : password = data . get ( ( 'password' , ) ,  None  ) if ( not password ) : errors [ key ] . append (   ( ' Missing   value' ) ) 
def fastica ( X , n components =  None  , algorithm = 'parallel' , whiten =  True  , fun = 'logcosh' , fun args =  None  , max iter = 200 , tol = 0.0001 , w init =  None  , random state =  None  , return X mean =  False  , compute sources =  True  , return n iter =  False  ) : random state = check random state ( random state ) fun args = ( { } if ( fun args is  None  ) else fun args ) X = check array ( X , copy = whiten , dtype = FLOAT DTYPES ) . T alpha = fun args . get ( 'alpha' , 1.0 ) if ( not ( 1 <= alpha <= 2 ) ) : raise  Value  Error  ( 'alpha  must  be  in  [1,2]' ) if ( fun == 'logcosh' ) : g =  logcosh elif ( fun == 'exp' ) : g =  exp elif ( fun == 'cube' ) : g =  cube elif callable ( fun ) : def g ( x , fun args ) : return fun ( x , ** fun args ) else : exc = (  Value  Error  if isinstance ( fun , six . string types ) else  Type  Error  ) raise exc ( ( " Unknown   function  %r;  should  be  one  of  'logcosh',  'exp',  'cube'  or  callable" % fun ) ) ( n , p ) = X . shape if ( ( not whiten ) and ( n components is not  None  ) ) : n components =  None  warnings . warn ( ' Ignoring   n components  with  whiten= False .' ) if ( n components is  None  ) : n components = min ( n , p ) if ( n components > min ( n , p ) ) : n components = min ( n , p ) warnings . warn ( ( 'n components  is  too  large:  it  will  be  set  to  %s' % n components ) ) if whiten : X mean = X . mean ( axis = ( - 1 ) ) X -= X mean [ : , np . newaxis ] ( u , d ,   ) = linalg . svd ( X , full matrices =  False  ) del   K = ( u / d ) . T [ : n components ] del u , d X1 = np . dot ( K , X ) X1 *= np . sqrt ( p ) else : X1 = as float array ( X , copy =  False  ) if ( w init is  None  ) : w init = np . asarray ( random state . normal ( size = ( n components , n components ) ) , dtype = X1 . dtype ) else : w init = np . asarray ( w init ) if ( w init . shape != ( n components , n components ) ) : raise  Value  Error  ( ( 'w init  has  invalid  shape  --  should  be  %(shape)s' % { 'shape' : ( n components , n components ) } ) ) kwargs = { 'tol' : tol , 'g' : g , 'fun args' : fun args , 'max iter' : max iter , 'w init' : w init } if ( algorithm == 'parallel' ) : ( W , n iter ) =  ica par ( X1 , ** kwargs ) elif ( algorithm == 'deflation' ) : ( W , n iter ) =  ica def ( X1 , ** kwargs ) else : raise  Value  Error  ( ' Invalid   algorithm:  must  be  either  `parallel`  or  `deflation`.' ) del X1 if whiten : if compute sources : S = fast dot ( fast dot ( W , K ) , X ) . T else : S =  None  if return X mean : if return n iter : return ( K , W , S , X mean , n iter ) else : return ( K , W , S , X mean ) elif return n iter : return ( K , W , S , n iter ) else : return ( K , W , S ) else : if compute sources : S = fast dot ( W , X ) . T else : S =  None  if return X mean : if return n iter : return (  None  , W , S ,  None  , n iter ) else : return (  None  , W , S ,  None  ) elif return n iter : return (  None  , W , S , n iter ) else : return (  None  , W , S ) 
def adadelta ( loss or grads , params , learning rate = 1.0 , rho = 0.95 , epsilon = 1e-06 ) : grads = get or compute grads ( loss or grads , params ) updates =  Ordered  Dict  ( ) one = T . constant ( 1 ) for ( param , grad ) in zip ( params , grads ) : value = param . get value ( borrow =  True  ) accu = theano . shared ( np . zeros ( value . shape , dtype = value . dtype ) , broadcastable = param . broadcastable ) delta accu = theano . shared ( np . zeros ( value . shape , dtype = value . dtype ) , broadcastable = param . broadcastable ) accu new = ( ( rho * accu ) + ( ( one - rho ) * ( grad ** 2 ) ) ) updates [ accu ] = accu new update = ( ( grad * T . sqrt ( ( delta accu + epsilon ) ) ) / T . sqrt ( ( accu new + epsilon ) ) ) updates [ param ] = ( param - ( learning rate * update ) ) delta accu new = ( ( rho * delta accu ) + ( ( one - rho ) * ( update ** 2 ) ) ) updates [ delta accu ] = delta accu new return updates 
def path split ( leaf , expr ) : last =  None  for node in list ( path ( expr , leaf ) ) [ : ( - 1 ) ] [ : : ( - 1 ) ] : if isinstance ( node , good to split ) : return node elif ( not isinstance ( node , can split ) ) : return last last = node return node 
def  Parse  Log  Entry  ( entry ) : try : return   Strict  Parse  Log  Entry  ( entry ) except  Value  Error  : return (   Current  Time  Micro  ( ) ,  DEFAULT LEVEL ,   Clean  ( entry ) ,  None  ) 
def  Key LEQ ( key1 , key2 ) : if ( ( key1 is  None  ) or ( key2 is  None  ) ) : return  True  return ( key1 <= key2 ) 
def trapz Warp  ( pic , cx , cy , ismask =  False  ) : ( Y , X ) = pic . shape [ : 2 ] src = np . array ( [ [ 0 , 0 ] , [ X , 0 ] , [ X , Y ] , [ 0 , Y ] ] ) dst = np . array ( [ [ ( cx * X ) , ( cy * Y ) ] , [ ( ( 1 - cx ) * X ) , ( cy * Y ) ] , [ X , Y ] , [ 0 , Y ] ] ) tform = tf .  Projective  Transform  ( ) tform . estimate ( src , dst ) im = tf . warp ( pic , tform . inverse , output shape = ( Y , X ) ) return ( im if ismask else ( im * 255 ) . astype ( 'uint8' ) ) 
@ contextmanager def open ( filename , ** credentials ) : kdb =  None  try : with io . open ( filename , 'rb' ) as stream : signature = read signature ( stream ) cls = get kdb reader ( signature ) kdb = cls ( stream , ** credentials ) ( yield kdb ) kdb . close ( ) except : if kdb : kdb . close ( ) raise 
def request server info ( server ) : if ( not server . request ) : server . request =  True   Thread  ( target =  retrieve info , args = ( server , ) ) . start ( ) 
def  check is permutation ( indices , n samples ) : if ( len ( indices ) != n samples ) : return  False  hit = np . zeros ( n samples , dtype = bool ) hit [ indices ] =  True  if ( not np . all ( hit ) ) : return  False  return  True  
def  dict with extra specs ( inst type query ) : inst type dict = dict ( inst type query ) extra specs = dict ( [ ( x [ 'key' ] , x [ 'value' ] ) for x in inst type query [ 'extra specs' ] ] ) inst type dict [ 'extra specs' ] = extra specs return inst type dict 
def create Profile  ( color Space  , color Temp  = ( - 1 ) ) : if ( color Space  not in [ 'LAB' , 'XYZ' , 'sRGB' ] ) : raise  Py CMS Error  ( ( ' Color   space  not  supported  for  on-the-fly  profile  creation  (%s)' % color Space  ) ) if ( color Space  == 'LAB' ) : try : color Temp  = float ( color Temp  ) except : raise  Py CMS Error  ( ( ' Color   temperature  must  be  numeric,  "%s"  not  valid' % color Temp  ) ) try : return core . create Profile  ( color Space  , color Temp  ) except (  Type  Error  ,  Value  Error  ) as v : raise  Py CMS Error  ( v ) 
def install sample ( path ) : log . info ( ' Installing   sample  in  the  device:  %s' , path ) try : args = [ '/system/bin/sh' , '/system/bin/pm' , 'install' , path ] output = subprocess . check output ( args ) except subprocess .  Called  Process  Error  as e : log . error ( ' Error   installing  sample:  %r' , e ) return log . info ( ' Installed   sample:  %r' , output ) 
def get saved rules ( conf file =  None  , family = 'ipv4' ) : if (  conf ( ) and ( not conf file ) ) : conf file =  conf ( ) with salt . utils . fopen ( conf file ) as fp  : lines = fp  . readlines ( ) rules = [ ] for line in lines : tmpline = line . strip ( ) if ( not tmpline ) : continue if tmpline . startswith ( '#' ) : continue rules . append ( line ) return rules 
def get pipeline definition ( pipeline id , version = 'latest' , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : client =  get client ( region , key , keyid , profile ) r = { } try : r [ 'result' ] = client . get pipeline definition ( pipeline Id  = pipeline id , version = version ) except ( botocore . exceptions .  Boto  Core  Error  , botocore . exceptions .  Client  Error  ) as e : r [ 'error' ] = str ( e ) return r 
def unload ( module ) : ret = { } fmadm =  check fmadm ( ) cmd = '{cmd}  unload  {module}' . format ( cmd = fmadm , module = module ) res =   salt   [ 'cmd.run all' ] ( cmd ) retcode = res [ 'retcode' ] result = { } if ( retcode != 0 ) : result [ ' Error ' ] = res [ 'stderr' ] else : result =  True  return result 
def main ( argv ) : try : driver = get demo driver ( ) except  Invalid  Creds  Error  : e = sys . exc info ( ) [ 1 ] print ( ' Invalid    Credentials :  ' + e . value ) return 1 try : print '>>   Loading   nodes...' pprint ( driver . list nodes ( ) ) print '>>   Loading   images...  (showing  up  to  10)' pprint ( driver . list images ( ) [ : 10 ] ) print '>>   Loading   sizes...  (showing  up  to  10)' pprint ( driver . list sizes ( ) [ : 10 ] ) except  Exception  : e = sys . exc info ( ) [ 1 ] print ( 'A  fatal  error  occurred:  ' + e ) return 1 
def bbox artist ( artist , renderer , props =  None  , fill =  True  ) : if ( props is  None  ) : props = { } props = props . copy ( ) pad = props . pop ( u'pad' , 4 ) pad = renderer . points to pixels ( pad ) bbox = artist . get window extent ( renderer ) ( l , b , w , h ) = bbox . bounds l -= ( pad / 2.0 ) b -= ( pad / 2.0 ) w += pad h += pad r =  Rectangle  ( xy = ( l , b ) , width = w , height = h , fill = fill ) r . set transform ( transforms .  Identity  Transform  ( ) ) r . set clip on (  False  ) r . update ( props ) r . draw ( renderer ) 
def cdist ( XA , XB , metric = 'euclidean' , p = 2 , V =  None  , VI =  None  , w =  None  ) : XA = np . asarray ( XA , order = 'c' ) XB = np . asarray ( XB , order = 'c' ) XA =  copy array if base present ( XA ) XB =  copy array if base present ( XB ) s = XA . shape sB = XB . shape if ( len ( s ) != 2 ) : raise  Value  Error  ( 'XA  must  be  a  2-dimensional  array.' ) if ( len ( sB ) != 2 ) : raise  Value  Error  ( 'XB  must  be  a  2-dimensional  array.' ) if ( s [ 1 ] != sB [ 1 ] ) : raise  Value  Error  ( 'XA  and  XB  must  have  the  same  number  of  columns  (i.e.  feature  dimension.)' ) mA = s [ 0 ] mB = sB [ 0 ] n = s [ 1 ] dm = np . zeros ( ( mA , mB ) , dtype = np . double ) if ( ( metric in [ 'wminkowski' , 'wmi' , 'wm' , 'wpnorm' , 'test wminkowski' ] ) or ( metric == wminkowski ) ) : w =  validate wminkowski w ( w ) if ( ( metric in [ 'seuclidean' , 'se' , 's' , 'test seuclidean' ] ) or ( metric == seuclidean ) ) : V =  validate seuclidean V ( V , np . vstack ( [ XA , XB ] ) , n ) if ( ( metric in [ 'mahalanobis' , 'mahal' , 'mah' , 'test mahalanobis' ] ) or ( metric == mahalanobis ) ) : VI =  validate mahalanobis VI ( VI , np . vstack ( [ XA , XB ] ) , ( mA + mB ) , n ) if callable ( metric ) : if ( metric in [ braycurtis , canberra , chebyshev , cityblock , correlation , cosine , euclidean , mahalanobis , minkowski , sqeuclidean , seuclidean , wminkowski ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB ) elif ( metric in [ dice , kulsinski , matching , rogerstanimoto , russellrao , sokalmichener , sokalsneath , yule ] ) : XA =  convert to bool ( XA ) XB =  convert to bool ( XB ) elif ( metric in [ hamming , jaccard ] ) : if ( XA . dtype == bool ) : XA =  convert to bool ( XA ) XB =  convert to bool ( XB ) else : XA =  convert to double ( XA ) XB =  convert to double ( XB ) if ( metric == minkowski ) : def dfun ( u , v ) : return minkowski ( u , v , p ) elif ( metric == wminkowski ) : def dfun ( u , v ) : return wminkowski ( u , v , p , w ) elif ( metric == seuclidean ) : def dfun ( u , v ) : return seuclidean ( u , v , V ) elif ( metric == mahalanobis ) : def dfun ( u , v ) : return mahalanobis ( u , v , VI ) else : dfun = metric for i in xrange ( 0 , mA ) : for j in xrange ( 0 , mB ) : dm [ ( i , j ) ] = dfun ( XA [ i , : ] , XB [ j , : ] ) elif isinstance ( metric , string types ) : mstr = metric . lower ( ) try : ( validate , cdist fn ) =  SIMPLE CDIST [ mstr ] XA = validate ( XA ) XB = validate ( XB ) cdist fn ( XA , XB , dm ) return dm except  Key  Error  : pass if ( mstr in [ 'hamming' , 'hamm' , 'ha' , 'h' ] ) : if ( XA . dtype == bool ) : XA =  convert to bool ( XA ) XB =  convert to bool ( XB )  distance wrap . cdist hamming bool wrap ( XA , XB , dm ) else : XA =  convert to double ( XA ) XB =  convert to double ( XB )  distance wrap . cdist hamming wrap ( XA , XB , dm ) elif ( mstr in [ 'jaccard' , 'jacc' , 'ja' , 'j' ] ) : if ( XA . dtype == bool ) : XA =  convert to bool ( XA ) XB =  convert to bool ( XB )  distance wrap . cdist jaccard bool wrap ( XA , XB , dm ) else : XA =  convert to double ( XA ) XB =  convert to double ( XB )  distance wrap . cdist jaccard wrap ( XA , XB , dm ) elif ( mstr in [ 'minkowski' , 'mi' , 'm' , 'pnorm' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB )  distance wrap . cdist minkowski wrap ( XA , XB , dm , p ) elif ( mstr in [ 'wminkowski' , 'wmi' , 'wm' , 'wpnorm' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB ) w =  convert to double ( w )  distance wrap . cdist weighted minkowski wrap ( XA , XB , dm , p , w ) elif ( mstr in [ 'seuclidean' , 'se' , 's' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB )  distance wrap . cdist seuclidean wrap ( XA , XB , V , dm ) elif ( mstr in [ 'cosine' , 'cos' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB )  cosine cdist ( XA , XB , dm ) elif ( mstr in [ 'correlation' , 'co' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB ) XA -= XA . mean ( axis = 1 ) [ : , np . newaxis ] XB -= XB . mean ( axis = 1 ) [ : , np . newaxis ]  cosine cdist ( XA , XB , dm ) elif ( mstr in [ 'mahalanobis' , 'mahal' , 'mah' ] ) : XA =  convert to double ( XA ) XB =  convert to double ( XB )  distance wrap . cdist mahalanobis wrap ( XA , XB , VI , dm ) elif ( metric == 'test euclidean' ) : dm = cdist ( XA , XB , euclidean ) elif ( metric == 'test seuclidean' ) : dm = cdist ( XA , XB , seuclidean , V = V ) elif ( metric == 'test sqeuclidean' ) : dm = cdist ( XA , XB , sqeuclidean ) elif ( metric == 'test braycurtis' ) : dm = cdist ( XA , XB , braycurtis ) elif ( metric == 'test mahalanobis' ) : dm = cdist ( XA , XB , mahalanobis , VI = VI ) elif ( metric == 'test canberra' ) : dm = cdist ( XA , XB , canberra ) elif ( metric == 'test cityblock' ) : dm = cdist ( XA , XB , cityblock ) elif ( metric == 'test cosine' ) : dm = cdist ( XA , XB , cosine ) elif ( metric == 'test minkowski' ) : dm = cdist ( XA , XB , minkowski , p = p ) elif ( metric == 'test wminkowski' ) : dm = cdist ( XA , XB , wminkowski , p = p , w = w ) elif ( metric == 'test correlation' ) : dm = cdist ( XA , XB , correlation ) elif ( metric == 'test hamming' ) : dm = cdist ( XA , XB , hamming ) elif ( metric == 'test jaccard' ) : dm = cdist ( XA , XB , jaccard ) elif ( ( metric == 'test chebyshev' ) or ( metric == 'test chebychev' ) ) : dm = cdist ( XA , XB , chebyshev ) elif ( metric == 'test yule' ) : dm = cdist ( XA , XB , yule ) elif ( metric == 'test matching' ) : dm = cdist ( XA , XB , matching ) elif ( metric == 'test dice' ) : dm = cdist ( XA , XB , dice ) elif ( metric == 'test kulsinski' ) : dm = cdist ( XA , XB , kulsinski ) elif ( metric == 'test rogerstanimoto' ) : dm = cdist ( XA , XB , rogerstanimoto ) elif ( metric == 'test russellrao' ) : dm = cdist ( XA , XB , russellrao ) elif ( metric == 'test sokalsneath' ) : dm = cdist ( XA , XB , sokalsneath ) elif ( metric == 'test sokalmichener' ) : dm = cdist ( XA , XB , sokalmichener ) else : raise  Value  Error  ( ( ' Unknown    Distance    Metric :  %s' % mstr ) ) else : raise  Type  Error  ( '2nd  argument  metric  must  be  a  string  identifier  or  a  function.' ) return dm 
def jabber ( registry , xml parent , data ) : j = XML .  Sub  Element  ( xml parent , 'hudson.plugins.jabber.im.transport. Jabber  Publisher ' ) j . set ( 'plugin' , 'jabber' ) t = XML .  Sub  Element  ( j , 'targets' ) if ( 'group-targets' in data ) : for group in data [ 'group-targets' ] : gcimt = XML .  Sub  Element  ( t , 'hudson.plugins.im. Group  Chat IM Message  Target ' ) gcimt . set ( 'plugin' , 'instant-messaging' ) XML .  Sub  Element  ( gcimt , 'name' ) . text = group XML .  Sub  Element  ( gcimt , 'notification Only ' ) . text = 'false' if ( 'individual-targets' in data ) : for individual in data [ 'individual-targets' ] : dimt = XML .  Sub  Element  ( t , 'hudson.plugins.im. Default IM Message  Target ' ) dimt . set ( 'plugin' , 'instant-messaging' ) XML .  Sub  Element  ( dimt , 'value' ) . text = individual strategy = data . get ( 'strategy' , 'all' ) strategydict = { 'all' : 'ALL' , 'failure' : 'ANY FAILURE' , 'failure-fixed' : 'FAILURE AND FIXED' , 'new-failure-fixed' : 'NEW FAILURE AND FIXED' , 'change' : 'STATECHANGE ONLY' } if ( strategy not in strategydict ) : raise  Jenkins  Jobs  Exception  ( ( ' Strategy   entered  is  not  valid,  must  be  ' + 'one  of:  all,  failure,  failure-fixed,  or  change' ) ) XML .  Sub  Element  ( j , 'strategy' ) . text = strategydict [ strategy ] mappings = [ ( 'notify-on-build-start' , 'notify On  Build  Start ' ,  False  ) , ( 'notify-scm-committers' , 'notify Suspects ' ,  False  ) , ( 'notify-scm-culprits' , 'notify Culprits ' ,  False  ) , ( 'notify-scm-fixers' , 'notify Fixers ' ,  False  ) , ( 'notify-upstream-committers' , 'notify Upstream  Committers ' ,  False  ) ] helpers . convert mapping to xml ( j , data , mappings , fail required =  True  ) message = data . get ( 'message' , 'summary-scm' ) messagedict = { 'summary-scm' : ' Default  Build  To  Chat  Notifier ' , 'summary' : ' Summary  Only  Build  To  Chat  Notifier ' , 'summary-build' : ' Build  Parameters  Build  To  Chat  Notifier ' , 'summary-scm-fail' : ' Print  Failing  Tests  Build  To  Chat  Notifier ' } if ( message not in messagedict ) : raise  Jenkins  Jobs  Exception  ( ' Message   entered  is  not  valid,  must  be  one  of:  summary-scm,  summary,  summary-build  or  summary-scm-fail' ) XML .  Sub  Element  ( j , 'build To  Chat  Notifier ' , { 'class' : ( 'hudson.plugins.im.build notify.' + messagedict [ message ] ) } ) XML .  Sub  Element  ( j , 'matrix Multiplier ' ) . text = 'ONLY CONFIGURATIONS' 
def linux distribution ( distname = '' , version = '' , id = '' , supported dists =  supported dists , full distribution name = 1 ) : try : etc = os . listdir ( '/etc' ) except os . error : return ( distname , version , id ) etc . sort ( ) for file in etc : m =  release filename . match ( file ) if ( m is not  None  ) : (  distname , dummy ) = m . groups ( ) if (  distname in supported dists ) : distname =  distname break else : return  dist try harder ( distname , version , id ) f = open ( ( '/etc/' + file ) , 'r' ) firstline = f . readline ( ) f . close ( ) (  distname ,  version ,  id ) =  parse release file ( firstline ) if (  distname and full distribution name ) : distname =  distname if  version : version =  version if  id : id =  id return ( distname , version , id ) 
def  read buckets cache file ( cache file ) : log . debug ( ' Reading   buckets  cache  file' ) with salt . utils . fopen ( cache file , 'rb' ) as fp  : data = pickle . load ( fp  ) return data 
def test json view normal response ( ) : expected = http .  Http  Response  Forbidden  ( ) def func ( request ) : return expected response = decorators . json view ( func ) ( mock .  Mock  ( ) ) assert ( expected is response ) assert ( response [ ' Content - Type ' ] == 'text/html;  charset=utf-8' ) 
@ hook . on start def on start ( bot ) : global dev key , wunder key dev key = bot . config . get ( 'api keys' , { } ) . get ( 'google dev key' ,  None  ) wunder key = bot . config . get ( 'api keys' , { } ) . get ( 'wunderground' ,  None  ) 
def verify ( cert , signature , data , digest ) : data =  text to bytes and warn ( 'data' , data ) digest obj =  lib . EVP get digestbyname (  byte string ( digest ) ) if ( digest obj ==  ffi . NULL ) : raise  Value  Error  ( ' No   such  digest  method' ) pkey =  lib . X509 get pubkey ( cert .  x509 ) if ( pkey ==  ffi . NULL ) :  raise current error ( ) pkey =  ffi . gc ( pkey ,  lib . EVP PKEY free ) md ctx =  ffi . new ( 'EVP MD CTX*' ) md ctx =  ffi . gc ( md ctx ,  lib . EVP MD CTX cleanup )  lib . EVP  Verify  Init  ( md ctx , digest obj )  lib . EVP  Verify  Update  ( md ctx , data , len ( data ) ) verify result =  lib . EVP  Verify  Final  ( md ctx , signature , len ( signature ) , pkey ) if ( verify result != 1 ) :  raise current error ( ) 
def simplify ( units ) : pass 
def message ( level , text ) : logger = logging . get Logger  ( 'MARKDOWN' ) if logger . handlers : logger . log ( level , text ) if ( level > WARN ) : sys . exit ( 0 ) elif ( level > WARN ) : raise  Markdown  Exception  , text else : warnings . warn ( text ,  Markdown  Warning  ) 
def  apply inverse epochs gen ( epochs , inverse operator , lambda2 , method = 'dSPM' , label =  None  , nave = 1 , pick ori =  None  , prepared =  False  , verbose =  None  ) : method =  check method ( method ) pick ori =  check ori ( pick ori )  check ch names ( inverse operator , epochs . info ) if ( not prepared ) : inv = prepare inverse operator ( inverse operator , nave , lambda2 , method ) else : inv = inverse operator sel =  pick channels inverse operator ( epochs . ch names , inv ) logger . info ( ( ' Picked   %d  channels  from  the  data' % len ( sel ) ) ) logger . info ( ' Computing   inverse...' ) ( K , noise norm , vertno ) =  assemble kernel ( inv , label , method , pick ori ) tstep = ( 1.0 / epochs . info [ 'sfreq' ] ) tmin = epochs . times [ 0 ] is free ori = ( ( inverse operator [ 'source ori' ] == FIFF . FIFFV MNE FREE ORI ) and ( pick ori is  None  ) ) if ( ( not is free ori ) and ( noise norm is not  None  ) ) : K *= noise norm subject =  subject from inverse ( inverse operator ) for ( k , e ) in enumerate ( epochs ) : logger . info ( ( ' Processing   epoch  :  %d' % ( k + 1 ) ) ) if is free ori : sol = np . dot ( K , e [ sel ] ) if is free ori : logger . info ( 'combining  the  current  components...' ) sol = combine xyz ( sol ) if ( noise norm is not  None  ) : sol *= noise norm elif ( len ( sel ) < K . shape [ 0 ] ) : sol = ( K , e [ sel ] ) else : sol = np . dot ( K , e [ sel ] ) stc =  make stc ( sol , vertices = vertno , tmin = tmin , tstep = tstep , subject = subject ) ( yield stc ) logger . info ( '[done]' ) 
def  representing matrices ( basis , G , ring ) : domain = ring . domain u = ( ring . ngens - 1 ) def var ( i ) : return tuple ( ( ( ( [ 0 ] * i ) + [ 1 ] ) + ( [ 0 ] * ( u - i ) ) ) ) def representing matrix ( m ) : M = [ ( [ domain . zero ] * len ( basis ) ) for   in range ( len ( basis ) ) ] for ( i , v ) in enumerate ( basis ) : r = ring . term new ( monomial mul ( m , v ) , domain . one ) . rem ( G ) for ( monom , coeff ) in r . terms ( ) : j = basis . index ( monom ) M [ j ] [ i ] = coeff return M return [ representing matrix ( var ( i ) ) for i in range ( ( u + 1 ) ) ] 
def libvlc media list player play ( p mlp ) : f = (   Cfunctions  . get ( 'libvlc media list player play' ,  None  ) or   Cfunction  ( 'libvlc media list player play' , ( ( 1 , ) , ) ,  None  ,  None  ,  Media  List  Player  ) ) return f ( p mlp ) 
@ contextmanager def reraise errors ( msg = u'{0!r}' , errors =  None  ) : assert ( crypto is not  None  ) errors = ( ( crypto .  Error  , ) if ( errors is  None  ) else errors ) try : ( yield ) except errors as exc : reraise (  Security  Error  ,  Security  Error  ( msg . format ( exc ) ) , sys . exc info ( ) [ 2 ] ) 
def read sheets ( archive ) : xml source = archive . read ( ARC WORKBOOK ) tree = fromstring ( xml source ) for element in safe iterator ( tree , ( '{%s}sheet' % SHEET MAIN NS ) ) : attrib = element . attrib attrib [ 'id' ] = attrib [ ( '{%s}id' % REL NS ) ] del attrib [ ( '{%s}id' % REL NS ) ] if attrib [ 'id' ] : ( yield attrib ) 
def url add parameters ( url , params ) : if params : fragments = list ( urlparse ( url ) ) value = parse qs ( fragments [ 4 ] ) value . update ( params ) fragments [ 4 ] = urlencode ( value ) url = urlunparse ( fragments ) return url 
def xl cell to rowcol ( cell str ) : if ( not cell str ) : return ( 0 , 0 ) match = range parts . match ( cell str ) col str = match . group ( 2 ) row str = match . group ( 4 ) expn = 0 col = 0 for char in reversed ( col str ) : col += ( ( ( ord ( char ) - ord ( 'A' ) ) + 1 ) * ( 26 ** expn ) ) expn += 1 row = ( int ( row str ) - 1 ) col -= 1 return ( row , col ) 
def get Plugins  Directory  Path  ( ) : return archive . get Absolute  Frozen  Folder  Path  (   file   , 'xml plugins' ) 
def decode filename ( filename ) : if isinstance ( filename , unicode ) : return filename else : return filename . decode ( sys . getfilesystemencoding ( ) ) 
def if search enabled ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : ' Wraps   the  decorated  function.' cls = args [ 0 ] if cls . search is enabled ( ) : return f ( * args , ** kwargs ) return wrapper 
def get New  Repository  ( ) : return  Lash  Repository  ( ) 
def test hsl to rgb part 13 ( ) : assert ( hsl to rgb ( 0 , 100 , 0 ) == ( 0 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 10 ) == ( 51 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 20 ) == ( 102 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 30 ) == ( 153 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 40 ) == ( 204 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 50 ) == ( 255 , 0 , 0 ) ) assert ( hsl to rgb ( 0 , 100 , 60 ) == ( 255 , 51 , 51 ) ) assert ( hsl to rgb ( 0 , 100 , 70 ) == ( 255 , 102 , 102 ) ) assert ( hsl to rgb ( 0 , 100 , 80 ) == ( 255 , 153 , 153 ) ) assert ( hsl to rgb ( 0 , 100 , 90 ) == ( 255 , 204 , 204 ) ) assert ( hsl to rgb ( 0 , 100 , 100 ) == ( 255 , 255 , 255 ) ) 
def discard config ( ) : return   proxy   [ 'napalm.call' ] ( 'discard config' , ** { } ) 
def linked data ( prefix , ignore channels =  False  ) : recs = linked data  . get ( prefix ) if ( recs is  None  ) : recs = linked data  [ prefix ] = odict ( ) meta dir = join ( prefix , u'conda-meta' ) if isdir ( meta dir ) : for fn in listdir ( meta dir ) : if fn . endswith ( u'.json' ) : dist name = fn [ : ( - 5 ) ] load linked data ( prefix , dist name , ignore channels = ignore channels ) return recs 
def read plain byte array ( file obj , count ) : return [ file obj . read ( struct . unpack ( '<i' , file obj . read ( 4 ) ) [ 0 ] ) for i in range ( count ) ] 
def get Geometry  Output  ( derivation , xml Element  ) : if ( derivation ==  None  ) : derivation =  Shaft  Derivation  ( ) derivation . set To XML Element  ( xml Element  ) shaft Path  = get Shaft  Path  ( derivation . depth Bottom  , derivation . depth Top  , derivation . radius , derivation . sides ) return lineation . get Geometry  Output  By  Loop  ( lineation .  Side  Loop  ( shaft Path  ) , xml Element  ) 
def  yaml result unicode to utf8 ( data ) : if six . PY3 : return data if isinstance ( data ,  Ordered  Dict  ) : for ( key , elt ) in six . iteritems ( data ) : data [ key ] =  yaml result unicode to utf8 ( elt ) elif isinstance ( data , list ) : for i in range ( len ( data ) ) : data [ i ] =  yaml result unicode to utf8 ( data [ i ] ) elif isinstance ( data , six . text type ) : data = data . encode ( 'utf-8' ) return data 
def install ( pkg , target = ' Local  System ' , store =  False  , allow untrusted =  False  ) : if ( '*.' not in pkg ) : pkg =  quote ( pkg ) target =  quote ( target ) cmd = 'installer  -pkg  {0}  -target  {1}' . format ( pkg , target ) if store : cmd += '  -store' if allow untrusted : cmd += '  -allow Untrusted ' python shell =  False  if ( '*.' in cmd ) : python shell =  True  return   salt   [ 'cmd.run all' ] ( cmd , python shell = python shell ) 
def fmt highlights ( raw value , value , unit ) : if ( unit is  None  ) : return value highlights = highlight string ( raw value , unit ) start search = 0 for highlight in highlights : htext = escape ( force text ( highlight [ 2 ] ) ) find highlight = value . find ( htext , start search ) if ( find highlight >= 0 ) : newpart = HL CHECK . format ( htext ) next part = value [ ( find highlight + len ( htext ) ) : ] value = ( ( value [ : find highlight ] + newpart ) + next part ) start search = ( find highlight + len ( newpart ) ) return value 
def report messages by module stats ( sect , stats ,   ) : if ( len ( stats [ 'by module' ] ) == 1 ) : raise utils .  Empty  Report  ( ) by mod = collections . defaultdict ( dict ) for m type in ( 'fatal' , 'error' , 'warning' , 'refactor' , 'convention' ) : total = stats [ m type ] for module in six . iterkeys ( stats [ 'by module' ] ) : mod total = stats [ 'by module' ] [ module ] [ m type ] if ( total == 0 ) : percent = 0 else : percent = ( float ( ( mod total * 100 ) ) / total ) by mod [ module ] [ m type ] = percent sorted result = [ ] for ( module , mod info ) in six . iteritems ( by mod ) : sorted result . append ( ( mod info [ 'error' ] , mod info [ 'warning' ] , mod info [ 'refactor' ] , mod info [ 'convention' ] , module ) ) sorted result . sort ( ) sorted result . reverse ( ) lines = [ 'module' , 'error' , 'warning' , 'refactor' , 'convention' ] for line in sorted result : if all ( ( ( entry == 0 ) for entry in line [ : ( - 1 ) ] ) ) : continue lines . append ( line [ ( - 1 ) ] ) for val in line [ : ( - 1 ) ] : lines . append ( ( '%.2f' % val ) ) if ( len ( lines ) == 5 ) : raise utils .  Empty  Report  ( ) sect . append ( ureports .  Table  ( children = lines , cols = 5 , rheaders = 1 ) ) 
def render ( yaml data , saltenv = 'base' , sls = '' , argline = '' , ** kws ) : if ( not isinstance ( yaml data , string types ) ) : yaml data = yaml data . read ( ) with warnings . catch warnings ( record =  True  ) as warn list : try : data = load ( yaml data ,  Loader  = get yaml loader ( argline ) ) except  Scanner  Error  as exc : err type =  ERROR MAP . get ( exc . problem , exc . problem ) line num = ( exc . problem mark . line + 1 ) raise  Salt  Render  Error  ( err type , line num , exc . problem mark . buffer ) except (  Parser  Error  ,  Constructor  Error  ) as exc : raise  Salt  Render  Error  ( exc ) if ( len ( warn list ) > 0 ) : for item in warn list : log . warning ( '{warn}  found  in  {sls}  saltenv={env}' . format ( warn = item . message , sls = salt . utils . url . create ( sls ) , env = saltenv ) ) if ( not data ) : data = { } elif ( 'config.get' in   salt   ) : if   salt   [ 'config.get' ] ( 'yaml utf8' ,  False  ) : data =  yaml result unicode to utf8 ( data ) elif   opts   . get ( 'yaml utf8' ) : data =  yaml result unicode to utf8 ( data ) log . debug ( ' Results   of  YAML  rendering:  \n{0}' . format ( data ) ) def  validate data ( data ) : "\n                         Py YAML  will  for  some  reason  allow  improper  YAML  to  be  formed  into\n                        an  unhashable  dict  (that  is,  one  with  a  dict  as  a  key).   This \n                        function  will  recursively  go  through  and  check  the  keys  to  make\n                        sure  they're  not  dicts.\n                        " if isinstance ( data , dict ) : for ( key , value ) in six . iteritems ( data ) : if isinstance ( key , dict ) : raise  Salt  Render  Error  ( ' Invalid   YAML,  possible  double  curly-brace' )  validate data ( value ) elif isinstance ( data , list ) : for item in data :  validate data ( item )  validate data ( data ) return data 
def teardown databases ( old config , verbosity , parallel = 0 , keepdb =  False  ) : for ( connection , old name , destroy ) in old config : if destroy : if ( parallel > 1 ) : for index in range ( parallel ) : connection . creation . destroy test db ( number = ( index + 1 ) , verbosity = verbosity , keepdb = keepdb ) connection . creation . destroy test db ( old name , verbosity , keepdb ) 
def candlestick2 ochl ( ax , opens , closes , highs , lows , width = 4 , colorup = u'k' , colordown = u'r' , alpha = 0.75 ) : candlestick2 ohlc ( ax , opens , highs , lows , closes , width = width , colorup = colorup , colordown = colordown , alpha = alpha ) 
def  Class  Doc  ( path ) : try : from com . sun . tools . javadoc import  Javadoc  Tool  ,  Messager  ,  Modifier  Filter  from com . sun . tools . javac . util import  List  ,  Context  from com . sun . tools . javac . code .  Flags  import PUBLIC except  Import  Error  : raise  Data  Error  ( " Creating   documentation  from   Java   source  files  requires  'tools.jar'  to  be  in  CLASSPATH." ) context =  Context  ( )  Messager  . pre Register  ( context , 'libdoc' ) jdoctool =  Javadoc  Tool  . make0 ( context ) filter =  Modifier  Filter  ( PUBLIC ) java names =  List  . of ( path ) if ( sys . platform [ 4 : 7 ] < '1.8' ) : root = jdoctool . get Root  Doc  Impl  ( 'en' , 'utf-8' , filter , java names ,  List  . nil ( ) ,  False  ,  List  . nil ( ) ,  List  . nil ( ) ,  False  ,  False  ,  True  ) else : root = jdoctool . get Root  Doc  Impl  ( 'en' , 'utf-8' , filter , java names ,  List  . nil ( ) ,  List  . nil ( ) ,  False  ,  List  . nil ( ) ,  List  . nil ( ) ,  False  ,  False  ,  True  ) return root . classes ( ) [ 0 ] 
def  Open  Document  Chart  ( ) : doc =  Open  Document  ( 'application/vnd.oasis.opendocument.chart' ) doc . chart =  Chart  ( ) doc . body . add Element  ( doc . chart ) return doc 
def  get Year  Cent RE ( cent = ( 0 , 3 ) , distance = 3 , now = (  My  Time  . now ( ) ,  My  Time  . alternate Now  ) ) : cent = ( lambda year , f = cent [ 0 ] , t = cent [ 1 ] : str ( year ) [ f : t ] ) exprset = set ( ( cent ( ( now [ 0 ] . year + i ) ) for i in ( ( - 1 ) , distance ) ) ) if ( len ( now ) and now [ 1 ] ) : exprset |= set ( ( cent ( ( now [ 1 ] . year + i ) ) for i in ( ( - 1 ) , distance ) ) ) return ( ( '(?:%s)' % '|' . join ( exprset ) ) if ( len ( exprset ) > 1 ) else '' . join ( exprset ) ) 
def   virtual   ( ) : if CAN USE NAMECHEAP : return 'namecheap domains' return  False  
def rs tanh ( p , x , prec ) : if rs is puiseux ( p , x ) : return rs puiseux ( rs tanh , p , x , prec ) R = p . ring const = 0 if  has constant term ( p , x ) : zm = R . zero monom c = p [ zm ] if ( R . domain is EX ) : c expr = c . as expr ( ) const = tanh ( c expr ) elif isinstance ( c ,  Poly  Element  ) : try : c expr = c . as expr ( ) const = R ( tanh ( c expr ) ) except  Value  Error  : raise  Domain  Error  ( " The   given  series  can't  be  expanded  in  this  domain." ) else : try : const = R ( tanh ( c ) ) except  Value  Error  : raise  Domain  Error  ( " The   given  series  can't  be  expanded  in  this  domain." ) p1 = ( p - c ) t1 = rs tanh ( p1 , x , prec ) t = rs series inversion ( ( 1 + ( const * t1 ) ) , x , prec ) return rs mul ( ( const + t1 ) , t , x , prec ) if ( R . ngens == 1 ) : return  tanh ( p , x , prec ) else : return rs fun ( p ,  tanh , x , prec ) 
def  parse tokens ( tokens ) : index = 0 parsed tokens = [ ] num tokens = len ( tokens ) while ( index < num tokens ) : tok =  Token  ( * tokens [ index ] ) assert ( tok . token type != token . INDENT ) if ( tok . token type == tokenize . NEWLINE ) : break if ( tok . token string in u'([{' ) : ( container , index ) =  parse container ( tokens , index ) if ( not container ) : return  None  parsed tokens . append ( container ) else : parsed tokens . append (  Atom  ( tok ) ) index += 1 return parsed tokens 
def   virtual   ( ) : if HAS LIBS : return   virtualname   return (  False  , ' The   splunk search  execution  module  failed  to  load:  requires  both  the  requests  and  the  splunk-sdk  python  library  to  be  installed.' ) 
def get user ( username =  None  ) : if username : user = models .  User  . objects . get ( login = username ) else : user = models .  User  . current user ( ) return user 
def episode by id ( episode id , session =  None  ) : return session . query (  Episode  ) . filter ( (  Episode  . id == episode id ) ) . one ( ) 
def get temper devices ( ) : from temperusb . temper import  Temper  Handler  return  Temper  Handler  ( ) . get devices ( ) 
def  move to next ( fid , byte = 8 ) : now = fid . tell ( ) if ( ( now % byte ) != 0 ) : now = ( ( now - ( now % byte ) ) + byte ) fid . seek ( now , 0 ) 
def  align ( terms ) : try : terms = list ( com . flatten ( terms ) ) except  Type  Error  : if isinstance ( terms . value , pd . core . generic . ND Frame  ) : typ = type ( terms . value ) return ( typ ,  zip axes from type ( typ , terms . value . axes ) ) return ( np . result type ( terms . type ) ,  None  ) if all ( ( term . isscalar for term in terms ) ) : return (  result type many ( * ( term . value for term in terms ) ) . type ,  None  ) ( typ , axes ) =  align core ( terms ) return ( typ , axes ) 
def cnn pool ( pool dim , convolved features ) : num images = convolved features . shape [ 1 ] num features = convolved features . shape [ 0 ] convolved dim = convolved features . shape [ 2 ] assert ( ( convolved dim % pool dim ) == 0 ) , ' Pooling   dimension  is  not  an  exact  multiple  of  convolved  dimension' pool size = ( convolved dim / pool dim ) pooled features = np . zeros ( shape = ( num features , num images , pool size , pool size ) , dtype = np . float64 ) for i in range ( pool size ) : for j in range ( pool size ) : pool = convolved features [ : , : , ( i * pool dim ) : ( ( i + 1 ) * pool dim ) , ( j * pool dim ) : ( ( j + 1 ) * pool dim ) ] pooled features [ : , : , i , j ] = np . mean ( np . mean ( pool , 2 ) , 2 ) return pooled features 
def lanl graph ( ) : import networkx as nx try : fh = open ( 'lanl routes.edgelist' , 'r' ) except IO Error  : print 'lanl.edges  not  found' raise G = nx .  Graph  ( ) time = { } time [ 0 ] = 0 for line in fh . readlines ( ) : ( head , tail , rtt ) = line . split ( ) G . add edge ( int ( head ) , int ( tail ) ) time [ int ( head ) ] = float ( rtt ) G0 = sorted ( nx . connected component subgraphs ( G ) , key = len , reverse =  True  ) [ 0 ] G0 . rtt = { } for n in G0 : G0 . rtt [ n ] = time [ n ] return G0 
def draw spectral ( G , ** kwargs ) : draw ( G , spectral layout ( G ) , ** kwargs ) 
@ requires segment info def readonly indicator ( pl , segment info , text = u'RO' ) : return ( text if int ( vim getbufoption ( segment info , u'readonly' ) ) else  None  ) 
def cpu times ( ) : ret = cext . per cpu times ( ) return scputimes ( * [ sum ( x ) for x in zip ( * ret ) ] ) 
def fix local scheme ( home dir , symlink =  True  ) : try : import sysconfig except  Import  Error  : pass else : if ( sysconfig .  get default scheme ( ) == 'posix local' ) : local path = os . path . join ( home dir , 'local' ) if ( not os . path . exists ( local path ) ) : os . mkdir ( local path ) for subdir name in os . listdir ( home dir ) : if ( subdir name == 'local' ) : continue copyfile ( os . path . abspath ( os . path . join ( home dir , subdir name ) ) , os . path . join ( local path , subdir name ) , symlink ) 
def  flatsax Parse  ( fl ) : parser = make parser ( ) parser . set Feature  ( handler . feature validation , 0 ) parser . set Feature  ( handler . feature namespaces , 1 ) parser . set Feature  ( handler . feature external ges , 0 ) parser . set Feature  ( handler . feature external pes , 0 ) s =   To  Stan  ( getattr ( fl , 'name' ,  None  ) ) parser . set Content  Handler  ( s ) parser . set Entity  Resolver  ( s ) parser . set Property  ( handler . property lexical handler , s ) parser . parse ( fl ) return s . document 
def get ignore scope ( line , keyword ) : toignore = line [ ( line . rfind ( keyword ) + len ( keyword ) ) : ] if toignore . startswith ( 'all' ) : return [ ] else : return list (  String  Converter  ( toignore , list delimiters = ',  ' ) ) 
@ receiver (  Signal  Handler  . course published ) def trigger update xblocks cache task ( sender , course key , ** kwargs ) : tasks = import module ( 'openedx.core.djangoapps.bookmarks.tasks' ) tasks . update xblocks cache . apply async ( [ unicode ( course key ) ] , countdown = 0 ) 
def  reverse cmap spec ( spec ) : if ( u'listed' in spec ) : return { u'listed' : spec [ u'listed' ] [ : : ( - 1 ) ] } if ( u'red' in spec ) : return revcmap ( spec ) else : revspec = list ( reversed ( spec ) ) if ( len ( revspec [ 0 ] ) == 2 ) : revspec = [ ( ( 1.0 - a ) , b ) for ( a , b ) in revspec ] return revspec 
def check partitioners ( partitioners , keys ) : if ( partitioners is  None  ) : return { } keys = set ( keys ) if ( not ( set ( partitioners ) <= keys ) ) : extra keys = ( set ( partitioners ) - keys ) raise  Key  Error  ( ' Invalid   partitioner  keys  {},  partitioners  can  only  be  provided  for  {}' . format ( ',  ' . join ( ( "'{}'" . format ( key ) for key in extra keys ) ) , ',  ' . join ( ( "'{}'" . format ( key ) for key in keys ) ) ) ) def check nested callables ( dictionary ) : for ( key , entry ) in dictionary . iteritems ( ) : if isinstance ( entry , dict ) : check nested callables ( entry ) elif ( not callable ( entry ) ) : raise  Type  Error  ( " Partitioner   for  '{}'  is  not  a  callable  function  or  dictionary" . format ( key ) ) check nested callables ( partitioners ) return partitioners 
def get New  Derivation  ( element Node  ) : return  Grid  Derivation  ( element Node  ) 
def get Crafted  Text  From  Text  ( gcode Text  , repository =  None  ) : if gcodec . is Procedure  Done  Or  File  Is  Empty  ( gcode Text  , 'temperature' ) : return gcode Text  if ( repository ==  None  ) : repository = settings . get Read  Repository  (  Temperature  Repository  ( ) ) if ( not repository . activate Temperature  . value ) : return gcode Text  return  Temperature  Skein  ( ) . get Crafted  Gcode  ( gcode Text  , repository ) 
def inv recv rheader ( r ) : if ( ( r . representation == 'html' ) and ( r . name == 'recv' ) ) : record = r . record if record : T = current . T s3db = current . s3db tabs = [ ( T ( ' Edit    Details ' ) ,  None  ) , ( T ( ' Items ' ) , 'track item' ) ] rheader tabs = s3 rheader tabs ( r , tabs ) table = r . table tracktable = s3db . inv track item recv id = record . id site id = record . site id stable = s3db . org site site = current . db ( ( stable . site id == site id ) ) . select ( stable . organisation id , limitby = ( 0 , 1 ) ) . first ( ) try : org id = site . organisation id except : org id =  None  logo = s3db . org organisation logo ( org id ) r Data  = TABLE ( TR ( TD ( T ( current . deployment settings . get inv recv form name ( ) ) ,  colspan = 2 ,  class = 'pdf title' ) , TD ( logo ,  colspan = 2 ) ) , TR ( TH ( ( '%s:  ' % table . recv ref . label ) ) , TD ( table . recv ref . represent ( record . recv ref ) ) ) , TR ( TH ( ( '%s:  ' % table . status . label ) ) , table . status . represent ( record . status ) ) , TR ( TH ( ( '%s:  ' % table . eta . label ) ) , table . eta . represent ( record . eta ) , TH ( ( '%s:  ' % table . date . label ) ) , table . date . represent ( record . date ) ) , TR ( TH ( ( '%s:  ' % table . from site id . label ) ) , table . from site id . represent ( record . from site id ) , TH ( ( '%s:  ' % table . site id . label ) ) , table . site id . represent ( record . site id ) ) , TR ( TH ( ( '%s:  ' % table . sender id . label ) ) , s3 fullname ( record . sender id ) , TH ( ( '%s:  ' % table . recipient id . label ) ) , s3 fullname ( record . recipient id ) ) , TR ( TH ( ( '%s:  ' % table . send ref . label ) ) , table . send ref . represent ( record . send ref ) , TH ( ( '%s:  ' % table . recv ref . label ) ) , table . recv ref . represent ( record . recv ref ) ) , TR ( TH ( ( '%s:  ' % table . comments . label ) ) , TD ( ( record . comments or '' ) ,  colspan = 3 ) ) ) rfooter = TAG [ '' ] ( ) action = DIV ( ) query = ( ( tracktable . recv id == recv id ) & ( tracktable . deleted ==  False  ) ) cnt = current . db ( query ) . count ( ) if ( ( record . status == SHIP STATUS SENT ) or ( record . status == SHIP STATUS IN PROCESS ) ) : if current . auth . s3 has permission ( 'update' , 'inv recv' , record id = record . id ) : if ( cnt > 0 ) : action . append ( A ( T ( ' Receive    Shipment ' ) ,  href = URL ( c = 'inv' , f = 'recv process' , args = [ record . id ] ) ,  id = 'recv process' ,  class = 'action-btn' ) ) recv btn confirm = SCRIPT ( ( "S3.confirm Click ('#recv process',  '%s')" % T ( ' Do   you  want  to  receive  this  shipment?' ) ) ) rfooter . append ( recv btn confirm ) else : msg = T ( ' You   need  to  check  all  item  quantities  and  allocate  to  bins  before  you  can  receive  the  shipment' ) rfooter . append ( SPAN ( msg ) ) msg = '' if ( cnt == 1 ) : msg = T ( ' This   shipment  contains  one  line  item' ) elif ( cnt > 1 ) : msg = ( T ( ' This   shipment  contains  %s  items' ) % cnt ) r Data  . append ( TR ( TH ( action ,  colspan = 2 ) , TD ( msg ) ) ) current . response . s3 . rfooter = rfooter rheader = DIV ( r Data  , rheader tabs ) return rheader return  None  
def build model ( dataset , frcn rois per img , train pre nms N = 12000 , train post nms N = 2000 , test pre nms N = 6000 , test post nms N = 300 , inference =  False  ) : num classes = dataset . num classes b1 =  Branch  Node  ( name = 'conv branch' ) b2 =  Branch  Node  ( name = 'rpn branch' ) b3 =  Branch  Node  ( name = 'roi branch' ) VGG = util . add vgg layers ( ) rpn init = dict ( strides = 1 , init =  Gaussian  ( scale = 0.01 ) , bias =  Constant  ( 0 ) ) RPN 3x3 =  Conv  ( ( 3 , 3 , 512 ) , activation =  Rectlin  ( ) , padding = 1 , ** rpn init ) RPN 1x1 obj =  Conv  ( ( 1 , 1 , 18 ) , activation =  Pixelwise  Softmax  ( c = 2 ) , padding = 0 , ** rpn init ) RPN 1x1 bbox =  Conv  ( ( 1 , 1 , 36 ) , activation =  Identity  ( ) , padding = 0 , ** rpn init ) if ( not inference ) : pre nms N = train pre nms N post nms N = train post nms N else : pre nms N = test pre nms N post nms N = test post nms N proposal Layer  =  Proposal  Layer  ( [ RPN 1x1 obj , RPN 1x1 bbox ] , dataset , pre nms N = pre nms N , post nms N = post nms N , num rois = frcn rois per img , inference = inference ) ROI = [ proposal Layer  ,  Roi  Pooling  ( HW = ( 7 , 7 ) ) ,  Affine  ( nout = 4096 , init =  Gaussian  ( scale = 0.005 ) , bias =  Constant  ( 0.1 ) , activation =  Rectlin  ( ) ) ,  Dropout  ( keep = 0.5 ) ,  Affine  ( nout = 4096 , init =  Gaussian  ( scale = 0.005 ) , bias =  Constant  ( 0.1 ) , activation =  Rectlin  ( ) ) ,  Dropout  ( keep = 0.5 ) ] ROI category =  Affine  ( nout = num classes , init =  Gaussian  ( scale = 0.01 ) , bias =  Constant  ( 0 ) , activation =  Softmax  ( ) ) ROI bbox =  Affine  ( nout = ( 4 * num classes ) , init =  Gaussian  ( scale = 0.001 ) , bias =  Constant  ( 0 ) , activation =  Identity  ( ) ) frcn tree =  Tree  ( [ ( ROI + [ b3 , ROI category ] ) , [ b3 , ROI bbox ] ] ) model =  Model  ( layers =  Tree  ( [ ( VGG + [ b1 , RPN 3x3 , b2 , RPN 1x1 obj ] ) , [ b2 , RPN 1x1 bbox ] , ( [ b1 ] + [ frcn tree ] ) ] ) ) if inference : return ( model , proposal Layer  ) else : return model 
def luhn Check  ( value ) : arr = [ ] for c in value : if c . isdigit ( ) : arr . append ( int ( c ) ) arr . reverse ( ) for idx in [ i for i in range ( len ( arr ) ) if ( i % 2 ) ] : d = ( arr [ idx ] * 2 ) if ( d > 9 ) : d = ( ( d / 10 ) + ( d % 10 ) ) arr [ idx ] = d sm = sum ( arr ) return ( not ( sm % 10 ) ) 
@ public def interpolate ( data , x ) : n = len ( data ) if isinstance ( data , dict ) : ( X , Y ) = list ( zip ( * data . items ( ) ) ) elif isinstance ( data [ 0 ] , tuple ) : ( X , Y ) = list ( zip ( * data ) ) else : X = list ( range ( 1 , ( n + 1 ) ) ) Y = list ( data ) poly = interpolating poly ( n , x , X , Y ) return poly . expand ( ) 
@ xthreaded @ public def apart ( f , x =  None  , full =  False  , ** options ) : allowed flags ( options , [ ] ) f = sympify ( f ) if f . is  Atom  : return f else : ( P , Q ) = f . as numer denom ( )  options = options . copy ( ) options = set defaults ( options , extension =  True  ) try : ( ( P , Q ) , opt ) = parallel poly from expr ( ( P , Q ) , x , ** options ) except  Polynomial  Error  as msg : if f . is commutative : raise  Polynomial  Error  ( msg ) if f . is  Mul  : ( c , nc ) = f . args cnc ( split 1 =  False  ) nc = f . func ( * nc ) if c : c = apart ( f . func .  from args ( c ) , x = x , full = full , **  options ) return ( c * nc ) else : return nc elif f . is  Add  : c = [ ] nc = [ ] for i in f . args : if i . is commutative : c . append ( i ) else : try : nc . append ( apart ( i , x = x , full = full , **  options ) ) except  Not  Implemented  Error  : nc . append ( i ) return ( apart ( f . func ( * c ) , x = x , full = full , **  options ) + f . func ( * nc ) ) else : reps = [ ] pot = preorder traversal ( f ) next ( pot ) for e in pot : try : reps . append ( ( e , apart ( e , x = x , full = full , **  options ) ) ) pot . skip ( ) except  Not  Implemented  Error  : pass return f . xreplace ( dict ( reps ) ) if P . is multivariate : fc = f . cancel ( ) if ( fc != f ) : return apart ( fc , x = x , full = full , **  options ) raise  Not  Implemented  Error  ( 'multivariate  partial  fraction  decomposition' ) ( common , P , Q ) = P . cancel ( Q ) ( poly , P ) = P . div ( Q , auto =  True  ) ( P , Q ) = P . rat clear denoms ( Q ) if ( Q . degree ( ) <= 1 ) : partial = ( P / Q ) elif ( not full ) : partial = apart undetermined coeffs ( P , Q ) else : partial = apart full decomposition ( P , Q ) terms = S .  Zero  for term in  Add  . make args ( partial ) : if term . has (  Root  Sum  ) : terms += term else : terms += factor ( term ) return ( common * ( poly . as expr ( ) + terms ) ) 
def  tanh ( p , x , prec ) : R = p . ring p1 = R ( 0 ) for precx in  giant steps ( prec ) : tmp = ( p - rs atanh ( p1 , x , precx ) ) tmp = rs mul ( tmp , ( 1 - rs square ( p1 , x , prec ) ) , x , precx ) p1 += tmp return p1 
def normalize dotted fields ( document ) : if isinstance ( document , list ) : prev = document for i in prev : normalize dotted fields ( i ) elif isinstance ( document , dict ) : for field in list ( document ) : if ( '.' in field ) : parts = field . split ( '.' ) prev = document for part in parts [ : ( - 1 ) ] : if ( part not in prev ) : prev [ part ] = { } prev = prev [ part ] if isinstance ( document [ field ] , ( dict , list ) ) : normalize dotted fields ( document [ field ] ) prev [ parts [ ( - 1 ) ] ] = document [ field ] document . pop ( field ) elif isinstance ( document [ field ] , ( dict , list ) ) : normalize dotted fields ( document [ field ] ) 
def  tag added ( sender , question id , tag name , ** kwargs ) : if ( tag name == config . ESCALATE TAG NAME ) : escalate question . delay ( question id ) 
def process urlencoded ( entity ) : qs = entity . fp . read ( ) for charset in entity . attempt charsets : try : params = { } for aparam in qs . split ( ntob ( '&' ) ) : for pair in aparam . split ( ntob ( ';' ) ) : if ( not pair ) : continue atoms = pair . split ( ntob ( '=' ) , 1 ) if ( len ( atoms ) == 1 ) : atoms . append ( ntob ( '' ) ) key = unquote plus ( atoms [ 0 ] ) . decode ( charset ) value = unquote plus ( atoms [ 1 ] ) . decode ( charset ) if ( key in params ) : if ( not isinstance ( params [ key ] , list ) ) : params [ key ] = [ params [ key ] ] params [ key ] . append ( value ) else : params [ key ] = value except  Unicode  Decode  Error  : pass else : entity . charset = charset break else : raise cherrypy . HTTP Error  ( 400 , ( ' The   request  entity  could  not  be  decoded.   The   following  charsets  were  attempted:  %s' % repr ( entity . attempt charsets ) ) ) for ( key , value ) in params . items ( ) : if ( key in entity . params ) : if ( not isinstance ( entity . params [ key ] , list ) ) : entity . params [ key ] = [ entity . params [ key ] ] entity . params [ key ] . append ( value ) else : entity . params [ key ] = value 
def  parse char metrics ( fh ) : ascii d = { } name d = { } while 1 : line = fh . readline ( ) if ( not line ) : break line = line . rstrip ( ) if line . startswith ( ' End  Char  Metrics ' ) : return ( ascii d , name d ) vals = line . split ( ';' ) [ : 4 ] if ( len ( vals ) != 4 ) : raise  Runtime  Error  ( ( ' Bad   char  metrics  line:  %s' % line ) ) num =  to int ( vals [ 0 ] . split ( ) [ 1 ] ) wx =  to float ( vals [ 1 ] . split ( ) [ 1 ] ) name = vals [ 2 ] . split ( ) [ 1 ] bbox =  to list of ints ( vals [ 3 ] [ 2 : ] ) if ( name == ' Euro ' ) : num = 128 if ( num != ( - 1 ) ) : ascii d [ num ] = ( wx , name , bbox ) name d [ name ] = ( wx , bbox ) raise  Runtime  Error  ( ' Bad   parse' ) 
def send event ( event , users ) : queue json publish ( 'notify tornado' , dict ( event = event , users = users ) , send notification http ) 
def assoc laguerre ( x , n , k = 0.0 ) : return orthogonal . eval genlaguerre ( n , k , x ) 
def handle File  Collision  ( file Name  , file Collision  Method  ) : if ( file Collision  Method  == 'overwrite' ) : logging . warning ( ( ' Data   file,  %s,  will  be  overwritten' % file Name  ) ) elif ( file Collision  Method  == 'fail' ) : msg = ' Data   file  %s  already  exists.   Set   argument  file Collision  Method   to  overwrite.' raise IO Error  ( ( msg % file Name  ) ) elif ( file Collision  Method  == 'rename' ) : ( root Name  , extension ) = os . path . splitext ( file Name  ) matching Files  = glob . glob ( ( '%s*%s' % ( root Name  , extension ) ) ) if ( not matching Files  ) : file Name  = ( '%s%s' % ( root Name  , extension ) ) else : file Name  = ( '%s %d%s' % ( root Name  , len ( matching Files  ) , extension ) ) if os . path . exists ( file Name  ) : msg = ' New   file Name   %s  has  already  been  taken.   Something   is  wrong  with  the  append  counter.' raise IO Error  ( ( msg % file Name  ) ) else : msg = ' Argument   file Collision  Method   was  invalid:  %s' raise  Value  Error  ( ( msg % str ( file Collision  Method  ) ) ) return file Name  
@ click . command ( name = 'delete snapshots' ) @ click . option ( '--repository' , type = str , required =  True  , help = ' Snapshot   repository  name' ) @ click . option ( '--retry count' , type = int , help = ' Number   of  times  to  retry  (max  3)' ) @ click . option ( '--retry interval' , type = int , help = ' Time   in  seconds  between  retries' ) @ click . option ( '--ignore empty list' , is flag =  True  , help = ' Do   not  raise  exception  if  there  are  no  actionable  snapshots' ) @ click . option ( '--filter list' , callback = validate filter json , help = 'JSON  string  representing  an  array  of  filters.' , required =  True  ) @ click . pass context def delete snapshots singleton ( ctx , repository , retry count , retry interval , ignore empty list , filter list ) : action = 'delete snapshots' action class = CLASS MAP [ action ] c args = ctx . obj [ 'config' ] [ 'client' ] client = get client ( ** c args ) logger = logging . get Logger  (   name   ) raw options = { 'repository' : repository , 'retry count' : retry count , 'retry interval' : retry interval } logger . debug ( ' Validating   provided  options:  {0}' . format ( raw options ) ) mykwargs = option schema check ( action , raw options ) del mykwargs [ 'repository' ] logger . debug ( ' Validating   provided  filters:  {0}' . format ( filter list ) ) clean filters = { 'filters' : filter schema check ( action , filter list ) } slo =  Snapshot  List  ( client , repository = repository )  do filters ( slo , clean filters , ignore empty list ) action obj = action class ( slo , ** mykwargs )  actionator ( action , action obj , dry run = ctx . parent . params [ 'dry run' ] ) 
def get url ( endpoint or url ) : try : return url for ( endpoint or url ) except : return endpoint or url 
def test bool symbol ( ) : assert (  And  ( A ,  True  ) == A ) assert (  And  ( A ,  True  ,  True  ) == A ) assert (  And  ( A ,  False  ) is false ) assert (  And  ( A ,  True  ,  False  ) is false ) assert (  Or  ( A ,  True  ) is true ) assert (  Or  ( A ,  False  ) == A ) 
def   ip addr ( addr , address family = socket . AF INET ) : mask max = '32' if ( address family == socket . AF INET6 ) : mask max = '128' try : if ( '/' not in addr ) : addr = '{addr}/{mask max}' . format ( addr = addr , mask max = mask max ) except  Type  Error  : return  False  ( ip , mask ) = addr . rsplit ( '/' , 1 ) try : socket . inet pton ( address family , ip ) except socket . error : return  False  try : mask = int ( mask ) except  Value  Error  : return  False  else : if ( not ( 1 <= mask <= int ( mask max ) ) ) : return  False  return  True  
def region invalidate ( namespace , region , * args ) : if callable ( namespace ) : if ( not region ) : region = namespace .  arg region namespace = namespace .  arg namespace if ( not region ) : raise  Beaker  Exception  ( ' Region   or  callable  function  namespace  is  required' ) else : region = cache regions [ region ] cache =  Cache  .  get cache ( namespace , region ) cache key = '  ' . join ( ( str ( x ) for x in args ) ) cache . remove value ( cache key ) 
def firebase put ( path , value =  None  ) : ( response , content ) =  get http ( ) . request ( path , method = 'PUT' , body = value ) return json . loads ( content ) 
def write Output  ( file Name  = '' ) : start Time  = time . time ( ) print ( ( ' File   ' + archive . get Summarized  File  Name  ( file Name  ) ) + '  is  being  cleaved.' ) repository =  Cleave  Repository  ( ) settings . get Read  Repository  ( repository ) cleave Gcode  = get Crafted  Text  ( file Name  , '' , repository ) if ( cleave Gcode  == '' ) : return suffix File  Name  = ( file Name  [ : file Name  . rfind ( '.' ) ] + ' cleave.svg' ) suffix Directory  Name  = os . path . dirname ( suffix File  Name  ) suffix Replaced  Base  Name  = os . path . basename ( suffix File  Name  ) . replace ( '  ' , ' ' ) suffix File  Name  = os . path . join ( suffix Directory  Name  , suffix Replaced  Base  Name  ) archive . write File  Text  ( suffix File  Name  , cleave Gcode  ) print ( ' The   cleaved  file  is  saved  as  ' + archive . get Summarized  File  Name  ( suffix File  Name  ) ) print ( ' It   took  %s  to  cleave  the  file.' % euclidean . get Duration  String  ( ( time . time ( ) - start Time  ) ) ) settings . openSVG Page  ( suffix File  Name  , repository . svg Viewer  . value ) 
def  get catalogs ( service type , catalog db , ** kwargs ) : if ( catalog db is  None  ) : catalog db = get remote catalog db ( service type , ** kwargs ) catalogs = catalog db . get catalogs ( ) elif isinstance ( catalog db , VOS Database  ) : catalogs = catalog db . get catalogs ( ) elif isinstance ( catalog db , ( VOS Catalog  , six . string types ) ) : catalogs = [ (  None  , catalog db ) ] elif isinstance ( catalog db , list ) : for x in catalog db : assert ( isinstance ( x , ( VOS Catalog  , six . string types ) ) and ( not isinstance ( x , VOS Database  ) ) ) catalogs = [ (  None  , x ) for x in catalog db ] else : raise VOS Error  ( u'catalog db  must  be  a  catalog  database,  a  list  of  catalogs,  or  a  catalog' ) return catalogs 
def slugify ( text , entities =  True  , decimal =  True  , hexadecimal =  True  , max length = 0 , word boundary =  False  , separator = '-' ) : if ( not isinstance ( text , types .  Unicode  Type  ) ) : text = unicode ( text , 'utf-8' , 'ignore' ) text = unidecode ( text ) if ( not isinstance ( text , types .  Unicode  Type  ) ) : text = unicode ( text , 'utf-8' , 'ignore' ) if entities : text = CHAR ENTITY REXP . sub ( ( lambda m : unichr ( name2codepoint [ m . group ( 1 ) ] ) ) , text ) if decimal : try : text = DECIMAL REXP . sub ( ( lambda m : unichr ( int ( m . group ( 1 ) ) ) ) , text ) except : pass if hexadecimal : try : text = HEX REXP . sub ( ( lambda m : unichr ( int ( m . group ( 1 ) , 16 ) ) ) , text ) except : pass text = unicodedata . normalize ( 'NFKD' , text ) if ( sys . version info < ( 3 , ) ) : text = text . encode ( 'ascii' , 'ignore' ) text = REPLACE1 REXP . sub ( '' , text . lower ( ) ) text = REPLACE2 REXP . sub ( '-' , text . lower ( ) ) text = REMOVE REXP . sub ( '-' , text ) . strip ( '-' ) if ( max length > 0 ) : text = smart truncate ( text , max length , word boundary , '-' ) if ( separator != '-' ) : text = text . replace ( '-' , separator ) return text 
def create app ( config =  None  , app name =  None  ) : if ( app name is  None  ) : app name =  Default  Config  . PROJECT app =  Flask  ( app name , instance path = INSTANCE FOLDER PATH , instance relative config =  True  ) configure app ( app , config ) configure hook ( app ) configure blueprints ( app ) configure extensions ( app ) configure logging ( app ) configure template filters ( app ) configure error handlers ( app ) configure cli ( app ) return app 
def  attempt YYYYMMDD ( arg , errors ) : def calc ( carg ) : carg = carg . astype ( object ) parsed = lib . try parse year month day ( ( carg / 10000 ) , ( ( carg / 100 ) % 100 ) , ( carg % 100 ) ) return tslib . array to datetime ( parsed , errors = errors ) def calc with mask ( carg , mask ) : result = np . empty ( carg . shape , dtype = 'M8[ns]' ) iresult = result . view ( 'i8' ) iresult [ ( ~ mask ) ] = tslib . i Na T result [ mask ] = calc ( carg [ mask ] . astype ( np . float64 ) . astype ( np . int64 ) ) . astype ( 'M8[ns]' ) return result try : return calc ( arg . astype ( np . int64 ) ) except : pass try : carg = arg . astype ( np . float64 ) return calc with mask ( carg , notnull ( carg ) ) except : pass try : mask = ( ~ lib . ismember ( arg , tslib .  nat strings ) ) return calc with mask ( arg , mask ) except : pass return  None  
def get board ( state ) : planes = np . zeros ( ( 3 , state . size , state . size ) ) planes [ 0 , : , : ] = ( state . board == state . current player ) planes [ 1 , : , : ] = ( state . board == ( - state . current player ) ) planes [ 2 , : , : ] = ( state . board == go . EMPTY ) return planes 
def get translated storefile ( store , pootle path =  None  ) : storeclass = store . syncer . file class filestore = store . syncer . convert ( storeclass ) for unit in filestore . units : if ( not unit . istranslated ( ) ) : unit . target = ( ' Translation   of  %s' % unit . source ) path = ( pootle path if ( pootle path is not  None  ) else store . pootle path ) filestore . updateheader ( add =  True  , X  Pootle   Path  = path ) filestore . updateheader ( add =  True  , X  Pootle   Revision  = store . get max unit revision ( ) ) return filestore 
def make Mimi  ( upid ) : str Seed  = 'g Gddg  Pfeaf  gzyr' prehash = ( ( upid + ' ' ) + str Seed  ) return md5 ( prehash . encode ( 'utf-8' ) ) . hexdigest ( ) 
def is multigraphical ( sequence ) : deg sequence = list ( sequence ) if ( not nx . utils . is list of ints ( deg sequence ) ) : return  False  ( dsum , dmax ) = ( 0 , 0 ) for d in deg sequence : if ( d < 0 ) : return  False  ( dsum , dmax ) = ( ( dsum + d ) , max ( dmax , d ) ) if ( ( dsum % 2 ) or ( dsum < ( 2 * dmax ) ) ) : return  False  return  True  
def add ssh public keys ( name , filenames ) : from fabtools . require . files import directory as  require directory , file as  require file ssh dir = posixpath . join ( home directory ( name ) , '.ssh' )  require directory ( ssh dir , mode = '700' , owner = name , use sudo =  True  ) authorized keys filename = posixpath . join ( ssh dir , 'authorized keys' )  require file ( authorized keys filename , mode = '600' , owner = name , use sudo =  True  ) for filename in filenames : with open ( filename ) as public key file : public keys = public key file . read ( ) . strip ( ) . split ( '\n' ) for public key in public keys : if ( public key not in authorized keys ( name ) ) : sudo ( ( 'echo  %s  >>%s' % ( quote ( public key ) , quote ( authorized keys filename ) ) ) ) 
def discretize bilinear 2D ( model , x range , y range ) : x = np . arange ( ( x range [ 0 ] - 0.5 ) , ( x range [ 1 ] + 0.5 ) ) y = np . arange ( ( y range [ 0 ] - 0.5 ) , ( y range [ 1 ] + 0.5 ) ) ( x , y ) = np . meshgrid ( x , y ) values intermediate grid = model ( x , y ) values = ( 0.5 * ( values intermediate grid [ 1 : , : ] + values intermediate grid [ : ( - 1 ) , : ] ) ) values = ( 0.5 * ( values [ : , 1 : ] + values [ : , : ( - 1 ) ] ) ) return values 
def  git Config  ( path ) : run Command  ( [ 'git' , 'config' , '--file' , path . child ( '.git' ) . child ( 'config' ) . path , 'user.name' , '"someone"' ] ) run Command  ( [ 'git' , 'config' , '--file' , path . child ( '.git' ) . child ( 'config' ) . path , 'user.email' , '"someone@someplace.com"' ] ) 
def exc info to str ( exc info ) : return '' . join ( traceback . format exception ( * exc info ) ) 
def binop ( x , op , y , lineno =  None  , col =  None  ) : lineno = ( x . lineno if ( lineno is  None  ) else lineno ) col = ( x . col offset if ( col is  None  ) else col ) return ast .  Bin  Op  ( left = x , op = op , right = y , lineno = lineno , col offset = col ) 
def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : ( i + MAXBINSIZE ) ] pieces . append ( binascii . b2a base64 ( chunk ) ) return '' . join ( pieces ) 
def get Branch  Matrix  ( element Node  ) : branch Matrix  =  Matrix  ( ) matrix Child  Element  = element Node  . get First  Child  By  Local  Name  ( 'matrix' ) if ( matrix Child  Element  !=  None  ) : branch Matrix  = branch Matrix  . get From  Element  Node  ( matrix Child  Element  , '' ) branch Matrix  = branch Matrix  . get From  Element  Node  ( element Node  , 'matrix.' ) if ( element Node  . xml Object  ==  None  ) : return branch Matrix  element Node  Matrix  = element Node  . xml Object  . get Matrix 4X4 ( ) if ( element Node  Matrix  ==  None  ) : return branch Matrix  return element Node  Matrix  . get Other  Times  Self  ( branch Matrix  . tetragrid ) 
def userkey required ( ) : def  decorator ( view ) : def wrapped view ( request , * args , ** kwargs ) : try : uk =  User  Key  . objects . get ( user = request . user ) except  User  Key  .  Does  Not  Exist  : messages . warning ( request , u" This   operation  requires  an  active  user  key,  but  you  don't  have  one." ) return redirect ( 'users:userkey' ) if ( not uk . is active ( ) ) : messages . warning ( request , u' This   operation  is  not  available.   Your   user  key  has  not  been  activated.' ) return redirect ( 'users:userkey' ) return view ( request , * args , ** kwargs ) return wrapped view return  decorator 
def create symlink ( src , dest ) : os . symlink ( src , dest ) atexit . register ( delete symlink , dest ) 
def isnan ( a ) : return reshape ( array ( [  isnan ( i ) for i in ravel ( a ) ] , 'b' ) , shape ( a ) ) 
def  Get  Request  Context  ( ) : global  thread Local  Context  return  thread Local  Context  .   dict   . setdefault ( 'req Ctx ' ,  String  Dict  ( ) ) 
def perform ( level , box , options ) : optmap = { ' Tree    Height ' : 'CENTERHEIGHT' } def set Option  ( opt ) : OPT = optmap . get ( opt , opt . replace ( '  ' , '' ) . upper ( ) ) if ( OPT in dir (  Forester  ) ) : val = options [ opt ] if isinstance ( val , str ) : val = val . replace ( '  ' , '' ) . lower ( ) setattr (  Forester  , OPT , val ) for option in options : set Option  ( option )  Forester  . EDGEHEIGHT =  Forester  . CENTERHEIGHT wood = options [ ' Wood    Material ' ] leaf = options [ ' Leaf    Material ' ] grass = options [ ' Plant    On ' ]  Forester  . WOODINFO = { 'B' : wood . ID , 'D' : wood . block Data  }  Forester  . LEAFINFO = { 'B' : leaf . ID , 'D' : leaf . block Data  }  Forester  . PLANTON = [ grass . ID ] x center = int ( ( box . minx + ( box . width / 2 ) ) ) z center = int ( ( box . minz + ( box . length / 2 ) ) ) edge padding = int ( (  Forester  . EDGEHEIGHT * 0.618 ) ) max dim = min ( box . width , box . length ) planting radius = ( ( max dim / 2 ) - edge padding ) if ( planting radius <= 1 ) : planting radius = 1  Forester  . TREECOUNT = 1 print " Box   isn't  wide  and/or  long  enough.   Only   planting  one  tree."  Forester  . X = x center  Forester  . Z = z center  Forester  . RADIUS = planting radius print ( ' Plant   radius  =  ' + str ( planting radius ) )  Forester  . LIGHTINGFIX =  False   Forester  . MAXTRIES = 5000  Forester  . VERBOSE =  True  mcmap = mc Interface  . MC Level  Adapter  ( level , box )  Forester  . main ( mcmap ) level . mark Dirty  Box  ( box ) 
def get dhcp leases ( context , network ref ) : hosts = [ ] host =  None  if network ref [ 'multi host' ] : host = CONF . host for fixedip in objects .  Fixed IP List  . get by network ( context , network ref , host = host ) : if fixedip . leased : hosts . append (  host lease ( fixedip ) ) return '\n' . join ( hosts ) 
def write worksheet cols ( doc , worksheet ) : if worksheet . column dimensions : start tag ( doc , 'cols' ) for ( column string , columndimension ) in worksheet . column dimensions . items ( ) : col index = column index from string ( column string ) col def = { } col def [ 'collapsed' ] = str ( columndimension . style index ) col def [ 'min' ] = str ( col index ) col def [ 'max' ] = str ( col index ) if ( columndimension . width != worksheet . default column dimension . width ) : col def [ 'custom Width ' ] = 'true' if ( not columndimension . visible ) : col def [ 'hidden' ] = 'true' if ( columndimension . outline level > 0 ) : col def [ 'outline Level ' ] = str ( columndimension . outline level ) if columndimension . collapsed : col def [ 'collapsed' ] = 'true' if columndimension . auto size : col def [ 'best Fit ' ] = 'true' if ( columndimension . width > 0 ) : col def [ 'width' ] = str ( columndimension . width ) else : col def [ 'width' ] = '9.10' tag ( doc , 'col' , col def ) end tag ( doc , 'cols' ) 
def monit status ( summary , service ) : for line in summary . split ( '\n' ) : if ( service in line ) : return '  ' . join ( line . split ( ) [ 2 : ] ) raise  Service  Exception  ( ' Unable   to  find   Monit   entry  for  {}' . format ( service ) ) 
def replace pool members ( hostname , username , password , name , members ) : payload = { } payload [ 'name' ] = name if ( members is not  None  ) : if isinstance ( members , str ) : members = members . split ( ',' ) pool members = [ ] for member in members : if isinstance ( member , dict ) : if ( 'member state' in member . keys ( ) ) : member [ 'state' ] = member . pop ( 'member state' ) for key in member . keys ( ) : new key = key . replace ( ' ' , '-' ) member [ new key ] = member . pop ( key ) pool members . append ( member ) else : pool members . append ( { 'name' : member , 'address' : member . split ( ':' ) [ 0 ] } ) payload [ 'members' ] = pool members bigip session =  build session ( username , password ) try : response = bigip session . put ( ( BIG IP URL BASE . format ( host = hostname ) + '/ltm/pool/{name}' . format ( name = name ) ) , data = json . dumps ( payload ) ) except requests . exceptions .  Connection  Error  as e : return  load connection error ( hostname , e ) return  load response ( response ) 
def stineman interp ( xi , x , y , yp =  None  ) : x = np . asarray ( x , float ) y = np . asarray ( y , float ) if ( x . shape != y . shape ) : raise  Value  Error  ( u"'x'  and  'y'  must  be  of  same  shape" ) if ( yp is  None  ) : yp = slopes ( x , y ) else : yp = np . asarray ( yp , float ) xi = np . asarray ( xi , float ) yi = np . zeros ( xi . shape , float ) dx = ( x [ 1 : ] - x [ : ( - 1 ) ] ) dy = ( y [ 1 : ] - y [ : ( - 1 ) ] ) s = ( dy / dx ) idx = np . searchsorted ( x [ 1 : ( - 1 ) ] , xi ) sidx = s . take ( idx ) xidx = x . take ( idx ) yidx = y . take ( idx ) xidxp1 = x . take ( ( idx + 1 ) ) yo = ( yidx + ( sidx * ( xi - xidx ) ) ) dy1 = ( ( yp . take ( idx ) - sidx ) * ( xi - xidx ) ) dy2 = ( ( yp . take ( ( idx + 1 ) ) - sidx ) * ( xi - xidxp1 ) ) dy1dy2 = ( dy1 * dy2 ) yi = ( yo + ( dy1dy2 * np . choose ( ( np . array ( np . sign ( dy1dy2 ) , np . int32 ) + 1 ) , ( ( ( ( ( 2 * xi ) - xidx ) - xidxp1 ) / ( ( dy1 - dy2 ) * ( xidxp1 - xidx ) ) ) , 0.0 , ( 1 / ( dy1 + dy2 ) ) ) ) ) ) return yi 
def takes all arguments ( function , * named arguments ) : return bool ( ( takes arguments ( function , * named arguments ) == set ( named arguments ) ) ) 
def competency ( ) : s3 . filter = ( FS ( 'person id$human resource.type' ) == 1 ) field = s3db . hrm competency . person id field . widget = S3 Person  Autocomplete  Widget  ( ajax filter = '~.human resource.type=1' ) return s3db . hrm competency controller ( ) 
def  sqrt nearest ( n , a ) : if ( ( n <= 0 ) or ( a <= 0 ) ) : raise  Value  Error  ( ' Both   arguments  to   sqrt nearest  should  be  positive.' ) b = 0 while ( a != b ) : ( b , a ) = ( a , ( ( a - ( ( - n ) // a ) ) >> 1 ) ) return a 
def coerce url to protocol ( url , protocol = 'http' ) : parsed url =  Url  Parser  ( url ) parsed url . scheme = protocol return parsed url . unparse ( ) 
def add dicts ( d1 , d2 ) : result = d1 . copy ( ) result . update ( d2 ) return result 
@ require POST @ csrf protect def replicate ( request ) : if ( not test user authenticated ( request ) ) : return login ( request , next = '/cobbler web/replicate' , expired =  True  ) return  Http  Response  Redirect  ( '/cobbler web/task created' ) 
def test scharr h horizontal ( ) : ( i , j ) = np . mgrid [ ( - 5 ) : 6 , ( - 5 ) : 6 ] image = ( i >= 0 ) . astype ( float ) result = filters . scharr h ( image ) i [ ( np . abs ( j ) == 5 ) ] = 10000 assert np . all ( ( result [ ( i == 0 ) ] == 1 ) ) assert np . all ( ( result [ ( np . abs ( i ) > 1 ) ] == 0 ) ) 
def write Output  ( file Name  = '' ) : file Name  = fabmetheus interpret . get First  Translator  File  Name  Unmodified  ( file Name  ) if ( file Name  != '' ) : skeinforge craft . write Chain  Text  With  Noun  Message  ( file Name  , 'speed' ) 
def write log file ( log data , log f ) : log f . write ( ' Details   for  removal  of  reverse  primers\n' ) log f . write ( ( ' Original   fasta  filepath:  %s\n' % log data [ 'fasta fp' ] ) ) log f . write ( ( ' Total   seqs  in  fasta:  %d\n' % log data [ 'total seqs' ] ) ) log f . write ( ( ' Mapping   filepath:  %s\n' % log data [ 'mapping fp' ] ) ) log f . write ( ( ' Truncation   option:  %s\n' % log data [ 'truncate option' ] ) ) log f . write ( ( ' Mismatches   allowed:  %d\n' % log data [ 'primer mismatches' ] ) ) log f . write ( ( ' Total   seqs  written:  %d\n' % log data [ 'seqs written' ] ) ) log f . write ( ( ' Sample I Ds   not  found:  %d\n' % log data [ 'sample id not found' ] ) ) log f . write ( ( ' Reverse   primers  not  found:  %d\n' % log data [ 'reverse primer not found' ] ) ) 
def set Repository  To  Line  ( line Index  , lines , short Dictionary  ) : line = lines [ line Index  ] split Line  = line . split ( global Spreadsheet  Separator  ) if ( len ( split Line  ) < 2 ) : return file Setting  Name  = split Line  [ 0 ] if ( file Setting  Name  in global Setting  Replacements  ) : file Setting  Name  = global Setting  Replacements  [ file Setting  Name  ] short Dictionary  Keys  = short Dictionary  . keys ( ) short Dictionary  Keys  . sort ( key = len , reverse =  True  ) for short Dictionary  Key  in short Dictionary  Keys  : if ( file Setting  Name  [ : len ( short Dictionary  Key  ) ] . lower ( ) == short Dictionary  Key  ) : short Dictionary  [ short Dictionary  Key  ] . set Value  To  Split  Line  ( line Index  , lines , split Line  ) return 
def test read no header names ( ) : table = '\n|     John     |  555-1234  |192.168.1.10|\n|     Mary     |  555-2134  |192.168.1.12|\n|       Bob     |  555-4527  |  192.168.1.9|\n' dat = ascii . read ( table ,  Reader  = ascii .  Fixed  Width  , guess =  False  , header start =  None  , data start = 0 , names = ( ' Name ' , ' Phone ' , 'TCP' ) ) assert equal ( tuple ( dat . dtype . names ) , ( ' Name ' , ' Phone ' , 'TCP' ) ) assert equal ( dat [ 1 ] [ 0 ] , ' Mary ' ) assert equal ( dat [ 0 ] [ 1 ] , '555-1234' ) assert equal ( dat [ 2 ] [ 2 ] , '192.168.1.9' ) 
def openable ( string , ** kwargs ) : f = tempfile .  Named  Temporary  File  ( ** kwargs ) f . write ( string ) f . seek ( 0 )  TEMPORARY FILES . append ( f ) return f . name 
def  minify ( source path , target path ) : cmd = ( 'java  -jar  %s  %s  -o  %s' % ( YUICOMPRESSOR DIR , source path , target path ) ) subprocess . check call ( cmd , shell =  True  ) 
def CDLHIKKAKEMOD ( bar Ds  , count ) : return call talib with ohlc ( bar Ds  , count , talib . CDLHIKKAKEMOD ) 
def get root of ( doctype ) : result = frappe . db . sql list ( ( u'select  name  from  `tab%s`\n DCTB  DCTB where  lft=1  and  rgt=(select  max(rgt)  from  `tab%s`  where  docstatus  <  2)' % ( doctype , doctype ) ) ) return ( result [ 0 ] if result else  None  ) 
def assert phase almost equal ( a , b , * args , ** kwargs ) : shift = ( ( 2 * np . pi ) * np . round ( ( ( b . mean ( ) - a . mean ( ) ) / ( 2 * np . pi ) ) ) ) with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' ) print ( 'assert phase allclose,  abs' , np . max ( np . abs ( ( a - ( b - shift ) ) ) ) ) print ( 'assert phase allclose,  rel' , np . max ( np . abs ( ( ( a - ( b - shift ) ) / a ) ) ) ) if np . ma . is Masked  Array  ( a ) : assert  ( np . ma . is Masked  Array  ( b ) ) assert array equal ( a . mask , b . mask ) au = np . asarray ( a ) bu = np . asarray ( b ) with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' ) print ( 'assert phase allclose,  no  mask,  abs' , np . max ( np . abs ( ( au - ( bu - shift ) ) ) ) ) print ( 'assert phase allclose,  no  mask,  rel' , np . max ( np . abs ( ( ( au - ( bu - shift ) ) / au ) ) ) ) assert array almost equal nulp ( ( a + shift ) , b , * args , ** kwargs ) 
def get data ( connection , vm name =  None  ) : vms service = connection . system service ( ) . vms service ( ) clusters service = connection . system service ( ) . clusters service ( ) if vm name : vm = ( vms service . list ( search = ( 'name=%s' % vm name ) ) or [  None  ] ) data = get dict of struct ( connection = connection , vm = vm [ 0 ] ) else : vms = dict ( ) data = defaultdict ( list ) for vm in vms service . list ( ) : name = vm . name vm service = vms service . vm service ( vm . id ) cluster service = clusters service . cluster service ( vm . cluster . id ) vms [ name ] = get dict of struct ( connection , vm ) cluster name = connection . follow link ( vm . cluster ) . name data [ ( 'cluster %s' % cluster name ) ] . append ( name ) tags service = vm service . tags service ( ) for tag in tags service . list ( ) : data [ ( 'tag %s' % tag . name ) ] . append ( name ) data [ ( 'status %s' % vm . status ) ] . append ( name ) for group in cluster service . affinity groups service ( ) . list ( ) : if ( vm . name in [ v . name for v in connection . follow link ( group . vms ) ] ) : data [ ( 'affinity group %s' % group . name ) ] . append ( vm . name ) affinity labels service = vm service . affinity labels service ( ) for label in affinity labels service . list ( ) : data [ ( 'affinity label %s' % label . name ) ] . append ( name ) data [ ' meta' ] = { 'hostvars' : vms } return data 
def organisation type ( ) : return s3 rest controller ( ) 
def  boolrelextrema ( data , comparator , axis = 0 , order = 1 , mode = 'clip' ) : if ( ( int ( order ) != order ) or ( order < 1 ) ) : raise  Value  Error  ( ' Order   must  be  an  int  >=  1' ) datalen = data . shape [ axis ] locs = np . arange ( 0 , datalen ) results = np . ones ( data . shape , dtype = bool ) main = data . take ( locs , axis = axis , mode = mode ) for shift in xrange ( 1 , ( order + 1 ) ) : plus = data . take ( ( locs + shift ) , axis = axis , mode = mode ) minus = data . take ( ( locs - shift ) , axis = axis , mode = mode ) results &= comparator ( main , plus ) results &= comparator ( main , minus ) if ( ~ results . any ( ) ) : return results return results 
def id shown with wait ( context , id str , ** kwargs ) : return  shown elem with wait ( context , (  By  . ID , id str ) , ** kwargs ) 
def send msg v2 ( module , token , room , msg from , msg , msg format = 'text' , color = 'yellow' , notify =  False  , api = NOTIFY URI V2 ) : print ' Sending   message  to  v2  server' headers = { ' Authorization ' : ( ' Bearer   %s' % token ) , ' Content - Type ' : 'application/json' } body = dict ( ) body [ 'message' ] = msg body [ 'color' ] = color body [ 'message format' ] = msg format body [ 'notify' ] = notify POST URL = ( api + NOTIFY URI V2 ) url = POST URL . replace ( '{id or name}' , urllib . pathname2url ( room ) ) data = json . dumps ( body ) if module . check mode : module . exit json ( changed =  False  ) ( response , info ) = fetch url ( module , url , data = data , headers = headers , method = 'POST' ) if ( info [ 'status' ] == 200 ) : return response . read ( ) else : module . fail json ( msg = ( 'failed  to  send  message,  return  status=%s' % str ( info [ 'status' ] ) ) ) 
def dlsim ( system , u , t =  None  , x0 =  None  ) : if isinstance ( system , lti ) : raise  Attribute  Error  ( 'dlsim  can  only  be  used  with  discrete-time  dlti  systems.' ) elif ( not isinstance ( system , dlti ) ) : system = dlti ( dt = system [ ( - 1 ) ] , * system [ : ( - 1 ) ] ) is ss input = isinstance ( system ,  State  Space  ) system = system .  as ss ( ) u = np . atleast 1d ( u ) if ( u . ndim == 1 ) : u = np . atleast 2d ( u ) . T if ( t is  None  ) : out samples = len ( u ) stoptime = ( ( out samples - 1 ) * system . dt ) else : stoptime = t [ ( - 1 ) ] out samples = ( int ( np . floor ( ( stoptime / system . dt ) ) ) + 1 ) xout = np . zeros ( ( out samples , system . A . shape [ 0 ] ) ) yout = np . zeros ( ( out samples , system . C . shape [ 0 ] ) ) tout = np . linspace ( 0.0 , stoptime , num = out samples ) if ( x0 is  None  ) : xout [ 0 , : ] = np . zeros ( ( system . A . shape [ 1 ] , ) ) else : xout [ 0 , : ] = np . asarray ( x0 ) if ( t is  None  ) : u dt = u else : if ( len ( u . shape ) == 1 ) : u = u [ : , np . newaxis ] u dt interp = interp1d ( t , u . transpose ( ) , copy =  False  , bounds error =  True  ) u dt = u dt interp ( tout ) . transpose ( ) for i in range ( 0 , ( out samples - 1 ) ) : xout [ ( i + 1 ) , : ] = ( np . dot ( system . A , xout [ i , : ] ) + np . dot ( system . B , u dt [ i , : ] ) ) yout [ i , : ] = ( np . dot ( system . C , xout [ i , : ] ) + np . dot ( system . D , u dt [ i , : ] ) ) yout [ ( out samples - 1 ) , : ] = ( np . dot ( system . C , xout [ ( out samples - 1 ) , : ] ) + np . dot ( system . D , u dt [ ( out samples - 1 ) , : ] ) ) if is ss input : return ( tout , yout , xout ) else : return ( tout , yout ) 
def  check outlines ( pos , outlines , head pos =  None  ) : pos = np . array ( pos , float ) [ : , : 2 ] head pos = ( dict ( ) if ( head pos is  None  ) else head pos ) if ( not isinstance ( head pos , dict ) ) : raise  Type  Error  ( 'head pos  must  be  dict  or   None ' ) head pos = copy . deepcopy ( head pos ) for key in head pos . keys ( ) : if ( key not in ( 'center' , 'scale' ) ) : raise  Key  Error  ( 'head pos  must  only  contain  "center"  and  "scale"' ) head pos [ key ] = np . array ( head pos [ key ] , float ) if ( head pos [ key ] . shape != ( 2 , ) ) : raise  Value  Error  ( ( 'head pos["%s"]  must  have  shape  (2,),  not  %s' % ( key , head pos [ key ] . shape ) ) ) if ( outlines in ( 'head' , 'skirt' ,  None  ) ) : radius = 0.5 l = np . linspace ( 0 , ( 2 * np . pi ) , 101 ) head x = ( np . cos ( l ) * radius ) head y = ( np . sin ( l ) * radius ) nose x = ( np . array ( [ 0.18 , 0 , ( - 0.18 ) ] ) * radius ) nose y = np . array ( [ ( radius - 0.004 ) , ( radius * 1.15 ) , ( radius - 0.004 ) ] ) ear x = np . array ( [ 0.497 , 0.51 , 0.518 , 0.5299 , 0.5419 , 0.54 , 0.547 , 0.532 , 0.51 , 0.489 ] ) ear y = np . array ( [ 0.0555 , 0.0775 , 0.0783 , 0.0746 , 0.0555 , ( - 0.0055 ) , ( - 0.0932 ) , ( - 0.1313 ) , ( - 0.1384 ) , ( - 0.1199 ) ] ) if ( 'center' not in head pos ) : head pos [ 'center' ] = ( 0.5 * ( pos . max ( axis = 0 ) + pos . min ( axis = 0 ) ) ) pos -= head pos [ 'center' ] if ( outlines is not  None  ) : outlines dict = dict ( head = ( head x , head y ) , nose = ( nose x , nose y ) , ear left = ( ear x , ear y ) , ear right = ( ( - ear x ) , ear y ) ) else : outlines dict = dict ( ) if ( outlines == 'skirt' ) : if ( 'scale' not in head pos ) : head pos [ 'scale' ] = ( 1.0 / ( pos . max ( axis = 0 ) - pos . min ( axis = 0 ) ) ) pos *= head pos [ 'scale' ] mask scale = ( 1.25 * ( pos . max ( axis = 0 ) - pos . min ( axis = 0 ) ) ) outlines dict [ 'autoshrink' ] =  False  outlines dict [ 'mask pos' ] = ( ( mask scale [ 0 ] * head x ) , ( mask scale [ 1 ] * head y ) ) outlines dict [ 'clip radius' ] = ( mask scale / 2.0 ) else : if ( 'scale' not in head pos ) : head pos [ 'scale' ] = ( 0.85 / ( pos . max ( axis = 0 ) - pos . min ( axis = 0 ) ) ) pos *= head pos [ 'scale' ] outlines dict [ 'autoshrink' ] =  True  outlines dict [ 'mask pos' ] = ( head x , head y ) outlines dict [ 'clip radius' ] = ( 0.5 , 0.5 ) outlines = outlines dict elif isinstance ( outlines , dict ) : if ( 'mask pos' not in outlines ) : raise  Value  Error  ( ' You   must  specify  the  coordinates  of  the  imagemask' ) else : raise  Value  Error  ( ' Invalid   value  for  `outlines' ) return ( pos , outlines ) 
def export languages json ( ) : languages = frappe . db . get all ( u' Language ' , fields = [ u'name' , u'language name' ] ) languages = [ { u'name' : d . language name , u'code' : d . name } for d in languages ] languages . sort ( ( lambda a , b : ( 1 if ( a [ u'code' ] > b [ u'code' ] ) else ( - 1 ) ) ) ) with open ( frappe . get app path ( u'frappe' , u'geo' , u'languages.json' ) , u'w' ) as f : f . write ( frappe . as json ( languages ) ) 
def flavor extra delete ( request , flavor id , keys ) : flavor = novaclient ( request ) . flavors . get ( flavor id ) return flavor . unset keys ( keys ) 
def splprep ( x , w =  None  , u =  None  , ub =  None  , ue =  None  , k = 3 , task = 0 , s =  None  , t =  None  , full output = 0 , nest =  None  , per = 0 , quiet = 1 ) : res =  impl . splprep ( x , w , u , ub , ue , k , task , s , t , full output , nest , per , quiet ) return res 
def export key ( keyids =  None  , secret =  False  , user =  None  , gnupghome =  None  ) : gpg =  create gpg ( user , gnupghome ) if isinstance ( keyids , six . string types ) : keyids = keyids . split ( ',' ) return gpg . export keys ( keyids , secret ) 
def  log multivariate normal density diag ( X , means , covars ) : ( n samples , n dim ) = X . shape lpr = ( ( - 0.5 ) * ( ( ( ( ( n dim * np . log ( ( 2 * np . pi ) ) ) + np . sum ( np . log ( covars ) , 1 ) ) + np . sum ( ( ( means ** 2 ) / covars ) , 1 ) ) - ( 2 * np . dot ( X , ( means / covars ) . T ) ) ) + np . dot ( ( X ** 2 ) , ( 1.0 / covars ) . T ) ) ) return lpr 
def wire HTTP Connection  to WSGI ( host , app ) : class HTTP Connection  Decorator  ( object , ) : ' Wraps   the  real  HTTP Connection   class  so  that  when  you  instantiate\n                the  class  you  might  instead  get  a  fake  instance.' def   init   ( self , wrapped ) : self . wrapped = wrapped def   call   ( self , connection host , * args , ** kwargs ) : if ( connection host == host ) : return  Fake  Httplib  Connection  ( app , host ) else : return self . wrapped ( connection host , * args , ** kwargs ) oldHTTP Connection  = httplib . HTTP Connection  httplib . HTTP Connection  = HTTP Connection  Decorator  ( httplib . HTTP Connection  ) return oldHTTP Connection  
def create file ( path ) : with open ( path , 'w' ) : pass 
def  move webengine data ( ) : old data dir = Q Standard  Paths  . writable Location  ( Q Standard  Paths  .  Data  Location  ) old cache dir = Q Standard  Paths  . writable Location  ( Q Standard  Paths  .  Cache  Location  ) new data dir = os . path . join ( data ( ) , 'webengine' ) new cache dir = os . path . join ( cache ( ) , 'webengine' ) if ( ( not os . path . exists ( os . path . join ( old data dir , ' Qt  Web  Engine ' ) ) ) and ( not os . path . exists ( os . path . join ( old cache dir , ' Qt  Web  Engine ' ) ) ) ) : return log . init . debug ( ' Moving    Qt  Web  Engine   data  from  {}  to  {}' . format ( old data dir , new data dir ) ) log . init . debug ( ' Moving    Qt  Web  Engine   cache  from  {}  to  {}' . format ( old cache dir , new cache dir ) ) if os . path . exists ( new data dir ) : log . init . warning ( ' Failed   to  move  old   Qt  Web  Engine   data  as  {}  already  exists!' . format ( new data dir ) ) return if os . path . exists ( new cache dir ) : log . init . warning ( ' Failed   to  move  old   Qt  Web  Engine   cache  as  {}  already  exists!' . format ( new cache dir ) ) return try : shutil . move ( os . path . join ( old data dir , ' Qt  Web  Engine ' , ' Default ' ) , new data dir ) shutil . move ( os . path . join ( old cache dir , ' Qt  Web  Engine ' , ' Default ' ) , new cache dir ) if ( old data dir . split ( os . sep ) [ ( - 2 ) : ] == [ 'qutebrowser' , 'qutebrowser' ] ) : log . init . debug ( ' Removing   {}  /  {}' . format ( old data dir , old cache dir ) ) for old dir in ( old data dir , old cache dir ) : os . rmdir ( os . path . join ( old dir , ' Qt  Web  Engine ' ) ) os . rmdir ( old dir ) except OS Error  as e : log . init . exception ( ' Failed   to  move  old   Qt  Web  Engine   data/cache:  {}' . format ( e ) ) 
@ pytest . mark . usefixtures ( 'table type' ) def test init and ref from multidim ndarray ( table type ) : for copy in (  False  ,  True  ) : nd = np . array ( [ ( 1 , [ 10 , 20 ] ) , ( 3 , [ 30 , 40 ] ) ] , dtype = [ ( str ( 'a' ) , 'i8' ) , ( str ( 'b' ) , 'i8' , ( 2 , ) ) ] ) t = table type ( nd , copy = copy ) assert ( t . colnames == [ 'a' , 'b' ] ) assert ( t [ 'a' ] . shape == ( 2 , ) ) assert ( t [ 'b' ] . shape == ( 2 , 2 ) ) t [ 'a' ] [ 0 ] = ( - 200 ) t [ 'b' ] [ 1 ] [ 1 ] = ( - 100 ) if copy : assert ( nd [ str ( 'a' ) ] [ 0 ] == 1 ) assert ( nd [ str ( 'b' ) ] [ 1 ] [ 1 ] == 40 ) else : assert ( nd [ str ( 'a' ) ] [ 0 ] == ( - 200 ) ) assert ( nd [ str ( 'b' ) ] [ 1 ] [ 1 ] == ( - 100 ) ) 
def make property ( fact ) : def getit ( self ) : try : return self .  assumptions [ fact ] except  Key  Error  : if ( self .  assumptions is self . default assumptions ) : self .  assumptions = self . default assumptions . copy ( ) return  ask ( fact , self ) getit . func name = as property ( fact ) return property ( getit ) 
def vdot ( m1 , m2 ) : err code = ct . c int ( 0 ) res =  eigenmat . vdot ( m1 . p mat , m2 . p mat , ct . byref ( err code ) ) if err code : raise generate exception ( err code . value ) return res 
def do nothing ( ) : return 
def  check region for parsing ( number , default region ) : if ( not  is valid region code ( default region ) ) : if ( ( number is  None  ) or ( len ( number ) == 0 ) ) : return  False  match =  PLUS CHARS PATTERN . match ( number ) if ( match is  None  ) : return  False  return  True  
def get dependencies ( ) : return config . check driver dependencies (   virtualname   , { 'softlayer' : HAS SLLIBS } ) 
def rewriter ( field , rules ) : def fieldfunc ( item ) : value = item .  values fixed [ field ] for ( pattern , replacement ) in rules : if pattern . match ( value . lower ( ) ) : return replacement return value return fieldfunc 
def create connection ( conf , new =  True  ) : return  Connection  ( ) 
def is url ( filename ) : return ( isinstance ( filename , six . string types ) and ( URL REGEX . match ( filename ) is not  None  ) ) 
def  is String  ( obj ) : return ( type ( obj ) in types .  String  Types  ) 
def json formatter ( fstring ) : if ( fstring == '||' ) : fstring = '[]' else : fstring = fstring . replace ( '  ' , '' ) fstring = fstring . replace ( '|' , '' ) fstring = fstring . replace ( '}{' , '},{' ) fstring = fstring . replace ( "{u'" , "{'" ) fstring = fstring . replace ( ":u'" , ":'" ) fstring = fstring . replace ( ",u'" , ",'" ) fstring = fstring . replace ( "'" , '"' ) fstring = ( '[%s]' % fstring ) return fstring 
@ public def pquo ( f , g , * gens , ** args ) : options . allowed flags ( args , [ 'polys' ] ) try : ( ( F , G ) , opt ) = parallel poly from expr ( ( f , g ) , * gens , ** args ) except  Polification  Failed  as exc : raise  Computation  Failed  ( 'pquo' , 2 , exc ) try : q = F . pquo ( G ) except  Exact  Quotient  Failed  : raise  Exact  Quotient  Failed  ( f , g ) if ( not opt . polys ) : return q . as expr ( ) else : return q 
def urlsafe b64encode ( s ) : return b64encode ( s ) . translate (  urlsafe encode translation ) 
@ login required @ view def importer ( request , test js =  False  ) : person = request . user . get profile ( ) data = get personal data ( person ) data [ 'citation form' ] = mysite . profile . forms .  Manually  Add A Citation  Form  ( auto id =  False  ) data [ 'test js' ] = ( test js or request . GET . get ( 'test' ,  None  ) ) return ( request , 'profile/importer.html' , data ) 
def  sub clade ( clade , term names ) : term clades = [ clade . find any ( name ) for name in term names ] sub clade = clade . common ancestor ( term clades ) if ( len ( term names ) != sub clade . count terminals ( ) ) : temp clade =  Base  Tree  .  Clade  ( ) temp clade . clades . extend ( term clades ) for c in sub clade . find clades ( terminal =  False  , order = 'preorder' ) : if ( c == sub clade . root ) : continue childs = ( set ( c . find clades ( terminal =  True  ) ) & set ( term clades ) ) if childs : for tc in temp clade . find clades ( terminal =  False  , order = 'preorder' ) : tc childs = set ( tc . clades ) tc new clades = ( tc childs - childs ) if ( childs . issubset ( tc childs ) and tc new clades ) : tc . clades = list ( tc new clades ) child clade =  Base  Tree  .  Clade  ( ) child clade . clades . extend ( list ( childs ) ) tc . clades . append ( child clade ) sub clade = temp clade return sub clade 
def get host id ( kwargs =  None  , call =  None  ) : if ( call == 'action' ) : raise  Salt  Cloud  System  Exit  ( ' The   get host id  function  must  be  called  with  -f  or  --function.' ) if ( kwargs is  None  ) : kwargs = { } name = kwargs . get ( 'name' ,  None  ) if ( name is  None  ) : raise  Salt  Cloud  System  Exit  ( ' The   get host id  function  requires  a  name.' ) try : ret = avail locations ( ) [ name ] [ 'id' ] except  Key  Error  : raise  Salt  Cloud  System  Exit  ( " The   host  '{0}'  could  not  be  found" . format ( name ) ) return ret 
def equal args ( * args , ** kwargs ) : key = args if kwargs : key += (  kwargs separator + tuple ( sorted ( kwargs . items ( ) ) ) ) return key 
def lexists ( path ) : try : os . lstat ( path ) except os . error : return  False  return  True  
def wait for occupied port ( host , port ) : if ( not host ) : raise  Value  Error  ( " Host   values  of  ''  or   None   are  not  allowed." ) for trial in range ( 50 ) : try : check port ( host , port ) except IO Error  : return else : time . sleep ( 0.1 ) raise IO Error  ( ( ' Port   %r  not  bound  on  %r' % ( port , host ) ) ) 
def  system state change ( state , device ) : if ( state == 'present' ) : if device : return  False  return  True  if ( state == 'absent' ) : if device : return  True  return  False  return  False  
def request content ( url , ** kwargs ) : response = request response ( url , ** kwargs ) if ( response is not  None  ) : return response . content 
def latlon to grid ( latlon ) : from MAV Proxy  . modules . lib . ANUGA import redfearn ( zone , easting , northing ) = redfearn . redfearn ( latlon [ 0 ] , latlon [ 1 ] ) if ( latlon [ 0 ] < 0 ) : hemisphere = 'S' else : hemisphere = 'N' return UTM Grid  ( zone , easting , northing , hemisphere = hemisphere ) 
def shuffle ( X , y ) : idx = np . argsort ( [ random . random ( ) for i in range ( len ( y ) ) ] ) y = np . asarray ( y ) X = [ X [ i ] for i in idx ] y = y [ idx ] return ( X , y ) 
def memoize ( timeout , dynamic timeout =  False  ) : cache = { 'timeout' : timeout } def decorator ( func ) : def wrapper ( * args , ** kwargs ) : start = time ( ) if ( ( not ( 'time' in cache ) ) or ( ( start - cache [ 'time' ] ) > cache [ 'timeout' ] ) ) : cache [ 'result' ] = func ( * args , ** kwargs ) cache [ 'time' ] = time ( ) if ( dynamic timeout and ( ( cache [ 'time' ] - start ) > cache [ 'timeout' ] ) ) : cache [ 'timeout' ] *= 2 return cache [ 'result' ] def clear cache ( ) : if ( 'time' in cache ) : del cache [ 'time' ] if ( 'result' in cache ) : del cache [ 'result' ] wrapper . clear cache = clear cache return wrapper return decorator 
def server security groups ( request , instance id ) : security groups = [ ] nclient = novaclient ( request ) ( resp , body ) = nclient . client . get ( ( '/servers/%s/os-security-groups' % instance id ) ) if body : sg objs = [  Nova  Security  Group  ( nclient . security groups , sg , loaded =  True  ) for sg in body . get ( 'security groups' , [ ] ) ] security groups = [  Security  Group  ( sg ) for sg in sg objs ] for sg in security groups : rule objects = [  Security  Group  Rule  ( rule ) for rule in sg . rules ] sg . rules = rule objects return security groups 
def is threshold graph ( G ) : return is threshold sequence ( list ( ( d for ( n , d ) in G . degree ( ) ) ) ) 
def get preferred file contents encoding ( ) : return ( locale . getpreferredencoding ( ) or u'utf-8' ) 
def tp write ( fd , buf ) : return get hub ( ) . threadpool . apply (  write , ( fd , buf ) ) 
@ contextmanager def use plugin ( * plugins ) : p = load ( * plugins ) try : ( yield p ) finally : unload ( * plugins ) 
def choose ncv ( k ) : return max ( ( ( 2 * k ) + 1 ) , 20 ) 
def pathstrip ( path , n ) : pathlist = [ path ] while ( os . path . dirname ( pathlist [ 0 ] ) != '' ) : pathlist [ 0 : 1 ] = os . path . split ( pathlist [ 0 ] ) return '/' . join ( pathlist [ n : ] ) 
def dummy 1d ( x , varname =  None  ) : if ( varname is  None  ) : labels = [ ( 'level %d' % i ) for i in range ( ( x . max ( ) + 1 ) ) ] return ( ( x [ : ,  None  ] == np . arange ( ( x . max ( ) + 1 ) ) ) . astype ( int ) , labels ) else : grouplabels = np . unique ( x ) labels = [ ( varname + ( ' %s' % str ( i ) ) ) for i in grouplabels ] return ( ( x [ : ,  None  ] == grouplabels ) . astype ( int ) , labels ) 
def  escaped text from text ( text , escapes = 'eol' ) : import re if isinstance ( escapes , base string type ) : if ( escapes == 'eol' ) : escapes = { '\r\n' : '\\r\\n\r\n' , '\n' : '\\n\n' , '\r' : '\\r\r' } elif ( escapes == 'whitespace' ) : escapes = { '\r\n' : '\\r\\n\r\n' , '\n' : '\\n\n' , '\r' : '\\r\r' , ' DCTB ' : '\\t' , '  ' : '.' } elif ( escapes == 'eol-one-line' ) : escapes = { '\n' : '\\n' , '\r' : '\\r' } elif ( escapes == 'whitespace-one-line' ) : escapes = { '\n' : '\\n' , '\r' : '\\r' , ' DCTB ' : '\\t' , '  ' : '.' } else : raise  Value  Error  ( ( 'unknown  text  escape  style:  %r' % escapes ) ) escapes keys = list ( escapes . keys ( ) ) try : escapes keys . sort ( key = ( lambda a : len ( a ) ) , reverse =  True  ) except  Type  Error  : escapes keys . sort ( ( lambda a , b : cmp ( len ( a ) , len ( b ) ) ) ) escapes keys . reverse ( ) def repl ( match ) : val = escapes [ match . group ( 0 ) ] return val escaped = re . sub ( ( '(%s)' % '|' . join ( [ re . escape ( k ) for k in escapes keys ] ) ) , repl , text ) return escaped 
def  migrate collection to latest schema ( versioned collection ) : collection schema version = versioned collection [ 'schema version' ] if ( not ( 1 <= collection schema version <= feconf . CURRENT COLLECTION SCHEMA VERSION ) ) : raise  Exception  ( ( ' Sorry ,  we  can  only  process  v1-v%d  collection  schemas  at  present.' % feconf . CURRENT COLLECTION SCHEMA VERSION ) ) 
def collect addon assets ( node ) : return { 'tree js' : list ( collect addon js ( node ) ) , 'tree css' : list ( collect addon css ( node ) ) } 
def copy plural forms ( msgs , locale , domain , verbosity ) : import django django dir = os . path . normpath ( os . path . join ( os . path . dirname ( django .   file   ) ) ) if ( domain == 'djangojs' ) : domains = ( 'djangojs' , 'django' ) else : domains = ( 'django' , ) for domain in domains : django po = os . path . join ( django dir , 'conf' , 'locale' , locale , 'LC MESSAGES' , ( '%s.po' % domain ) ) if os . path . exists ( django po ) : m = plural forms re . search ( open ( django po , 'rU' ) . read ( ) ) if m : if ( verbosity > 1 ) : sys . stderr . write ( ( 'copying  plural  forms:  %s\n' % m . group ( 'value' ) ) ) lines = [ ] seen =  False  for line in msgs . split ( '\n' ) : if ( ( not line ) and ( not seen ) ) : line = ( '%s\n' % m . group ( 'value' ) ) seen =  True  lines . append ( line ) msgs = '\n' . join ( lines ) break return msgs 
def fix all git symlinked ( topdir ) : with io . open ( ( topdir + u'\\nikola\\data\\symlink-test-link.txt' ) , u'r' , encoding = u'utf-8' ) as f : text = f . read ( ) if text . startswith ( u'NIKOLA SYMLINKS=OK' ) : return ( - 1 ) with io . open ( ( topdir + u'\\nikola\\data\\symlinked.txt' ) , u'r' , encoding = u'utf-8' ) as f : text = f . read ( ) if text . startswith ( u'.' ) : raise  Exception  ( u' Bad   data  in  \\nikola\\data\\symlinked.txt' ) relnames = text . split ( u'\n' ) relnames = [ name . strip ( ) . replace ( u'/' , u'\\' ) for name in relnames ] relnames = [ name for name in relnames if name ] failures = 0 for name in relnames : dst = os . path . join ( topdir , name ) if ( not is file into dir ( dst , topdir ) ) : continue if os . path . isdir ( dst ) : continue with io . open ( os . path . join ( topdir , dst ) , u'r' , encoding = u'utf-8' ) as f : text = f . read ( ) dst dir = os . path . dirname ( dst ) try : src = os . path . normpath ( os . path . join ( dst dir , text ) ) if ( not os . path . exists ( src ) ) : continue if ( not is file into dir ( src , topdir ) ) : continue except  Exception  : continue try : if os . path . isdir ( src ) : os . unlink ( dst ) shutil . copytree ( src , dst ) else : shutil . copy2 ( src , dst ) except  Exception  : failures += 1 print ( u'***  copy  failed  for' ) print ( u' DCTB   src:' , src ) print ( u' DCTB   dst:' , dst ) return failures 
def ini format ( stream , options , encoding ) : for ( optname , optdict , value ) in options : value = format option value ( optdict , value ) help = optdict . get ( 'help' ) if help : help = normalize text ( help , line len = 79 , indent = '#  ' ) print ( file = stream ) print (  encode ( help , encoding ) , file = stream ) else : print ( file = stream ) if ( value is  None  ) : print ( ( '#%s=' % optname ) , file = stream ) else : value =  encode ( value , encoding ) . strip ( ) print ( ( '%s=%s' % ( optname , value ) ) , file = stream ) 
def featured map ( request , site ) : map obj = resolve object ( request ,  Map  , { 'featuredurl' : site } , permission = 'base.view resourcebase' , permission msg =  PERMISSION MSG VIEW ) return map view ( request , str ( map obj . id ) ) 
def  get score from submissions ( submissions scores , block ) : if submissions scores : submission value = submissions scores . get ( unicode ( block . location ) ) if submission value : attempted =  True  ( weighted earned , weighted possible ) = submission value assert ( ( weighted earned >= 0.0 ) and ( weighted possible > 0.0 ) ) return ( ( (  None  ,  None  ) + ( weighted earned , weighted possible ) ) + ( attempted , ) ) 
@ require http methods ( 'GET' ) @ login required @ expect json def xblock outline handler ( request , usage key string ) : usage key = usage key with run ( usage key string ) if ( not has studio read access ( request . user , usage key . course key ) ) : raise  Permission  Denied  ( ) response format = request . GET . get ( 'format' , 'html' ) if ( ( response format == 'json' ) or ( 'application/json' in request . META . get ( 'HTTP ACCEPT' , 'application/json' ) ) ) : store = modulestore ( ) with store . bulk operations ( usage key . course key ) : root xblock = store . get item ( usage key , depth =  None  ) return  Json  Response  ( create xblock info ( root xblock , include child info =  True  , course outline =  True  , include children predicate = ( lambda xblock : ( not ( xblock . category == 'vertical' ) ) ) ) ) else : return  Http 404 
def test cache existing metadata file ( config stub , tmpdir ) : config stub . data = { 'storage' : { 'cache-size' : 1024 } , 'general' : { 'private-browsing' :  False  } } url = 'http://qutebrowser.org' content = 'foobar' metadata = Q Network  Cache  Meta  Data  ( ) metadata . set Url  ( Q Url  ( url ) ) assert metadata . is Valid  ( ) disk cache = cache .  Disk  Cache  ( str ( tmpdir ) ) device = disk cache . prepare ( metadata ) assert ( device is not  None  ) device . write ( content ) disk cache . insert ( device ) disk cache . update Meta  Data  ( metadata ) files = list ( tmpdir . visit ( fil = ( lambda path : path . isfile ( ) ) ) ) assert ( len ( files ) == 1 ) assert ( disk cache . file Meta  Data  ( str ( files [ 0 ] ) ) == metadata ) 
def  Smart  Connect  ( protocol = 'https' , host = 'localhost' , port = 443 , user = 'root' , pwd = '' , service = 'hostd' , path = '/sdk' , connection Pool  Timeout  = CONNECTION POOL IDLE TIMEOUT SEC , preferred Api  Versions  =  None  , key File  =  None  , cert File  =  None  , thumbprint =  None  , ssl Context  =  None  , b64token =  None  , mechanism = 'userpass' ) : if ( preferred Api  Versions  is  None  ) : preferred Api  Versions  =  Get  Service  Versions  ( 'vim25' ) ssl Context  = local Ssl  Fixup  ( host , ssl Context  ) supported Version  =    Find  Supported  Version  ( protocol , host , port , path , preferred Api  Versions  , ssl Context  ) if ( supported Version  is  None  ) : raise  Exception  ( ( '%s:%s  is  not  a  VIM  server' % ( host , port ) ) ) port Number  = ( ( ( protocol == 'http' ) and ( - int ( port ) ) ) or int ( port ) ) return  Connect  ( host = host , port = port Number  , user = user , pwd = pwd , service = service , adapter = 'SOAP' , version = supported Version  , path = path , connection Pool  Timeout  = connection Pool  Timeout  , key File  = key File  , cert File  = cert File  , thumbprint = thumbprint , ssl Context  = ssl Context  , b64token = b64token , mechanism = mechanism ) 
def   Test  Update  Device  ( tester , user cookie , request dict ) : validator = tester . validator ( user id , device id ) = tester .  Get  Ids  From  Cookie  ( user cookie ) request dict = deepcopy ( request dict ) device dict = request dict [ 'device dict' ] actual dict = tester .  Send  Request  ( 'update device' , user cookie , request dict ) device dict [ 'user id' ] = user id if ( 'push token' in device dict ) : device dict [ 'alert user id' ] = user id device dict . pop ( 'device uuid' ,  None  ) device dict . pop ( 'test udid' ,  None  ) device = validator .  Validate  Update DB Object  (  Device  , last access = util .  TEST TIME , ** device dict ) if ( 'push token' in device dict ) : predicate = ( lambda d : ( ( d . push token == device dict [ 'push token' ] ) and ( d . device id != device dict [ 'device id' ] ) ) ) for other device in validator .  Query  Model  Objects  (  Device  , predicate = predicate ) : validator .  Validate  Update DB Object  (  Device  , user id = other device . user id , device id = other device . device id , push token =  None  , alert user id =  None  ) tester .   Compare  Response  Dicts  ( 'update device' , user id , request dict , { } , actual dict ) return actual dict 
def write to file ( path , contents , umask =  None  ) : if umask : saved umask = os . umask ( umask ) try : with open ( path , 'w' ) as f : f . write ( contents ) finally : if umask : os . umask ( saved umask ) 
def register save ( id , driver ) : SAVE [ id . upper ( ) ] = driver 
def change playlist ( new playlist ) : global current music , current playlist , next change delay if ( music and ( new playlist is not current playlist ) ) : current playlist = new playlist if music enabled : music . fadeout ( ( fadeout time * 1000 ) ) next change delay = max ( 0 , ( change delay - fadeout time ) ) jog music ( ) else : current music =  None  
def retcode ( code = 42 ) :   context   [ 'retcode' ] = code return  True  
def safe parse date ( date hdr ) : try : if ( ';' in date hdr ) : date hdr = date hdr . split ( ';' ) [ ( - 1 ) ] . strip ( ) msg ts = long ( rfc822 . mktime tz ( rfc822 . parsedate tz ( date hdr ) ) ) if ( ( msg ts > ( time . time ( ) + ( 24 * 3600 ) ) ) or ( msg ts < 1 ) ) : return  None  else : return msg ts except (  Value  Error  ,  Type  Error  ,  Overflow  Error  ) : return  None  
def template instantiate ( call =  None  , kwargs =  None  ) : if ( call != 'function' ) : raise  Salt  Cloud  System  Exit  ( ' The   template instantiate  function  must  be  called  with  -f  or  --function.' ) if ( kwargs is  None  ) : kwargs = { } vm name = kwargs . get ( 'vm name' ,  None  ) template id = kwargs . get ( 'template id' ,  None  ) template name = kwargs . get ( 'template name' ,  None  ) if ( vm name is  None  ) : raise  Salt  Cloud  System  Exit  ( " The   template instantiate  function  requires  a  'vm name'  to  be  provided." ) if template id : if template name : log . warning ( " Both   the  'template id'  and  'template name'  arguments  were  provided.  'template id'  will  take  precedence." ) elif template name : template id = get template id ( kwargs = { 'name' : template name } ) else : raise  Salt  Cloud  System  Exit  ( " The   template instantiate  function  requires  either  a  'template id'  or  a  'template name'  to  be  provided." ) ( server , user , password ) =  get xml rpc ( ) auth = ':' . join ( [ user , password ] ) response = server . one . template . instantiate ( auth , int ( template id ) , vm name ) data = { 'action' : 'template.instantiate' , 'instantiated' : response [ 0 ] , 'instantiated vm id' : response [ 1 ] , 'vm name' : vm name , 'error code' : response [ 2 ] } return data 
def is gzip ( fp ) : return ( open ( fp , 'rb' ) . read ( 2 ) == '\x1f\x8b' ) 
def function range ( f , symbol , domain ) : from sympy . solvers . solveset import solveset vals = S .  Empty  Set  period = periodicity ( f , symbol ) if ( not any ( ( ( period is i ) for i in (  None  , S .  Zero  ) ) ) ) : inf = domain . inf inf period = ( S .  Zero  if inf . is infinite else inf ) sup period = ( inf period + period ) periodic interval =  Interval  ( inf period , sup period ) domain = domain . intersect ( periodic interval ) intervals = continuous domain ( f , symbol , domain ) range int = S .  Empty  Set  if isinstance ( intervals ,  Interval  ) : interval iter = ( intervals , ) else : interval iter = intervals . args for interval in interval iter : critical points = S .  Empty  Set  critical values = S .  Empty  Set  bounds = ( ( interval . left open , interval . inf , '+' ) , ( interval . right open , interval . sup , '-' ) ) for ( is open , limit point , direction ) in bounds : if is open : critical values +=  Finite  Set  ( limit ( f , symbol , limit point , direction ) ) vals += critical values else : vals +=  Finite  Set  ( f . subs ( symbol , limit point ) ) critical points += solveset ( f . diff ( symbol ) , symbol , domain ) for critical point in critical points : vals +=  Finite  Set  ( f . subs ( symbol , critical point ) ) ( left open , right open ) = (  False  ,  False  ) if ( critical values is not S .  Empty  Set  ) : if ( critical values . inf == vals . inf ) : left open =  True  if ( critical values . sup == vals . sup ) : right open =  True  range int +=  Interval  ( vals . inf , vals . sup , left open , right open ) return range int 
def load module from modpath ( parts , path =  None  , use sys =  True  ) : if use sys : try : return sys . modules [ '.' . join ( parts ) ] except  Key  Error  : pass modpath = [ ] prevmodule =  None  for part in parts : modpath . append ( part ) curname = '.' . join ( modpath ) module =  None  if ( len ( modpath ) != len ( parts ) ) : module = sys . modules . get ( curname ) elif use sys : module = sys . modules . get ( curname ) if ( module is  None  ) : ( mp file , mp filename , mp desc ) = find module ( part , path ) module = load module ( curname , mp file , mp filename , mp desc ) if prevmodule : setattr ( prevmodule , part , module )  file = getattr ( module , '  file  ' , '' ) prevmodule = module if ( ( not  file ) and  is namespace ( curname ) ) : continue if ( ( not  file ) and ( len ( modpath ) != len ( parts ) ) ) : raise  Import  Error  ( ( 'no  module  in  %s' % '.' . join ( parts [ len ( modpath ) : ] ) ) ) path = [ dirname (  file ) ] return module 
def expand to packages ( names , env =  None  ) : if ( env is  None  ) : env = os . environ ros paths = rospkg . get ros paths ( env ) rospack = rospkg .  Ros  Pack  ( ros paths ) rosstack = rospkg .  Ros  Stack  ( ros paths ) return rospkg . expand to packages ( names , rospack , rosstack ) 
def text to word sequence ( text , filters = '!"#$%&()*+,-./:;<=>?@[\\]^ `{|}~ DCTB \n' , lower =  True  , split = '  ' ) : if lower : text = text . lower ( ) text = text . translate ( maketrans ( filters , ( split * len ( filters ) ) ) ) seq = text . split ( split ) return [ i for i in seq if i ] 
def get Truncated  Rotated  Boundary  Layers  ( loop Layers  , repository ) : return loop Layers  [ repository . layers From  . value : repository . layers To  . value ] 
def compress ( body , content type ) : ( encoder , content type ) = get encoder ( content type ) return ( encoder ( ensure bytes ( body ) ) , content type ) 
def format html ( html ) : return html . replace ( u'\n' , u'' ) . replace ( u'  ' , u'' ) 
@ cronjobs . register def reload question traffic stats ( ) : if settings . STAGE : return  Question  Visits  . reload from analytics ( verbose = settings . DEBUG ) 
def get scheme names ( ) : return tuple ( sorted (  INSTALL SCHEMES ) ) 
def get static index page ( with shutdown ) : template = '\n<!DOCTYPE  HTML  PUBLIC  "-//W3C//DTD  HTML  4.01   Frameset //EN"    "http://www.w3.org/TR/html4/frameset.dtd">\n<HTML>\n          <!--   Natural    Language    Toolkit :   Wordnet    Interface :   Graphical    Wordnet    Browser \n                         Copyright   (C)  2001-2017  NLTK   Project \n                         Author :   Jussi    Salmela   <jtsalmela@users.sourceforge.net>\n                        URL:  <http://nltk.org/>\n                         For   license  information,  see  LICENSE.TXT  -->\n          <HEAD>\n                  <TITLE>NLTK   Wordnet    Browser </TITLE>\n          </HEAD>\n\n<frameset  rows="7%%,93%%">\n        <frame  src="%s"  name="header">\n        <frame  src="start page"  name="body">\n</frameset>\n</HTML>\n' if with shutdown : upper link = 'upper.html' else : upper link = 'upper 2.html' return ( template % upper link ) 
@ register . filter ( is safe =  True  ) @ stringfilter def force escape ( value ) : return mark safe ( escape ( value ) ) 
@ constructor def std ( input , axis =  None  , ddof = 0 , keepdims =  False  , corrected =  False  ) : if isinstance ( ddof , bool ) : raise  Value  Error  ( ' Parameter   keepdims  is  now  at  index  3:  (input,                                                      axis= None ,  ddof=0,  keepdims= False ,  corrected= False )' ) ret = sqrt ( var ( input = input , axis = axis , ddof = ddof , keepdims = keepdims , corrected = corrected ) ) ret . name = 'std' return ret 
def  make seqfeature ( name , from res , to res , description , ft id ) : loc =  Seq  Feature  .  Feature  Location  (  make position ( from res , ( - 1 ) ) ,  make position ( to res , 0 ) ) if ( not ft id ) : ft id = '<unknown  id>' return  Seq  Feature  .  Seq  Feature  ( loc , type = name , id = ft id , qualifiers = { 'description' : description } ) 
def  get config value ( key , defkey , path =  None  ) : value =  parse environ ( key ) if ( value is  None  ) : value =  parse environ ( defkey ) if ( value is not  None  ) : return value return ( os . path . exists ( path ) if ( path is not  None  ) else  False  ) 
def pytest generate tests ( metafunc ) : test indices = [ 0 ] test funcs = [  Test  Funcs  . func dot reduction mix ,  Test  Funcs  . func dot reduction transpose mix ] test tensor flags = [ 'pos rand' , 'neg rand' , 'rand' ] test tensor dims = [ ( 2 , 2 ) ] if ( 'custom args' in metafunc . fixturenames ) : fargs = itertools . product ( test indices , test funcs , test tensor flags , test tensor dims ) metafunc . parametrize ( 'custom args' , fargs ) 
def tokey ( * args ) : salt = u'||' . join ( [ force text ( arg ) for arg in args ] ) hash  = hashlib . md5 ( encode ( salt ) ) return hash  . hexdigest ( ) 
def floating ip get pools ( context ) : return IMPL . floating ip get pools ( context ) 
def read zlib chunks ( read some , unpacked , include comp =  False  , buffer size =  ZLIB BUFSIZE ) : if ( unpacked . decomp len <= ( - 1 ) ) : raise  Value  Error  ( 'non-negative  zlib  data  stream  size  expected' ) decomp obj = zlib . decompressobj ( ) comp chunks = [ ] decomp chunks = unpacked . decomp chunks decomp len = 0 crc32 = unpacked . crc32 while  True  : add = read some ( buffer size ) if ( not add ) : raise zlib . error ( 'EOF  before  end  of  zlib  stream' ) comp chunks . append ( add ) decomp = decomp obj . decompress ( add ) decomp len += len ( decomp ) decomp chunks . append ( decomp ) unused = decomp obj . unused data if unused : left = len ( unused ) if ( crc32 is not  None  ) : crc32 = binascii . crc32 ( add [ : ( - left ) ] , crc32 ) if include comp : comp chunks [ ( - 1 ) ] = add [ : ( - left ) ] break elif ( crc32 is not  None  ) : crc32 = binascii . crc32 ( add , crc32 ) if ( crc32 is not  None  ) : crc32 &= 4294967295 if ( decomp len != unpacked . decomp len ) : raise zlib . error ( 'decompressed  data  does  not  match  expected  size' ) unpacked . crc32 = crc32 if include comp : unpacked . comp chunks = comp chunks return unused 
def STDERR ( v ) : context . log console = sys . stderr 
@ with setup ( prepare stdout , registry . clear ) def test xunit output with no errors ( ) : called = [ ] def assert correct xml ( filename , content ) : called . append (  True  ) assert xsd valid ( filename , content ) root = etree . fromstring ( content ) assert equals ( root . get ( 'tests' ) , '1' ) assert equals ( len ( root . getchildren ( ) ) , 1 ) assert equals ( root . find ( 'testcase' ) . get ( 'name' ) , ' Given   I  do  nothing' ) assert true ( ( float ( root . find ( 'testcase' ) . get ( 'time' ) ) > 0 ) ) old = xunit output . wrt output xunit output . wrt output = assert correct xml runner =  Runner  ( feature name ( 'commented feature' ) , enable xunit =  True  ) runner . run ( ) assert equals ( 1 , len ( called ) , ' Function   not  called' ) xunit output . wrt output = old 
def  nested variable ( init , name =  None  , trainable =  False  ) : if ( isinstance ( init , list ) or isinstance ( init , tuple ) ) : result = [  nested variable ( i , name , trainable ) for i in init ] if isinstance ( init , tuple ) : return tuple ( result ) return result else : return tf .  Variable  ( init , name = name , trainable = trainable ) 
def build Libraries  ( ) : print '' print ' Building   required  libraries' print '' universal = os . path . join ( WORKDIR , 'libraries' ) os . mkdir ( universal ) os . makedirs ( os . path . join ( universal , 'usr' , 'local' , 'lib' ) ) os . makedirs ( os . path . join ( universal , 'usr' , 'local' , 'include' ) ) for recipe in library recipes ( ) : build Recipe  ( recipe , universal , ARCHLIST ) 
def get status ( ) : nodes = [ ] for line in check output ( [ NODE TOOL , 'status' , KEYSPACE ] ) . splitlines ( ) : fields = line . split ( ) if ( len ( fields ) != 8 ) : continue nodes . append ( { 'state' : fields [ 0 ] , 'ip' : fields [ 1 ] , 'tokens' : int ( fields [ 4 ] ) , 'owns' : float ( fields [ 5 ] [ : ( - 1 ) ] ) } ) return nodes 
def total physical memory ( ) : try : return psutil . virtual memory ( ) . total except  Attribute  Error  : return psutil . TOTAL PHYMEM 
def interpret in shape ( xshape ) : if isinstance ( xshape , int ) : ( nin , nsteps ) = ( xshape , 1 ) elif ( len ( xshape ) == 2 ) : ( nin , nsteps ) = xshape else : nin = np . prod ( xshape [ : ( - 1 ) ] ) nsteps = xshape [ ( - 1 ) ] return ( nin , nsteps ) 
@ printing func def isolate resources ( resources ) : return  Classpath  ( creator = u'isolate resources' ) 
def  lenient lowercase ( lst ) : lowered = [ ] for value in lst : try : lowered . append ( value . lower ( ) ) except  Attribute  Error  : lowered . append ( value ) return lowered 
def test xbm ( h , f ) : s = '#define  ' if ( h [ : len ( s ) ] == s ) : return 'xbm' 
def get Plugins  ( interface , package =  None  ) : if ( package is  None  ) : import twisted . plugins as package all Dropins  = get Cache  ( package ) for ( key , dropin ) in iteritems ( all Dropins  ) : for plugin in dropin . plugins : try : adapted = interface ( plugin ,  None  ) except : log . err ( ) else : if ( adapted is not  None  ) : ( yield adapted ) 
def get model field ( model , field name ) : fields = get field parts ( model , field name ) return ( fields [ ( - 1 ) ] if fields else  None  ) 
def  embed bpython shell ( namespace = { } , banner = '' ) : import bpython @ wraps (  embed bpython shell ) def wrapper ( namespace = namespace , banner = '' ) : bpython . embed ( locals  = namespace , banner = banner ) return wrapper 
def check status ( status , api ) : if ( status == 'REQUEST DENIED' ) : return ( ( ' The   ' + api ) + '  API  is  off  in  the   Google    Developers    Console .' ) elif ( status == 'ZERO RESULTS' ) : return ' No   results  found.' elif ( status == 'OVER QUERY LIMIT' ) : return ( ( ' The   ' + api ) + '  API  quota  has  run  out.' ) elif ( status == 'UNKNOWN ERROR' ) : return ' Unknown    Error .' elif ( status == 'INVALID REQUEST' ) : return ' Invalid    Request .' elif ( status == 'OK' ) : return  None  else : return ' Unknown    Demons .' 
def generic repr ( obj , additional kw = ( ) , to inspect =  None  ) : if ( to inspect is  None  ) : to inspect = [ obj ] else : to inspect =  collections . to list ( to inspect ) missing = object ( ) pos args = [ ] kw args =  collections .  Ordered  Dict  ( ) vargs =  None  for ( i , insp ) in enumerate ( to inspect ) : try : (  args ,  vargs , vkw , defaults ) = inspect . getargspec ( insp .   init   ) except  Type  Error  : continue else : default len = ( ( defaults and len ( defaults ) ) or 0 ) if ( i == 0 ) : if  vargs : vargs =  vargs if default len : pos args . extend (  args [ 1 : ( - default len ) ] ) else : pos args . extend (  args [ 1 : ] ) else : kw args . update ( [ ( arg , missing ) for arg in  args [ 1 : ( - default len ) ] ] ) if default len : kw args . update ( [ ( arg , default ) for ( arg , default ) in zip (  args [ ( - default len ) : ] , defaults ) ] ) output = [ ] output . extend ( ( repr ( getattr ( obj , arg ,  None  ) ) for arg in pos args ) ) if ( ( vargs is not  None  ) and hasattr ( obj , vargs ) ) : output . extend ( [ repr ( val ) for val in getattr ( obj , vargs ) ] ) for ( arg , defval ) in kw args . items ( ) : try : val = getattr ( obj , arg , missing ) if ( ( val is not missing ) and ( val != defval ) ) : output . append ( ( '%s=%r' % ( arg , val ) ) ) except : pass if additional kw : for ( arg , defval ) in additional kw : try : val = getattr ( obj , arg , missing ) if ( ( val is not missing ) and ( val != defval ) ) : output . append ( ( '%s=%r' % ( arg , val ) ) ) except : pass return ( '%s(%s)' % ( obj .   class   .   name   , ',  ' . join ( output ) ) ) 
def get installed distributions ( local only =  True  , skip = stdlib pkgs , include editables =  True  , editables only =  False  , user only =  False  ) : if local only : local test = dist is local else : def local test ( d ) : return  True  if include editables : def editable test ( d ) : return  True  else : def editable test ( d ) : return ( not dist is editable ( d ) ) if editables only : def editables only test ( d ) : return dist is editable ( d ) else : def editables only test ( d ) : return  True  if user only : user test = dist in usersite else : def user test ( d ) : return  True  return [ d for d in pkg resources . working set if ( local test ( d ) and ( d . key not in skip ) and editable test ( d ) and editables only test ( d ) and user test ( d ) ) ] 
def render to kml ( * args , ** kwargs ) : return  Http  Response  ( loader . render to string ( * args , ** kwargs ) , content type = 'application/vnd.google-earth.kml+xml' ) 
def lambda notation ( tokens , local dict , global dict ) : result = [ ] flag =  False  ( toknum , tokval ) = tokens [ 0 ] tok Len  = len ( tokens ) if ( ( toknum == NAME ) and ( tokval == 'lambda' ) ) : if ( tok Len  == 2 ) : result . extend ( tokens ) elif ( tok Len  > 2 ) : result . extend ( [ ( NAME , ' Lambda ' ) , ( OP , '(' ) , ( OP , '(' ) , ( OP , ')' ) , ( OP , ')' ) ] ) for ( tok Num  , tok Val  ) in tokens [ 1 : ] : if ( ( tok Num  == OP ) and ( tok Val  == ':' ) ) : tok Val  = ',' flag =  True  if ( ( not flag ) and ( tok Num  == OP ) and ( tok Val  in [ '*' , '**' ] ) ) : raise  Token  Error  ( ' Starred   arguments  in  lambda  not  supported' ) if flag : result . insert ( ( - 1 ) , ( tok Num  , tok Val  ) ) else : result . insert ( ( - 2 ) , ( tok Num  , tok Val  ) ) else : result . extend ( tokens ) return result 
def get distribution id ( vm  ) : distributions =  query ( 'avail' , 'distributions' ) [ 'DATA' ] vm image name = config . get cloud config value ( 'image' , vm  ,   opts   ) distro id = '' for distro in distributions : if ( vm image name == distro [ 'LABEL' ] ) : distro id = distro [ 'DISTRIBUTIONID' ] return distro id if ( not distro id ) : raise  Salt  Cloud  Not  Found  ( " The    Distribution ID  for  the  '{0}'  profile  could  not  be  found.\n The   '{1}'  instance  could  not  be  provisioned.   The   following  distributions  are  available:\n{2}" . format ( vm image name , vm  [ 'name' ] , pprint . pprint ( sorted ( [ distro [ 'LABEL' ] . encode (   salt system encoding   ) for distro in distributions ] ) ) ) ) 
def put assign to buffer ( lhs cname , rhs cname , buf entry , is initialized , pos , code ) : ( buffer aux , buffer type ) = ( buf entry . buffer aux , buf entry . type ) code . globalstate . use utility code ( acquire utility code ) pybuffernd struct = buffer aux . buflocal nd var . cname flags = get flags ( buffer aux , buffer type ) code . putln ( '{' ) code . putln ( ( '   Pyx   Buf  Fmt   Stack  Elem     pyx stack[%d];' % buffer type . dtype . struct nesting depth ( ) ) ) getbuffer = get getbuffer call ( code , '%s' , buffer aux , buffer type ) if is initialized : code . putln ( ( '   Pyx   Safe  Release  Buffer (&%s.rcbuffer->pybuffer);' % pybuffernd struct ) ) retcode cname = code . funcstate . allocate temp (  Pyrex  Types  . c int type , manage ref =  False  ) code . putln ( ( '%s  =  %s;' % ( retcode cname , ( getbuffer % rhs cname ) ) ) ) code . putln ( ( 'if  (%s)  {' % code . unlikely ( ( '%s  <  0' % retcode cname ) ) ) ) ( type , value , tb ) = [ code . funcstate . allocate temp (  Pyrex  Types  . py object type , manage ref =  False  ) for i in range ( 3 ) ] code . putln ( ( ' Py  Err   Fetch (&%s,  &%s,  &%s);' % ( type , value , tb ) ) ) code . putln ( ( 'if  (%s)  {' % code . unlikely ( ( '%s  ==  -1' % ( getbuffer % lhs cname ) ) ) ) ) code . putln ( ( ' Py  XDECREF(%s);   Py  XDECREF(%s);   Py  XDECREF(%s);' % ( type , value , tb ) ) ) code . globalstate . use utility code ( raise buffer fallback code ) code . putln ( '   Pyx   Raise  Buffer  Fallback  Error ();' ) code . putln ( '}  else  {' ) code . putln ( ( ' Py  Err   Restore (%s,  %s,  %s);' % ( type , value , tb ) ) ) for t in ( type , value , tb ) : code . funcstate . release temp ( t ) code . putln ( '}' ) code . putln ( '}' ) put unpack buffer aux into scope ( buf entry , code ) code . putln ( code . error goto if neg ( retcode cname , pos ) ) code . funcstate . release temp ( retcode cname ) else : code . putln ( ( 'if  (%s)  {' % code . unlikely ( ( '%s  ==  -1' % ( getbuffer % rhs cname ) ) ) ) ) code . putln ( ( '%s  =  %s;     Pyx  INCREF( Py   None );  %s.rcbuffer->pybuffer.buf  =  NULL;' % ( lhs cname ,  Pyrex  Types  . typecast ( buffer type ,  Pyrex  Types  . py object type , ' Py   None ' ) , pybuffernd struct ) ) ) code . putln ( code . error goto ( pos ) ) code . put ( '}  else  {' ) put unpack buffer aux into scope ( buf entry , code ) code . putln ( '}' ) code . putln ( '}' ) 
def message list ( request , slug , topic id , template name = 'groups/messages/message list.html' ) : group = get object or 404 (  Group  , slug = slug , is active =  True  ) topic = get object or 404 (  Group  Topic  , pk = topic id , group = group , is active =  True  ) return render ( request , template name , { 'group' : group , 'topic' : topic , 'message list' : topic . messages . all ( ) } ) 
def assert raises ( exc Class  , callable Obj  , * args , ** kwargs ) : try : callable Obj  ( * args , ** kwargs ) except exc Class  as e : return e else : if hasattr ( exc Class  , '  name  ' ) : exc Name  = exc Class  .   name   else : exc Name  = str ( exc Class  ) raise  Assertion  Error  ( ( '%s  not  raised' % exc Name  ) ) 
def get Module  With  Path  ( path ) : return get Module  With  Directory  Path  ( os . path . dirname ( path ) , os . path . basename ( path ) ) 
def make multilabel classification ( n samples = 100 , n features = 20 , n classes = 5 , n labels = 2 , length = 50 , allow unlabeled =  True  , sparse =  False  , return indicator = 'dense' , return distributions =  False  , random state =  None  ) : generator = check random state ( random state ) p c = generator . rand ( n classes ) p c /= p c . sum ( ) cumulative p c = np . cumsum ( p c ) p w c = generator . rand ( n features , n classes ) p w c /= np . sum ( p w c , axis = 0 ) def sample example ( ) : (   , n classes ) = p w c . shape y size = ( n classes + 1 ) while ( ( ( not allow unlabeled ) and ( y size == 0 ) ) or ( y size > n classes ) ) : y size = generator . poisson ( n labels ) y = set ( ) while ( len ( y ) != y size ) : c = np . searchsorted ( cumulative p c , generator . rand ( ( y size - len ( y ) ) ) ) y . update ( c ) y = list ( y ) n words = 0 while ( n words == 0 ) : n words = generator . poisson ( length ) if ( len ( y ) == 0 ) : words = generator . randint ( n features , size = n words ) return ( words , y ) cumulative p w sample = p w c . take ( y , axis = 1 ) . sum ( axis = 1 ) . cumsum ( ) cumulative p w sample /= cumulative p w sample [ ( - 1 ) ] words = np . searchsorted ( cumulative p w sample , generator . rand ( n words ) ) return ( words , y ) X indices = array . array ( 'i' ) X indptr = array . array ( 'i' , [ 0 ] ) Y = [ ] for i in range ( n samples ) : ( words , y ) = sample example ( ) X indices . extend ( words ) X indptr . append ( len ( X indices ) ) Y . append ( y ) X data = np . ones ( len ( X indices ) , dtype = np . float64 ) X = sp . csr matrix ( ( X data , X indices , X indptr ) , shape = ( n samples , n features ) ) X . sum duplicates ( ) if ( not sparse ) : X = X . toarray ( ) if ( return indicator in (  True  , 'sparse' , 'dense' ) ) : lb =  Multi  Label  Binarizer  ( sparse output = ( return indicator == 'sparse' ) ) Y = lb . fit ( [ range ( n classes ) ] ) . transform ( Y ) elif ( return indicator is not  False  ) : raise  Value  Error  ( "return indicator  must  be  either  'sparse',  'dense'  or   False ." ) if return distributions : return ( X , Y , p c , p w c ) return ( X , Y ) 
def delete pool ( hostname , username , password , name ) : ret = { 'name' : name , 'changes' : { } , 'result' :  False  , 'comment' : '' } if   opts   [ 'test' ] : return  test output ( ret , 'delete' , params = { 'hostname' : hostname , 'username' : username , 'password' : password , 'name' : name } ) existing =   salt   [ 'bigip.list pool' ] ( hostname , username , password , name ) if ( existing [ 'code' ] == 200 ) : deleted =   salt   [ 'bigip.delete pool' ] ( hostname , username , password , name ) if ( deleted [ 'code' ] == 200 ) : ret [ 'result' ] =  True  ret [ 'comment' ] = ' Pool   was  successfully  deleted.' ret [ 'changes' ] [ 'old' ] = existing [ 'content' ] ret [ 'changes' ] [ 'new' ] = { } else : ret =  load result ( deleted , ret ) elif ( existing [ 'code' ] == 404 ) : ret [ 'result' ] =  True  ret [ 'comment' ] = ' This   pool  already  does  not  exist.   No   changes  made.' ret [ 'changes' ] [ 'old' ] = { } ret [ 'changes' ] [ 'new' ] = { } else : ret =  load result ( existing , ret ) return ret 
def reverse timestamp ( dt ) : epoch = datetime . datetime ( 1970 , 1 , 1 ) td = ( dt - epoch ) ts = ( ( td . microseconds + ( ( td . seconds + ( ( td . days * 24 ) * 3600 ) ) * 100000 ) ) / 100000 ) return ( 9223372036854775807 - ts ) 
def tostr ( object , encoding =  None  ) : if isinstance ( object , basestring ) : if ( encoding is  None  ) : return object else : return object . encode ( encoding ) if isinstance ( object , tuple ) : s = [ '(' ] for item in object : if isinstance ( item , basestring ) : s . append ( item ) else : s . append ( tostr ( item ) ) s . append ( ',  ' ) s . append ( ')' ) return '' . join ( s ) if isinstance ( object , list ) : s = [ '[' ] for item in object : if isinstance ( item , basestring ) : s . append ( item ) else : s . append ( tostr ( item ) ) s . append ( ',  ' ) s . append ( ']' ) return '' . join ( s ) if isinstance ( object , dict ) : s = [ '{' ] for item in object . items ( ) : if isinstance ( item [ 0 ] , basestring ) : s . append ( item [ 0 ] ) else : s . append ( tostr ( item [ 0 ] ) ) s . append ( '  =  ' ) if isinstance ( item [ 1 ] , basestring ) : s . append ( item [ 1 ] ) else : s . append ( tostr ( item [ 1 ] ) ) s . append ( ',  ' ) s . append ( '}' ) return '' . join ( s ) try : return unicode ( object ) except : return str ( object ) 
def test all mail missing ( monkeypatch ) : g = get auth handler ( monkeypatch , { 'inbox' : 'INBOX' } ) with pytest . raises (  Gmail  Setting  Error  ) : g . verify account (  Account  Stub  ( ) ) 
def volume attach ( provider , names , ** kwargs ) : client =  get client ( ) info = client . extra action ( provider = provider , names = names , action = 'volume attach' , ** kwargs ) return info 
def xor ( d1 , d2 ) : if ( type ( d1 ) != six . binary type ) : raise  Exception  ( 'invalid  type  {}  for  d1  -  must  be  binary' . format ( type ( d1 ) ) ) if ( type ( d2 ) != six . binary type ) : raise  Exception  ( 'invalid  type  {}  for  d2  -  must  be  binary' . format ( type ( d2 ) ) ) if ( len ( d1 ) != len ( d2 ) ) : raise  Exception  ( 'cannot  XOR  binary  string  of  differing  length  ({}  !=  {})' . format ( len ( d1 ) , len ( d2 ) ) ) d1 = array ( 'B' , d1 ) d2 = array ( 'B' , d2 ) for i in range ( len ( d1 ) ) : d1 [ i ] ^= d2 [ i ] if six . PY3 : return d1 . tobytes ( ) else : return d1 . tostring ( ) 
def rsa encrypt ( data , rsa pub key str ) : key = RSA . import Key  ( rsa pub key str ) cipher = PKCS1 OAEP . new ( key ) encrypted data = cipher . encrypt ( data ) return encrypted data 
@ require context @ require snapshot exists def volume snapshot glance metadata get ( context , snapshot id , session =  None  ) : if ( not session ) : session = get session ( ) return session . query ( models .  Volume  Glance  Metadata  ) . filter by ( snapshot id = snapshot id ) . filter by ( deleted =  False  ) . all ( ) 
def line 2d to 3d ( line , zs = 0 , zdir = u'z' ) : line .   class   =  Line 3D line . set 3d properties ( zs , zdir ) 
def bytes  ( s , encoding = 'latin-1' , errors = 'strict' ) : if isinstance ( s , text type ) : return s . encode ( encoding , errors ) return s 
def row2feature ( row , id field , geometry field ) : feature = { 'type' : ' Feature ' , 'properties' :  copy ( row ) } geometry = feature [ 'properties' ] . pop ( geometry field ) feature [ 'geometry' ] =  loadshape (  unhexlify ( geometry ) ) feature [ 'id' ] = feature [ 'properties' ] . pop ( id field ) return feature 
def get cosine ( vec1 , vec2 ) : return ( numpy . dot ( vec1 , vec2 ) / ( numpy . linalg . norm ( vec1 ) * numpy . linalg . norm ( vec2 ) ) ) 
def fix w602 ( source , aggressive =  True  ) : if ( not aggressive ) : return source return refactor ( source , [ u'raise' ] , ignore = u'with traceback' ) 
def dominant ( expr , n ) : terms =  Add  . make args ( expr . expand ( func =  True  ) ) term0 = terms [ ( - 1 ) ] comp = [ term0 ] for t in terms [ : ( - 1 ) ] : e = ( term0 / t ) . combsimp ( ) l = limit seq ( e , n ) if ( l is S .  Zero  ) : term0 = t comp = [ term0 ] elif ( l is  None  ) : return  None  elif ( l not in [ S .  Infinity  , ( - S .  Infinity  ) ] ) : comp . append ( t ) if ( len ( comp ) > 1 ) : return  None  return term0 
def validate positive scalars ( ** kwargs ) : for ( key , val ) in kwargs . items ( ) : try : if ( val <= 0 ) : raise  Value  Error  ( '{}  must  be  >  0,  got  {}' . format ( key , val ) ) except  Type  Error  : raise exceptions .  Plotly  Error  ( '{}  must  be  a  number,  got  {}' . format ( key , val ) ) 
def  postorder traverse ( root , get children ) : def dfs ( elem ) : for v in get children ( elem ) : for u in dfs ( v ) : ( yield u ) ( yield elem ) for elem in dfs ( root ) : ( yield elem ) 
def split Quoted  ( s ) : out = [ ] quot =  None  phrase =  None  for word in s . split ( ) : if ( phrase is  None  ) : if ( word and ( word [ 0 ] in ( '"' , "'" ) ) ) : quot = word [ 0 ] word = word [ 1 : ] phrase = [ ] if ( phrase is  None  ) : out . append ( word ) elif ( word and ( word [ ( - 1 ) ] == quot ) ) : word = word [ : ( - 1 ) ] phrase . append ( word ) out . append ( '  ' . join ( phrase ) ) phrase =  None  else : phrase . append ( word ) return out 
@ ship . command ( 'shoot' ) @ click . argument ( 'ship' ) @ click . argument ( 'x' , type = float ) @ click . argument ( 'y' , type = float ) def ship shoot ( ship , x , y ) : click . echo ( ( ' Ship   %s  fires  to  %s,%s' % ( ship , x , y ) ) ) 
@ newrelic . agent . function trace ( ) @ block user agents @ require http methods ( [ 'GET' , 'POST' ] ) @ login required @ ratelimit ( key = 'user' , rate = limit banned ip to 0 , block =  True  ) @ process document path @ check readonly @ prevent indexing @ never cache def edit ( request , document slug , document locale , revision id =  None  ) : doc = get object or 404 (  Document  , locale = document locale , slug = document slug ) if ( doc . parent and ( doc . parent . id != doc . id ) ) : return translate ( request , doc . parent . slug , doc . locale , revision id , bypass process document path =  True  ) if revision id : rev = get object or 404 (  Revision  , pk = revision id , document = doc ) else : rev = ( doc . current revision or doc . revisions . order by ( '-created' , '-id' ) [ 0 ] ) slug dict = split slug ( document slug ) rev . slug = slug dict [ 'specific' ] section id = request . GET . get ( 'section' ,  None  ) if ( section id and ( not request . is ajax ( ) ) ) : return  Http  Response  ( ugettext ( ' Sections   may  only  be  edited  inline.' ) ) disclose description = bool ( request . GET . get ( 'opendescription' ) ) doc form = rev form =  None  if doc . allows revision by ( request . user ) : rev form =  Revision  Form  ( request = request , instance = rev , initial = { 'based on' : rev . id , 'current rev' : rev . id , 'comment' : '' } , section id = section id ) if doc . allows editing by ( request . user ) : doc form =  Document  Form  ( initial = document form initial ( doc ) ) show translation parent block = ( ( document locale != settings . WIKI DEFAULT LANGUAGE ) and ( not doc . parent id ) ) if ( request . method == 'GET' ) : if ( not ( rev form or doc form ) ) : raise  Permission  Denied  else : is async submit = request . is ajax ( ) is raw = request . GET . get ( 'raw' ,  False  ) need edit links = request . GET . get ( 'edit links' ,  False  ) parent id = request . POST . get ( 'parent id' , '' ) if ( show translation parent block and parent id ) : try : parent doc = get object or 404 (  Document  , id = parent id ) doc . parent = parent doc except  Document  .  Does  Not  Exist  : pass which form = request . POST . get ( 'form-type' ) if ( which form == 'doc' ) : if doc . allows editing by ( request . user ) : post data = request . POST . copy ( ) post data . update ( { 'locale' : document locale } ) doc form =  Document  Form  ( post data , instance = doc ) if doc form . is valid ( ) : if ( 'slug' in post data ) : post data [ 'slug' ] = u'/' . join ( [ slug dict [ 'parent' ] , post data [ 'slug' ] ] ) doc = doc form . save ( parent =  None  ) return redirect ( urlparams ( doc . get edit url ( ) , opendescription = 1 ) ) disclose description =  True  else : raise  Permission  Denied  elif ( which form == 'rev' ) : if ( not doc . allows revision by ( request . user ) ) : raise  Permission  Denied  else : post data = request . POST . copy ( ) rev form =  Revision  Form  ( request = request , data = post data , is async submit = is async submit , section id = section id ) rev form . instance . document = doc orig rev id = request . POST . get ( 'current rev' ,  False  ) if ( orig rev id is  False  ) : orig rev =  None  else : orig rev =  Revision  . objects . get ( pk = orig rev id ) curr rev = doc . current revision if ( not rev form . is valid ( ) ) : if is async submit : if ( 'current rev' in rev form .  errors ) : rev form . errors [ 'current rev' ] [ 0 ] = mark safe ( rev form . errors [ 'current rev' ] [ 0 ] ) errors = [ rev form . errors [ key ] [ 0 ] for key in rev form . errors . keys ( ) ] data = { 'error' :  True  , 'error message' : errors , 'new revision id' : curr rev . id } return  Json  Response  ( data = data ) return  edit document collision ( request , orig rev , curr rev , is async submit , is raw , rev form , doc form , section id , rev , doc ) if ( is async submit and ( '  all  ' in rev form .  errors ) ) : data = { 'error' :  True  , 'error message' : mark safe ( rev form . errors [ '  all  ' ] [ 0 ] ) , 'new revision id' : curr rev . id } return  Json  Response  ( data = data ) if rev form . is valid ( ) : rev form . save ( doc ) if ( is raw and ( orig rev is not  None  ) and ( curr rev . id != orig rev . id ) ) : response =  Http  Response  ( 'RESET' ) response [ 'X- Frame - Options ' ] = 'SAMEORIGIN' response . status code = 205 return response if is async submit : new rev id = rev . document . revisions . order by ( '-id' ) . first ( ) . id data = { 'error' :  False  , 'new revision id' : new rev id } return  Json  Response  ( data ) if rev form . instance . is approved : view = 'wiki.document' else : view = 'wiki.document revisions' url = reverse ( view , args = [ doc . slug ] , locale = doc . locale ) params = { } if is raw : params [ 'raw' ] = 'true' if need edit links : params [ 'edit links' ] = 'true' if section id : params [ 'section' ] = section id params [ 'document saved' ] = 'true' url = ( '%s?%s' % ( url , urlencode ( params ) ) ) if ( ( not is raw ) and section id ) : url = ( '%s#%s' % ( url , section id ) ) return redirect ( url ) parent path = parent slug = '' if slug dict [ 'parent' ] : parent slug = slug dict [ 'parent' ] if doc . parent topic id : parent doc =  Document  . objects . get ( pk = doc . parent topic id ) parent path = parent doc . get absolute url ( ) parent slug = parent doc . slug context = { 'revision form' : rev form , 'document form' : doc form , 'section id' : section id , 'disclose description' : disclose description , 'parent slug' : parent slug , 'parent path' : parent path , 'revision' : rev , 'document' : doc , 'attachment form' :  Attachment  Revision  Form  ( ) } return render ( request , 'wiki/edit.html' , context ) 
def parse config ( ) : global CONFIG try : for line in open ( 'config/zarp.conf' , 'r' ) . readlines ( ) : if ( ( line [ 0 ] == '#' ) or ( '=' not in line ) or ( len ( line ) < 1 ) ) : continue vals = [ k . strip ( ) . replace ( '\n' , '' ) for k in line . split ( '=' ) ] if ( len ( vals ) == 2 ) : set ( vals [ 0 ] , vals [ 1 ] ) except  Exception  as e : util .  Error  ( e ) 
@ contextmanager def update changed requirements ( ) : reqs path = join ( env . proj path , env . reqs path ) get reqs = ( lambda : run ( ( u'cat  %s' % reqs path ) , show =  False  ) ) old reqs = ( get reqs ( ) if env . reqs path else u'' ) ( yield ) if old reqs : new reqs = get reqs ( ) if ( old reqs == new reqs ) : for req in new reqs . split ( u'\n' ) : if req . startswith ( u'-e' ) : if ( u'@' not in req ) : break elif ( req . strip ( ) and ( not req . startswith ( u'#' ) ) ) : if ( not ( set ( u'>=<' ) & set ( req ) ) ) : break else : return pip ( ( u'-r  %s/%s' % ( env . proj path , env . reqs path ) ) ) 
@  define event def post run cell ( ) : pass 
def survey get All  Sections  For  Series  ( series id ) : table = current . s3db . survey series row = current . db ( ( table . id == series id ) ) . select ( table . template id , limitby = ( 0 , 1 ) ) . first ( ) return survey get All  Sections  For  Template  ( row . template id ) 
def add event ( c , type , payload ) : SQL = '\n                INSERT  INTO  events  (type,  payload)\n                VALUES  (%s,  %s)\n        ' c . run ( SQL , ( type , psycopg2 . extras .  Json  ( payload ) ) ) 
def test disconnect all ( timer ) : func = mock .  Mock  ( ) timer . timeout . connect ( func ) timer . timeout . disconnect ( ) timer . timeout . emit ( ) assert ( not func . called ) 
@ newrelic . agent . function trace ( ) def get threads ( request , course , user info , discussion id =  None  , per page = THREADS PER PAGE ) : default query params = { 'page' : 1 , 'per page' : per page , 'sort key' : 'activity' , 'text' : '' , 'course id' : unicode ( course . id ) , 'user id' : request . user . id , 'context' :  Thread  Context  . COURSE , 'group id' : get group id for comments service ( request , course . id , discussion id ) } if ( discussion id is not  None  ) : default query params [ 'commentable id' ] = discussion id if ( get team ( discussion id ) is not  None  ) : default query params [ 'context' ] =  Thread  Context  . STANDALONE if ( not request . GET . get ( 'sort key' ) ) : default query params [ 'sort key' ] = ( user info . get ( 'default sort key' ) or default query params [ 'sort key' ] ) elif ( request . GET . get ( 'sort key' ) != user info . get ( 'default sort key' ) ) : cc user = cc .  User  . from django user ( request . user ) cc user . default sort key = request . GET . get ( 'sort key' ) cc user . save ( ) query params = merge dict ( default query params , strip none ( extract ( request . GET , [ 'page' , 'sort key' , 'text' , 'commentable ids' , 'flagged' , 'unread' , 'unanswered' ] ) ) ) paginated results = cc .  Thread  . search ( query params ) threads = paginated results . collection if ( discussion id is  None  ) : discussion category ids = set ( utils . get discussion categories ids ( course , request . user ) ) threads = [ thread for thread in threads if ( thread . get ( 'commentable id' ) in discussion category ids ) ] for thread in threads : if ( 'pinned' not in thread ) : thread [ 'pinned' ] =  False  query params [ 'page' ] = paginated results . page query params [ 'num pages' ] = paginated results . num pages query params [ 'corrected text' ] = paginated results . corrected text return ( threads , query params ) 
def  Load  Single  Conf  ( stream ) : return yaml object .  Build  Single  Object  ( YAML Configuration  , stream ) 
def  sanitize url components ( comp list , field ) : if ( len ( comp list ) == 0 ) : return '' elif comp list [ 0 ] . startswith ( '{0}=' . format ( field ) ) : ret = '{0}=XXXXXXXXXX&' . format ( field ) comp list . remove ( comp list [ 0 ] ) return ( ret +  sanitize url components ( comp list , field ) ) else : ret = '{0}&' . format ( comp list [ 0 ] ) comp list . remove ( comp list [ 0 ] ) return ( ret +  sanitize url components ( comp list , field ) ) 
def constructor copy ( obj , cls , ** kw ) : names = get cls kwargs ( cls ) kw . update ( ( ( k , obj .   dict   [ k ] ) for k in names if ( k in obj .   dict   ) ) ) return cls ( ** kw ) 
def check access ( node , auth , action , cas resp ) : permission = permission map . get ( action ,  None  ) if ( permission is  None  ) : raise HTTP Error  ( httplib . BAD REQUEST ) if cas resp : if ( permission == 'read' ) : if node . is public : return  True  required scope = oauth scopes .  Core  Scopes  . NODE FILE READ else : required scope = oauth scopes .  Core  Scopes  . NODE FILE WRITE if ( ( not cas resp . authenticated ) or ( required scope not in oauth scopes . normalize scopes ( cas resp . attributes [ 'access Token  Scope ' ] ) ) ) : raise HTTP Error  ( httplib . FORBIDDEN ) if ( permission == 'read' ) : if node . can view ( auth ) : return  True  if ( node . is registration and node . registered from . can view ( auth ) ) : return  True  if ( ( permission == 'write' ) and node . can edit ( auth ) ) : return  True  if ( ( action == 'copyfrom' ) or ( ( action == 'copyto' ) and node . is registration ) ) : parent = node . parent node while parent : if parent . can edit ( auth ) : return  True  parent = parent . parent node try : prereg schema =  Meta  Schema  . find one ( ( Q ( 'name' , 'eq' , ' Prereg    Challenge ' ) & Q ( 'schema version' , 'eq' , 2 ) ) ) allowed nodes = ( [ node ] + node . parents ) prereg draft registration =  Draft  Registration  . find ( ( Q ( 'branched from' , 'in' , [ n for n in allowed nodes ] ) & Q ( 'registration schema' , 'eq' , prereg schema ) ) ) if ( ( action == 'download' ) and ( auth . user is not  None  ) and ( prereg draft registration . count ( ) > 0 ) and ( settings . PREREG ADMIN TAG in auth . user . system tags ) ) : return  True  except  No  Results  Found  : pass raise HTTP Error  ( ( httplib . FORBIDDEN if auth . user else httplib . UNAUTHORIZED ) ) 
def convert wo prefix ( string ) : factors = { 'K' : 1000 , 'M' : ( 1000 * 1000 ) , 'G' : ( ( 1000 * 1000 ) * 1000 ) , 'T' : ( ( ( 1000 * 1000 ) * 1000 ) * 1000 ) , 'P' : ( ( ( ( 1000 * 1000 ) * 1000 ) * 1000 ) * 1000 ) , 'E' : ( ( ( ( ( 1000 * 1000 ) * 1000 ) * 1000 ) * 1000 ) * 1000 ) } if ( string == '-' ) : return ( - 1 ) for ( f , fm ) in factors . items ( ) : if string . endswith ( f ) : number = float ( string [ : ( - 1 ) ] ) number = ( number * fm ) return long ( number ) return long ( string ) 
@ simple decorator def check login required ( view func ) : def  check ( * args , ** kwargs ) : siteconfig =  Site  Configuration  . objects . get current ( ) if siteconfig . get ( u'auth require sitewide login' ) : return login required ( view func ) ( * args , ** kwargs ) else : return view func ( * args , ** kwargs ) return  check 
def snappy installed ( ) : try : import snappy return  True  except  Import  Error  : return  False  except : logging . exception ( 'failed  to  verify  if  snappy  is  installed' ) return  False  
def save traceback ( app ) : import sphinx import jinja2 import docutils import platform exc = sys . exc info ( ) [ 1 ] if isinstance ( exc ,  Sphinx  Parallel  Error  ) : exc format = ( '( Error   in  parallel  process)\n' + exc . traceback ) else : exc format = traceback . format exc ( ) ( fd , path ) = tempfile . mkstemp ( '.log' , 'sphinx-err-' ) last msgs = '' if ( app is not  None  ) : last msgs = '\n' . join ( ( ( '#      %s' % strip colors ( force decode ( s , 'utf-8' ) ) . strip ( ) ) for s in app . messagelog ) ) os . write ( fd , (  DEBUG HEADER % ( sphinx .   display version   , platform . python version ( ) , platform . python implementation ( ) , docutils .   version   , docutils .   version details   , jinja2 .   version   , last msgs ) ) . encode ( 'utf-8' ) ) if ( app is not  None  ) : for ( extname , extmod ) in iteritems ( app .  extensions ) : modfile = getattr ( extmod , '  file  ' , 'unknown' ) if isinstance ( modfile , bytes ) : modfile = modfile . decode ( fs encoding , 'replace' ) version = app .  extension metadata [ extname ] [ 'version' ] if ( version != 'builtin' ) : os . write ( fd , ( '#      %s  (%s)  from  %s\n' % ( extname , version , modfile ) ) . encode ( 'utf-8' ) ) os . write ( fd , exc format . encode ( 'utf-8' ) ) os . close ( fd ) return path 
def libvlc audio equalizer get preamp ( p equalizer ) : f = (   Cfunctions  . get ( 'libvlc audio equalizer get preamp' ,  None  ) or   Cfunction  ( 'libvlc audio equalizer get preamp' , ( ( 1 , ) , ) ,  None  , ctypes . c float , ctypes . c void p ) ) return f ( p equalizer ) 
def supported locales ( ) : family = distrib family ( ) if ( family == 'debian' ) : return  parse locales ( '/usr/share/i18n/SUPPORTED' ) elif ( family == 'arch' ) : return  parse locales ( '/etc/locale.gen' ) elif ( family == 'redhat' ) : return  supported locales redhat ( ) else : raise  Unsupported  Family  ( supported = [ 'debian' , 'arch' , 'redhat' ] ) 
def require http methods ( request method list ) : def decorator ( func ) : @ wraps ( func , assigned = available attrs ( func ) ) def inner ( request , * args , ** kwargs ) : if ( request . method not in request method list ) : logger . warning ( ' Method    Not    Allowed   (%s):  %s' , request . method , request . path , extra = { 'status code' : 405 , 'request' : request } ) return  Http  Response  Not  Allowed  ( request method list ) return func ( request , * args , ** kwargs ) return inner return decorator 
def perm above ( accessing obj , accessed obj , * args , ** kwargs ) : kwargs [ ' greater than' ] =  True  return perm ( accessing obj , accessed obj , * args , ** kwargs ) 
def with foreground server thread ( startextra = { } ) : def  deco wrapper ( f ) : @ with tmpdir @ wraps ( f ) def wrapper ( self , tmp , * args , ** kwargs ) : th =  None  phase = dict ( ) try : startparams =  start params ( tmp , logtarget = 'INHERITED' , ** startextra ) th =  Thread  ( name = '  Test  Case  Worker ' , target = self .  test Start  Foreground  , args = ( tmp , startparams , phase ) ) th . daemon =  True  th . start ( ) try :  Utils  . wait for ( ( lambda : ( phase . get ( 'start' ,  None  ) is not  None  ) ) , MAX WAITTIME ) self . assert True  ( phase . get ( 'start' ,  None  ) ) self .  wait for srv ( tmp ,  True  , startparams = startparams )  Def  Log  Sys  . info ( '===  within  server:  begin  ===' ) self . prune Log  ( ) return f ( self , tmp , startparams , * args , ** kwargs ) finally :  Def  Log  Sys  . info ( '===  within  server:  end.    ===' ) self . prune Log  ( ) self . exec Success  ( startparams , 'stop' )  Utils  . wait for ( ( lambda : ( phase . get ( 'end' ,  None  ) is not  None  ) ) , MAX WAITTIME ) self . assert True  ( phase . get ( 'end' ,  None  ) ) self . assert Logged  ( ' Shutdown   successful' , ' Exiting    Fail 2ban' ) finally : if th : if phase . get ( 'end' ,  None  ) : th . join ( ) return wrapper return  deco wrapper 
def make msgid ( domain ) : timeval = time . time ( ) utcdate = time . strftime ( '%Y%m%d%H%M%S' , time . gmtime ( timeval ) ) pid = os . getpid ( ) randint = randrange ( 100000 ) msgid = ( '<%s.%s.%s@%s>' % ( utcdate , pid , randint , domain ) ) return msgid 
def thumbnail url ( link ) : if link . has thumbnail : if hasattr ( link , 'thumbnail url' ) : return link . thumbnail url else : return '' else : return '' 
def is library missing ( name ) : ( path , module ) = name . rsplit ( '.' , 1 ) try : package = import module ( path ) return ( not module has submodule ( package , module ) ) except  Import  Error  : return is library missing ( path ) 
def find max occupancy node ( dir list ) : count = 0 number = 0 length = 0 for dirs in dir list : if ( length < len ( dirs ) ) : length = len ( dirs ) number = count count += 1 return number 
@ task def gitrepos ( branch =  None  , fork = 'sympy' ) : with cd ( '/home/vagrant' ) : if ( not exists ( 'sympy-cache.git' ) ) : error ( ' Run   fab  vagrant  prepare  first' ) if ( not branch ) : branch = local ( 'git  rev-parse  --abbrev-ref  HEAD' , capture =  True  ) if ( branch == 'master' ) : raise  Exception  ( ' Cannot   release  from  master' ) run ( 'mkdir  -p  repos' ) with cd ( '/home/vagrant/repos' ) : run ( 'git  clone  --reference  ../sympy-cache.git  https://github.com/{fork}/sympy.git' . format ( fork = fork ) ) with cd ( '/home/vagrant/repos/sympy' ) : run ( ( 'git  checkout  -t  origin/%s' % branch ) ) 
def sphinxify ( docstring , context , buildername = 'html' ) : srcdir = mkdtemp ( ) srcdir = encoding . to unicode from fs ( srcdir ) base name = osp . join ( srcdir , 'docstring' ) rst name = ( base name + '.rst' ) if ( buildername == 'html' ) : suffix = '.html' else : suffix = '.txt' output name = ( base name + suffix ) if ( context [ 'right sphinx version' ] and context [ 'math on' ] ) : docstring = docstring . replace ( '\\\\' , '\\\\\\\\' ) argspec = escape ( context [ 'argspec' ] ) for char in [ '=' , ',' , '(' , ')' , '*' , '**' ] : argspec = argspec . replace ( char , ( ( '<span  class="argspec-highlight">' + char ) + '</span>' ) ) context [ 'argspec' ] = argspec doc file = codecs . open ( rst name , 'w' , encoding = 'utf-8' ) doc file . write ( docstring ) doc file . close ( ) temp confdir =  False  if temp confdir : confdir = mkdtemp ( ) confdir = encoding . to unicode from fs ( confdir ) generate configuration ( confdir ) else : confdir = osp . join ( get module source path ( 'spyder.utils.help' ) ) confoverrides = { 'html context' : context } doctreedir = osp . join ( srcdir , 'doctrees' ) sphinx app =  Sphinx  ( srcdir , confdir , srcdir , doctreedir , buildername , confoverrides , status =  None  , warning =  None  , freshenv =  True  , warningiserror =  False  , tags =  None  ) try : sphinx app . build (  None  , [ rst name ] ) except  System  Message  : output =   ( ' It   was  not  possible  to  generate  rich  text  help  for  this  object.</br> Please   see  it  in  plain  text.' ) return warning ( output ) if osp . exists ( output name ) : output = codecs . open ( output name , 'r' , encoding = 'utf-8' ) . read ( ) output = output . replace ( '<pre>' , '<pre  class="literal-block">' ) else : output =   ( ' It   was  not  possible  to  generate  rich  text  help  for  this  object.</br> Please   see  it  in  plain  text.' ) return warning ( output ) if temp confdir : shutil . rmtree ( confdir , ignore errors =  True  ) shutil . rmtree ( srcdir , ignore errors =  True  ) return output 
def  mdb get database ( uri , ** kwargs ) : if ( not ( 'tz aware' in kwargs ) ) : kwargs [ 'tz aware' ] =  True  connection factory =  Mongo  Client   parsed uri = { } try :  parsed uri = pymongo . uri parser . parse uri ( uri ) except pymongo . errors .  Invalid URI : db name = uri  conn =  Mongo  Client  ( ) pass else : if ( 'replicaset' in  parsed uri [ 'options' ] ) : connection factory =  Mongo  Replica  Set  Client  db name =  parsed uri . get ( 'database' , 'pysaml2' )  conn = connection factory ( uri , ** kwargs )  db =  conn [ db name ] if ( 'username' in  parsed uri ) :  db . authenticate (  parsed uri . get ( 'username' ,  None  ) ,  parsed uri . get ( 'password' ,  None  ) ) return  db 
def save load ( jid , load , minions =  None  ) : serv =  get serv ( ret =  None  ) serv . set ( jid , json . dumps ( load ) )  append list ( serv , 'jids' , jid ) 
def curve ( n turns ) : phi = np . linspace ( 0 , ( 2 * np . pi ) , 2000 ) return [ ( np . cos ( phi ) * ( 1 + ( 0.5 * np . cos ( ( n turns * phi ) ) ) ) ) , ( np . sin ( phi ) * ( 1 + ( 0.5 * np . cos ( ( n turns * phi ) ) ) ) ) , ( 0.5 * np . sin ( ( n turns * phi ) ) ) ] 
@ image comparison ( baseline images = [ u' Event  Collection  plot  add positions' ] ) def test   Event  Collection   add positions ( ) : ( splt , coll , props ) = generate  Event  Collection  plot ( ) new positions = np . hstack ( [ props [ u'positions' ] , props [ u'extra positions' ] [ 0 ] ] ) coll . add positions ( props [ u'extra positions' ] [ 0 ] ) np . testing . assert array equal ( new positions , coll . get positions ( ) ) check segments ( coll , new positions , props [ u'linelength' ] , props [ u'lineoffset' ] , props [ u'orientation' ] ) splt . set title ( u' Event  Collection :  add positions' ) splt . set xlim ( ( - 1 ) , 35 ) 
def test alias args commented ( ) :  ip . magic ( 'alias  commetarg  echo  this  is  %%s  a  commented  out  arg' ) with capture output ( ) as cap :  ip . run cell ( 'commetarg' ) nt . assert equal ( cap . stdout , 'this  is  %s  a  commented  out  arg' ) 
def switch ( name , ip =  None  , netmask =  None  , gateway =  None  , dhcp =  None  , password =  None  , snmp =  None  ) : ret = { 'name' : name , 'result' :  True  , 'changes' : { } , 'comment' : '' } current nic =   salt   [ 'chassis.cmd' ] ( 'network info' , module = name ) try : if ( current nic . get ( 'retcode' , 0 ) != 0 ) : ret [ 'result' ] =  False  ret [ 'comment' ] = current nic [ 'stdout' ] return ret if ( ip or netmask or gateway ) : if ( not ip ) : ip = current nic [ ' Network ' ] [ 'IP   Address ' ] if ( not netmask ) : ip = current nic [ ' Network ' ] [ ' Subnet    Mask ' ] if ( not gateway ) : ip = current nic [ ' Network ' ] [ ' Gateway ' ] if ( ( current nic [ ' Network ' ] [ 'DHCP   Enabled ' ] == '0' ) and dhcp ) : ret [ 'changes' ] . update ( { 'DHCP' : { ' Old ' : { 'DHCP   Enabled ' : current nic [ ' Network ' ] [ 'DHCP   Enabled ' ] } , ' New ' : { 'DHCP   Enabled ' : dhcp } } } ) if ( ( ip or netmask or gateway ) and ( not dhcp ) and ( ( ip != current nic [ ' Network ' ] [ 'IP   Address ' ] ) or ( netmask != current nic [ ' Network ' ] [ ' Subnet    Mask ' ] ) or ( gateway != current nic [ ' Network ' ] [ ' Gateway ' ] ) ) ) : ret [ 'changes' ] . update ( { 'IP' : { ' Old ' : current nic [ ' Network ' ] , ' New ' : { 'IP   Address ' : ip , ' Subnet    Mask ' : netmask , ' Gateway ' : gateway } } } ) if password : if ( ' New ' not in ret [ 'changes' ] ) : ret [ 'changes' ] [ ' New ' ] = { } ret [ 'changes' ] [ ' New ' ] . update ( { ' Password ' : '*****' } ) if snmp : if ( ' New ' not in ret [ 'changes' ] ) : ret [ 'changes' ] [ ' New ' ] = { } ret [ 'changes' ] [ ' New ' ] . update ( { 'SNMP' : '*****' } ) if ( ret [ 'changes' ] == { } ) : ret [ 'comment' ] = ( ( ' Switch   ' + name ) + '  is  already  in  desired  state' ) return ret except  Attribute  Error  : ret [ 'changes' ] = { } ret [ 'comment' ] = ' Something   went  wrong  retrieving  the  switch  details' return ret if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = ( ( ' Switch   ' + name ) + '  configuration  will  change' ) return ret dhcp ret = net ret = password ret = snmp ret =  True  if dhcp : dhcp ret =   salt   [ 'chassis.cmd' ] ( 'set niccfg' , module = name , dhcp = dhcp ) if ( ip or netmask or gateway ) : net ret =   salt   [ 'chassis.cmd' ] ( 'set niccfg' , ip , netmask , gateway , module = name ) if password : password ret =   salt   [ 'chassis.cmd' ] ( 'deploy password' , 'root' , password , module = name ) if snmp : snmp ret =   salt   [ 'chassis.cmd' ] ( 'deploy snmp' , snmp , module = name ) if ( any ( [ password ret , snmp ret , net ret , dhcp ret ] ) is  False  ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' There   was  an  error  setting  the  switch  {0}.' . format ( name ) ret [ 'comment' ] = ' Dell   chassis  switch  {0}  was  updated.' . format ( name ) return ret 
def directory browser ( request , path = '/' ) : directories =  Dojo  File  Store  ( path , dirsonly =  True  , root = request . GET . get ( 'root' , '/' ) ) . items ( ) context = directories content = json . dumps ( context ) return  Http  Response  ( content , content type = 'application/json' ) 
def reboot ( at time =  None  ) : cmd = [ 'shutdown' , '-r' , ( '{0}' . format ( at time ) if at time else 'now' ) ] ret =   salt   [ 'cmd.run' ] ( cmd , python shell =  False  ) return ret 
@ should dump thread stack def start thread stack dump ( ) : dump data every thread ( dump thread stack , DELAY MINUTES , SAVE THREAD PTR ) 
@ login required @ ensure csrf cookie @ require http methods ( ( 'GET' , 'POST' , 'PUT' ) ) @ ensure valid course key def import handler ( request , course key string ) : courselike key =  Course  Key  . from string ( course key string ) library = isinstance ( courselike key ,  Library  Locator  ) if library : root name = LIBRARY ROOT successful url = reverse library url ( 'library handler' , courselike key ) context name = 'context library' courselike module = modulestore ( ) . get library ( courselike key ) import func = import library from xml else : root name = COURSE ROOT successful url = reverse course url ( 'course handler' , courselike key ) context name = 'context course' courselike module = modulestore ( ) . get course ( courselike key ) import func = import course from xml return  import handler ( request , courselike key , root name , successful url , context name , courselike module , import func ) 
def get edit extensions ( ) : edit filetypes = get edit filetypes ( ) return (  get extensions ( edit filetypes ) + [ '' ] ) 
@ requires csrf token def page not found ( request , template name = '404.html' ) : try : template = loader . get template ( template name ) content type =  None  except  Template  Does  Not  Exist  : template =  Template  ( '<h1> Not    Found </h1><p> The   requested  URL  {{  request path  }}  was  not  found  on  this  server.</p>' ) content type = 'text/html' body = template . render (  Request  Context  ( request , { 'request path' : request . path } ) ) return http .  Http  Response  Not  Found  ( body , content type = content type ) 
def dup gf factor ( f , K ) : f = dup convert ( f , K , K . dom ) ( coeff , factors ) = gf factor ( f , K . mod , K . dom ) for ( i , ( f , k ) ) in enumerate ( factors ) : factors [ i ] = ( dup convert ( f , K . dom , K ) , k ) return ( K . convert ( coeff , K . dom ) , factors ) 
def match descriptors ( descriptors1 , descriptors2 , metric =  None  , p = 2 , max distance = np . inf , cross check =  True  ) : if ( descriptors1 . shape [ 1 ] != descriptors2 . shape [ 1 ] ) : raise  Value  Error  ( ' Descriptor   length  must  equal.' ) if ( metric is  None  ) : if np . issubdtype ( descriptors1 . dtype , np . bool ) : metric = 'hamming' else : metric = 'euclidean' distances = cdist ( descriptors1 , descriptors2 , metric = metric , p = p ) indices1 = np . arange ( descriptors1 . shape [ 0 ] ) indices2 = np . argmin ( distances , axis = 1 ) if cross check : matches1 = np . argmin ( distances , axis = 0 ) mask = ( indices1 == matches1 [ indices2 ] ) indices1 = indices1 [ mask ] indices2 = indices2 [ mask ] matches = np . column stack ( ( indices1 , indices2 ) ) if ( max distance < np . inf ) : matches = matches [ ( distances [ ( indices1 , indices2 ) ] < max distance ) ] return matches 
def  get dev port var ( backend , instance =  None  ) : port var = ( 'BACKEND PORT.%s' % str ( backend ) . lower ( ) ) if ( instance is not  None  ) : port var = ( '%s.%d' % ( port var , instance ) ) return port var 
def transform ( shape , func ) : construct = shape .   class   if shape . type . startswith ( ' Multi ' ) : parts = [ transform ( geom , func ) for geom in shape . geoms ] return construct ( parts ) if ( shape . type in ( ' Point ' , ' Line  String ' ) ) : return construct ( map ( func , shape . coords ) ) if ( shape . type == ' Polygon ' ) : exterior = map ( func , shape . exterior . coords ) rings = [ map ( func , ring . coords ) for ring in shape . interiors ] return construct ( exterior , rings ) if ( shape . type == ' Geometry  Collection ' ) : return construct ( ) raise  Value  Error  ( ( ' Unknown   geometry  type,  "%s"' % shape . type ) ) 
def calc column lighting ( x , z , mclevel ) : cur light = 15 y = 127 get block = mclevel . block set block = mclevel . set block get height = mclevel . retrieve heightmap set height = mclevel . set heightmap cur height = get height ( x , z ) height updated =  False  if ( cur height is  None  ) : return  None  light reduction lookup = { 0 : 0 , 20 : 0 , 18 : 1 , 8 : 2 , 79 : 2 } while  True  : block info = get block ( x , y , z , 'BS' ) block light = block info [ 'S' ] block type = block info [ 'B' ] if ( ( not height updated ) and ( block type not in ( 0 , 20 ) ) ) : new height = ( y + 1 ) if ( new height == 128 ) : new height = 127 set height ( x , new height , z ) height updated =  True  if ( ( block light == 0 ) and ( cur light == 0 ) ) : break if ( block light != cur light ) : set block ( x , y , z , { 'S' : cur light } ) if ( block type in light reduction lookup ) : light reduction = light reduction lookup [ block type ] else : light reduction = 16 cur light += ( - light reduction ) if ( cur light < 0 ) : cur light = 0 y += ( - 1 ) if ( y < 0 ) : break 
def sorted score ( scores ) : score lst = [ ( scores [ k ] , k ) for k in scores ] sort lst = sorted ( score lst , reverse =  True  ) return [ ( i [ 1 ] , i [ 0 ] ) for i in sort lst ] 
def rldecode ( data ) : decoded = [ ] i = 0 while ( i < len ( data ) ) : length = ord ( data [ i ] ) if ( length == 128 ) : break if ( ( length >= 0 ) and ( length < 128 ) ) : run = data [ ( i + 1 ) : ( ( i + 1 ) + ( length + 1 ) ) ] decoded . append ( run ) i = ( ( i + 1 ) + ( length + 1 ) ) if ( length > 128 ) : run = ( data [ ( i + 1 ) ] * ( 257 - length ) ) decoded . append ( run ) i = ( ( i + 1 ) + 1 ) return '' . join ( decoded ) 
def clean results ( test results dir , output dir , filter ) : for d in os . listdir ( test results dir ) : if ( filter and ( d in filter ) ) : continue print ( 'looking  at' , d ) test dir = os . path . join ( test results dir , d ) if ( not os . path . isdir ( test dir ) ) : continue base test name = os . path . basename ( test dir ) for file in os . listdir ( test dir ) : if file . endswith ( '.xml' ) : test name = ( ( base test name + '.' ) + file [ : ( - 4 ) ] ) file = os . path . join ( test dir , file ) try : result = junitxml . read ( file , test name ) output path = os . path . join ( output dir , ( '%s.xml' % test name ) ) with open ( output path , 'w' ) as f : print ( 're-writing' , output path ) f . write ( result . xml ( ) . encode ( 'utf-8' ) ) except  Exception  as e : sys . stderr . write ( ( 'ignoring  [%s]:  %s\n' % ( file , e ) ) ) 
def  parse settings eth ( opts , iface type , enabled , iface ) : result = { 'name' : iface } if ( 'proto' in opts ) : valid = [ 'none' , 'bootp' , 'dhcp' ] if ( opts [ 'proto' ] in valid ) : result [ 'proto' ] = opts [ 'proto' ] else :  raise error iface ( iface , opts [ 'proto' ] , valid ) if ( 'dns' in opts ) : result [ 'dns' ] = opts [ 'dns' ] result [ 'peerdns' ] = 'yes' if ( 'mtu' in opts ) : try : result [ 'mtu' ] = int ( opts [ 'mtu' ] ) except  Value  Error  :  raise error iface ( iface , 'mtu' , [ 'integer' ] ) if ( iface type not in [ 'bridge' ] ) : ethtool =  parse ethtool opts ( opts , iface ) if ethtool : result [ 'ethtool' ] = ethtool if ( iface type == 'slave' ) : result [ 'proto' ] = 'none' if ( iface type == 'bond' ) : bonding =  parse settings bond ( opts , iface ) if bonding : result [ 'bonding' ] = bonding result [ 'devtype' ] = ' Bond ' if ( iface type == 'vlan' ) : vlan =  parse settings vlan ( opts , iface ) if vlan : result [ 'devtype' ] = ' Vlan ' for opt in vlan : result [ opt ] = opts [ opt ] if ( iface type not in [ 'bond' , 'vlan' , 'bridge' , 'ipip' ] ) : if ( 'addr' in opts ) : if salt . utils . validate . net . mac ( opts [ 'addr' ] ) : result [ 'addr' ] = opts [ 'addr' ] else :  raise error iface ( iface , opts [ 'addr' ] , [ 'AA:BB:CC:DD:EE:FF' ] ) elif ( iface type != 'slave' ) : ifaces =   salt   [ 'network.interfaces' ] ( ) if ( ( iface in ifaces ) and ( 'hwaddr' in ifaces [ iface ] ) ) : result [ 'addr' ] = ifaces [ iface ] [ 'hwaddr' ] if ( iface type == 'eth' ) : result [ 'devtype' ] = ' Ethernet ' if ( iface type == 'bridge' ) : result [ 'devtype' ] = ' Bridge ' bypassfirewall =  True  valid = (  CONFIG TRUE +  CONFIG FALSE ) for opt in [ 'bypassfirewall' ] : if ( opt in opts ) : if ( opts [ opt ] in  CONFIG TRUE ) : bypassfirewall =  True  elif ( opts [ opt ] in  CONFIG FALSE ) : bypassfirewall =  False  else :  raise error iface ( iface , opts [ opt ] , valid ) if bypassfirewall :   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-ip6tables' , '0' )   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-iptables' , '0' )   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-arptables' , '0' ) else :   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-ip6tables' , '1' )   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-iptables' , '1' )   salt   [ 'sysctl.persist' ] ( 'net.bridge.bridge-nf-call-arptables' , '1' ) elif ( 'bridge' in opts ) : result [ 'bridge' ] = opts [ 'bridge' ] if ( iface type == 'ipip' ) : result [ 'devtype' ] = 'IPIP' for opt in [ 'my inner ipaddr' , 'my outer ipaddr' ] : if ( opt not in opts ) :  raise error iface ( iface , opts [ opt ] , [ '1.2.3.4' ] ) else : result [ opt ] = opts [ opt ] if ( iface type == 'ib' ) : result [ 'devtype' ] = ' Infini  Band ' if ( 'prefix' in opts ) : if ( 'netmask' in opts ) : msg = ' Cannot   use  prefix  and  netmask  together' log . error ( msg ) raise  Attribute  Error  ( msg ) result [ 'prefix' ] = opts [ 'prefix' ] elif ( 'netmask' in opts ) : result [ 'netmask' ] = opts [ 'netmask' ] for opt in [ 'ipaddr' , 'master' , 'srcaddr' , 'delay' , 'domain' , 'gateway' , 'uuid' , 'nickname' , 'zone' ] : if ( opt in opts ) : result [ opt ] = opts [ opt ] for opt in [ 'ipv6addr' , 'ipv6gateway' ] : if ( opt in opts ) : result [ opt ] = opts [ opt ] for opt in [ 'ipaddrs' , 'ipv6addrs' ] : if ( opt in opts ) : result [ opt ] = opts [ opt ] if ( 'enable ipv6' in opts ) : result [ 'enable ipv6' ] = opts [ 'enable ipv6' ] valid = (  CONFIG TRUE +  CONFIG FALSE ) for opt in [ 'onparent' , 'peerdns' , 'peerroutes' , 'slave' , 'vlan' , 'defroute' , 'stp' , 'ipv6 peerdns' , 'ipv6 defroute' , 'ipv6 peerroutes' , 'ipv6 autoconf' , 'ipv4 failure fatal' , 'dhcpv6c' ] : if ( opt in opts ) : if ( opts [ opt ] in  CONFIG TRUE ) : result [ opt ] = 'yes' elif ( opts [ opt ] in  CONFIG FALSE ) : result [ opt ] = 'no' else :  raise error iface ( iface , opts [ opt ] , valid ) if ( 'onboot' in opts ) : log . warning ( " The   'onboot'  option  is  controlled  by  the  'enabled'  option.   Interface :  {0}   Enabled :  {1}" . format ( iface , enabled ) ) if enabled : result [ 'onboot' ] = 'yes' else : result [ 'onboot' ] = 'no' if ( 'userctl' in opts ) : if ( opts [ 'userctl' ] in  CONFIG TRUE ) : result [ 'userctl' ] = 'yes' elif ( opts [ 'userctl' ] in  CONFIG FALSE ) : result [ 'userctl' ] = 'no' else :  raise error iface ( iface , opts [ 'userctl' ] , valid ) else : result [ 'userctl' ] = 'no' if ( 'vlan' in opts ) : if ( opts [ 'vlan' ] in  CONFIG TRUE ) : result [ 'vlan' ] = 'yes' elif ( opts [ 'vlan' ] in  CONFIG FALSE ) : result [ 'vlan' ] = 'no' else :  raise error iface ( iface , opts [ 'vlan' ] , valid ) if ( 'arpcheck' in opts ) : if ( opts [ 'arpcheck' ] in  CONFIG FALSE ) : result [ 'arpcheck' ] = 'no' if ( 'ipaddr start' in opts ) : result [ 'ipaddr start' ] = opts [ 'ipaddr start' ] if ( 'ipaddr end' in opts ) : result [ 'ipaddr end' ] = opts [ 'ipaddr end' ] if ( 'clonenum start' in opts ) : result [ 'clonenum start' ] = opts [ 'clonenum start' ] if ( 'nm controlled' in opts ) : if ( opts [ 'nm controlled' ] in  CONFIG TRUE ) : result [ 'nm controlled' ] = 'yes' elif ( opts [ 'nm controlled' ] in  CONFIG FALSE ) : result [ 'nm controlled' ] = 'no' else :  raise error iface ( iface , opts [ 'nm controlled' ] , valid ) else : result [ 'nm controlled' ] = 'no' return result 
def get current site ( request ) : if apps . is installed ( 'django.contrib.sites' ) : from . models import  Site  return  Site  . objects . get current ( request ) else : from . requests import  Request  Site  return  Request  Site  ( request ) 
def run wsgi ( conf path , app section , * args , ** kwargs ) : try : ( conf , logger , log name ) =  initrp ( conf path , app section , * args , ** kwargs ) except  Config  File  Error  as e : print ( e ) return 1 utils . modify priority ( conf , logger ) servers per port = int ( ( conf . get ( 'servers per port' , '0' ) or 0 ) ) if ( servers per port and ( app section == 'object-server' ) ) : strategy =  Servers  Per  Port  Strategy  ( conf , logger , servers per port = servers per port ) else : strategy =  Workers  Strategy  ( conf , logger ) error msg = strategy . bind ports ( ) if error msg : logger . error ( error msg ) print ( error msg ) return 1 global conf = { 'log name' : log name } if ( 'global conf callback' in kwargs ) : kwargs [ 'global conf callback' ] ( conf , global conf ) loadapp ( conf path , global conf = global conf ) ( utils . FALLOCATE RESERVE , utils . FALLOCATE IS PERCENT ) = utils . config fallocate value ( conf . get ( 'fallocate reserve' , '1%' ) ) capture stdio ( logger ) no fork sock = strategy . no fork sock ( ) if no fork sock : run server ( conf , logger , no fork sock , global conf = global conf ) return 0 def kill children ( * args ) : ' Kills   the  entire  process  group.' logger . error ( 'SIGTERM  received' ) signal . signal ( signal . SIGTERM , signal . SIG IGN ) running [ 0 ] =  False  os . killpg ( 0 , signal . SIGTERM ) def hup ( * args ) : ' Shuts   down  the  server,  but  allows  running  requests  to  complete' logger . error ( 'SIGHUP  received' ) signal . signal ( signal . SIGHUP , signal . SIG IGN ) running [ 0 ] =  False  running = [  True  ] signal . signal ( signal . SIGTERM , kill children ) signal . signal ( signal . SIGHUP , hup ) while running [ 0 ] : for ( sock , sock info ) in strategy . new worker socks ( ) : pid = os . fork ( ) if ( pid == 0 ) : signal . signal ( signal . SIGHUP , signal . SIG DFL ) signal . signal ( signal . SIGTERM , signal . SIG DFL ) strategy . post fork hook ( ) run server ( conf , logger , sock ) strategy . log sock exit ( sock , sock info ) return 0 else : strategy . register worker start ( sock , sock info , pid ) loop timeout = strategy . loop timeout ( ) with  Timeout  ( loop timeout , exception =  False  ) : try : try : ( pid , status ) = green os . wait ( ) if ( os . WIFEXITED ( status ) or os . WIFSIGNALED ( status ) ) : strategy . register worker exit ( pid ) except OS Error  as err : if ( err . errno not in ( errno . EINTR , errno . ECHILD ) ) : raise if ( err . errno == errno . ECHILD ) : sleep ( 0.01 ) except  Keyboard  Interrupt  : logger . notice ( ' User   quit' ) running [ 0 ] =  False  break strategy . shutdown sockets ( ) logger . notice ( ' Exited ' ) return 0 
def ismount ( path ) : if islink ( path ) : return  False  try : s1 = os . lstat ( path ) s2 = os . lstat ( join ( path , '..' ) ) except os . error : return  False  dev1 = s1 . st dev dev2 = s2 . st dev if ( dev1 != dev2 ) : return  True  ino1 = s1 . st ino ino2 = s2 . st ino if ( ino1 == ino2 ) : return  True  return  False  
def filter entity ( entity ref ) : if entity ref : entity ref . pop ( 'dn' ,  None  ) return entity ref 
def config from file ( filename , config =  None  ) : if config : try : with open ( filename , 'w' ) as fdesc : fdesc . write ( json . dumps ( config ) ) except IO Error  as error :  LOGGER . error ( ' Saving   configuration  file  failed:  %s' , error ) return  False  return  True  elif os . path . isfile ( filename ) : try : with open ( filename , 'r' ) as fdesc : return json . loads ( fdesc . read ( ) ) except IO Error  as error :  LOGGER . error ( ' Reading   config  file  failed:  %s' , error ) return  False  else : return { } 
def unesc ( s , esc chars ) : for c in esc chars : esc str = ( u'\\' + c ) s = s . replace ( esc str , c ) return s 
def strip entities ( value ) : return re . sub ( '&(?:\\w+|#\\d+);' , '' , force unicode ( value ) ) 
def import dashboard config ( modules ) : config = collections . defaultdict ( dict ) for module in modules : for ( key , submodule ) in import submodules ( module ) . items ( ) : if hasattr ( submodule , 'DASHBOARD' ) : dashboard = submodule . DASHBOARD config [ dashboard ] . update ( submodule .   dict   ) elif ( hasattr ( submodule , 'PANEL' ) or hasattr ( submodule , 'PANEL GROUP' ) or hasattr ( submodule , 'FEATURE' ) ) : config [ submodule .   name   ] = submodule .   dict   else : logging . warning ( " Skipping   %s  because  it  doesn't  have  DASHBOARD,  PANEL,  PANEL GROUP,  or  FEATURE  defined." , submodule .   name   ) return sorted ( config . items ( ) , key = ( lambda c : c [ 1 ] [ '  name  ' ] . rsplit ( '.' , 1 ) [ 1 ] ) ) 
def  get milestones stats for backlog ( project , milestones ) : current evolution = 0 current team increment = 0 current client increment = 0 optimal points per sprint = 0 optimal points = 0 team increment = 0 client increment = 0 total story points = ( project . total story points if ( project . total story points not in [  None  , 0 ] ) else project .  defined points ) total milestones = ( project . total milestones if ( project . total milestones not in [  None  , 0 ] ) else len ( milestones ) ) if ( total story points and total milestones ) : optimal points per sprint = ( total story points / total milestones ) milestones count = len ( milestones ) milestones stats = [ ] for current milestone pos in range ( 0 , max ( milestones count , total milestones ) ) : optimal points = ( total story points - ( optimal points per sprint * current milestone pos ) ) evolution = ( ( total story points - current evolution ) if ( current evolution is not  None  ) else  None  ) if ( current milestone pos < milestones count ) : current milestone = list ( milestones . values ( ) ) [ current milestone pos ] milestone name = current milestone . name team increment = current team increment client increment = current client increment current evolution += current milestone .  closed points current team increment += current milestone .  team increment points current client increment += current milestone .  client increment points else : milestone name =   ( ' Future   sprint' ) current team increment += project .  future team increment current client increment += project .  future client increment team increment = current team increment client increment = current client increment current evolution =  None  milestones stats . append ( { 'name' : milestone name , 'optimal' : optimal points , 'evolution' : evolution , 'team-increment' : team increment , 'client-increment' : client increment } ) optimal points -= optimal points per sprint evolution = ( ( total story points - current evolution ) if ( ( current evolution is not  None  ) and total story points ) else  None  ) milestones stats . append ( { 'name' :   ( ' Project    End ' ) , 'optimal' : optimal points , 'evolution' : evolution , 'team-increment' : current team increment , 'client-increment' : current client increment } ) return milestones stats 
def fetch 20newsgroups vectorized ( subset = 'train' , remove = ( ) , data home =  None  ) : data home = get data home ( data home = data home ) filebase = '20newsgroup vectorized' if remove : filebase += ( 'remove-' + '-' . join ( remove ) ) target file =  pkl filepath ( data home , ( filebase + '.pkl' ) ) data train = fetch 20newsgroups ( data home = data home , subset = 'train' , categories =  None  , shuffle =  True  , random state = 12 , remove = remove ) data test = fetch 20newsgroups ( data home = data home , subset = 'test' , categories =  None  , shuffle =  True  , random state = 12 , remove = remove ) if os . path . exists ( target file ) : ( X train , X test ) = joblib . load ( target file ) else : vectorizer =  Count  Vectorizer  ( dtype = np . int16 ) X train = vectorizer . fit transform ( data train . data ) . tocsr ( ) X test = vectorizer . transform ( data test . data ) . tocsr ( ) joblib . dump ( ( X train , X test ) , target file , compress = 9 ) X train = X train . astype ( np . float64 ) X test = X test . astype ( np . float64 ) normalize ( X train , copy =  False  ) normalize ( X test , copy =  False  ) target names = data train . target names if ( subset == 'train' ) : data = X train target = data train . target elif ( subset == 'test' ) : data = X test target = data test . target elif ( subset == 'all' ) : data = sp . vstack ( ( X train , X test ) ) . tocsr ( ) target = np . concatenate ( ( data train . target , data test . target ) ) else : raise  Value  Error  ( ( "%r  is  not  a  valid  subset:  should  be  one  of  ['train',  'test',  'all']" % subset ) ) return  Bunch  ( data = data , target = target , target names = target names ) 
def folder exists ( folder Name  ) : return os . path . isdir ( folder Name  ) 
def  nt quote args ( args ) : for ( i , arg ) in enumerate ( args ) : if ( '  ' in arg ) : args [ i ] = ( '"%s"' % arg ) return args 
def  dec from triple ( sign , coefficient , exponent , special =  False  ) : self = object .   new   (  Decimal  ) self .  sign = sign self .  int = coefficient self .  exp = exponent self .  is special = special return self 
def fft freqs ( n fft , fs ) : return ( ( np . arange ( 0 , ( ( n fft // 2 ) + 1 ) ) / float ( n fft ) ) * float ( fs ) ) 
def mask or ( clip , other clip ) : if isinstance ( other clip ,  Image  Clip  ) : other clip = other clip . img if isinstance ( other clip , np . ndarray ) : return clip . fl image ( ( lambda f : np . maximum ( f , other clip ) ) ) else : return clip . fl ( ( lambda gf , t : np . maximum ( gf ( t ) , other clip . get frame ( t ) ) ) ) 
def password present ( name , password ) : ret = { 'name' : name , 'result' :  True  , 'changes' : { 'old' : 'unknown' , 'new' : '********' } , 'comment' : ' Host   password  was  updated.' } esxi cmd = 'esxi.cmd' if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = ' Host   password  will  change.' return ret else : try :   salt   [ esxi cmd ] ( 'update host password' , new password = password ) except  Command  Execution  Error  as err : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Error :  {0}' . format ( err ) return ret return ret 
def  compute multivariate sample acovf ( endog , maxlag ) : endog = np . array ( endog ) if ( endog . ndim == 1 ) : endog = endog [ : , np . newaxis ] endog -= np . mean ( endog , axis = 0 ) ( nobs , k endog ) = endog . shape sample autocovariances = [ ] for s in range ( ( maxlag + 1 ) ) : sample autocovariances . append ( np . zeros ( ( k endog , k endog ) ) ) for t in range ( ( nobs - s ) ) : sample autocovariances [ s ] += np . outer ( endog [ t ] , endog [ ( t + s ) ] ) sample autocovariances [ s ] /= nobs return sample autocovariances 
def  index list ( key or list , direction =  None  ) : if ( direction is not  None  ) : return [ ( key or list , direction ) ] else : if isinstance ( key or list , string type ) : return [ ( key or list , ASCENDING ) ] elif ( not isinstance ( key or list , ( list , tuple ) ) ) : raise  Type  Error  ( 'if  no  direction  is  specified,  key or list  must  be  an  instance  of  list' ) return key or list 
def normalize col name ( col name , used column names , is relation ) : field params = { } field notes = [ ] new name = col name . lower ( ) if ( new name != col name ) : field notes . append ( u' Field   name  made  lowercase.' ) if is relation : if new name . endswith ( u' id' ) : new name = new name [ : ( - 3 ) ] else : field params [ u'db column' ] = col name ( new name , num repl ) = re . subn ( u'\\W' , u' ' , new name ) if ( num repl > 0 ) : field notes . append ( u' Field   renamed  to  remove  unsuitable  characters.' ) if ( new name . find ( u'  ' ) >= 0 ) : while ( new name . find ( u'  ' ) >= 0 ) : new name = new name . replace ( u'  ' , u' ' ) if ( col name . lower ( ) . find ( u'  ' ) >= 0 ) : field notes . append ( u" Field   renamed  because  it  contained  more  than  one  ' '  in  a  row." ) if new name . startswith ( u' ' ) : new name = ( u'field%s' % new name ) field notes . append ( u" Field   renamed  because  it  started  with  ' '." ) if new name . endswith ( u' ' ) : new name = ( u'%sfield' % new name ) field notes . append ( u" Field   renamed  because  it  ended  with  ' '." ) if keyword . iskeyword ( new name ) : new name += u' field' field notes . append ( u' Field   renamed  because  it  was  a   Python   reserved  word.' ) if new name [ 0 ] . isdigit ( ) : new name = ( u'number %s' % new name ) field notes . append ( u" Field   renamed  because  it  wasn't  a  valid   Python   identifier." ) if ( new name in used column names ) : num = 0 while ( ( u'%s %d' % ( new name , num ) ) in used column names ) : num += 1 new name = ( u'%s %d' % ( new name , num ) ) field notes . append ( u' Field   renamed  because  of  name  conflict.' ) if ( ( col name != new name ) and field notes ) : field params [ u'db column' ] = col name return ( new name , field params , field notes ) 
def is numlike ( obj ) : try : ( obj + 1 ) except : return  False  else : return  True  
def write Output  ( file Name  , should Analyze  =  True  ) : skeinforge craft . write Chain  Text  With  Noun  Message  ( file Name  , 'wipe' , should Analyze  ) 
def   virtual   ( ) : if HAS KEYSTONE : return 'keystone' return (  False  , 'keystone  execution  module  cannot  be  loaded:  keystoneclient  python  library  not  available.' ) 
def thing type present ( name , thing Type  Name  , thing Type  Description  , searchable Attributes  List  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : ret = { 'name' : thing Type  Name  , 'result' :  True  , 'comment' : '' , 'changes' : { } } r =   salt   [ 'boto iot.thing type exists' ] ( thing Type  Name  = thing Type  Name  , region = region , key = key , keyid = keyid , profile = profile ) if ( 'error' in r ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  create  thing  type:  {0}.' . format ( r [ 'error' ] [ 'message' ] ) return ret if r . get ( 'exists' ) : ret [ 'result' ] =  True  ret [ 'comment' ] = ' Thing   type  with  given  name  {0}  already  exists' . format ( thing Type  Name  ) return ret if   opts   [ 'test' ] : ret [ 'comment' ] = ' Thing   type  {0}  is  set  to  be  created.' . format ( thing Type  Name  ) ret [ 'result' ] =  None  return ret r =   salt   [ 'boto iot.create thing type' ] ( thing Type  Name  = thing Type  Name  , thing Type  Description  = thing Type  Description  , searchable Attributes  List  = searchable Attributes  List  , region = region , key = key , keyid = keyid , profile = profile ) if ( not r . get ( 'created' ) ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  create  thing  type:  {0}.' . format ( r [ 'error' ] [ 'message' ] ) return ret  describe =   salt   [ 'boto iot.describe thing type' ] ( thing Type  Name  = thing Type  Name  , region = region , key = key , keyid = keyid , profile = profile ) ret [ 'changes' ] [ 'old' ] = { 'thing type' :  None  } ret [ 'changes' ] [ 'new' ] =  describe ret [ 'comment' ] = ' Thing    Type   {0}  created.' . format ( thing Type  Name  ) return ret 
def get network timezone ( network ) : orig network = network if network : network = network . lower ( ) network tz name = network dict . get ( network ) if ( network and ( not ( network tz name or ( network in missing network timezones ) ) ) ) : missing network timezones . add ( network ) logger . log ( u' Missing   time  zone  for  network:  {0}.   Check   valid  network  is  set  in  indexer  (theTVDB)  before  filing  issue.' . format ( orig network ) , logger . ERROR ) try : network tz = ( ( tz . gettz ( network tz name ) or sb timezone ) if network tz name else sb timezone ) except  Exception  : return sb timezone return network tz 
def test discretize callable 2d ( ) : def f ( x , y ) : return ( ( x ** 2 ) + ( y ** 2 ) ) actual = discretize model ( f , ( ( - 5 ) , 6 ) , ( ( - 5 ) , 6 ) ) ( y , x ) = ( np . indices ( ( 11 , 11 ) ) - 5 ) desired = ( ( x ** 2 ) + ( y ** 2 ) ) assert allclose ( actual , desired ) 
def  Locate  Path  ( file Name  , search Paths  ) : import os return os . path . abspath ( os . path . split (  Locate  File  Name  ( file Name  , search Paths  ) ) [ 0 ] ) 
def  Init  And  Run  ( ) : options . parse command line ( ) def   On  Signal  ( signum , frame ) : logging . info ( ( 'process  stopped  with  signal  %d' % signum ) )  Shutdown  ( ) signal . signal ( signal . SIGHUP ,   On  Signal  ) signal . signal ( signal . SIGINT ,   On  Signal  ) signal . signal ( signal . SIGQUIT ,   On  Signal  ) signal . signal ( signal . SIGTERM ,   On  Signal  ) logging . get Logger  ( ) . set Level  ( logging . INFO ) logging . get Logger  ( ) . handlers [ 0 ] . set Level  ( logging . INFO )  Server  Environment  .  Init  Server  Environment  ( ) watchdog =  Watchdog  (  Scenario  Device  ( 'user1' ) , SCENARIO LIST ) lock file = os . path . expanduser ( LOCK FILE NAME ) if ( not os . path . exists ( os . path . dirname ( lock file ) ) ) : os . makedirs ( os . path . dirname ( lock file ) ) dm = daemon .  Daemon  Manager  ( lock file )  setup error = [  False  ] def   On  Init  Exception  ( t , v , tb ) : logging . error ( ' Exception   during  watchdog  initialization.' , exc info = ( t , v , tb ) )  setup error [ 0 ] =  True   Shutdown  ( ) def   On  Init IO Loop  ( shutdown callback ) : SMS Manager  .  Set  Instance  ( SMS Manager  ( ) ) watchdog .  Run  ( shutdown callback ) def   On  Init  Daemon  ( shutdown callback ) :   Start IO Loop  ( partial (   On  Init IO Loop  , shutdown callback ) ) def   On  Authenticated  ( auth complete ) : if ( not auth complete ) : print ' Waiting   for  user  to  authorize...' cb = partial ( watchdog . device .  Poll  For  Authentication  ,   On  Authenticated  ) ioloop . IO Loop  . current ( ) . add timeout ( ( time . time ( ) + 15 ) , cb ) else : io loop = ioloop . IO Loop  . current ( ) io loop . stop ( ) io loop . close ( ) def   On  Get  User  Code  ( user code , verification url ) : print ( ' Please   visit  url:\n      %s\n  and  input  user  code:\n      %s\n  to  authorize  scenario  login.' % ( verification url , user code ) )   On  Authenticated  (  False  ) def   Init  Watchdog  ( ) : if watchdog . device .  Is  Authenticated  ( ) :   On  Authenticated  (  True  ) else : watchdog . device .  Get  User  Code  (   On  Get  User  Code  ) def   Init  Secrets  ( ) : with util .  Exception  Barrier  (   On  Init  Exception  ) : secrets .  Init  Secrets  (   Init  Watchdog  , can prompt = sys . stderr . isatty ( ) ) if ( options . options . daemon . lower ( ) == 'stop' ) : dm .  Setup  From  Command  Line  ( util .  No  Callback  , util .  No  Callback  ) else :   Start IO Loop  (   Init  Secrets  ) if ( not  setup error [ 0 ] ) : dm .  Setup  From  Command  Line  (   On  Init  Daemon  ,  Shutdown  ) 
def  psd func ( epoch , noverlap , nfft , fs , freq mask , func ) : return func ( epoch , fs = fs , nperseg = nfft , noverlap = noverlap , nfft = nfft , window = 'hann' ) [ 2 ] [ ... , freq mask , : ] 
def  update rs with primary from member ( sds , replica set name , server description ) : assert ( replica set name is not  None  ) if ( replica set name != server description . replica set name ) : sds . pop ( server description . address ) elif ( server description . me and ( server description . address != server description . me ) ) : sds . pop ( server description . address ) return  check has primary ( sds ) 
def compute chunk ( source , target , chunk , chunk expr , parts ) : ( source part , target part ) = parts part = source [ source part ] result = compute ( chunk expr , { chunk : part } , return type = 'native' ) target [ target part ] = result 
def handle clear ( request , basket , ** kwargs ) : basket . clear all ( ) return { u'ok' :  True  } 
def  unsugar count from ( ** kw ) : count from = kw . pop ( 'count from' ,  None  ) if ( ( kw . get ( 'ordering func' ,  None  ) is  None  ) and ( count from is not  None  ) ) : if ( count from == 0 ) : kw [ 'ordering func' ] = count from 0 elif ( count from == 1 ) : kw [ 'ordering func' ] = count from 1 else : kw [ 'ordering func' ] = count from n factory ( count from ) return kw 
def layer from config ( config , custom objects =  None  ) : if custom objects : get custom objects ( ) . update ( custom objects ) class name = config [ 'class name' ] if ( class name == ' Sequential ' ) : layer class =  Sequential  elif ( class name in [ ' Model ' , ' Container ' ] ) : layer class =  Model  else : layer class = get from module ( class name , globals ( ) , 'layer' , instantiate =  False  ) arg spec = inspect . getargspec ( layer class . from config ) if ( 'custom objects' in arg spec . args ) : return layer class . from config ( config [ 'config' ] , custom objects = custom objects ) else : return layer class . from config ( config [ 'config' ] ) 
def existing user fields ( fields ) : user fields = get user model ( ) .  meta . fields user field names = [ field . name for field in user fields ] return [ field for field in fields if ( field in user field names ) ] 
def  get size ( fileno ) : import fcntl import termios buf = array . array ( ( 'h' if six . PY2 else u'h' ) , [ 0 , 0 , 0 , 0 ] ) fcntl . ioctl ( fileno , termios . TIOCGWINSZ , buf ) return ( buf [ 0 ] , buf [ 1 ] ) 
def create image ( module , ec2 ) : instance id = module . params . get ( 'instance id' ) name = module . params . get ( 'name' ) wait = module . params . get ( 'wait' ) wait timeout = int ( module . params . get ( 'wait timeout' ) ) description = module . params . get ( 'description' ) no reboot = module . params . get ( 'no reboot' ) device mapping = module . params . get ( 'device mapping' ) tags = module . params . get ( 'tags' ) launch permissions = module . params . get ( 'launch permissions' ) try : params = { 'instance id' : instance id , 'name' : name , 'description' : description , 'no reboot' : no reboot } if device mapping : bdm =  Block  Device  Mapping  ( ) for device in device mapping : if ( 'device name' not in device ) : module . fail json ( msg = ' Device   name  must  be  set  for  volume' ) device name = device [ 'device name' ] del device [ 'device name' ] bd =  Block  Device  Type  ( ** device ) bdm [ device name ] = bd params [ 'block device mapping' ] = bdm image id = ec2 . create image ( ** params ) except boto . exception .  Boto  Server  Error  as e : if ( e . error code == ' Invalid AMI Name . Duplicate ' ) : images = ec2 . get all images ( ) for img in images : if ( img . name == name ) : module . exit json ( msg = 'AMI  name  already  present' , image id = img . id , state = img . state , changed =  False  ) else : module . fail json ( msg = ' Error   in  retrieving  duplicate  AMI  details' ) else : module . fail json ( msg = ( '%s:  %s' % ( e . error code , e . error message ) ) ) for i in range ( wait timeout ) : try : img = ec2 . get image ( image id ) break except boto . exception . EC2 Response  Error  as e : if ( ( ' Invalid AMIID. Not  Found ' in e . error code ) and wait ) : time . sleep ( 1 ) else : module . fail json ( msg = ' Error   while  trying  to  find  the  new  image.   Using   wait=yes  and/or  a  longer  wait timeout  may  help.' ) else : module . fail json ( msg = 'timed  out  waiting  for  image  to  be  recognized' ) wait timeout = ( time . time ( ) + wait timeout ) while ( wait and ( wait timeout > time . time ( ) ) and ( ( img is  None  ) or ( img . state != 'available' ) ) ) : img = ec2 . get image ( image id ) time . sleep ( 3 ) if ( wait and ( wait timeout <= time . time ( ) ) ) : module . fail json ( msg = 'timed  out  waiting  for  image  to  be  created' ) if tags : try : ec2 . create tags ( image id , tags ) except boto . exception . EC2 Response  Error  as e : module . fail json ( msg = ( ' Image   tagging  failed  =>  %s:  %s' % ( e . error code , e . error message ) ) ) if launch permissions : try : img = ec2 . get image ( image id ) img . set launch permissions ( ** launch permissions ) except boto . exception .  Boto  Server  Error  as e : module . fail json ( msg = ( '%s:  %s' % ( e . error code , e . error message ) ) , image id = image id ) module . exit json ( msg = 'AMI  creation  operation  complete' , image id = image id , state = img . state , changed =  True  ) 
def attach translations ( addons ) : attach trans dict (  Addon  , addons ) 
def validate string ( option , value ) : if isinstance ( value , string type ) : return value raise  Type  Error  ( ( ' Wrong   type  for  %s,  value  must  be  an  instance  of  %s' % ( option , string type .   name   ) ) ) 
def   Get  Blobstore  Metadata  ( blob key ) : try : blob info = datastore .  Get  ( datastore .  Key  . from path ( blobstore . BLOB INFO KIND , blob key , namespace = '' ) ) return ( blob info [ 'size' ] , blob info [ 'content type' ] , blob key ) except datastore errors .  Entity  Not  Found  Error  : return (  None  ,  None  ,  None  ) 
def add Hook  ( hook , func ) : if ( not  hooks . get ( hook ,  None  ) ) :  hooks [ hook ] = [ ] if ( func not in  hooks [ hook ] ) :  hooks [ hook ] . append ( func ) 
def test terrain import exception ( ) : string = ' Lettuce   has  tried  to  load  the  conventional  environment  module  "terrain"\nbut  it  has  errors,  check  its  contents  and  try  to  run  lettuce  again.\n\n Original   traceback  below:\n\n' mox =  Mox  ( ) mox .  Stub  Out  With  Mock  ( lettuce . fs , ' File  System ' ) mox .  Stub  Out  With  Mock  ( lettuce . exceptions , 'traceback' ) mox .  Stub  Out  With  Mock  ( lettuce . sys , 'stderr' ) exc =  Exception  ( 'foo  bar' ) lettuce . fs .  File  System  .  import ( 'terrain' ) .  And  Raise  ( exc ) lettuce . exceptions . traceback . format exc ( exc ) .  And  Return  ( 'I  AM  THE  TRACEBACK  FOR  IMPORT  ERROR' ) lettuce . sys . stderr . write ( string ) lettuce . sys . stderr . write ( 'I  AM  THE  TRACEBACK  FOR  IMPORT  ERROR' ) mox .  Replay  All  ( ) try : reload ( lettuce ) except  Lettuce  Runner  Error  : mox .  Verify  All  ( ) finally : mox .  Unset  Stubs  ( ) 
def to scipy sparse matrix ( G , nodelist =  None  , dtype =  None  , weight = 'weight' , format = 'csr' ) : from scipy import sparse if ( nodelist is  None  ) : nodelist = list ( G ) nlen = len ( nodelist ) if ( nlen == 0 ) : raise nx .  Network X Error  ( ' Graph   has  no  nodes  or  edges' ) if ( len ( nodelist ) != len ( set ( nodelist ) ) ) : msg = ' Ambiguous   ordering:  `nodelist`  contained  duplicates.' raise nx .  Network X Error  ( msg ) index = dict ( zip ( nodelist , range ( nlen ) ) ) coefficients = zip ( * ( ( index [ u ] , index [ v ] , d . get ( weight , 1 ) ) for ( u , v , d ) in G . edges ( nodelist , data =  True  ) if ( ( u in index ) and ( v in index ) ) ) ) try : ( row , col , data ) = coefficients except  Value  Error  : ( row , col , data ) = ( [ ] , [ ] , [ ] ) if G . is directed ( ) : M = sparse . coo matrix ( ( data , ( row , col ) ) , shape = ( nlen , nlen ) , dtype = dtype ) else : d = ( data + data ) r = ( row + col ) c = ( col + row ) selfloops = list ( G . selfloop edges ( data =  True  ) ) if selfloops : ( diag index , diag data ) = zip ( * ( ( index [ u ] , ( - d . get ( weight , 1 ) ) ) for ( u , v , d ) in selfloops if ( ( u in index ) and ( v in index ) ) ) ) d += diag data r += diag index c += diag index M = sparse . coo matrix ( ( d , ( r , c ) ) , shape = ( nlen , nlen ) , dtype = dtype ) try : return M . asformat ( format ) except  Attribute  Error  : raise nx .  Network X Error  ( ( ' Unknown   sparse  matrix  format:  %s' % format ) ) 
def  read ( obj ) : if  is url ( obj ) : with urlopen ( obj ) as url : text = url . read ( ) elif hasattr ( obj , 'read' ) : text = obj . read ( ) elif isinstance ( obj , char types ) : text = obj try : if os . path . isfile ( text ) : with open ( text , 'rb' ) as f : return f . read ( ) except (  Type  Error  ,  Value  Error  ) : pass else : raise  Type  Error  ( ( ' Cannot   read  object  of  type  %r' % type ( obj ) .   name   ) ) return text 
def apply path wildcard ( stats , path wildcard ) : paths =  Paths  ( tuple ( (  Path  ( normpath ( join ( path wildcard . symbolic path , basename ( s . path ) ) ) , s ) for s in stats . dependencies if fnmatch ( basename ( s . path ) , path wildcard . wildcard ) ) ) ) return  Paths  Expansion  ( paths , tuple ( ) ) 
def parse plain scalar indent (  Token  Class  ) : def callback ( lexer , match , context ) : text = match . group ( ) if ( len ( text ) <= context . indent ) : context . stack . pop ( ) context . stack . pop ( ) return if text : ( yield ( match . start ( ) ,  Token  Class  , text ) ) context . pos = match . end ( ) return callback 
def   Was  Gyp  Include  File  Modified  ( params , files ) : if params [ 'options' ] . includes : for include in params [ 'options' ] . includes : if (   To  Gyp  Path  ( os . path . normpath ( include ) ) in files ) : print ' Include   file  modified,  assuming  all  changed' , include return  True  return  False  
def avail locations ( call =  None  ) : if ( call == 'action' ) : raise  Salt  Cloud  Exception  ( ' The   avail locations  function  must  be  called  with  -f  or  --function.' ) vm  = get configured provider ( ) manager = packet .  Manager  ( auth token = vm  [ 'token' ] ) ret = { } for facility in manager . list facilities ( ) : ret [ facility . name ] = facility .   dict   return ret 
def  suggest donation if appropriate ( config , action ) : if ( config . staging or ( config . verb == 'renew' ) ) : return if ( action not in [ 'renew' , 'newcert' ] ) : return reporter util = zope . component . get Utility  ( interfaces . I Reporter  ) msg = " If   you  like   Certbot ,  please  consider  supporting  our  work  by:\n\n Donating   to  ISRG  /   Let 's   Encrypt :      https://letsencrypt.org/donate\n Donating   to  EFF:                                        https://eff.org/donate-le\n\n" reporter util . add message ( msg , reporter util . LOW PRIORITY ) 
def config course cohorts legacy ( course , discussions , cohorted , cohorted discussions =  None  , auto cohort groups =  None  , always cohort inline discussions =  None  ) : def to id ( name ) : '\n                 Helper   method  to  convert  a  discussion  topic  name  to  a  database  identifier\n                ' return topic name to id ( course , name ) topics = dict ( ( ( name , { 'sort key' : 'A' , 'id' : to id ( name ) } ) for name in discussions ) ) course . discussion topics = topics config = { 'cohorted' : cohorted } if ( cohorted discussions is not  None  ) : config [ 'cohorted discussions' ] = [ to id ( name ) for name in cohorted discussions ] if ( auto cohort groups is not  None  ) : config [ 'auto cohort groups' ] = auto cohort groups if ( always cohort inline discussions is not  None  ) : config [ 'always cohort inline discussions' ] = always cohort inline discussions course . cohort config = config try : modulestore ( ) . update item ( course ,  Module  Store  Enum  .  User ID . test ) except  Not  Implemented  Error  : pass 
def scrape video ( youtube id , format = 'mp4' , force =  False  , quiet =  False  , callback =  None  ) : video filename = ( '%(id)s.%(ext)s' % { 'id' : youtube id , 'ext' : format } ) video file download path = os . path . join ( settings . CONTENT ROOT , video filename ) if ( os . path . exists ( video file download path ) and ( not force ) ) : return yt dl = youtube dl .  Youtube DL ( { 'outtmpl' : video file download path , 'quiet' : quiet } ) yt dl . add default info extractors ( ) if callback : yt dl . add progress hook ( callback ) yt dl . extract info ( ( 'www.youtube.com/watch?v=%s' % youtube id ) , download =  True  ) 
def isdir ( s ) : try : st = os . stat ( s ) except os . error : return  False  return stat . S ISDIR ( st . st mode ) 
def render inclusion ( func , file name , takes context , django context , * args , ** kwargs ) : if takes context : args = ( [ django context ] + list ( args ) )  dict = func ( * args , ** kwargs ) if isinstance ( file name ,  Template  ) : t = file name elif ( ( not isinstance ( file name , basestring ) ) and is iterable ( file name ) ) : t = select template ( file name ) else : t = get template ( file name ) nodelist = t . nodelist new context =  Context  (  dict ) csrf token = django context . get ( 'csrf token' ,  None  ) if ( csrf token is not  None  ) : new context [ 'csrf token' ] = csrf token return nodelist . render ( new context ) 
def format datetime ( dt ) : return dateformat . format ( make naive ( dt ) , u'r' ) 
def load vector ( * names ) : return pkg resources . resource string (   name   , os . path . join ( 'testdata' , * names ) ) 
def make dense ( targets , noclass ) : with tf . device ( '/cpu:0' ) : shape = tf . shape ( targets ) batch size = shape [ 0 ] indices = ( targets + ( noclass * tf . range ( 0 , batch size ) ) ) length = tf . expand dims ( ( batch size * noclass ) , 0 ) dense = tf . sparse to dense ( indices , length , 1.0 , 0.0 ) return tf . reshape ( dense , [ ( - 1 ) , noclass ] ) 
def owner to ( dbname , ownername , user =  None  , host =  None  , port =  None  , password =  None  , runas =  None  ) : sqlfile = tempfile .  Named  Temporary  File  ( ) sqlfile . write ( 'begin;\n' ) sqlfile . write ( 'alter  database  "{0}"  owner  to  "{1}";\n' . format ( dbname , ownername ) ) queries = ( ( 'alter  schema  {n}  owner  to  {owner};' , 'select  quote ident(schema name)  as  n  from  information schema.schemata;' ) , ( 'alter  table  {n}  owner  to  {owner};' , "select  quote ident(table schema)||'.'||quote ident(table name)  as  n  from  information schema.tables  where  table schema  not  in  ('pg catalog',  'information schema');" ) , ( 'alter  function  {n}  owner  to  {owner};' , "select  p.oid::regprocedure::text  as  n  from  pg catalog.pg proc  p  join  pg catalog.pg namespace  ns  on  p.pronamespace=ns.oid  where  ns.nspname  not  in  ('pg catalog',  'information schema')    and  not  p.proisagg;" ) , ( 'alter  aggregate  {n}  owner  to  {owner};' , "select  p.oid::regprocedure::text  as  n  from  pg catalog.pg proc  p  join  pg catalog.pg namespace  ns  on  p.pronamespace=ns.oid  where  ns.nspname  not  in  ('pg catalog',  'information schema')  and  p.proisagg;" ) , ( 'alter  sequence  {n}  owner  to  {owner};' , "select  quote ident(sequence schema)||'.'||quote ident(sequence name)  as  n  from  information schema.sequences;" ) ) for ( fmt , query ) in queries : ret = psql query ( query , user = user , host = host , port = port , maintenance db = dbname , password = password , runas = runas ) for row in ret : sqlfile . write ( ( fmt . format ( owner = ownername , n = row [ 'n' ] ) + '\n' ) ) sqlfile . write ( 'commit;\n' ) sqlfile . flush ( ) os . chmod ( sqlfile . name , 420 ) cmdret =  psql prepare and run ( [ '-f' , sqlfile . name ] , user = user , runas = runas , host = host , port = port , password = password , maintenance db = dbname ) return cmdret 
def detect unboundedness ( R , s , t ) : q = deque ( [ s ] ) seen = set ( [ s ] ) inf = R . graph [ 'inf' ] while q : u = q . popleft ( ) for ( v , attr ) in R [ u ] . items ( ) : if ( ( attr [ 'capacity' ] == inf ) and ( v not in seen ) ) : if ( v == t ) : raise nx .  Network X Unbounded  ( ' Infinite   capacity  path,  flow  unbounded  above.' ) seen . add ( v ) q . append ( v ) 
def  load csv ( F ) : if PY2 : names = F . readline ( ) . strip ( ) . split ( ',' ) else : names = F . readline ( ) . decode ( 'ascii' ) . strip ( ) . split ( ',' ) rec = np . loadtxt ( F , skiprows = 0 , delimiter = ',' , dtype = 'a22,f4,f4' ) rec . dtype . names = names return rec 
def do Auth  ( realm ) : return ( ( digest Auth  ( realm ) + '  ' ) + basic Auth  ( realm ) ) 
def filemode ( mode ) : perm = [ ] for table in filemode table : for ( bit , char ) in table : if ( ( mode & bit ) == bit ) : perm . append ( char ) break else : perm . append ( '-' ) return '' . join ( perm ) 
def test mixeddiv ( ) : i = iscalar ( ) d = dscalar ( ) assert ( 0 == function ( [ i , d ] , ( d * ( i // ( i + 1 ) ) ) ) ( 3 , 1.0 ) ) 
def  encode params ( ** kw ) : def  encode ( L , k , v ) : if isinstance ( v , unicode ) : L . append ( ( '%s=%s' % ( k , urllib . quote ( v . encode ( 'utf-8' ) ) ) ) ) elif isinstance ( v , str ) : L . append ( ( '%s=%s' % ( k , urllib . quote ( v ) ) ) ) elif isinstance ( v , collections .  Iterable  ) : for x in v :  encode ( L , k , x ) else : L . append ( ( '%s=%s' % ( k , urllib . quote ( str ( v ) ) ) ) ) args = [ ] for ( k , v ) in kw . iteritems ( ) :  encode ( args , k , v ) return '&' . join ( args ) 
def get hex from color ( color ) : return ( '#' + '' . join ( [ '{0:02x}' . format ( int ( ( x * 255 ) ) ) for x in color ] ) ) 
def create script ( command ) : import os import os . path as path import subprocess import tempfile ( fd , script file ) = tempfile . mkstemp ( prefix = 'lxc-attach-script' ) f = os . fdopen ( fd , 'wb' ) try : f . write ( ( ATTACH TEMPLATE % { 'container command' : command } ) ) f . flush ( ) finally : f . close ( ) os . chmod ( script file , int ( '0700' , 8 ) ) stdout file = os . fdopen ( tempfile . mkstemp ( prefix = 'lxc-attach-script-log' ) [ 0 ] , 'ab' ) stderr file = os . fdopen ( tempfile . mkstemp ( prefix = 'lxc-attach-script-err' ) [ 0 ] , 'ab' ) try : subprocess .  Popen  ( [ script file ] , stdout = stdout file , stderr = stderr file ) . communicate ( ) finally : stderr file . close ( ) stdout file . close ( ) os . remove ( script file ) 
def  make voxel ras trans ( move , ras , voxel size ) : assert ( voxel size . ndim == 1 ) assert ( voxel size . size == 3 ) rot = ( ras . T * voxel size [ np . newaxis , : ] ) assert ( rot . ndim == 2 ) assert ( rot . shape [ 0 ] == 3 ) assert ( rot . shape [ 1 ] == 3 ) trans = np . c  [ ( np . r  [ ( rot , np . zeros ( ( 1 , 3 ) ) ) ] , np . r  [ ( move , 1.0 ) ] ) ] t =  Transform  ( 'mri voxel' , 'mri' , trans ) return t 
@ sopel . module . event ( u'KICK' ) @ sopel . module . rule ( u'.*' ) @ sopel . module . priority ( u'low' ) def hold ground ( bot , trigger ) : if bot . config . admin . hold ground : channel = trigger . sender if ( trigger . args [ 1 ] == bot . nick ) : bot . join ( channel ) 
def dmp inflate ( f , M , u , K ) : if ( not u ) : return dup inflate ( f , M [ 0 ] , K ) if all ( ( ( m == 1 ) for m in M ) ) : return f else : return  rec inflate ( f , M , u , 0 , K ) 
def get instance path at destination ( instance , migrate data =  None  ) : instance relative path =  None  if migrate data : instance relative path = migrate data . instance relative path if instance relative path : instance dir = os . path . join ( CONF . instances path , instance relative path ) else : instance dir = get instance path ( instance ) return instance dir 
def parse atom ( tokens , options ) : token = tokens . current ( ) result = [ ] if ( token in '([' ) : tokens . move ( ) ( matching , pattern ) = { '(' : [ ')' ,  Required  ] , '[' : [ ']' ,  Optional  ] } [ token ] result = pattern ( * parse expr ( tokens , options ) ) if ( tokens . move ( ) != matching ) : raise tokens . error ( ( "unmatched  '%s'" % token ) ) return [ result ] elif ( token == 'options' ) : tokens . move ( ) return [  Any  Options  ( ) ] elif ( token . startswith ( '--' ) and ( token != '--' ) ) : return parse long ( tokens , options ) elif ( token . startswith ( '-' ) and ( token not in ( '-' , '--' ) ) ) : return parse shorts ( tokens , options ) elif ( ( token . startswith ( '<' ) and token . endswith ( '>' ) ) or token . isupper ( ) ) : return [  Argument  ( tokens . move ( ) ) ] else : return [  Command  ( tokens . move ( ) ) ] 
def   Match  Postfix  ( postfix props , index props ) : index props rev = reversed ( index props ) for property group in reversed ( postfix props ) : index group iter = itertools . islice ( index props rev , len ( property group ) ) if isinstance ( property group , ( frozenset , set ) ) : index group = set ( ( prop for ( prop ,   ) in index group iter ) ) if ( index group != property group ) : return  None  else : index group = list ( index group iter ) if ( len ( index group ) != len ( property group ) ) : return  None  for ( ( index prop , index dir ) , ( prop , direction ) ) in itertools . izip ( index group , reversed ( property group ) ) : if ( ( index prop != prop ) or ( direction and ( index dir != direction ) ) ) : return  None  remaining = list ( index props rev ) remaining . reverse ( ) return remaining 
def debug ( msg , html =  False  ) : write ( msg , 'DEBUG' , html ) 
@ decorator . decorator def add mask if none ( f , clip , * a , ** k ) : if ( clip . mask is  None  ) : clip = clip . add mask ( ) return f ( clip , * a , ** k ) 
def  parse prefix as idd ( idd pattern , number ) : match = idd pattern . match ( number ) if match : match end = match . end ( ) digit match =  CAPTURING DIGIT PATTERN . search ( number [ match end : ] ) if digit match : normalized group = normalize digits only ( digit match . group ( 1 ) ) if ( normalized group == U ZERO ) : return (  False  , number ) return (  True  , number [ match end : ] ) return (  False  , number ) 
@ require role def get session user info ( request ) : return [ request . user . id , request . user . username , request . user ] 
def get figure ( file owner or url , file id =  None  , raw =  False  ) : plotly rest url = get config ( ) [ 'plotly domain' ] if ( file id is  None  ) : url = file owner or url if ( url [ : len ( plotly rest url ) ] != plotly rest url ) : raise exceptions .  Plotly  Error  ( " Because   you  didn't  supply  a  'file id'  in  the  call,  we're  assuming  you're  trying  to  snag  a  figure  from  a  url.   You   supplied  the  url,  '{0}',  we  expected  it  to  start  with  '{1}'.\n Run   help  on  this  function  for  more  information." . format ( url , plotly rest url ) ) head = ( plotly rest url + '/~' ) file owner = url . replace ( head , '' ) . split ( '/' ) [ 0 ] file id = url . replace ( head , '' ) . split ( '/' ) [ 1 ] else : file owner = file owner or url try : int ( file id ) except  Value  Error  : raise exceptions .  Plotly  Error  ( " The   'file id'  argument  was  not  able  to  be  converted  into  an  integer  number.   Make   sure  that  the  positional  'file id'  argument  is  a  number  that  can  be  converted  into  an  integer  or  a  string  that  can  be  converted  into  an  integer." ) if ( int ( file id ) < 0 ) : raise exceptions .  Plotly  Error  ( " The   'file id'  argument  must  be  a  non-negative  number." ) fid = '{}:{}' . format ( file owner , file id ) response = v2 . plots . content ( fid , inline data =  True  ) figure = response . json ( ) for ( index , entry ) in enumerate ( figure [ 'data' ] ) : try : if all ( ( ( entry [ 'type' ] == 'histogramy' ) , ( 'xbins' in entry ) , ( 'ybins' not in entry ) ) ) : entry [ 'ybins' ] = entry . pop ( 'xbins' ) if ( entry [ 'type' ] in [ 'histogramx' , 'histogramy' ] ) : entry [ 'type' ] = 'histogram' if ( 'bardir' in entry ) : entry [ 'orientation' ] = entry . pop ( 'bardir' ) if ( entry [ 'type' ] == 'bar' ) : if ( entry [ 'orientation' ] == 'h' ) :  swap xy data ( entry ) if ( entry [ 'type' ] == 'histogram' ) : if ( ( 'x' in entry ) and ( 'y' not in entry ) ) : if ( entry [ 'orientation' ] == 'h' ) :  swap xy data ( entry ) del entry [ 'orientation' ] if ( ( 'y' in entry ) and ( 'x' not in entry ) ) : if ( entry [ 'orientation' ] == 'v' ) :  swap xy data ( entry ) del entry [ 'orientation' ] figure [ 'data' ] [ index ] = entry except  Key  Error  : pass for ( index , entry ) in enumerate ( figure [ 'data' ] ) : if ( 'stream' in entry ) : del figure [ 'data' ] [ index ] [ 'stream' ] if raw : return figure return tools . get valid graph obj ( figure , obj type = ' Figure ' ) 
def model form ( model , base class =  Form  , only =  None  , exclude =  None  , field args =  None  , converter =  None  ) : field dict = model fields ( model , only , exclude , field args , converter ) return type ( ( model . kind ( ) + ' Form ' ) , ( base class , ) , field dict ) 
def absent ( name , user =  None  , password =  None  , host =  None  , port =  None  ) : ret = { 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : '' } if   salt   [ 'influxdb08.db exists' ] ( name , user , password , host , port ) : if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = ' Database   {0}  is  present  and  needs  to  be  removed' . format ( name ) return ret if   salt   [ 'influxdb08.db remove' ] ( name , user , password , host , port ) : ret [ 'comment' ] = ' Database   {0}  has  been  removed' . format ( name ) ret [ 'changes' ] [ name ] = ' Absent ' return ret else : ret [ 'comment' ] = ' Failed   to  remove  database  {0}' . format ( name ) ret [ 'result' ] =  False  return ret ret [ 'comment' ] = ' Database   {0}  is  not  present,  so  it  cannot  be  removed' . format ( name ) return ret 
def create tmp dir ( function ) : @ functools . wraps ( function ) def decorated function ( * args , ** kwargs ) : tmp dir path = tempfile . mkdtemp ( ) kwargs [ 'tmp dir path' ] = tmp dir path try : return function ( * args , ** kwargs ) finally : utils . execute ( 'rm' , '-rf' , tmp dir path ) return decorated function 
@ app . route ( '/libtoggle' , methods = [ 'POST' ] ) def review ( ) : if ( not g . user ) : return 'NO' idvv = request . form [ 'pid' ] if ( not isvalidid ( idvv ) ) : return 'NO' pid = strip version ( idvv ) if ( not ( pid in db ) ) : return 'NO' uid = session [ 'user id' ] record = query db ( 'select  *  from  library  where\n                    user id  =  ?  and  paper id  =  ?' , [ uid , pid ] , one =  True  ) print record ret = 'NO' if record : g . db . execute ( 'delete  from  library  where  user id  =  ?  and  paper id  =  ?' , [ uid , pid ] ) g . db . commit ( ) ret = 'OFF' else : rawpid = strip version ( pid ) g . db . execute ( 'insert  into  library  (paper id,  user id,  update time)  values  (?,  ?,  ?)' , [ rawpid , uid , int ( time . time ( ) ) ] ) g . db . commit ( ) ret = 'ON' return ret 
def parse List  Of  Points  ( s ) : i = 0 ws nums = re . split ( '\\s*,?\\s*' , s . strip ( ) ) nums = [ ] for i in xrange ( len ( ws nums ) ) : negcoords = ws nums [ i ] . split ( '-' ) if ( len ( negcoords ) == 1 ) : nums . append ( negcoords [ 0 ] ) else : for j in xrange ( len ( negcoords ) ) : if ( j == 0 ) : if ( negcoords [ 0 ] != '' ) : nums . append ( negcoords [ 0 ] ) else : prev = nums [ ( len ( nums ) - 1 ) ] if ( prev [ ( len ( prev ) - 1 ) ] in [ 'e' , 'E' ] ) : nums [ ( len ( nums ) - 1 ) ] = ( ( prev + '-' ) + negcoords [ j ] ) else : nums . append ( ( '-' + negcoords [ j ] ) ) if ( ( len ( nums ) % 2 ) != 0 ) : return [ ] i = 0 while ( i < len ( nums ) ) : try : nums [ i ] = getcontext ( ) . create decimal ( nums [ i ] ) nums [ ( i + 1 ) ] = getcontext ( ) . create decimal ( nums [ ( i + 1 ) ] ) except decimal .  Invalid  Operation  : return [ ] i += 2 return nums 
def catch exceptions if in safe mode ( function ) : def wrapped ( method , url , ** kwargs ) : if ( ( kwargs . get ( 'config' ) and kwargs . get ( 'config' ) . get ( 'safe mode' ) ) or ( kwargs . get ( 'session' ) and kwargs . get ( 'session' ) . config . get ( 'safe mode' ) ) ) : try : return function ( method , url , ** kwargs ) except (  Request  Exception  ,  Connection  Error  , HTTP Error  , socket . timeout , socket . gaierror ) as e : r =  Response  ( ) r . error = e r . raw = HTTP Response  ( ) r . status code = 0 return r return function ( method , url , ** kwargs ) return wrapped 
def jaccard ( firsts , seconds ) : return ( sum ( [ ( 1 / frequency . get ( brand , 100 ) ) for brand in ( firsts & seconds ) ] ) / sum ( [ ( 1 / frequency . get ( brand , 100 ) ) for brand in ( firsts | seconds ) ] ) ) 
@ gen . coroutine def  Update  Episode  ( client , obj store , user id , device id , request ) : ( yield  Activity  .  Verify  Activity  Id  ( client , user id , device id , request [ 'activity' ] [ 'activity id' ] ) ) headers = request . pop ( 'headers' ) activity = request . pop ( 'activity' ) request = { 'headers' : headers , 'user id' : user id , 'activity' : activity , 'episode' : request } ( yield gen .  Task  (  Operation  .  Create  And  Execute  , client , user id , device id , ' Update  Episode  Operation . Execute ' , request ) ) logging . info ( ( 'UPDATE  EPISODE:  user:  %d,  device:  %d,  episode:  %s' % ( user id , device id , request [ 'episode' ] [ 'episode id' ] ) ) ) raise gen .  Return  ( { } ) 
def two scales ( ax1 , time , data1 , data2 , c1 , c2 ) : ax2 = ax1 . twinx ( ) ax1 . plot ( time , data1 , color = c1 ) ax1 . set xlabel ( 'time  (s)' ) ax1 . set ylabel ( 'exp' ) ax2 . plot ( time , data2 , color = c2 ) ax2 . set ylabel ( 'sin' ) return ( ax1 , ax2 ) 
def   Add  Has  Field  Method  ( message descriptor , cls ) : singular fields = { } for field in message descriptor . fields : if ( field . label !=   Field  Descriptor  . LABEL REPEATED ) : singular fields [ field . name ] = field def  Has  Field  ( self , field name ) : try : field = singular fields [ field name ] except  Key  Error  : raise  Value  Error  ( ( ' Protocol   message  has  no  singular  "%s"  field.' % field name ) ) if ( field . cpp type ==   Field  Descriptor  . CPPTYPE MESSAGE ) : value = self .  fields . get ( field ) return ( ( value is not  None  ) and value .  is present in parent ) else : return ( field in self .  fields ) cls .  Has  Field  =  Has  Field  
def get main running hub ( ) : hubs = get running hubs ( ) if ( not hubs ) : raise SAMP Hub  Error  ( u' Unable   to  find  a  running  SAMP   Hub .' ) if ( u'SAMP HUB' in os . environ ) : if os . environ [ u'SAMP HUB' ] . startswith ( u'std-lockurl:' ) : lockfilename = os . environ [ u'SAMP HUB' ] [ len ( u'std-lockurl:' ) : ] else : raise SAMP Hub  Error  ( u'SAMP   Hub   profile  not  supported.' ) else : lockfilename = os . path . join (  find home ( ) , u'.samp' ) return hubs [ lockfilename ] 
def volume type get ( context , id ) : return IMPL . volume type get ( context , id ) 
def require valid sender ( handler ) : def test user ( self , ** kwargs ) : ' Checks   if  the  user  is  logged  in  and  is  authorized  sender.' if ( not self . user id ) : self . redirect ( current user services . create login url ( self . request . uri ) ) return if ( self . username not in config domain . WHITELISTED EMAIL SENDERS . value ) : raise self .  Unauthorized  User  Exception  ( '%s  is  not  an  authorized  user  of  this  application' , self . user id ) return handler ( self , ** kwargs ) return test user 
def load model ( path to models , path to tables ) : path to umodel = ( path to models + 'uni skip.npz' ) path to bmodel = ( path to models + 'bi skip.npz' ) with open ( ( '%s.pkl' % path to umodel ) , 'rb' ) as f : uoptions = pkl . load ( f ) with open ( ( '%s.pkl' % path to bmodel ) , 'rb' ) as f : boptions = pkl . load ( f ) uparams = init params ( uoptions ) uparams = load params ( path to umodel , uparams ) utparams = init tparams ( uparams ) bparams = init params bi ( boptions ) bparams = load params ( path to bmodel , bparams ) btparams = init tparams ( bparams ) ( embedding , x mask , ctxw2v ) = build encoder ( utparams , uoptions ) f w2v = theano . function ( [ embedding , x mask ] , ctxw2v , name = 'f w2v' ) ( embedding , x mask , ctxw2v ) = build encoder bi ( btparams , boptions ) f w2v2 = theano . function ( [ embedding , x mask ] , ctxw2v , name = 'f w2v2' ) ( utable , btable ) = load tables ( path to tables ) model = { } model [ 'uoptions' ] = uoptions model [ 'boptions' ] = boptions model [ 'utable' ] = utable model [ 'btable' ] = btable model [ 'f w2v' ] = f w2v model [ 'f w2v2' ] = f w2v2 return model 
def   Make  Args  ( amazon collection map , google collection map ) : request list = [ ] for ( url , label ) in amazon collection map . iteritems ( ) : request list . append (  Cloud  Metadata  Request  ( bios version regex = AMAZON BIOS REGEX , service name regex = AMAZON SERVICE REGEX , instance type = 'AMAZON' , timeout = 1.0 , url = url , label = label ) ) for ( url , label ) in google collection map . iteritems ( ) : request list . append (  Cloud  Metadata  Request  ( bios version regex = GOOGLE BIOS REGEX , service name regex = GOOGLE SERVICE REGEX , headers = { ' Metadata - Flavor ' : ' Google ' } , instance type = 'GOOGLE' , timeout = 1.0 , url = url , label = label ) ) return request list 
def read int64 ( fid ) : return  unpack simple ( fid , '>u8' , np . int64 ) 
@ login required def edit service ( request , service id ) : service obj = get object or 404 (  Service  , pk = service id ) if ( request . method == 'POST' ) : service form =  Service  Form  ( request . POST , instance = service obj , prefix = 'service' ) if service form . is valid ( ) : service obj = service form . save ( commit =  False  ) service obj . keywords . clear ( ) service obj . keywords . add ( * service form . cleaned data [ 'keywords' ] ) service obj . save ( ) return  Http  Response  Redirect  ( service obj . get absolute url ( ) ) else : service form =  Service  Form  ( instance = service obj , prefix = 'service' ) return render to response ( 'services/service edit.html' ,  Request  Context  ( request , { 'service' : service obj , 'service form' : service form } ) ) 
def get mode ( path ) : if ( not os . path . exists ( path ) ) : return '' func name = '{0}.get mode' . format (   virtualname   ) if (   opts   . get ( 'fun' , '' ) == func name ) : log . info ( ' The   function  {0}  should  not  be  used  on   Windows   systems;  see  function  docs  for  details.   The   value  returned  is  always   None .' . format ( func name ) ) return  None  
def render to string ( template name , context =  None  , request =  None  , using =  None  ) : if isinstance ( template name , ( list , tuple ) ) : template = select template ( template name , using = using ) else : template = get template ( template name , using = using ) return template . render ( context , request ) 
def generate secret ( ) : from  Crypto  .  Random  import get random bytes bytes = get random bytes ( 20 ) encoded = base64 . b32encode ( bytes ) return encoded 
def test commented scenarios ( ) : scenario =  Scenario  . from string ( COMMENTED SCENARIO ) assert equals ( scenario . name , u' Adding   some  students  to  my  university  database' ) assert equals ( len ( scenario . steps ) , 4 ) 
def prep ( ) : long description = make long description ( ) write ( long description , RST DESCRIPTION PATH ) 
def random func ( lib , opts , args ) : query = decargs ( args ) if opts . album : objs = list ( lib . albums ( query ) ) else : objs = list ( lib . items ( query ) ) objs = random objs ( objs , opts . album , opts . number , opts . time , opts . equal chance ) for obj in objs : print  ( format ( obj ) ) 
def default fused keys renamer ( keys ) : typ = type ( keys [ 0 ] ) if ( ( typ is str ) or ( typ is unicode ) ) : names = [ key split ( x ) for x in keys [ : 0 : ( - 1 ) ] ] names . append ( keys [ 0 ] ) return '-' . join ( names ) elif ( ( typ is tuple ) and ( len ( keys [ 0 ] ) > 0 ) and isinstance ( keys [ 0 ] [ 0 ] , ( str , unicode ) ) ) : names = [ key split ( x ) for x in keys [ : 0 : ( - 1 ) ] ] names . append ( keys [ 0 ] [ 0 ] ) return ( ( '-' . join ( names ) , ) + keys [ 0 ] [ 1 : ] ) else : return  None  
def polarify ( eq , subs =  True  , lift =  False  ) : if lift : subs =  False  eq =  polarify ( sympify ( eq ) , lift ) if ( not subs ) : return eq reps = { s :  Dummy  ( s . name , polar =  True  ) for s in eq . free symbols } eq = eq . subs ( reps ) return ( eq , { r : s for ( s , r ) in reps . items ( ) } ) 
def update vote lookups ( user , thing , direction ) : key = prequeued vote key ( user , thing ) grace period = int ( g . vote queue grace period . total seconds ( ) ) direction =  Vote  . serialize direction ( direction ) g . gencache . set ( key , direction , time = ( grace period + 1 ) ) rel cls =  Votes  By  Account  . rel ( thing .   class   )  Last  Modified  . touch ( user .  fullname , rel cls .  last modified name ) 
def  str to num ( n ) : val = 0 col = long ( 1 ) if ( n [ : 1 ] == 'x' ) : n = ( '0' + n ) if ( n [ : 2 ] == '0x' ) : n = string . lower ( n [ 2 : ] ) while ( len ( n ) > 0 ) : l = n [ ( len ( n ) - 1 ) ] val = ( val + ( string . hexdigits . index ( l ) * col ) ) col = ( col * 16 ) n = n [ : ( len ( n ) - 1 ) ] elif ( n [ 0 ] == '\\' ) : n = n [ 1 : ] while ( len ( n ) > 0 ) : l = n [ ( len ( n ) - 1 ) ] if ( ( ord ( l ) < 48 ) or ( ord ( l ) > 57 ) ) : break val = ( val + ( int ( l ) * col ) ) col = ( col * 8 ) n = n [ : ( len ( n ) - 1 ) ] else : val = string . atol ( n ) return val 
def npath ( path ) : return path 
def query ( query type = 'list nodes' ) : client =  get client ( ) info = client . query ( query type ) return info 
def  get cost functions ( ) : cost fns conf = CONF . least cost functions if ( cost fns conf is  None  ) : fn str = 'nova.scheduler.least cost.compute fill first cost fn' cost fns conf = [ fn str ] cost fns = [ ] for cost fn str in cost fns conf : short name = cost fn str . split ( '.' ) [ ( - 1 ) ] if ( not ( short name . startswith ( 'compute ' ) or short name . startswith ( 'noop' ) ) ) : continue if cost fn str . startswith ( 'nova.scheduler.least cost.' ) : cost fn str = ( 'nova.scheduler.weights.least cost' + cost fn str [ 25 : ] ) try : cost fn = importutils . import class ( cost fn str ) except  Import  Error  : raise exception .  Scheduler  Cost  Function  Not  Found  ( cost fn str = cost fn str ) try : flag name = ( '%s weight' % cost fn .   name   ) weight = getattr ( CONF , flag name ) except  Attribute  Error  : raise exception .  Scheduler  Weight  Flag  Not  Found  ( flag name = flag name ) if ( ( flag name == 'compute fill first cost fn weight' ) and ( weight is  None  ) ) : weight = ( - 1.0 ) cost fns . append ( ( weight , cost fn ) ) return cost fns 
def ip missing ( mod attr ) : IPY SHOULD IMPL . write ( ( mod attr + '\n' ) ) IPY SHOULD IMPL . flush ( ) 
def set access token ( access token ) : global  access token  access token = access token 
def json from url ( url ) : error message = '' url handle = urllib . urlopen ( url ) url contents = url handle . read ( ) try : parsed json = json . loads ( url contents ) except  Exception  as e : error message = str ( url contents ) print ' Error   parsing  JSON  data  in  json from url():  ' , str ( e ) return (  None  , error message ) return ( parsed json , error message ) 
def create export tarball ( course module , course key , context ) : name = course module . url name export file =  Named  Temporary  File  ( prefix = ( name + '.' ) , suffix = '.tar.gz' ) root dir = path ( mkdtemp ( ) ) try : if isinstance ( course key ,  Library  Locator  ) : export library to xml ( modulestore ( ) , contentstore ( ) , course key , root dir , name ) else : export course to xml ( modulestore ( ) , contentstore ( ) , course module . id , root dir , name ) logging . debug ( u'tar  file  being  generated  at  %s' , export file . name ) with tarfile . open ( name = export file . name , mode = 'w:gz' ) as tar file : tar file . add ( ( root dir / name ) , arcname = name ) except  Serialization  Error  as exc : log . exception ( u' There   was  an  error  exporting  %s' , course key ) unit =  None  failed item =  None  parent =  None  try : failed item = modulestore ( ) . get item ( exc . location ) parent loc = modulestore ( ) . get parent location ( failed item . location ) if ( parent loc is not  None  ) : parent = modulestore ( ) . get item ( parent loc ) if ( parent . location . category == 'vertical' ) : unit = parent except : pass context . update ( { 'in err' :  True  , 'raw err msg' : str ( exc ) , 'failed module' : failed item , 'unit' : unit , 'edit unit url' : ( reverse usage url ( 'container handler' , parent . location ) if parent else '' ) } ) raise except  Exception  as exc : log . exception ( ' There   was  an  error  exporting  %s' , course key ) context . update ( { 'in err' :  True  , 'unit' :  None  , 'raw err msg' : str ( exc ) } ) raise finally : shutil . rmtree ( ( root dir / name ) ) return export file 
def assert not has text ( output , text ) : assert ( output . find ( text ) < 0 ) , ( " Output   file  contains  unexpected  text  '%s'" % text ) 
def  log runtime ( parameter , proc location , start time ) : runtime = ( time . time ( ) - start time ) log . debug ( ( 'proc  call  (%s):  %s  (runtime:  %0.4f)' % ( parameter , proc location , runtime ) ) ) 
def length ( xyz , along =  False  ) : xyz = np . asarray ( xyz ) if ( xyz . shape [ 0 ] < 2 ) : if along : return np . array ( [ 0 ] ) return 0 dists = np . sqrt ( ( np . diff ( xyz , axis = 0 ) ** 2 ) . sum ( axis = 1 ) ) if along : return np . cumsum ( dists ) return np . sum ( dists ) 
def best match ( supported , header ) : parsed header = [ parse media range ( r ) for r in header . split ( ',' ) ] weighted matches = [ ( fitness and quality parsed ( mime type , parsed header ) , mime type ) for mime type in supported ] weighted matches . sort ( ) return ( ( weighted matches [ ( - 1 ) ] [ 0 ] [ 1 ] and weighted matches [ ( - 1 ) ] [ 1 ] ) or '' ) 
def reset ( name , runas =  None  ) : return prlctl ( 'reset' ,  sdecode ( name ) , runas = runas ) 
def align log prob ( i , j , source sents , target sents , alignment , params ) : l s = sum ( ( source sents [ ( ( i - offset ) - 1 ) ] for offset in range ( alignment [ 0 ] ) ) ) l t = sum ( ( target sents [ ( ( j - offset ) - 1 ) ] for offset in range ( alignment [ 1 ] ) ) ) try : m = ( ( l s + ( l t / params . AVERAGE CHARACTERS ) ) / 2 ) delta = ( ( ( l s * params . AVERAGE CHARACTERS ) - l t ) / math . sqrt ( ( m * params . VARIANCE CHARACTERS ) ) ) except  Zero  Division  Error  : return float ( '-inf' ) return ( - ( ( LOG2 + norm logsf ( abs ( delta ) ) ) + math . log ( params . PRIORS [ alignment ] ) ) ) 
def plot gat times ( gat , train time = 'diagonal' , title =  None  , xmin =  None  , xmax =  None  , ymin =  None  , ymax =  None  , ax =  None  , show =  True  , color =  None  , xlabel =  True  , ylabel =  True  , legend =  True  , chance =  True  , label = ' Classif .  score' ) : if ( not hasattr ( gat , 'scores ' ) ) : raise  Runtime  Error  ( ' Please   score  your  data  before  trying  to  plot  scores' ) import matplotlib . pyplot as plt if ( ax is  None  ) : ( fig , ax ) = plt . subplots ( 1 , 1 ) if ( chance is not  False  ) : if ( chance is  True  ) : chance =  get chance level ( gat . scorer  , gat . y train  ) chance = float ( chance ) if np . isfinite ( chance ) : ax . axhline ( chance , color = 'k' , linestyle = '--' , label = ' Chance   level' ) ax . axvline ( 0 , color = 'k' , label = '' ) if isinstance ( train time , ( str , float ) ) : train time = [ train time ] label = [ label ] elif isinstance ( train time , ( list , np . ndarray ) ) : label = train time else : raise  Value  Error  ( "train time  must  be  'diagonal'  |  float  |  list  or  array  of  float." ) if ( ( color is  None  ) or isinstance ( color , str ) ) : color = np . tile ( color , len ( train time ) ) for (  train time ,  color ,  label ) in zip ( train time , color , label ) :  plot gat time ( gat ,  train time , ax ,  color ,  label ) if ( title is not  None  ) : ax . set title ( title ) if ( ( ymin is not  None  ) and ( ymax is not  None  ) ) : ax . set ylim ( ymin , ymax ) if ( ( xmin is not  None  ) and ( xmax is not  None  ) ) : ax . set xlim ( xmin , xmax ) if ( xlabel is  True  ) : ax . set xlabel ( ' Time   (s)' ) if ( ylabel is  True  ) : ax . set ylabel ( ' Classif .  score  ({0})' . format ( ( 'AUC' if ( 'roc' in repr ( gat . scorer  ) ) else '%' ) ) ) if ( legend is  True  ) : ax . legend ( loc = 'best' ) plt show ( show ) return ( fig if ( ax is  None  ) else ax . get figure ( ) ) 
def  is number matching desc ( national number , number desc ) : if ( number desc is  None  ) : return  False  national re = re . compile ( ( number desc . national number pattern or U EMPTY STRING ) ) return (  is number possible for desc ( national number , number desc ) and fullmatch ( national re , national number ) ) 
def  mergetreejinja ( src , dst , context ) : for item in os . listdir ( src ) : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if os . path . isdir ( s ) : log . info ( ' Copying   folder  {0}  to  {1}' . format ( s , d ) ) if os . path . exists ( d ) :  mergetreejinja ( s , d , context ) else : os . mkdir ( d )  mergetreejinja ( s , d , context ) elif ( item != TEMPLATE FILE NAME ) : d =  Template  ( d ) . render ( context ) log . info ( ' Copying   file  {0}  to  {1}' . format ( s , d ) ) with salt . utils . fopen ( s , 'r' ) as source file : src contents = source file . read ( ) dest contents =  Template  ( src contents ) . render ( context ) with salt . utils . fopen ( d , 'w' ) as dest file : dest file . write ( dest contents ) 
@ treeio login required def help page ( request , url = '/' , response format = 'html' ) : source = getattr ( settings , 'HARDTREE HELP SOURCE' , 'http://127.0.0.1:7000/help' ) if ( not url ) : url = '/' body = '' try : body = urllib2 . urlopen ( ( ( ( source + url ) + '?domain=' ) +  Request  Site  ( request ) . domain ) ) . read ( ) except : pass regexp = '<!--  module content inner  -->(?P<module inner>.*?)<!--  /module content inner  -->' blocks = re . finditer ( regexp , body , re . DOTALL ) for block in blocks : body = smart unicode ( block . group ( 'module inner' ) . strip ( ) ) return render to response ( 'core/help page' , { 'body' : body } , context instance =  Request  Context  ( request ) , response format = response format ) 
def get url ( handler name , key value , key name = 'usage key string' , kwargs =  None  ) : return reverse url ( handler name , key name , key value , kwargs ) 
def openshift img streams ( registry , xml parent , data ) : scm = XML .  Sub  Element  ( xml parent , 'scm' , { 'class' : 'com.openshift.jenkins.plugins.pipeline. Open  Shift  Image  Streams ' } ) mapping = [ ( 'image-stream-name' , 'image Stream  Name ' , 'nodejs-010-centos7' ) , ( 'tag' , 'tag' , 'latest' ) , ( 'api-url' , 'apiURL' , 'https://openshift.default.svc.cluster.local' ) , ( 'namespace' , 'namespace' , 'test' ) , ( 'auth-token' , 'auth Token ' , '' ) , ( 'verbose' , 'verbose' ,  False  ) ] convert mapping to xml ( scm , data , mapping , fail required =  True  ) 
def disable task ( name ) : return  run cmd ( 'kapacitor  disable  {0}' . format ( name ) ) 
@ composite def docker image strategy ( draw , repository strategy = unique name strategy ( ) , tag strategy = unique name strategy ( ) ) : return  Docker  Image  ( repository = draw ( repository strategy ) , tag = draw ( tag strategy ) ) 
def layer option ( option , opt , value , parser ) : try : dest = int ( value ) except  Value  Error  : dest = value setattr ( parser . values , option . dest , dest ) 
def get computer sleep ( ) : ret = salt . utils . mac utils . execute return result ( 'systemsetup  -getcomputersleep' ) return salt . utils . mac utils . parse return ( ret ) 
def  parse kern pairs ( fh ) : line = next ( fh ) if ( not line . startswith ( ' Start  Kern  Pairs ' ) ) : raise  Runtime  Error  ( ( u' Bad   start  of  kern  pairs  data:  %s' % line ) ) d = { } for line in fh : line = line . rstrip ( ) if ( not line ) : continue if line . startswith ( ' End  Kern  Pairs ' ) : next ( fh ) return d vals = line . split ( ) if ( ( len ( vals ) != 4 ) or ( vals [ 0 ] != 'KPX' ) ) : raise  Runtime  Error  ( ( u' Bad   kern  pairs  line:  %s' % line ) ) ( c1 , c2 , val ) = (  to str ( vals [ 1 ] ) ,  to str ( vals [ 2 ] ) ,  to float ( vals [ 3 ] ) ) d [ ( c1 , c2 ) ] = val raise  Runtime  Error  ( u' Bad   kern  pairs  parse' ) 
def redirect back ( url , source domain ) : parse data = urlparse ( url ) domain = parse data . netloc query = parse data . query if ( ( source domain in domain ) or ( domain in source domain ) ) : return url query item = parse qs ( query ) if query item . get ( 'url' ) : return query item [ 'url' ] [ 0 ] return url 
def getnodes ( tree ) : if isinstance ( tree , tuple ) : ( name , subtree ) = tree ab = [ name ] al = [ ] if ( len ( subtree ) == 1 ) : adeg = [ name ] else : adeg = [ ] for st in subtree : ( b , l , d ) = getnodes ( st ) ab . extend ( b ) al . extend ( l ) adeg . extend ( d ) return ( ab , al , adeg ) return ( [ ] , [ tree ] , [ ] ) 
def  raise error routes ( iface , option , expected ) : msg =  error msg routes ( iface , option , expected ) log . error ( msg ) raise  Attribute  Error  ( msg ) 
def update ( name , profile = 'splunk' , ** kwargs ) : client =  get splunk ( profile ) search = client . saved searches [ name ] props =  get splunk search props ( search ) updates = kwargs update needed =  False  update set = dict ( ) diffs = [ ] for key in sorted ( kwargs ) : old value = props . get ( key ,  None  ) new value = updates . get ( key ,  None  ) if isinstance ( old value , six . string types ) : old value = old value . strip ( ) if isinstance ( new value , six . string types ) : new value = new value . strip ( ) if ( old value != new value ) : update set [ key ] = new value update needed =  True  diffs . append ( "{0}:  '{1}'  =>  '{2}'" . format ( key , old value , new value ) ) if update needed : search . update ( ** update set ) . refresh ( ) return ( update set , diffs ) return  False  
def wf ( ) : global  wf if (  wf is  None  ) :  wf = workflow .  Workflow  ( ) return  wf 
@ utils . arg ( 'server' , metavar = '<server>' , help =   ( ' Name   or  ID  of  server.' ) ) @ utils . arg ( 'console type' , metavar = '<console-type>' , help =   ( ' Type   of  rdp  console  ("rdp-html5").' ) ) def do get rdp console ( cs , args ) : server =  find server ( cs , args . server ) data = server . get rdp console ( args . console type ) print console ( cs , data ) 
def iddr asvd ( A , k ) : A = np . asfortranarray ( A ) ( m , n ) = A . shape w = np . empty ( ( ( ( ( ( ( 2 * k ) + 28 ) * m ) + ( ( ( 6 * k ) + 21 ) * n ) ) + ( 25 * ( k ** 2 ) ) ) + 100 ) , order = 'F' ) w  = iddr aidi ( m , n , k ) w [ : w  . size ] = w  ( U , V , S , ier ) =  id . iddr asvd ( A , k , w ) if ( ier != 0 ) : raise  RETCODE ERROR return ( U , V , S ) 
@ pytest . fixture def js tester ( webview , qtbot ) : return JS Tester  ( webview , qtbot ) 
def fix eols ( s ) : s = re . sub ( '(?<!\\r)\\n' , CRLF , s ) s = re . sub ( '\\r(?!\\n)' , CRLF , s ) return s 
def  contains yieldpoint ( children ) : if isinstance ( children , dict ) : return any ( ( isinstance ( i ,  Yield  Point  ) for i in children . values ( ) ) ) if isinstance ( children , list ) : return any ( ( isinstance ( i ,  Yield  Point  ) for i in children ) ) return  False  
@ app . route ( '/library' ) def library ( ) : papers = papers from library ( ) ret = encode json ( papers , 500 ) if g . user : msg = ( '%d  papers  in  your  library:' % ( len ( ret ) , ) ) else : msg = ' You   must  be  logged  in.   Once   you  are,  you  can  save  papers  to  your  library  (with  the  save  icon  on  the  right  of  each  paper)  and  they  will  show  up  here.' ctx = default context ( papers , render format = 'library' , msg = msg ) return render template ( 'main.html' , ** ctx ) 
def add params to uri ( uri , params ) : ( sch , net , path , par , query , fra ) = urlparse . urlparse ( uri ) query = add params to qs ( query , params ) return urlparse . urlunparse ( ( sch , net , path , par , query , fra ) ) 
@ click . command ( name = 'import' ) @ click . argument ( 'src' , type = click .  File  ( 'rb' ) ) @ configuration def import  ( src ) : from django . core import serializers for obj in serializers . deserialize ( 'json' , src , stream =  True  , use natural keys =  True  ) : obj . save ( ) 
def recompose xfm ( in bval , in xfms ) : import numpy as np import os . path as op bvals = np . loadtxt ( in bval ) out matrix = np . array ( ( [ np . eye ( 4 ) ] * len ( bvals ) ) ) xfms = iter ( [ np . loadtxt ( xfm ) for xfm in in xfms ] ) out files = [ ] for ( i , b ) in enumerate ( bvals ) : if ( b == 0.0 ) : mat = np . eye ( 4 ) else : mat = next ( xfms ) out name = op . abspath ( ( u'eccor %04d.mat' % i ) ) out files . append ( out name ) np . savetxt ( out name , mat ) return out files 
def reader ( file obj , columns =  None  ) : if ( hasattr ( file obj , u'mode' ) and ( u'b' not in file obj . mode ) ) : logger . error ( u'parquet.reader  requires  the  fileobj  to  be  opened  in  binary  mode!' ) footer =  read footer ( file obj ) schema helper = schema .  Schema  Helper  ( footer . schema ) keys = ( columns if columns else [ s . name for s in footer . schema if s . type ] ) debug logging = logger . is Enabled  For  ( logging . DEBUG ) for row group in footer . row groups : res = defaultdict ( list ) row group rows = row group . num rows for col group in row group . columns : dict items = [ ] cmd = col group . meta data if ( columns and ( not ( u'.' . join ( cmd . path in schema ) in columns ) ) ) : continue offset =  get offset ( cmd ) file obj . seek ( offset , 0 ) values seen = 0 if debug logging : logger . debug ( u'reading  column  chunk  of  type:  %s' ,  get name ( parquet thrift .  Type  , cmd . type ) ) while ( values seen < row group rows ) : page header =  read page header ( file obj ) if debug logging : logger . debug ( u' Reading   page  (type=%s,  uncompressed=%s  bytes,  compressed=%s  bytes)' ,  get name ( parquet thrift .  Page  Type  , page header . type ) , page header . uncompressed page size , page header . compressed page size ) if ( page header . type == parquet thrift .  Page  Type  . DATA PAGE ) : values = read data page ( file obj , schema helper , page header , cmd , dict items ) res [ u'.' . join ( cmd . path in schema ) ] += values values seen += page header . data page header . num values elif ( page header . type == parquet thrift .  Page  Type  . DICTIONARY PAGE ) : if debug logging : logger . debug ( page header ) assert ( dict items == [ ] ) dict items =  read dictionary page ( file obj , schema helper , page header , cmd ) if debug logging : logger . debug ( u' Dictionary :  %s' , str ( dict items ) ) else : logger . info ( u' Skipping   unknown  page  type=%s' ,  get name ( parquet thrift .  Page  Type  , page header . type ) ) for i in range ( row group . num rows ) : ( yield [ res [ k ] [ i ] for k in keys if res [ k ] ] ) 
def unpooling 2d ( x , ksize , stride =  None  , pad = 0 , outsize =  None  , cover all =  True  ) : return  Unpooling 2D ( ksize , stride , pad , outsize , cover all ) ( x ) 
def update requirements main ( args , base path , top level ) : options =  Update  Requirements  Options  ( ) try : options . parse Options  ( args ) except  Usage  Error  as e : sys . stderr . write ( u'{}\n Usage    Error :  {}:  {}\n' . format ( unicode ( options ) , base path . basename ( ) , e ) . encode ( 'utf-8' ) ) raise  System  Exit  ( 1 ) requirements directory = top level . child ( 'requirements' ) dockerfile = top level . descendant ( [ 'admin' , 'requirements. Dockerfile ' ] ) if ( not options [ 'no-build' ] ) : build requirements image ( REQUIREMENTS IMAGE , dockerfile , requirements directory ) for infile in requirements directory . glob Children  ( '*.in' ) : requirements from infile ( infile ) 
def win find exe ( filename , installsubdir =  None  , env = ' Program  Files ' ) : if ( not WINDOWS ) : return for fn in [ filename , ( filename + '.exe' ) ] : try : if ( installsubdir is  None  ) : path =  where ( fn ) else : path =  where ( fn , dirs = [ os . path . join ( os . environ [ env ] , installsubdir ) ] ) except IO Error  : path = filename else : break return path 
def make weighted tree ( fn , ls , ** kwargs ) : if ( not ls ) : raise  Value  Error  ( ' Called   make weighted tree  with  empty  list' ) ls . sort ( ) while ( len ( ls ) > 1 ) : a = ls . pop ( 0 ) b = ls . pop ( 0 ) insort ( ls , ( ( a [ 0 ] + b [ 0 ] ) , fn ( a [ 1 ] , b [ 1 ] ) ) ) return ls [ 0 ] [ 1 ] 
def test deleted folder on fetch ( monkeypatch , generic client , constants ) : def raise invalid uid exc ( * args , ** kwargs ) : raise imapclient . IMAP Client  .  Error  ( '[UNAVAILABLE]  UID  FETCH   Server   error  while  fetching  messages' ) monkeypatch . setattr ( 'imapclient.IMAP Client .fetch' , raise invalid uid exc ) generic client . uids ( [ '125' ] ) 
def  int or float ( s ) : if isinstance ( s , float ) : return s try : return int ( s ) except (  Value  Error  ,  Type  Error  ) : try : return float ( s ) except (  Value  Error  ,  Type  Error  ) as e : raise  Value  Error  ( str ( e ) ) 
@ not implemented for ( 'directed' ) def is connected ( G ) : if ( len ( G ) == 0 ) : raise nx .  Network X Pointless  Concept  ( ' Connectivity   is  undefined  ' , 'for  the  null  graph.' ) return ( len ( set (  plain bfs ( G , arbitrary element ( G ) ) ) ) == len ( G ) ) 
def   Can  Break  Before  ( prev token , cur token ) : pval = prev token . value cval = cur token . value if py3compat . PY3 : if ( ( pval == 'yield' ) and ( cval == 'from' ) ) : return  False  if ( ( pval in { 'async' , 'await' } ) and ( cval in { 'def' , 'with' , 'for' } ) ) : return  False  if ( cur token . split penalty >= split penalty . UNBREAKABLE ) : return  False  if ( pval == '@' ) : return  False  if ( cval == ':' ) : return  False  if ( cval == ',' ) : return  False  if ( prev token . is name and ( cval == '(' ) ) : return  False  if ( prev token . is name and ( cval == '[' ) ) : return  False  if ( prev token . is name and ( cval == '.' ) ) : return  False  if ( cur token . is comment and ( prev token . lineno == cur token . lineno ) ) : return  False  if ( format token .  Subtype  . UNARY OPERATOR in prev token . subtypes ) : return  False  return  True  
def test deprecated class with super ( ) : @ deprecated ( u'100.0' ) class TB ( object , ) : def   init   ( self , a , b ) : super ( TB , self ) .   init   ( ) with catch warnings (  Astropy  Deprecation  Warning  ) as w : TB ( 1 , 2 ) assert ( len ( w ) == 1 ) if ( TB .   doc   is not  None  ) : assert ( u'function' not in TB .   doc   ) assert ( u'deprecated' in TB .   doc   ) assert ( u'function' not in TB .   init   .   doc   ) assert ( u'deprecated' in TB .   init   .   doc   ) 
def  quit editor ( caller ) : del caller . db .  multidesc editkey caller . msg ( ' Exited   editor.' ) 
def make link targets ( proj name , user name , repo name , known link fname , out link fname , url =  None  , ml url =  None  ) : link contents = open ( known link fname , 'rt' ) . readlines ( ) have url = ( not ( url is  None  ) ) have ml url = ( not ( ml url is  None  ) ) have gh url =  None  for line in link contents : if ( not have url ) : match = re . match ( ( '..\\s+ `%s`:\\s+' % proj name ) , line ) if match : have url =  True  if ( not have ml url ) : match = re . match ( ( '..\\s+ `%s  mailing  list`:\\s+' % proj name ) , line ) if match : have ml url =  True  if ( not have gh url ) : match = re . match ( ( '..\\s+ `%s  github`:\\s+' % proj name ) , line ) if match : have gh url =  True  if ( ( not have url ) or ( not have ml url ) ) : raise  Runtime  Error  ( ' Need   command  line  or  known  project  and  /  or  mailing  list  UR Ls ' ) lines = [ ] if ( not ( url is  None  ) ) : lines . append ( ( '..   `%s`:  %s\n' % ( proj name , url ) ) ) if ( not have gh url ) : gh url = ( 'http://github.com/%s/%s\n' % ( user name , repo name ) ) lines . append ( ( '..   `%s  github`:  %s\n' % ( proj name , gh url ) ) ) if ( not ( ml url is  None  ) ) : lines . append ( ( '..   `%s  mailing  list`:  %s\n' % ( proj name , ml url ) ) ) if ( len ( lines ) == 0 ) : return lines = ( [ ( '..  %s\n' % proj name ) ] + lines ) out links = open ( out link fname , 'wt' ) out links . writelines ( lines ) out links . close ( ) 
def supports librabbitmq ( ) : if (  detect environment ( ) == u'default' ) : try : import librabbitmq except  Import  Error  : pass else : return  True  
def  declarative constructor ( self , ** kwargs ) : cls  = type ( self ) for k in kwargs : if ( not hasattr ( cls  , k ) ) : raise  Type  Error  ( ( '%r  is  an  invalid  keyword  argument  for  %s' % ( k , cls  .   name   ) ) ) setattr ( self , k , kwargs [ k ] ) 
def safe string equals ( a , b ) : if ( len ( a ) != len ( b ) ) : return  False  result = 0 for ( x , y ) in zip ( a , b ) : result |= ( ord ( x ) ^ ord ( y ) ) return ( result == 0 ) 
def use setuptools ( version = DEFAULT VERSION , download base = DEFAULT URL , to dir = DEFAULT SAVE DIR , download delay = 15 ) : version =  resolve version ( version ) to dir = os . path . abspath ( to dir ) rep modules = ( 'pkg resources' , 'setuptools' ) imported = set ( sys . modules ) . intersection ( rep modules ) try : import pkg resources pkg resources . require ( ( 'setuptools>=' + version ) ) return except  Import  Error  : pass except pkg resources .  Distribution  Not  Found  : pass except pkg resources .  Version  Conflict  as VC err : if imported :  conflict bail ( VC err , version ) del pkg resources  unload pkg resources ( ) return  do download ( version , download base , to dir , download delay ) 
def get flow ( context , manager , db , driver , scheduler rpcapi , host , volume , allow reschedule , reschedule context , request spec , filter properties , image volume cache =  None  ) : flow name = ( ACTION . replace ( ':' , ' ' ) + ' manager' ) volume flow = linear flow .  Flow  ( flow name ) create what = { 'context' : context , 'filter properties' : filter properties , 'request spec' : request spec , 'volume' : volume } volume flow . add (  Extract  Volume  Ref  Task  ( db , host , set error =  False  ) ) retry = filter properties . get ( 'retry' ,  None  ) do reschedule = ( allow reschedule and request spec and retry ) volume flow . add (  On  Failure  Reschedule  Task  ( reschedule context , db , driver , scheduler rpcapi , do reschedule ) ) LOG . debug ( ' Volume   reschedule  parameters:  %(allow)s  retry:  %(retry)s' , { 'allow' : allow reschedule , 'retry' : retry } ) volume flow . add (  Extract  Volume  Spec  Task  ( db ) ,  Notify  Volume  Action  Task  ( db , 'create.start' ) ,  Create  Volume  From  Spec  Task  ( manager , db , driver , image volume cache ) ,  Create  Volume  On  Finish  Task  ( db , 'create.end' ) ) return taskflow . engines . load ( volume flow , store = create what ) 
def capwords ( s , sep =  None  ) : return ( sep or '  ' ) . join ( [ x . capitalize ( ) for x in s . split ( sep ) ] ) 
def is Prime  ( N ) : if ( N == 1 ) : return 0 if ( N in sieve ) : return 1 for i in sieve : if ( ( N % i ) == 0 ) : return 0 if (  fastmath is not  None  ) : return  fastmath . is Prime  ( N ) N1 = ( N - 1 L ) n = 1 L while ( n < N ) : n = ( n << 1 L ) n = ( n >> 1 L ) for c in sieve [ : 7 ] : a = long ( c ) d = 1 L t = n while t : x = ( ( d * d ) % N ) if ( ( x == 1 L ) and ( d != 1 L ) and ( d != N1 ) ) : return 0 if ( N1 & t ) : d = ( ( x * a ) % N ) else : d = x t = ( t >> 1 L ) if ( d != 1 L ) : return 0 return 1 
def test mapnode json ( tmpdir ) : wd = str ( tmpdir ) os . chdir ( wd ) from nipype import  Map  Node  ,  Function  ,  Workflow  def func1 ( in1 ) : return ( in1 + 1 ) n1 =  Map  Node  (  Function  ( input names = [ u'in1' ] , output names = [ u'out' ] , function = func1 ) , iterfield = [ u'in1' ] , name = u'n1' ) n1 . inputs . in1 = [ 1 ] w1 =  Workflow  ( name = u'test' ) w1 . base dir = wd w1 . config [ u'execution' ] [ u'crashdump dir' ] = wd w1 . add nodes ( [ n1 ] ) w1 . run ( ) n1 . inputs . in1 = [ 2 ] w1 . run ( ) n1 . inputs . in1 = [ 1 ] eg = w1 . run ( ) node = eg . nodes ( ) [ 0 ] outjson = glob ( os . path . join ( node . output dir ( ) , u' 0x*.json' ) ) assert ( len ( outjson ) == 1 ) with open ( os . path . join ( node . output dir ( ) , u'test.json' ) , u'wt' ) as fp : fp . write ( u'dummy  file' ) w1 . config [ u'execution' ] . update ( ** { u'stop on first rerun' :  True  } ) w1 . run ( ) 
def cbranch or continue ( builder , cond , bbtrue ) : bbcont = builder . append basic block ( '.continue' ) builder . cbranch ( cond , bbtrue , bbcont ) builder . position at end ( bbcont ) return bbcont 
def update live symlinks ( config ) : for renewal file in storage . renewal conf files ( config ) : storage .  Renewable  Cert  ( renewal file , config , update symlinks =  True  ) 
def norm ( x , ord =  None  , axis =  None  ) : if ( not issparse ( x ) ) : raise  Type  Error  ( 'input  is  not  sparse.  use  numpy.linalg.norm' ) if ( ( axis is  None  ) and ( ord in (  None  , 'fro' , 'f' ) ) ) : return  sparse frobenius norm ( x ) x = x . tocsr ( ) if ( axis is  None  ) : axis = ( 0 , 1 ) elif ( not isinstance ( axis , tuple ) ) : msg = "'axis'  must  be   None ,  an  integer  or  a  tuple  of  integers" try : int axis = int ( axis ) except  Type  Error  : raise  Type  Error  ( msg ) if ( axis != int axis ) : raise  Type  Error  ( msg ) axis = ( int axis , ) nd = 2 if ( len ( axis ) == 2 ) : ( row axis , col axis ) = axis if ( not ( ( ( - nd ) <= row axis < nd ) and ( ( - nd ) <= col axis < nd ) ) ) : raise  Value  Error  ( ( ' Invalid   axis  %r  for  an  array  with  shape  %r' % ( axis , x . shape ) ) ) if ( ( row axis % nd ) == ( col axis % nd ) ) : raise  Value  Error  ( ' Duplicate   axes  given.' ) if ( ord == 2 ) : raise  Not  Implemented  Error  elif ( ord == ( - 2 ) ) : raise  Not  Implemented  Error  elif ( ord == 1 ) : return abs ( x ) . sum ( axis = row axis ) . max ( axis = col axis ) [ ( 0 , 0 ) ] elif ( ord ==  Inf  ) : return abs ( x ) . sum ( axis = col axis ) . max ( axis = row axis ) [ ( 0 , 0 ) ] elif ( ord == ( - 1 ) ) : return abs ( x ) . sum ( axis = row axis ) . min ( axis = col axis ) [ ( 0 , 0 ) ] elif ( ord == ( -  Inf  ) ) : return abs ( x ) . sum ( axis = col axis ) . min ( axis = row axis ) [ ( 0 , 0 ) ] elif ( ord in (  None  , 'f' , 'fro' ) ) : return  sparse frobenius norm ( x ) else : raise  Value  Error  ( ' Invalid   norm  order  for  matrices.' ) elif ( len ( axis ) == 1 ) : ( a , ) = axis if ( not ( ( - nd ) <= a < nd ) ) : raise  Value  Error  ( ( ' Invalid   axis  %r  for  an  array  with  shape  %r' % ( axis , x . shape ) ) ) if ( ord ==  Inf  ) : M = abs ( x ) . max ( axis = a ) elif ( ord == ( -  Inf  ) ) : M = abs ( x ) . min ( axis = a ) elif ( ord == 0 ) : M = ( x != 0 ) . sum ( axis = a ) elif ( ord == 1 ) : M = abs ( x ) . sum ( axis = a ) elif ( ord in ( 2 ,  None  ) ) : M = sqrt ( abs ( x ) . power ( 2 ) . sum ( axis = a ) ) else : try : ( ord + 1 ) except  Type  Error  : raise  Value  Error  ( ' Invalid   norm  order  for  vectors.' ) M = np . power ( abs ( x ) . power ( ord ) . sum ( axis = a ) , ( 1 / ord ) ) return M . A . ravel ( ) else : raise  Value  Error  ( ' Improper   number  of  dimensions  to  norm.' ) 
def p relational expression 3 ( t ) : pass 
def scan used functions ( example file , gallery conf ) : example code obj = identify names ( open ( example file ) . read ( ) ) if example code obj : codeobj fname = ( example file [ : ( - 3 ) ] + ' codeobj.pickle' ) with open ( codeobj fname , 'wb' ) as fid : pickle . dump ( example code obj , fid , pickle . HIGHEST PROTOCOL ) backrefs = set ( ( '{module short}.{name}' . format ( ** entry ) for entry in example code obj . values ( ) if entry [ 'module' ] . startswith ( gallery conf [ 'doc module' ] ) ) ) return backrefs 
def transitivity ( G ) : triangles = sum ( ( t for ( v , d , t ,   ) in  triangles and degree iter ( G ) ) ) contri = sum ( ( ( d * ( d - 1 ) ) for ( v , d , t ,   ) in  triangles and degree iter ( G ) ) ) return ( 0 if ( triangles == 0 ) else ( triangles / contri ) ) 
def  make unique ( l ) : return sorted ( set ( l ) ) 
def  get repo options ( fromrepo =  None  , packagesite =  None  ) : root = ( fromrepo if ( fromrepo is not  None  ) else   salt   [ 'config.get' ] ( 'freebsdpkg.PACKAGEROOT' ,  None  ) ) site = ( packagesite if ( packagesite is not  None  ) else   salt   [ 'config.get' ] ( 'freebsdpkg.PACKAGESITE' ,  None  ) ) ret = { } if ( root is not  None  ) : ret [ 'PACKAGEROOT' ] = root if ( site is not  None  ) : ret [ 'PACKAGESITE' ] = site return ret 
def  Analyze APK ( filename , decompiler = 'dad' , session =  None  ) : androconf . debug ( ' Analyze APK' ) if ( not session ) : session = CONF [ 'SESSION' ] with open ( filename , 'r' ) as fd : data = fd . read ( ) session . add ( filename , data ) return session . get objects apk ( filename ) 
def copytree ( src , dst , symlinks =  False  , ignore =  None  ) : if ( not os . path . exists ( dst ) ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if ( x not in excl ) ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if ( symlinks and os . path . islink ( s ) ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S IMODE ( st . st mode ) os . lchmod ( d , mode ) except : pass elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d ) 
def run all ( delay seconds = 0 ) : default scheduler . run all ( delay seconds = delay seconds ) 
def numel ( x , ** kwargs ) : return chunk . sum ( np . ones like ( x ) , ** kwargs ) 
def  resolve name ( name , package , level ) : level -= 1 try : if ( package . count ( '.' ) < level ) : raise  Value  Error  ( 'attempted  relative  import  beyond  top-level  package' ) except  Attribute  Error  : raise  Value  Error  ( "'package'  not  set  to  a  string" ) try : dot rindex = package . rindex ( '.' , level ) [ 0 ] base = package [ : dot rindex ] except  Value  Error  : base = package if name : return ( '%s.%s' % ( base , name ) ) else : return base 
def attr as boolean ( val attr ) : return strutils . bool from string ( val attr , default =  True  ) 
def base repr ( number , base = 2 , padding = 0 ) : chars = u'0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ' if ( number < base ) : return ( ( ( padding - 1 ) * chars [ 0 ] ) + chars [ int ( number ) ] ) max exponent = int ( ( math . log ( number ) / math . log ( base ) ) ) max power = ( long ( base ) ** max exponent ) lead digit = int ( ( number / max power ) ) return ( chars [ lead digit ] + base repr ( ( number - ( max power * lead digit ) ) , base , max ( ( padding - 1 ) , max exponent ) ) ) 
def get session config ( ) : return copy . deepcopy (  session [ 'config' ] ) 
def parse cache control header ( value , on update =  None  , cls =  None  ) : if ( cls is  None  ) : cls =  Request  Cache  Control  if ( not value ) : return cls (  None  , on update ) return cls ( parse dict header ( value ) , on update ) 
def unique ( items ) : seen = set ( ) for item in items : if ( item in seen ) : continue else : ( yield item ) seen . add ( item ) 
def arbitrary string ( size = 4 , base text =  None  ) : if ( not base text ) : base text = 'test' return '' . join ( itertools . islice ( itertools . cycle ( base text ) , size ) ) 
def expand path ( path ) : return os . path . expandvars ( os . path . expanduser ( path ) ) 
def   virtual   ( ) : if ( 'win snmp.get agent settings' in   salt   ) : return  True  return  False  
def earned exp ( base exp , level ) : return ( ( base exp * level ) // 7 ) 
def timeit ( func , iter = 1000 , * args , ** kwargs ) : from time import time as current Time  r = range ( iter ) t = current Time  ( ) for i in r : func ( * args , ** kwargs ) return ( current Time  ( ) - t ) 
def configurable test state ( name , changes =  True  , result =  True  , comment = '' ) : ret = { 'name' : name , 'changes' : { } , 'result' :  False  , 'comment' : comment } change data = { 'testing' : { 'old' : ' Unchanged ' , 'new' : ' Something   pretended  to  change' } } if ( changes == ' Random ' ) : if random . choice ( [  True  ,  False  ] ) : ret [ 'changes' ] = change data elif ( changes is  True  ) : ret [ 'changes' ] = change data elif ( changes is  False  ) : ret [ 'changes' ] = { } else : err = " You   have  specified  the  state  option  ' Changes '  with  invalid  arguments.   It   must  be  either    ' True ',  ' False ',  or  ' Random '" raise  Salt  Invocation  Error  ( err ) if ( result == ' Random ' ) : ret [ 'result' ] = random . choice ( [  True  ,  False  ] ) elif ( result is  True  ) : ret [ 'result' ] =  True  elif ( result is  False  ) : ret [ 'result' ] =  False  else : raise  Salt  Invocation  Error  ( " You   have  specified  the  state  option  ' Result '  with  invalid  arguments.   It   must  be  either  ' True ',  ' False ',  or  ' Random '" ) if   opts   [ 'test' ] : ret [ 'result' ] = (  True  if ( changes is  False  ) else  None  ) ret [ 'comment' ] = ( ' This   is  a  test' if ( not comment ) else comment ) return ret 
def  fetch file ( url , file name , print destination =  True  ) : temp file name = ( file name + '.part' ) local file =  None  initial size = 0 n try = 3 for ii in range ( n try ) : try : data = urllib . request . urlopen ( url , timeout = 15.0 ) except  Exception  as e : if ( ii == ( n try - 1 ) ) : raise  Runtime  Error  ( ( ' Error   while  fetching  file  %s.\n Dataset   fetching  aborted  (%s)' % ( url , e ) ) ) try : file size = int ( data . headers [ ' Content - Length ' ] . strip ( ) ) print ( ' Downloading   data  from  %s  (%s)' % ( url , sizeof fmt ( file size ) ) ) local file = open ( temp file name , 'wb' )  chunk read ( data , local file , initial size = initial size ) if ( not local file . closed ) : local file . close ( ) shutil . move ( temp file name , file name ) if ( print destination is  True  ) : sys . stdout . write ( ( ' File   saved  as  %s.\n' % file name ) ) except  Exception  as e : raise  Runtime  Error  ( ( ' Error   while  fetching  file  %s.\n Dataset   fetching  aborted  (%s)' % ( url , e ) ) ) finally : if ( local file is not  None  ) : if ( not local file . closed ) : local file . close ( ) 
def plot ccpr grid ( results , exog idx =  None  , grid =  None  , fig =  None  ) : fig = utils . create mpl fig ( fig ) ( exog name , exog idx ) = utils . maybe name or idx ( exog idx , results . model ) if ( grid is not  None  ) : ( nrows , ncols ) = grid elif ( len ( exog idx ) > 2 ) : nrows = int ( np . ceil ( ( len ( exog idx ) / 2.0 ) ) ) ncols = 2 else : nrows = len ( exog idx ) ncols = 1 seen constant = 0 for ( i , idx ) in enumerate ( exog idx ) : if ( results . model . exog [ : , idx ] . var ( ) == 0 ) : seen constant = 1 continue ax = fig . add subplot ( nrows , ncols , ( ( i + 1 ) - seen constant ) ) fig = plot ccpr ( results , exog idx = idx , ax = ax ) ax . set title ( '' ) fig . suptitle ( ' Component - Component    Plus    Residual    Plot ' , fontsize = 'large' ) fig . tight layout ( ) fig . subplots adjust ( top = 0.95 ) return fig 
def to text ( obj , encoding = 'utf-8' , errors =  None  , nonstring = 'simplerepr' ) : if isinstance ( obj , text type ) : return obj if ( errors in (  None  , 'surrogate or replace' ) ) : if HAS SURROGATEESCAPE : errors = 'surrogateescape' else : errors = 'replace' elif ( errors == 'surrogate or strict' ) : if HAS SURROGATEESCAPE : errors = 'surrogateescape' else : errors = 'strict' if isinstance ( obj , binary type ) : return obj . decode ( encoding , errors ) if ( nonstring == 'simplerepr' ) : try : value = str ( obj ) except  Unicode  Error  : try : value = repr ( obj ) except  Unicode  Error  : return u'' elif ( nonstring == 'passthru' ) : return obj elif ( nonstring == 'empty' ) : return u'' elif ( nonstring == 'strict' ) : raise  Type  Error  ( 'obj  must  be  a  string  type' ) else : raise  Type  Error  ( ( " Invalid   value  %s  for  to text's  nonstring  parameter" % nonstring ) ) return to text ( value , encoding , errors ) 
def profile list ( request , page = 1 , template name = 'userena/profile list.html' , paginate by = 50 , extra context =  None  , ** kwargs ) : warnings . warn ( 'views.profile list  is  deprecated.   Use    Profile  List  View   instead' ,  Deprecation  Warning  , stacklevel = 2 ) try : page = int ( request . GET . get ( 'page' ,  None  ) ) except (  Type  Error  ,  Value  Error  ) : page = page if ( userena settings . USERENA DISABLE PROFILE LIST and ( not request . user . is staff ) ) : raise  Http 404 profile model = get profile model ( ) queryset = profile model . objects . get visible profiles ( request . user ) if ( not extra context ) : extra context = dict ( ) return  Profile  List  View  . as view ( queryset = queryset , paginate by = paginate by , page = page , template name = template name , extra context = extra context , ** kwargs ) ( request ) 
def  Get  Images  ( region , owner ids =  None  ) : ec2 =   Connect  ( region ) if ( not owner ids ) : return  None  return ec2 . get all images ( owners = owner ids ) 
@ check is trading @ export as api @  Execution  Context  . enforce phase ( EXECUTION PHASE . HANDLE BAR , EXECUTION PHASE . SCHEDULED ) def order target percent ( id or ins , percent , style =  None  ) : order book id = assure order book id ( id or ins ) bar dict =  Execution  Context  . get current bar dict ( ) price = bar dict [ order book id ] . close position = get simu exchange ( ) . account . portfolio . positions [ order book id ] current value = ( position . quantity * price ) portfolio value = get simu exchange ( ) . account . portfolio . portfolio value return order value ( order book id , ( ( portfolio value * percent ) - current value ) , style ) 
def  limit ( query , hints ) : if hints . limit : original len = query . count ( ) limit query = query . limit ( hints . limit [ 'limit' ] ) if ( limit query . count ( ) < original len ) : hints . limit [ 'truncated' ] =  True  query = limit query return query 
def randn ( * size , ** kwarg ) : dtype = kwarg . pop ( 'dtype' , float ) if kwarg : raise  Type  Error  ( ( 'randn()  got  unexpected  keyword  arguments  %s' % ',  ' . join ( kwarg . keys ( ) ) ) ) return distributions . normal ( size = size , dtype = dtype ) 
def get template ( template name , globals =  None  ) : try : return get env ( ) . get template ( template name , globals = globals ) except  Template  Not  Found  as e : raise  Template  Does  Not  Exist  ( str ( e ) ) 
def init ( ) : ret Val  =  False  if have Bits DLL : try : ret Val  =  bits . bits Init  ( ) except  Exception  : logging . error ( 'bits.init()  barfed!' ) return ret Val  
def get effective router ( appname ) : if ( ( not routers ) or ( appname not in routers ) ) : return  None  return  Storage  ( routers [ appname ] ) 
def gf crt ( U , M , K =  None  ) : p = prod ( M , start = K . one ) v = K . zero for ( u , m ) in zip ( U , M ) : e = ( p // m ) ( s ,   ,   ) = K . gcdex ( e , m ) v += ( e * ( ( u * s ) % m ) ) return ( v % p ) 
def is image extendable ( image ) : LOG . debug ( ' Checking   if  we  can  extend  filesystem  inside  %(image)s.' , { 'image' : image } ) if ( ( not isinstance ( image , imgmodel .  Local  Image  ) ) or ( image . format != imgmodel . FORMAT RAW ) ) : fs =  None  try : fs = vfs . VFS . instance for image ( image ,  None  ) fs . setup ( mount =  False  ) if ( fs . get image fs ( ) in SUPPORTED FS TO EXTEND ) : return  True  except exception .  Nova  Exception  as e : LOG . warning (  LW ( ' Unable   to  mount  image  %(image)s  with  error  %(error)s.   Cannot   resize.' ) , { 'image' : image , 'error' : e } ) finally : if ( fs is not  None  ) : fs . teardown ( ) return  False  else : try : utils . execute ( 'e2label' , image . path ) except processutils .  Process  Execution  Error  as e : LOG . debug ( ' Unable   to  determine  label  for  image  %(image)s  with  error  %(error)s.   Cannot   resize.' , { 'image' : image , 'error' : e } ) return  False  return  True  
def s3 validate ( table , field , value , record =  None  ) : default = ( value ,  None  ) if isinstance ( field , basestring ) : fieldname = field if ( fieldname in table . fields ) : field = table [ fieldname ] else : return default else : fieldname = field . name self id =  None  if ( record is not  None  ) : try : v = record [ field ] except : v =  None  if ( v and ( v == value ) ) : return default try : self id = record [ table .  id ] except : pass requires = field . requires if ( field . unique and ( not requires ) ) : field . requires = IS NOT IN DB ( current . db , str ( field ) ) if self id : field . requires . set self id ( self id ) elif self id : if ( not isinstance ( requires , ( list , tuple ) ) ) : requires = [ requires ] for r in requires : if hasattr ( r , 'set self id' ) : r . set self id ( self id ) if ( hasattr ( r , 'other' ) and hasattr ( r . other , 'set self id' ) ) : r . other . set self id ( self id ) try : ( value , error ) = field . validate ( value ) except : current . log . error ( ( ' Validate   %s:  %s  (ignored)' % ( field , sys . exc info ( ) [ 1 ] ) ) ) return (  None  ,  None  ) else : return ( value , error ) 
def read packet ( sock , timeout =  None  ) : sock . settimeout ( timeout ) ( dlen , data ) = (  None  ,  None  ) try : if ( os . name == 'nt' ) : datalen = sock . recv ( SZ ) ( dlen , ) = struct . unpack ( 'l' , datalen ) data = '' while ( len ( data ) < dlen ) : data += sock . recv ( dlen ) else : datalen = temp fail retry ( socket . error , sock . recv , SZ , socket . MSG WAITALL ) if ( len ( datalen ) == SZ ) : ( dlen , ) = struct . unpack ( 'l' , datalen ) data = temp fail retry ( socket . error , sock . recv , dlen , socket . MSG WAITALL ) except socket . timeout : raise except socket . error : data =  None  finally : sock . settimeout (  None  ) if ( data is not  None  ) : try : return pickle . loads ( data ) except  Exception  : if DEBUG EDITOR : traceback . print exc ( file = STDERR ) return 
def splitvalue ( attr ) : global  valueprog if (  valueprog is  None  ) :  valueprog = re . compile ( '^([^=]*)=(.*)$' ) match =  valueprog . match ( attr ) if match : return match . group ( 1 , 2 ) return ( attr ,  None  ) 
def case ( predicate , expression tuples , default =  None  ) : clauses = [ SQL ( 'CASE' ) ] simple case = ( predicate is not  None  ) if simple case : clauses . append ( predicate ) for ( expr , value ) in expression tuples : clauses . extend ( ( SQL ( 'WHEN' ) , expr , SQL ( 'THEN' ) , value ) ) if ( default is not  None  ) : clauses . extend ( ( SQL ( 'ELSE' ) , default ) ) clauses . append ( SQL ( 'END' ) ) return  Clause  ( * clauses ) 
@ sync performer def perform list s3 keys ( dispatcher , intent ) : s3 = boto . connect s3 ( ) bucket = s3 . get bucket ( intent . bucket ) return { key . name [ len ( intent . prefix ) : ] for key in bucket . list ( intent . prefix ) } 
def is valid ipv6 address ( address , allow brackets =  False  ) : if allow brackets : if ( address . startswith ( '[' ) and address . endswith ( ']' ) ) : address = address [ 1 : ( - 1 ) ] colon count = address . count ( ':' ) if ( colon count > 7 ) : return  False  elif ( ( colon count != 7 ) and ( '::' not in address ) ) : return  False  elif ( ( address . count ( '::' ) > 1 ) or ( ':::' in address ) ) : return  False  for entry in address . split ( ':' ) : if ( not re . match ( '^[0-9a-fA-f]{0,4}$' , entry ) ) : return  False  return  True  
def get json ( url ) : res = http . get ( url ) return http . json ( res ) 
def update all ( recommended =  False  , restart =  True  ) : to update =  get available ( recommended , restart ) if ( not to update ) : return { } for  update in to update : cmd = [ 'softwareupdate' , '--install' ,  update ] salt . utils . mac utils . execute return success ( cmd ) ret = { } updates left =  get available ( ) for  update in to update : ret [  update ] = (  True  if (  update not in updates left ) else  False  ) return ret 
def bootstrap url ( postfix ) : return ( get bootstrap setting ( u'base url' ) + postfix ) 
def pull tar ( url , name , verify =  False  ) : return  pull image ( 'tar' , url , name , verify = verify ) 
def  duration pb to timedelta ( duration pb ) : return datetime . timedelta ( seconds = duration pb . seconds , microseconds = ( duration pb . nanos / 1000.0 ) ) 
@ pytest . fixture def objects ( empty history ) : ( stream ,  data , user data ) = tabhistory . serialize ( ITEMS ) qtutils . deserialize stream ( stream , empty history ) return  Objects  ( history = empty history , user data = user data ) 
@ np . deprecate ( message = 'scipy.special.sph jnyn  is  deprecated  in  scipy  0.18.0.   Use   scipy.special.spherical jn  and  scipy.special.spherical yn  instead.   Note   that  the  new  function  has  a  different  signature.' ) def sph jnyn ( n , z ) : if ( not ( isscalar ( n ) and isscalar ( z ) ) ) : raise  Value  Error  ( 'arguments  must  be  scalars.' ) if ( ( n != floor ( n ) ) or ( n < 0 ) ) : raise  Value  Error  ( 'n  must  be  a  non-negative  integer.' ) if ( n < 1 ) : n1 = 1 else : n1 = n if ( iscomplex ( z ) or less ( z , 0 ) ) : ( nm , jn , jnp , yn , ynp ) = specfun . csphjy ( n1 , z ) else : ( nm , yn , ynp ) = specfun . sphy ( n1 , z ) ( nm , jn , jnp ) = specfun . sphj ( n1 , z ) return ( jn [ : ( n + 1 ) ] , jnp [ : ( n + 1 ) ] , yn [ : ( n + 1 ) ] , ynp [ : ( n + 1 ) ] ) 
def  chain procs ( procs args , ** kwargs ) : last stdout =  None  procs = [ ] for ( i , args ) in enumerate ( procs args ) : proc kwargs = kwargs . copy ( ) if ( i > 0 ) : proc kwargs [ 'stdin' ] = last stdout if ( i < ( len ( procs args ) - 1 ) ) : proc kwargs [ 'stdout' ] = PIPE proc =  Popen  ( args , ** proc kwargs ) last stdout = proc . stdout procs . append ( proc ) return procs 
def isqref ( object ) : return ( isinstance ( object , tuple ) and ( len ( object ) == 2 ) and isinstance ( object [ 0 ] , basestring ) and isinstance ( object [ 1 ] , basestring ) ) 
def bash wrap ( cmd str ) : log . warning ( 'bash wrap()  is  deprecated  and  will  be  removed  in  v0.6.0' ) return ( "bash  -c  '%s'" % cmd str . replace ( "'" , "'\\''" ) ) 
def safe join ( directory , filename ) : filename = posixpath . normpath ( filename ) for sep in  os alt seps : if ( sep in filename ) : return  None  if ( os . path . isabs ( filename ) or filename . startswith ( '../' ) ) : return  None  return os . path . join ( directory , filename ) 
def get bucket location or error ( access key , secret key , bucket name ) : try : connection = connect s3 ( access key , secret key ) except : raise  Invalid  Auth  Error  ( ) if ( ( bucket name != bucket name . lower ( ) ) or ( '.' in bucket name ) ) : connection . calling format =  Ordinary  Calling  Format  ( ) try : return connect s3 ( access key , secret key ) . get bucket ( bucket name , validate =  False  ) . get location ( ) except exception . S3 Response  Error  : raise  Invalid  Folder  Error  ( ) 
def test string literals are prefixed ( ) : errors = [ ] for ( abs path , rel path ) in walk python files ( ) : if ( rel path in  STRING LITERALS WHITELIST ) : continue problems = find unprefixed string literals ( abs path ) if problems : errors . append ( ( rel path , problems ) ) if errors : lines = [ u' Unprefixed   string  literals:' ] for ( filename , problems ) in errors : lines . append ( ( u'    ' + filename ) ) for ( line no , col no ) in problems : lines . append ( u'        line  {},  column  {}' . format ( line no , col no ) ) raise  Assertion  Error  ( u'\n' . join ( lines ) ) 
def get gravatar ( email , size = 80 , default = 'identicon' ) : if userena settings . USERENA MUGSHOT GRAVATAR SECURE : base url = 'https://secure.gravatar.com/avatar/' else : base url = '//www.gravatar.com/avatar/' gravatar url = ( '%(base url)s%(gravatar id)s?' % { 'base url' : base url , 'gravatar id' : md5 ( email . lower ( ) . encode ( 'utf-8' ) ) . hexdigest ( ) } ) gravatar url += urlencode ( { 's' : str ( size ) , 'd' : default } ) return gravatar url 
def get ratio ( old , new ) : if ( not all ( [ old , new ] ) ) : return VERSIONING RATIO if IS SPEEDUP : return (  Levenshtein  . distance ( old , new ) / ( len ( old ) / 100.0 ) ) else : return ( levenshtein distance ( old , new ) / ( len ( old ) / 100.0 ) ) 
def thumb scale size ( orig width , orig height , width , height ) : if ( width is  None  ) : width = scale aspect ( orig width , orig height , height ) elif ( height is  None  ) : height = scale aspect ( orig height , orig width , width ) elif ( ( orig width * height ) >= ( orig height * width ) ) : width = scale aspect ( orig width , orig height , height ) else : height = scale aspect ( orig height , orig width , width ) return ( width , height ) 
@ require GET def more tweets ( request ) : max id = request . GET . get ( 'max id' ) raw filter = request . GET . get ( 'filter' ) filter = ( raw filter if ( raw filter in FILTERS ) else 'recent' ) return render ( request , 'customercare/tweets.html' , { 'tweets' :  get tweets ( locale = request . LANGUAGE CODE , max id = max id , filter = filter , https = request . is secure ( ) ) } ) 
def parse user define ( text ) : text = text . strip ( ) if ( '=' in text ) : text = unqote ( text ) ( name , value ) = text . split ( '=' , 1 ) name = name . strip ( ) value = unqote ( value . strip ( ) ) else : name = text value = 'true' return ( name , value ) 
def arguments ( function , extra arguments = 0 ) : if ( not hasattr ( function , '  code  ' ) ) : return ( ) return function .   code   . co varnames [ : ( function .   code   . co argcount + extra arguments ) ] 
def error from serialization exception ( exception , included =  False  ) : type  = collection name ( get model ( exception . instance ) ) id  = primary key value ( exception . instance ) if ( exception . message is not  None  ) : detail = exception . message else : resource = ( 'included  resource' if included else 'resource' ) detail = ' Failed   to  serialize  {0}  of  type  {1}  and  ID  {2}' detail = detail . format ( resource , type  , id  ) return error ( status = 500 , detail = detail ) 
@ route ( bp , '/<store id>/products/<product id>' , methods = [ 'PUT' ] ) def add product ( store id , product id ) : return  stores . add product (  stores . get or 404 ( store id ) ,  products . get or 404 ( product id ) ) 
def get wsgi application ( ) : django . setup ( set prefix =  False  ) return WSGI Handler  ( ) 
def getsemod ( module ) : return list semod ( ) . get ( module , { } ) 
def reformat dict keys ( keymap =  None  , inputdict =  None  ) : keymap = ( keymap or { } ) inputdict = ( inputdict or { } ) return dict ( [ ( outk , inputdict [ ink ] ) for ( ink , outk ) in keymap . items ( ) if ( ink in inputdict ) ] ) 
def main ( ) : if ( len ( sys . argv ) > 1 ) : write Output  ( '  ' . join ( sys . argv [ 1 : ] ) ) else : settings . start Main  Loop  From  Constructor  ( get New  Repository  ( ) ) 
def rename blob ( bucket name , blob name , new name ) : storage client = storage .  Client  ( ) bucket = storage client . get bucket ( bucket name ) blob = bucket . blob ( blob name ) new blob = bucket . rename blob ( blob , new name ) print ' Blob   {}  has  been  renamed  to  {}' . format ( blob . name , new blob . name ) 
def check header dups ( header , errors ) : for curr elem in range ( len ( header ) ) : if ( header . count ( header [ curr elem ] ) != 1 ) : errors . append ( ( ( '%s  found  in  header  %d  times.    ' % ( header [ curr elem ] , header . count ( header [ curr elem ] ) ) ) + ( ' Header   fields  must  be  unique. DCTB %d,%d' % ( 0 , curr elem ) ) ) ) return errors 
def get Transfer  Closest  Surrounding  Loop  ( old Ordered  Location  , remaining Surrounding  Loops  , skein ) : if ( len ( remaining Surrounding  Loops  ) > 0 ) : old Ordered  Location  . z = remaining Surrounding  Loops  [ 0 ] . z closest Distance  = 1e+18 closest Surrounding  Loop  =  None  for remaining Surrounding  Loop  in remaining Surrounding  Loops  : distance = get Nearest  Distance  Index  ( old Ordered  Location  . drop Axis  ( 2 ) , remaining Surrounding  Loop  . boundary ) . distance if ( distance < closest Distance  ) : closest Distance  = distance closest Surrounding  Loop  = remaining Surrounding  Loop  remaining Surrounding  Loops  . remove ( closest Surrounding  Loop  ) closest Surrounding  Loop  . add To  Threads  ( old Ordered  Location  , skein ) return closest Surrounding  Loop  
def sync vlan allocations ( network vlan ranges ) : session = db . get session ( ) with session . begin ( ) : allocations = dict ( ) allocs = session . query ( ovs models v2 .  Vlan  Allocation  ) . all ( ) for alloc in allocs : if ( alloc . physical network not in allocations ) : allocations [ alloc . physical network ] = set ( ) allocations [ alloc . physical network ] . add ( alloc ) for ( physical network , vlan ranges ) in network vlan ranges . iteritems ( ) : vlan ids = set ( ) for vlan range in vlan ranges : vlan ids |= set ( xrange ( vlan range [ 0 ] , ( vlan range [ 1 ] + 1 ) ) ) if ( physical network in allocations ) : for alloc in allocations [ physical network ] : try : vlan ids . remove ( alloc . vlan id ) except  Key  Error  : if ( not alloc . allocated ) : LOG . debug (   ( ' Removing   vlan  %(vlan id)s  on  physical  network  %(physical network)s  from  pool' ) , { 'vlan id' : alloc . vlan id , 'physical network' : physical network } ) session . delete ( alloc ) del allocations [ physical network ] for vlan id in sorted ( vlan ids ) : alloc = ovs models v2 .  Vlan  Allocation  ( physical network , vlan id ) session . add ( alloc ) for allocs in allocations . itervalues ( ) : for alloc in allocs : if ( not alloc . allocated ) : LOG . debug (   ( ' Removing   vlan  %(vlan id)s  on  physical  network  %(physical network)s  from  pool' ) , { 'vlan id' : alloc . vlan id , 'physical network' : alloc . physical network } ) session . delete ( alloc ) 
@ contextlib . contextmanager def convert to writable filelike ( fd , compressed =  False  ) : if isinstance ( fd , six . string types ) : if ( fd . endswith ( u'.gz' ) or compressed ) : with gzip .  Gzip  File  ( fd , u'wb' ) as real fd : encoded fd = io .  Text IO Wrapper  ( real fd , encoding = u'utf8' ) ( yield encoded fd ) encoded fd . flush ( ) real fd . flush ( ) return else : with io . open ( fd , u'wt' , encoding = u'utf8' ) as real fd : ( yield real fd ) return elif hasattr ( fd , u'write' ) : assert six . callable ( fd . write ) if compressed : fd = gzip .  Gzip  File  ( fileobj = fd ) needs wrapper =  False  try : fd . write ( u'' ) except  Type  Error  : needs wrapper =  True  if ( ( not hasattr ( fd , u'encoding' ) ) or ( fd . encoding is  None  ) ) : needs wrapper =  True  if needs wrapper : ( yield codecs . getwriter ( u'utf-8' ) ( fd ) ) fd . flush ( ) else : ( yield fd ) fd . flush ( ) return else : raise  Type  Error  ( u' Can   not  be  coerced  to  writable  file-like  object' ) 
@ contextmanager def pushd ( directory ) : cwd = os . getcwd ( ) os . chdir ( directory ) try : ( yield directory ) finally : os . chdir ( cwd ) 
def processXML Element  ( xml Element  ) : targets = evaluate . getXML Elements  By  Key  ( 'target' , xml Element  ) if ( len ( targets ) < 1 ) : print ' Warning ,  processXML Element   in  write  could  not  get  targets  for:' print xml Element  return file Names  = [ ] for target in targets : writeXML Element  ( file Names  , target , xml Element  ) 
def  check for unavailable sdk (  config vars ) : cflags =  config vars . get ( 'CFLAGS' , '' ) m = re . search ( '-isysroot\\s+(\\S+)' , cflags ) if ( m is not  None  ) : sdk = m . group ( 1 ) if ( not os . path . exists ( sdk ) ) : for cv in  UNIVERSAL CONFIG VARS : if ( ( cv in  config vars ) and ( cv not in os . environ ) ) : flags =  config vars [ cv ] flags = re . sub ( '-isysroot\\s+\\S+(?:\\s|$)' , '  ' , flags )  save modified value (  config vars , cv , flags ) return  config vars 
def affiliation ( ) : return s3 rest controller ( ) 
def wait readwrite ( fileno , timeout =  None  , timeout exc = timeout ( 'timed  out' ) , event =  None  ) : io = get hub ( ) . loop . io ( fileno , 3 ) return wait ( io , timeout , timeout exc ) 
def setup pidfile fixtures ( testcase ) : testcase . mock tracker = scaffold .  Mock  Tracker  ( ) scenarios = make pidlockfile scenarios ( ) testcase . pidlockfile scenarios = scenarios def get scenario option ( testcase , key , default =  None  ) : value = default try : value = testcase . scenario [ key ] except (  Name  Error  ,  Type  Error  ,  Attribute  Error  ,  Key  Error  ) : pass return value scaffold . mock ( 'os.getpid' , returns = scenarios [ 'simple' ] [ 'pid' ] , tracker = testcase . mock tracker ) def make mock open funcs ( testcase ) : def mock open nonexist ( filename , mode , buffering ) : if ( 'r' in mode ) : raise IO Error  ( errno . ENOENT , ( ' No   such  file  %(filename)r' % vars ( ) ) ) else : result = testcase . scenario [ 'pidfile' ] return result def mock open read denied ( filename , mode , buffering ) : if ( 'r' in mode ) : raise IO Error  ( errno . EPERM , ( ' Read   denied  on  %(filename)r' % vars ( ) ) ) else : result = testcase . scenario [ 'pidfile' ] return result def mock open okay ( filename , mode , buffering ) : result = testcase . scenario [ 'pidfile' ] return result def mock os open nonexist ( filename , flags , mode ) : if ( flags & os . O CREAT ) : result = testcase . scenario [ 'pidfile' ] . fileno ( ) else : raise OS Error  ( errno . ENOENT , ( ' No   such  file  %(filename)r' % vars ( ) ) ) return result def mock os open read denied ( filename , flags , mode ) : if ( flags & os . O CREAT ) : result = testcase . scenario [ 'pidfile' ] . fileno ( ) else : raise OS Error  ( errno . EPERM , ( ' Read   denied  on  %(filename)r' % vars ( ) ) ) return result def mock os open okay ( filename , flags , mode ) : result = testcase . scenario [ 'pidfile' ] . fileno ( ) return result funcs = dict ( ( ( name , obj ) for ( name , obj ) in vars ( ) . items ( ) if hasattr ( obj , '  call  ' ) ) ) return funcs testcase . mock pidfile open funcs = make mock open funcs ( testcase ) def mock open ( filename , mode = 'r' , buffering =  None  ) : scenario path = get scenario option ( testcase , 'path' ) if ( filename == scenario path ) : func name = testcase . scenario [ 'open func name' ] mock open func = testcase . mock pidfile open funcs [ func name ] result = mock open func ( filename , mode , buffering ) else : result =  Fake  File  Descriptor  String IO ( ) return result scaffold . mock ( '  builtin  .open' , returns func = mock open , tracker = testcase . mock tracker ) def mock os open ( filename , flags , mode =  None  ) : scenario path = get scenario option ( testcase , 'path' ) if ( filename == scenario path ) : func name = testcase . scenario [ 'os open func name' ] mock os open func = testcase . mock pidfile open funcs [ func name ] result = mock os open func ( filename , flags , mode ) else : result =  Fake  File  Descriptor  String IO ( ) . fileno ( ) return result scaffold . mock ( 'os.open' , returns func = mock os open , tracker = testcase . mock tracker ) def mock os fdopen ( fd , mode = 'r' , buffering =  None  ) : scenario pidfile = get scenario option ( testcase , 'pidfile' ,  Fake  File  Descriptor  String IO ( ) ) if ( fd == testcase . scenario [ 'pidfile' ] . fileno ( ) ) : result = testcase . scenario [ 'pidfile' ] else : raise OS Error  ( errno . EBADF , ' Bad   file  descriptor' ) return result scaffold . mock ( 'os.fdopen' , returns func = mock os fdopen , tracker = testcase . mock tracker ) testcase . scenario =  Not  Implemented  
def cleanup html ( html ) : match =  body re . search ( html ) if match : html = html [ match . end ( ) : ] match =  end body re . search ( html ) if match : html = html [ : match . start ( ) ] html =  ins del re . sub ( '' , html ) return html 
def unichr safe ( index ) : try : return unichr ( index ) except  Value  Error  : return unichr ( 65533 ) 
def colorize ( msg , color ) : if DONT COLORIZE : return msg else : return '{}{}{}' . format ( COLORS [ color ] , msg , COLORS [ 'endc' ] ) 
def render formset ( formset , ** kwargs ) : renderer cls = get formset renderer ( ** kwargs ) return renderer cls ( formset , ** kwargs ) . render ( ) 
@ util . positional ( 2 ) def error ( status code , status message =  None  , content type = 'text/plain;  charset=utf-8' , headers =  None  , content =  None  ) : if ( status message is  None  ) : status message = httplib . responses . get ( status code , ' Unknown    Error ' ) if ( content is  None  ) : content = status message content = util . pad string ( content ) return static page ( content , status = ( status code , status message ) , content type = content type , headers = headers ) 
def  Canonical  Path  To  Local  Path  ( path ) : return path 
def doc ( * args ) : return os . path . join (  prefix , u'share' , u'doc' , u'git-cola' , * args ) 
def find files with lib ( dirname ) : try : try : repo = hg . repository ( ui . ui ( ) , path = dirname ) except  Repo  Error  : return ( modified , added , removed , deleted , unknown ) = repo . status ( ) [ : 5 ] excluded = ( ( removed + deleted ) + unknown ) if ( version in OLD VERSIONS ) : from mercurial import util node =  None  for ( src , abs , rel , exact ) in cmdutil . walk ( repo , [ ] , { } , node = node , badmatch = util . always , default = 'relglob' ) : if ( src == 'b' ) : continue if ( ( not node ) and ( abs not in repo . dirstate ) ) : continue if ( abs in excluded ) : continue ( yield abs ) else : rev =  None  try : match = cmdutil . match ( repo , [ ] , { } , default = 'relglob' ) except : from mercurial import scmutil match = scmutil . match ( repo [  None  ] , [ ] , { } , default = 'relglob' ) match . bad = ( lambda x , y :  False  ) for abs in repo [ rev ] . walk ( match ) : if ( ( not rev ) and ( abs not in repo . dirstate ) ) : continue if ( abs in excluded ) : continue ( yield abs ) except  Exception  as e : if log : log . warn ( ( ' Error   in  setuptools hg:  %s' % e ) ) find files with cmd ( dirname ) 
def median test ksample ( x , groups ) : x = np . asarray ( x ) gruni = np . unique ( groups ) xli = [ x [ ( groups == group ) ] for group in gruni ] xmedian = np . median ( x ) counts larger = np . array ( [ ( xg > xmedian ) . sum ( ) for xg in xli ] ) counts = np . array ( [ len ( xg ) for xg in xli ] ) counts smaller = ( counts - counts larger ) nobs = counts . sum ( ) n larger = ( x > xmedian ) . sum ( ) n smaller = ( nobs - n larger ) table = np . vstack ( ( counts smaller , counts larger ) ) expected = np . vstack ( ( ( ( ( counts * 1.0 ) / nobs ) * n smaller ) , ( ( ( counts * 1.0 ) / nobs ) * n larger ) ) ) if ( expected < 5 ) . any ( ) : print ( ' Warning :   There   are  cells  with  less  than  5  expectedobservations.   The   chisquare  distribution  might  not  be  a  goodapproximation  for  the  true  distribution.' ) return ( stats . chisquare ( table . ravel ( ) , expected . ravel ( ) , ddof = 1 ) , table , expected ) 
def handle process output ( process , stdout handler , stderr handler , finalizer =  None  , decode streams =  True  ) : def pump stream ( cmdline , name , stream , is decode , handler ) : try : for line in stream : if handler : if is decode : line = line . decode ( defenc ) handler ( line ) except  Exception  as ex : log . error ( ' Pumping   %r  of  cmd(%s)  failed  due  to:  %r' , name , cmdline , ex ) raise  Command  Error  ( ( [ ( '<%s-pump>' % name ) ] + cmdline ) , ex ) finally : stream . close ( ) cmdline = getattr ( process , 'args' , '' ) if ( not isinstance ( cmdline , ( tuple , list ) ) ) : cmdline = cmdline . split ( ) pumps = [ ] if process . stdout : pumps . append ( ( 'stdout' , process . stdout , stdout handler ) ) if process . stderr : pumps . append ( ( 'stderr' , process . stderr , stderr handler ) ) threads = [ ] for ( name , stream , handler ) in pumps : t = threading .  Thread  ( target = pump stream , args = ( cmdline , name , stream , decode streams , handler ) ) t . set Daemon  (  True  ) t . start ( ) threads . append ( t ) for t in threads : t . join ( ) if finalizer : return finalizer ( process ) 
def compare otu maps ( otu map1 , otu map2 , verbose =  False  ) : right = 0 wrong = 0 otus1 = set ( otu map1 . keys ( ) ) otus2 = set ( otu map2 . keys ( ) ) shared otus = otus1 . intersection ( otus2 ) for otu in shared otus : members1 = set ( otu map1 [ otu ] ) members2 = set ( otu map2 [ otu ] ) right += len ( ( members1 & members2 ) ) missing in 2 = ( members1 - members2 ) wrong += len ( missing in 2 ) if ( verbose and ( len ( missing in 2 ) > 0 ) ) : print ( 'OTU  id:  %s' % otu ) print list ( missing in 2 ) print for otu in ( otus1 - shared otus ) : wrong += len ( otu map1 [ otu ] ) if verbose : print ( 'OTU  id:  %s' % otu ) print list ( otu map1 [ otu ] ) return ( float ( wrong ) / ( right + wrong ) ) 
def debugerror ( ) : return web .   Internal  Error  ( djangoerror ( ) ) 
@ functools . lru cache ( maxsize = 1000 ) def check for language ( lang code ) : if ( ( lang code is  None  ) or ( not language code re . search ( lang code ) ) ) : return  False  for path in all locale paths ( ) : if ( gettext module . find ( 'django' , path , [ to locale ( lang code ) ] ) is not  None  ) : return  True  return  False  
def  inflate g ( g , n ) : def inflate ( params , n ) : '  (a1,  ..,  ak)  ->  (a1/n,  (a1+1)/n,  ...,  (ak  +  n-1)/n)  ' res = [ ] for a in params : for i in range ( n ) : res . append ( ( ( a + i ) / n ) ) return res v = S ( ( len ( g . ap ) - len ( g . bq ) ) ) C = ( n ** ( ( 1 + g . nu ) + ( v / 2 ) ) ) C /= ( ( 2 * pi ) ** ( ( n - 1 ) * g . delta ) ) return ( C , meijerg ( inflate ( g . an , n ) , inflate ( g . aother , n ) , inflate ( g . bm , n ) , inflate ( g . bother , n ) , ( ( g . argument ** n ) * ( n ** ( n * v ) ) ) ) ) 
@ lower builtin ( 'is' , types .  Any  , types .  Any  ) def generic is ( context , builder , sig , args ) : ( lhs type , rhs type ) = sig . args if ( lhs type == rhs type ) : if lhs type . mutable : raise  Not  Implemented  Error  ( 'no  default  `is`  implementation' ) else : try : eq impl = context . get function ( '==' , sig ) except  Not  Implemented  Error  : return cgutils . false bit else : return eq impl ( builder , args ) else : return cgutils . false bit 
@ app . route ( '/ add numbers' ) def add numbers ( ) : a = request . args . get ( 'a' , 0 , type = int ) b = request . args . get ( 'b' , 0 , type = int ) return jsonify ( result = ( a + b ) ) 
def push ( repo , tag =  None  , quiet =  False  , insecure registry =  False  ) : client =  get client ( ) status = base status . copy ( ) ( registry , repo name ) = docker . auth . resolve repository name ( repo ) try : kwargs = { 'tag' : tag } if salt . utils . compare versions ( ver1 = docker .   version   , oper = '>=' , ver2 = '0.5.0' ) : kwargs [ 'insecure registry' ] = insecure registry ret = client . push ( repo , ** kwargs ) if ret : ( image logs , infos ) =  parse image multilogs string ( ret ) if image logs : repotag = repo name if tag : repotag = '{0}:{1}' . format ( repo , tag ) if ( not quiet ) : status [ 'out' ] = image logs else : status [ 'out' ] =  None  laststatus = image logs [ 2 ] . get ( 'status' ,  None  ) if ( laststatus and ( ( 'already  pushed' in laststatus ) or ( ' Pushing   tags  for  rev' in laststatus ) or ( ' Pushing   tag  for  rev' in laststatus ) ) ) : status [ 'status' ] =  True  status [ 'id' ] =  get image infos ( repo ) [ ' Id ' ] status [ 'comment' ] = ' Image   {0}({1})  was  pushed' . format ( repotag , status [ 'id' ] ) else :  push assemble error status ( status , ret , image logs ) else : status [ 'out' ] = ret  push assemble error status ( status , ret , image logs ) else :  invalid ( status ) except  Exception  :  invalid ( status , id  = repo , out = traceback . format exc ( ) ) return status 
def  preparse ( source , f = compose (  replace locals ,  replace booleans ,  rewrite assign ) ) : assert callable ( f ) , 'f  must  be  callable' return tokenize . untokenize ( lmap ( f , tokenize string ( source ) ) ) 
def dfs ( graph , start , path = [ ] ) : if ( ( start not in graph ) or ( graph [ start ] is  None  ) or ( graph [ start ] == [ ] ) ) : return  None  path = ( path + [ start ] ) for edge in graph [ start ] : if ( edge not in path ) : path = dfs ( graph , edge , path ) return path 
def mv ( source , destination ,  action = shutil . move ) : sources = glob . glob ( source ) if ( len ( sources ) > 1 ) : assert isdir ( destination ) for filename in sources :  action ( filename , join ( destination , basename ( filename ) ) ) else : try : source = sources [ 0 ] except  Index  Error  : raise OS Error  ( ( ' No   file  matching  %s' % source ) ) if ( isdir ( destination ) and exists ( destination ) ) : destination = join ( destination , basename ( source ) ) try :  action ( source , destination ) except OS Error  as ex : raise OS Error  ( ( ' Unable   to  move  %r  to  %r  (%s)' % ( source , destination , ex ) ) ) 
def register database ( app ) : db . init app ( app ) db . app = app cache . init app ( app ) 
def upgrade ( ) : manager = pkg manager ( ) run as root ( ( '%(manager)s  - Su ' % locals ( ) ) , pty =  False  ) 
def do if ( parser , token ) : bits = token . contents . split ( ) del bits [ 0 ] if ( not bits ) : raise  Template  Syntax  Error  , "'if'  statement  requires  at  least  one  argument" bitstr = '  ' . join ( bits ) boolpairs = bitstr . split ( '  and  ' ) boolvars = [ ] if ( len ( boolpairs ) == 1 ) : link type =  If  Node  .  Link  Types  . or  boolpairs = bitstr . split ( '  or  ' ) else : link type =  If  Node  .  Link  Types  . and  if ( '  or  ' in bitstr ) : raise  Template  Syntax  Error  , "'if'  tags  can't  mix  'and'  and  'or'" for boolpair in boolpairs : if ( '  ' in boolpair ) : try : ( not  , boolvar ) = boolpair . split ( ) except  Value  Error  : raise  Template  Syntax  Error  , "'if'  statement  improperly  formatted" if ( not  != 'not' ) : raise  Template  Syntax  Error  , " Expected   'not'  in  if  statement" boolvars . append ( (  True  , parser . compile filter ( boolvar ) ) ) else : boolvars . append ( (  False  , parser . compile filter ( boolpair ) ) ) nodelist true = parser . parse ( ( 'else' , 'endif' ) ) token = parser . next token ( ) if ( token . contents == 'else' ) : nodelist false = parser . parse ( ( 'endif' , ) ) parser . delete first token ( ) else : nodelist false =  Node  List  ( ) return  If  Node  ( boolvars , nodelist true , nodelist false , link type ) 
def  scale mpl figure ( fig , scale ) : fig . set size inches ( ( fig . get size inches ( ) * scale ) ) fig . set dpi ( ( fig . get dpi ( ) * scale ) ) import matplotlib as mpl if ( scale >= 1 ) : sfactor = ( scale ** 2 ) elif ( scale < 1 ) : sfactor = ( - ( ( 1.0 / scale ) ** 2 ) ) for text in fig . findobj ( mpl . text .  Text  ) : fs = text . get fontsize ( ) new size = ( fs + sfactor ) if ( new size <= 0 ) : raise  Value  Error  ( 'could  not  rescale  matplotlib  fonts,  consider  increasing  "scale"' ) text . set fontsize ( new size ) fig . canvas . draw ( ) 
def cast ( context , topic , msg ) : return  get impl ( ) . cast ( CONF , context , topic , msg ) 
def attach Accept  ( P Tmsi  Signature  presence = 0 ,  Gprs  Timer  presence = 0 ,  Mobile  Id  presence = 0 ,  Mobile  Id  presence1 = 0 ,  Gmm  Cause  presence = 0 ) : a =  Tp  Pd  ( pd = 3 ) b =  Message  Type  ( mes Type  = 2 ) c =  Attach  Result  ( ) d =  Force  To  Standby  ( ) e =  Gprs  Timer  ( ) f =  Radio  Priority  And  Spare  Half  Octets  ( ) h =  Routing  Area  Identification  ( ) packet = ( ( ( ( ( ( a / b ) / c ) / d ) / e ) / f ) / h ) if ( P Tmsi  Signature  presence is 1 ) : i = P Tmsi  Signature  ( ieiPTS = 25 ) packet = ( packet / i ) if (  Gprs  Timer  presence is 1 ) : j =  Gprs  Timer  ( ieiGT = 23 ) packet = ( packet / j ) if (  Mobile  Id  presence is 1 ) : k =  Mobile  Id  Hdr  ( ieiMI = 24 , eight Bit MI = 0 ) packet = ( packet / k ) if (  Mobile  Id  presence1 is 1 ) : l =  Mobile  Id  Hdr  ( ieiMI = 35 , eight Bit MI = 0 ) packet = ( packet / l ) if (  Gmm  Cause  presence is 1 ) : m =  Gmm  Cause  ( ieiGC = 37 ) packet = ( packet / m ) return packet 
def  compute signature ( parameters , access key secret , method , path ) : parameters [ 'signature method' ] = ' Hmac SHA256' string to sign = '{0}\n{1}\n' . format ( method . upper ( ) , path ) keys = sorted ( parameters . keys ( ) ) pairs = [ ] for key in keys : val = str ( parameters [ key ] ) . encode ( 'utf-8' ) pairs . append ( ( (  quote ( key , safe = '' ) + '=' ) +  quote ( val , safe = '- ~' ) ) ) qs = '&' . join ( pairs ) string to sign += qs h = hmac . new ( access key secret , digestmod = sha256 ) h . update ( string to sign ) signature = base64 . b64encode ( h . digest ( ) ) . strip ( ) return signature 
def bigmemtest ( minsize , memuse , overhead = ( 5 *  1M ) ) : def decorator ( f ) : def wrapper ( self ) : if ( not max memuse ) : maxsize = 5147 self . assert False  ( ( ( ( maxsize * memuse ) + overhead ) > ( 20 *  1M ) ) ) else : maxsize = int ( ( ( max memuse - overhead ) / memuse ) ) if ( maxsize < minsize ) : if verbose : sys . stderr . write ( ( ' Skipping   %s  because  of  memory  constraint\n' % ( f .   name   , ) ) ) return maxsize = max ( ( maxsize - ( 50 *  1M ) ) , minsize ) return f ( self , maxsize ) wrapper . minsize = minsize wrapper . memuse = memuse wrapper . overhead = overhead return wrapper return decorator 
def commit ( ) : return   proxy   [ 'napalm.call' ] ( 'commit config' , ** { } ) 
def instantiate Add  Callbacks  After  Result  ( n ) : d = defer .  Deferred  ( ) def f ( result ) : return result d . callback ( 1 ) for i in xrange ( n ) : d . add Callback  ( f ) d . add Errback  ( f ) d . add Both  ( f ) d . add Callbacks  ( f ) 
def conn from flowtuple ( ft ) : ( sip , sport , dip , dport , offset , relts ) = ft return { 'src' : sip , 'sport' : sport , 'dst' : dip , 'dport' : dport , 'offset' : offset , 'time' : relts } 
def to id ( s ) : if ( s == '+' ) : return 11 if ( s == '*' ) : return 12 return ( int ( s ) + 1 ) 
def resolve Imports  ( sheet , target =  None  ) : if ( not target ) : target = css . CSS Style  Sheet  ( href = sheet . href , media = sheet . media , title = sheet . title ) def get Replacer  ( targetbase ) : ' Return   a  replacer  which  uses  base  to  return  adjusted  UR Ls ' ( basesch , baseloc , basepath , basequery , basefrag ) = urlparse . urlsplit ( targetbase ) ( basepath , basepathfilename ) = os . path . split ( basepath ) def replacer ( uri ) : ( scheme , location , path , query , fragment ) = urlparse . urlsplit ( uri ) if ( ( not scheme ) and ( not location ) and ( not path . startswith ( u'/' ) ) ) : ( path , filename ) = os . path . split ( path ) combined = os . path . normpath ( os . path . join ( basepath , path , filename ) ) return urllib . pathname2url ( combined ) else : return uri return replacer for rule in sheet . css Rules  : if ( rule . type == rule . CHARSET RULE ) : pass elif ( rule . type == rule . IMPORT RULE ) : log . info ( ( u' Processing   @import  %r' % rule . href ) , neverraise =  True  ) if rule . href Found  : target . add ( css . CSS Comment  ( css Text  = ( u'/*  START  @import  "%s"  */' % rule . href ) ) ) try : imported Sheet  = resolve Imports  ( rule . style Sheet  ) except xml . dom .  Hierarchy  Request  Err  as e : log . warn ( ( u'@import:   Cannot   resolve  target,  keeping  rule:  %s' % e ) , neverraise =  True  ) target . add ( rule ) else : log . info ( ( u'@import:   Adjusting   paths  for  %r' % rule . href ) , neverraise =  True  ) replace Urls  ( imported Sheet  , get Replacer  ( rule . href ) , ignore Import  Rules  =  True  ) if ( rule . media . media Text  == u'all' ) : mediaproxy =  None  else : keepimport =  False  for r in imported Sheet  : if ( r . type not in ( r . COMMENT , r . STYLE RULE , r . IMPORT RULE ) ) : keepimport =  True  break if keepimport : log . warn ( ( u' Cannot   combine  imported  sheet  with  given  media  as  other  rules  then  comments  or  stylerules  found  %r,  keeping  %r' % ( r , rule . css Text  ) ) , neverraise =  True  ) target . add ( rule ) continue log . info ( ( u'@import:   Wrapping   some  rules  in  @media    to  keep  media:  %s' % rule . media . media Text  ) , neverraise =  True  ) mediaproxy = css . CSS Media  Rule  ( rule . media . media Text  ) for r in imported Sheet  : if mediaproxy : mediaproxy . add ( r ) else : target . add ( r ) if mediaproxy : target . add ( mediaproxy ) else : log . error ( ( u' Cannot   get  referenced  stylesheet  %r,  keeping  rule' % rule . href ) , neverraise =  True  ) target . add ( rule ) else : target . add ( rule ) return target 
@ register . filter def has permissions ( user , component ) : return user . has perms ( getattr ( component , 'permissions' , set ( ) ) ) 
def fftconvolve old ( in1 , in2 , in3 =  None  , mode = 'full' ) : s1 = array ( in1 . shape ) s2 = array ( in2 . shape ) complex result = ( np . issubdtype ( in1 . dtype , np . complex ) or np . issubdtype ( in2 . dtype , np . complex ) ) size = ( ( s1 + s2 ) - 1 ) fsize = ( 2 ** np . ceil ( np . log2 ( size ) ) ) IN1 = fftn ( in1 , fsize ) IN1 /= fftn ( in2 , fsize ) fslice = tuple ( [ slice ( 0 , int ( sz ) ) for sz in size ] ) ret = ifftn ( IN1 ) [ fslice ] . copy ( ) del IN1 if ( not complex result ) : ret = ret . real if ( mode == 'full' ) : return ret elif ( mode == 'same' ) : if ( product ( s1 , axis = 0 ) > product ( s2 , axis = 0 ) ) : osize = s1 else : osize = s2 return  centered ( ret , osize ) elif ( mode == 'valid' ) : return  centered ( ret , ( abs ( ( s2 - s1 ) ) + 1 ) ) 
def testopendocx ( ) : if isinstance ( opendocx ( TEST FILE ) , lxml . etree .   Element  ) : pass else : assert  False  
def capacity indicator ( ) : return s3 rest controller ( ) 
def  PPI Guess  Payload  Class  ( p , ** kargs ) : if ( len ( p ) >= 4 ) : ( t , pfh len ) = struct . unpack ( '<HH' , p [ : 4 ] ) cls = getPPI Type  ( t , 'default' ) pfh len += 4 out = cls ( p [ : pfh len ] , ** kargs ) if out . payload : out . payload = conf . raw layer ( out . payload . load ) if ( len ( p ) > pfh len ) : out . payload . payload = conf . padding layer ( p [ pfh len : ] ) elif ( len ( p ) > pfh len ) : out . payload = conf . padding layer ( p [ pfh len : ] ) else : out = conf . raw layer ( p , ** kargs ) return out 
def arfilter old ( x , a ) : x = np . asarray ( x ) a = np . asarray ( a ) if ( x . ndim == 1 ) : x = x [ : ,  None  ] if ( x . ndim > 2 ) : raise  Value  Error  ( 'x  array  has  to  be  1d  or  2d' ) nvar = x . shape [ 1 ] nlags = a . shape [ 0 ] ntrim = ( nlags // 2 ) if ( a . ndim == 1 ) : return signal . convolve ( x , a [ : ,  None  ] , mode = 'valid' ) elif ( a . ndim == 2 ) : if ( min ( a . shape ) == 1 ) : return signal . convolve ( x , a , mode = 'valid' ) result = np . zeros ( ( ( ( x . shape [ 0 ] - nlags ) + 1 ) , nvar ) ) for i in range ( nvar ) : result [ : , i ] = signal . convolve ( x [ : , i ] , a [ : , i ] , mode = 'valid' ) return result elif ( a . ndim == 3 ) : yf = signal . convolve ( x [ : , : ,  None  ] , a ) yvalid = yf [ ntrim : ( - ntrim ) , ( yf . shape [ 1 ] // 2 ) , : ] return yvalid 
def test resize icon enlarge ( ) : resize size = 350 final size = ( 339 , 128 )  uploader ( resize size , final size ) 
def fuzzy and ( args ) : rv =  True  for ai in args : ai = fuzzy bool ( ai ) if ( ai is  False  ) : return  False  if rv : rv = ai return rv 
def   virtual   ( ) : if ( not HAS ELEMENT TREE ) : return (  False  , ' Cannot   load  {0}  state:   Element  Tree   library  unavailable' . format (   virtualname   ) ) if ( 'boto iam.get user' in   salt   ) : return  True  else : return (  False  , ' Cannot   load  {0}  state:  boto iam  module  unavailable' . format (   virtualname   ) ) 
@ register . tag def autoescape ( parser , token ) : args = token . contents . split ( ) if ( len ( args ) != 2 ) : raise  Template  Syntax  Error  ( "'autoescape'  tag  requires  exactly  one  argument." ) arg = args [ 1 ] if ( arg not in ( u'on' , u'off' ) ) : raise  Template  Syntax  Error  ( "'autoescape'  argument  should  be  'on'  or  'off'" ) nodelist = parser . parse ( ( 'endautoescape' , ) ) parser . delete first token ( ) return  Auto  Escape  Control  Node  ( ( arg == 'on' ) , nodelist ) 
def fclusterdata ( X , t , criterion = 'inconsistent' , metric = 'euclidean' , depth = 2 , method = 'single' , R =  None  ) : X = np . asarray ( X , order = 'c' , dtype = np . double ) if ( ( type ( X ) != np . ndarray ) or ( len ( X . shape ) != 2 ) ) : raise  Type  Error  ( ' The   observation  matrix  X  must  be  an  n  by  m  numpy  array.' ) Y = distance . pdist ( X , metric = metric ) Z = linkage ( Y , method = method ) if ( R is  None  ) : R = inconsistent ( Z , d = depth ) else : R = np . asarray ( R , order = 'c' ) T = fcluster ( Z , criterion = criterion , depth = depth , R = R , t = t ) return T 
def  fit lbfgs ( f , score , start params , fargs , kwargs , disp =  True  , maxiter = 100 , callback =  None  , retall =  False  , full output =  True  , hess =  None  ) : bounds = kwargs . setdefault ( 'bounds' , ( [ (  None  ,  None  ) ] * len ( start params ) ) ) kwargs . setdefault ( 'iprint' , 0 ) names = ( 'm' , 'pgtol' , 'factr' , 'maxfun' , 'epsilon' , 'approx grad' ) extra kwargs = dict ( ( ( x , kwargs [ x ] ) for x in names if ( x in kwargs ) ) ) approx grad = kwargs . get ( 'approx grad' ,  False  ) loglike and score = kwargs . get ( 'loglike and score' ,  None  ) epsilon = kwargs . get ( 'epsilon' ,  None  ) if approx grad : score =  None  if ( epsilon and ( not approx grad ) ) : raise  Value  Error  ( 'a  finite-differences  epsilon  was  provided  even  though  we  are  not  using  approx grad' ) if ( approx grad and loglike and score ) : raise  Value  Error  ( 'gradient  approximation  was  requested  even  though  an  analytic  loglike and score  function  was  given' ) if loglike and score : func = ( lambda p , * a : tuple ( ( ( - x ) for x in loglike and score ( p , * a ) ) ) ) elif score : func = f extra kwargs [ 'fprime' ] = score elif approx grad : func = f scipy version curr = distutils . version .  Loose  Version  ( scipy version ) scipy version 12 = distutils . version .  Loose  Version  ( '0.12.0' ) if ( scipy version curr < scipy version 12 ) : retvals = optimize . fmin l bfgs b ( func , start params , args = fargs , bounds = bounds , disp = disp , ** extra kwargs ) else : retvals = optimize . fmin l bfgs b ( func , start params , maxiter = maxiter , callback = callback , args = fargs , bounds = bounds , disp = disp , ** extra kwargs ) if full output : ( xopt , fopt , d ) = retvals warnflag = d [ 'warnflag' ] converged = ( warnflag == 0 ) gopt = d [ 'grad' ] fcalls = d [ 'funcalls' ] retvals = { 'fopt' : fopt , 'gopt' : gopt , 'fcalls' : fcalls , 'warnflag' : warnflag , 'converged' : converged } else : xopt = retvals [ 0 ] retvals =  None  return ( xopt , retvals ) 
def escapedCDATA ( data ) : if isinstance ( data , unicode ) : data = data . encode ( 'utf-8' ) return data . replace ( ']]>' , ']]]]><![CDATA[>' ) 
def parse stories ( lines , only supporting =  False  ) : data = [ ] story = [ ] for line in lines : line = line . decode ( 'utf-8' ) . strip ( ) ( nid , line ) = line . split ( '  ' , 1 ) nid = int ( nid ) if ( nid == 1 ) : story = [ ] if ( ' DCTB ' in line ) : ( q , a , supporting ) = line . split ( ' DCTB ' ) q = tokenize ( q ) substory =  None  if only supporting : supporting = map ( int , supporting . split ( ) ) substory = [ story [ ( i - 1 ) ] for i in supporting ] else : substory = [ x for x in story if x ] data . append ( ( substory , q , a ) ) story . append ( '' ) else : sent = tokenize ( line ) story . append ( sent ) return data 
def abort ( code , message ) : raise JSONHTTP Exception  ( message , code ) 
def construct url ( environ , with query string =  True  , with path info =  True  , script name =  None  , path info =  None  , querystring =  None  ) : url = ( environ [ 'wsgi.url scheme' ] + '://' ) if environ . get ( 'HTTP HOST' ) : host = environ [ 'HTTP HOST' ] port =  None  if ( ':' in host ) : ( host , port ) = host . split ( ':' , 1 ) if ( environ [ 'wsgi.url scheme' ] == 'https' ) : if ( port == '443' ) : port =  None  elif ( environ [ 'wsgi.url scheme' ] == 'http' ) : if ( port == '80' ) : port =  None  url += host if port : url += ( ':%s' % port ) else : url += environ [ 'SERVER NAME' ] if ( environ [ 'wsgi.url scheme' ] == 'https' ) : if ( environ [ 'SERVER PORT' ] != '443' ) : url += ( ':' + environ [ 'SERVER PORT' ] ) elif ( environ [ 'SERVER PORT' ] != '80' ) : url += ( ':' + environ [ 'SERVER PORT' ] ) if ( script name is  None  ) : url += quote ( environ . get ( 'SCRIPT NAME' , '' ) ) else : url += quote ( script name ) if with path info : if ( path info is  None  ) : url += quote ( environ . get ( 'PATH INFO' , '' ) ) else : url += quote ( path info ) if with query string : if ( querystring is  None  ) : if environ . get ( 'QUERY STRING' ) : url += ( '?' + environ [ 'QUERY STRING' ] ) elif querystring : url += ( '?' + querystring ) return url 
def parse config ( file name = '/usr/local/etc/pkg.conf' ) : ret = { } if ( not os . path . isfile ( file name ) ) : return ' Unable   to  find  {0}  on  file  system' . format ( file name ) with salt . utils . fopen ( file name ) as ifile : for line in ifile : if ( line . startswith ( '#' ) or line . startswith ( '\n' ) ) : pass else : ( key , value ) = line . split ( ' DCTB ' ) ret [ key ] = value ret [ 'config file' ] = file name return ret 
@ validate ( 'form' ) def valid type in colspan ( arch ) : return all ( ( attrib . isdigit ( ) for attrib in arch . xpath ( '//@colspan' ) ) ) 
@ yield once def icollect ( file paths , ignored globs =  None  ) : if isinstance ( file paths , str ) : file paths = [ file paths ] for file path in file paths : for match in iglob ( file path ) : if ( ( not ignored globs ) or ( not fnmatch ( match , ignored globs ) ) ) : ( yield ( match , file path ) ) 
def gf irred p rabin ( f , p , K ) : n = gf degree ( f ) if ( n <= 1 ) : return  True  (   , f ) = gf monic ( f , p , K ) x = [ K . one , K . zero ] indices = { ( n // d ) for d in factorint ( n ) } b = gf frobenius monomial base ( f , p , K ) h = b [ 1 ] for i in range ( 1 , n ) : if ( i in indices ) : g = gf sub ( h , x , p , K ) if ( gf gcd ( f , g , p , K ) != [ K . one ] ) : return  False  h = gf frobenius map ( h , f , b , p , K ) return ( h == x ) 
def enable ( iface ) : if is enabled ( iface ) : return  True  cmd = [ 'netsh' , 'interface' , 'set' , 'interface' , iface , 'admin=ENABLED' ]   salt   [ 'cmd.run' ] ( cmd , python shell =  False  ) return is enabled ( iface ) 
def header property ( name , doc , transform =  None  ) : normalized name = name . lower ( ) def fget ( self ) : try : return self .  headers [ normalized name ] except  Key  Error  : return  None  if ( transform is  None  ) : if six . PY2 : def fset ( self , value ) : self .  headers [ normalized name ] = str ( value ) else : def fset ( self , value ) : self .  headers [ normalized name ] = value else : def fset ( self , value ) : self .  headers [ normalized name ] = transform ( value ) def fdel ( self ) : del self .  headers [ normalized name ] return property ( fget , fset , fdel , doc ) 
def get local size ( * args , ** kargs ) : raise  stub error 
def scale ( X , axis = 0 , with mean =  True  , with std =  True  , copy =  True  ) : X = check array ( X , accept sparse = 'csc' , copy = copy , ensure 2d =  False  , warn on dtype =  True  , estimator = 'the  scale  function' , dtype = FLOAT DTYPES ) if sparse . issparse ( X ) : if with mean : raise  Value  Error  ( ' Cannot   center  sparse  matrices:  pass  `with mean= False `  instead   See   docstring  for  motivation  and  alternatives.' ) if ( axis != 0 ) : raise  Value  Error  ( ( ' Can   only  scale  sparse  matrix  on  axis=0,    got  axis=%d' % axis ) ) if with std : (   , var ) = mean variance axis ( X , axis = 0 ) var =  handle zeros in scale ( var , copy =  False  ) inplace column scale ( X , ( 1 / np . sqrt ( var ) ) ) else : X = np . asarray ( X ) if with mean : mean  = np . mean ( X , axis ) if with std : scale  = np . std ( X , axis )  Xr  = np . rollaxis ( X , axis ) if with mean :  Xr  -= mean  mean 1 =  Xr  . mean ( axis = 0 ) if ( not np . allclose ( mean 1 , 0 ) ) : warnings . warn ( ' Numerical   issues  were  encountered  when  centering  the  data  and  might  not  be  solved.   Dataset   may  contain  too  large  values.   You   may  need  to  prescale  your  features.' )  Xr  -= mean 1 if with std : scale  =  handle zeros in scale ( scale  , copy =  False  )  Xr  /= scale  if with mean : mean 2 =  Xr  . mean ( axis = 0 ) if ( not np . allclose ( mean 2 , 0 ) ) : warnings . warn ( ' Numerical   issues  were  encountered  when  scaling  the  data  and  might  not  be  solved.   The   standard  deviation  of  the  data  is  probably  very  close  to  0.  ' )  Xr  -= mean 2 return X 
def get user id for username ( user name , allow none =  False  ) : try : if ( c . userobj and ( c . userobj . name == user name ) ) : return c . userobj . id except  Type  Error  : pass user = model .  User  . get ( user name ) if user : return user . id if allow none : return  None  raise  Exception  ( ' Not   logged  in  user' ) 
def is conflict free ( path ) : rgx = re . compile ( u'^(<<<<<<<|\\|\\|\\|\\|\\|\\|\\||>>>>>>>)  ' ) try : with core . xopen ( path , u'r' ) as f : for line in f : line = core . decode ( line , errors = u'ignore' ) if rgx . match ( line ) : if should stage conflicts ( path ) : return  True  else : return  False  except IO Error  : pass return  True  
def send Message  ( qry ) : try : get User  Name  ( ) except : return  skype Error  ( ) if ( qry == 'skype  update' ) :  write Friends  ( )  get Avatars  ( ) return ( len (  read Friends  ( ) ) .   str   ( ) + '  friends  found  and  cached!' ) else : m = qry . partition ( ':  ' ) ret = skype ( ( ( ( 'MESSAGE  ' + m [ 0 ] ) + '  ' ) + m [ 2 ] ) ) if ( 'SENDING' in ret ) : return ( ' Message   sent  to  ' + m [ 0 ] ) else : return ( 'ERROR  sending  message  to:  ' + m [ 0 ] ) 
def parse Glob  ( filename ) : if ( not filename ) : raise files .  Invalid  File  Name  Error  ( 'filename  is   None .' ) if ( not isinstance ( filename , basestring ) ) : raise files .  Invalid  File  Name  Error  ( ( 'filename  %s  should  be  of  type  string' % filename ) ) match =  GS FILEPATH REGEX . match ( filename ) if ( not match ) : raise files .  Invalid  File  Name  Error  ( 'filename  %s  should  start  with/gs/bucketname' , filename ) bucketname = match . group ( 0 ) rest = filename [ len ( bucketname ) : ] if ( ( not rest ) or ( ( len ( rest ) == 1 ) and ( rest [ 0 ] == '/' ) ) ) : return ( bucketname , '' ) if ( not rest . startswith ( '/' ) ) : raise files .  Invalid  File  Name  Error  ( ( ' Expect   /  to  separate  bucketname  and  filename  in  %s' % filename ) ) i = 1 prefix =  False  processed = '' while ( i < len ( rest ) ) : char = rest [ i ] if ( char == '\\' ) : if ( ( i + 1 ) == len ( rest ) ) : processed += char else : processed += rest [ ( i + 1 ) ] i += 1 elif ( char == '*' ) : if ( ( i + 1 ) != len ( rest ) ) : raise files .  Invalid  File  Name  Error  ( '*  as  a  wildcard  is  not  the  last.' ) prefix =  True  else : processed += char i += 1 if prefix : return ( bucketname , processed ) else : return ( ( bucketname + '/' ) + processed ) 
def get ( name , profile = 'splunk' ) : client =  get splunk ( profile ) search =  None  try : search = client . saved searches [ name ] except  Key  Error  : pass return search 
def dgap l21 ( M , G , X , active set , alpha , n orient ) : GX = np . dot ( G [ : , active set ] , X ) R = ( M - GX ) penalty = norm l21 ( X , n orient , copy =  True  ) nR2 = sum squared ( R ) pobj = ( ( 0.5 * nR2 ) + ( alpha * penalty ) ) dual norm = norm l2inf ( np . dot ( G . T , R ) , n orient , copy =  False  ) scaling = ( alpha / dual norm ) scaling = min ( scaling , 1.0 ) dobj = ( ( ( 0.5 * ( scaling ** 2 ) ) * nR2 ) + ( scaling * np . sum ( ( R * GX ) ) ) ) gap = ( pobj - dobj ) return ( gap , pobj , dobj , R ) 
def  ll geom ( y , X , beta ) : ll =  ll nbp ( y , X , beta , alph = 1 , Q = 0 ) return ll 
def create options for optionables ( optionables , extra scopes =  None  , options =  None  ) : all options = defaultdict ( dict ) bootstrap option values =  None  def complete scopes ( scopes ) : u' Return   all  enclosing  scopes.\n\n         This   is  similar  to  what  `complete scopes`  does  in  `pants.option.options. Options `  without\n        creating  ` Scope  Info `s.\n        ' completed scopes = set ( scopes ) for scope in scopes : while ( scope != u'' ) : if ( scope not in completed scopes ) : completed scopes . add ( scope ) scope = enclosing scope ( scope ) return completed scopes def register func ( on scope ) : scoped options = all options [ on scope ] register =  options registration function ( scoped options ) register . bootstrap = bootstrap option values register . scope = on scope return register  Global  Options  Registrar  . register bootstrap options ( register func ( GLOBAL SCOPE ) ) bootstrap option values = create option values ( all options [ GLOBAL SCOPE ] . copy ( ) )  Global  Options  Registrar  . register options ( register func ( GLOBAL SCOPE ) ) for optionable in optionables : optionable . register options ( register func ( optionable . options scope ) ) all scopes = set ( all options . keys ( ) ) if extra scopes : all scopes . update ( extra scopes ) all scopes = complete scopes ( all scopes ) if options : for ( scope , opts ) in options . items ( ) : all options [ scope ] . update ( opts ) for s in sorted ( all scopes ) : if ( s != GLOBAL SCOPE ) : scope = enclosing scope ( s ) opts = all options [ s ] for ( key , val ) in all options . get ( scope , { } ) . items ( ) : if ( key not in opts ) : opts [ key ] = val return create options ( all options ) 
def download video ( youtube id , format = 'mp4' , callback =  None  ) : download url = ( ( 'http://%s/download/videos/' % settings . CENTRAL SERVER HOST ) + '%s/%s' ) return videos . download video ( youtube id , settings . CONTENT ROOT , download url , format , callback ) 
def hex version ( ) : try : return  dot2int (   version   . split ( '-' ) [ ( - 1 ) ] ) except (  Name  Error  ,  Value  Error  ) : return 0 
def test message delete ( db , gmail account ) : api client = new api client ( db , gmail account . namespace ) generic thread = add fake thread ( db . session , gmail account . namespace . id ) gen message = add fake message ( db . session , gmail account . namespace . id , generic thread ) category ids = [ ] for i in xrange ( 10 ) : po data = api client . post data ( '/labels/' , { 'display name' : str ( i ) } ) assert ( po data . status code == 200 ) category ids . append ( json . loads ( po data . data ) [ 'id' ] ) data = { 'label ids' : category ids } resp = api client . put data ( '/messages/{}' . format ( gen message . public id ) , data ) assert ( resp . status code == 200 ) associated mcs = db . session . query (  Message  Category  ) . filter ( (  Message  Category  . message id == gen message . id ) ) . all ( ) assert ( len ( associated mcs ) == 10 ) db . session . delete ( gen message ) db . session . commit ( ) assert ( db . session . query (  Message  Category  ) . filter ( (  Message  Category  . message id == gen message . id ) ) . all ( ) == [ ] ) 
def hash timestamp ( afile ) : md5hex =  None  if os . path . isfile ( afile ) : md5obj = md5 ( ) stat = os . stat ( afile ) md5obj . update ( str ( stat . st size ) . encode ( ) ) md5obj . update ( str ( stat . st mtime ) . encode ( ) ) md5hex = md5obj . hexdigest ( ) return md5hex 
def argmin ( x , axis = ( - 1 ) ) : if ( axis < 0 ) : axis = ( axis % len ( x . get shape ( ) ) ) return tf . argmin ( x , axis ) 
def register opts ( conf ) : p = get engine ( conf ) p . register opts ( conf ) 
def application ( environ , start response ) : path = environ . get ( 'PATH INFO' , '' ) . lstrip ( '/' ) if ( path == 'metadata' ) : return metadata ( environ , start response ) kaka = environ . get ( 'HTTP COOKIE' ,  None  ) logger . info ( ( '<application>  PATH:  %s' % path ) ) if kaka : logger . info ( '=  KAKA  =' ) ( user , authn ref ) = info from cookie ( kaka ) environ [ 'idp.authn ref' ] = authn ref else : try : query = parse qs ( environ [ 'QUERY STRING' ] ) logger . debug ( ( 'QUERY:  %s' % query ) ) user = IDP . cache . uid2user [ query [ 'id' ] [ 0 ] ] except  Key  Error  : user =  None  url patterns = AUTHN URLS if ( not user ) : logger . info ( '--   No   USER  --' ) url patterns = ( NON AUTHN URLS + url patterns ) for ( regex , callback ) in url patterns : match = re . search ( regex , path ) if ( match is not  None  ) : try : environ [ 'myapp.url args' ] = match . groups ( ) [ 0 ] except  Index  Error  : environ [ 'myapp.url args' ] = path logger . debug ( ( ' Callback :  %s' % ( callback , ) ) ) if isinstance ( callback , tuple ) : cls = callback [ 0 ] ( environ , start response , user ) func = getattr ( cls , callback [ 1 ] ) return func ( ) return callback ( environ , start response , user ) if ( re . search ( 'static/.*' , path ) is not  None  ) : return staticfile ( environ , start response ) return not found ( environ , start response ) 
def  make link ( volume path , backup path , vol id ) : try : utils . execute ( 'ln' , volume path , backup path , run as root =  True  , check exit code =  True  ) except processutils .  Process  Execution  Error  as exc : err = (   ( 'backup:  %(vol id)s  failed  to  create  device  hardlink  from  %(vpath)s  to  %(bpath)s.\nstdout:  %(out)s\n  stderr:  %(err)s' ) % { 'vol id' : vol id , 'vpath' : volume path , 'bpath' : backup path , 'out' : exc . stdout , 'err' : exc . stderr } ) LOG . error ( err ) raise exception .  Invalid  Backup  ( reason = err ) 
def get exception data ( e ) : err data = { 'type' : e .   class   .   name   , 'message' : e , 'detail' : '' } return err data 
def run ( flow , storage ) : if FLAGS . auth local webserver : success =  False  port number = 0 for port in FLAGS . auth host port : port number = port try : httpd =  Base HTTP Server  . HTTP Server  ( ( FLAGS . auth host name , port ) ,  Client  Redirect  Handler  ) except socket . error as e : pass else : success =  True  break FLAGS . auth local webserver = success if FLAGS . auth local webserver : oauth callback = ( 'http://%s:%s/' % ( FLAGS . auth host name , port number ) ) else : oauth callback = 'oob' authorize url = flow . step1 get authorize url ( oauth callback ) print ' Go   to  the  following  link  in  your  browser:' print authorize url print if FLAGS . auth local webserver : print ' If   your  browser  is  on  a  different  machine  then  exit  and  re-run  this' print 'application  with  the  command-line  parameter  --noauth local webserver.' print if FLAGS . auth local webserver : httpd . handle request ( ) if ( 'error' in httpd . query params ) : sys . exit ( ' Authentication   request  was  rejected.' ) if ( 'oauth verifier' in httpd . query params ) : code = httpd . query params [ 'oauth verifier' ] else : accepted = 'n' while ( accepted . lower ( ) == 'n' ) : accepted = raw input ( ' Have   you  authorized  me?  (y/n)  ' ) code = raw input ( ' What   is  the  verification  code?  ' ) . strip ( ) try : credentials = flow . step2 exchange ( code ) except  Request  Error  : sys . exit ( ' The   authentication  has  failed.' ) storage . put ( credentials ) credentials . set store ( storage . put ) print ' You   have  successfully  authenticated.' return credentials 
def email is not mit mailing list ( email ) : if ( '@mit.edu' in email ) : username = email . rsplit ( '@' , 1 ) [ 0 ] try : DNS . dnslookup ( ( '%s.pobox.ns.athena.mit.edu' % username ) , DNS .  Type  . TXT ) except DNS .  Base  .  Server  Error  as e : if ( e . rcode == DNS .  Status  . NXDOMAIN ) : raise  Validation  Error  ( mark safe ( MIT VALIDATION ERROR ) ) else : raise 
def parse http basic ( authorization header ) : ( auth scheme , auth token ) = require split ( authorization header , 2 ) require ( ( auth scheme . lower ( ) == 'basic' ) ) try : auth data = base64 . b64decode ( auth token ) except  Type  Error  : raise  Requirement  Exception  return require split ( auth data , 2 , ':' ) 
def test process never started ( qtbot , quit pyproc ) : quit pyproc . after test ( ) 
def process iter ( ) : def add ( pid ) : proc =  Process  ( pid )  pmap [ proc . pid ] = proc return proc def remove ( pid ) :  pmap . pop ( pid ,  None  ) a = set ( pids ( ) ) b = set (  pmap . keys ( ) ) new pids = ( a - b ) gone pids = ( b - a ) for pid in gone pids : remove ( pid ) for ( pid , proc ) in sorted ( ( list (  pmap . items ( ) ) + list ( dict . fromkeys ( new pids ) . items ( ) ) ) ) : try : if ( proc is  None  ) : ( yield add ( pid ) ) elif proc . is running ( ) : ( yield proc ) else : ( yield add ( pid ) ) except  No  Such  Process  : remove ( pid ) except  Access  Denied  : if ( ( proc is  None  ) and ( pid in  pmap ) ) : try : ( yield  pmap [ pid ] ) except  Key  Error  : pass else : raise 
def abspath ( path ) : if ( not isabs ( path ) ) : if isinstance ( path ,  unicode ) : cwd = os . getcwdu ( ) else : cwd = os . getcwd ( ) path = join ( cwd , path ) return normpath ( path ) 
def is full path ( file ) : if ( file . startswith ( '\\' ) or file . startswith ( '/' ) ) : return  True  try : if ( file [ 1 : 3 ] == ':\\' ) : return  True  except : pass return  False  
def guess language ( text ) : ( guess , confidence ) = classify ( text ) if ( confidence < 0.7 ) : return  None  elif ( confidence < 0.9 ) : word count = len ( re . findall ( "[\\w']+" , text ) ) if ( word count <= 3 ) : return  None  return guess 
def has key ( dictionary , key ) : if ( PY MAJOR VERSION > 2 ) : return ( key in dictionary ) else : return dictionary . has key ( key ) 
@ manager . command ( ) def version ( ) : print   version   
def validate column specs ( events , next value columns , previous value columns ) : required = required event fields ( next value columns , previous value columns ) received = set ( events . columns ) missing = ( required - received ) if missing : raise  Value  Error  ( ' Events  Loader   missing  required  columns  {missing}.\n Got    Columns :  {received}\n Expected    Columns :  {required}' . format ( missing = sorted ( missing ) , received = sorted ( received ) , required = sorted ( required ) ) ) 
def  tcp listener ( address , backlog = 50 , reuse addr =  None  , family =  socket . AF INET ) : sock = socket ( family = family ) if ( reuse addr is not  None  ) : sock . setsockopt (  socket . SOL SOCKET ,  socket . SO REUSEADDR , reuse addr ) try : sock . bind ( address ) except  socket . error as ex : strerror = getattr ( ex , 'strerror' ,  None  ) if ( strerror is not  None  ) : ex . strerror = ( ( strerror + ':  ' ) + repr ( address ) ) raise sock . listen ( backlog ) sock . setblocking ( 0 ) return sock 
def single source dijkstra path length ( G , source , cutoff =  None  , weight = 'weight' ) : return multi source dijkstra path length ( G , { source } , cutoff = cutoff , weight = weight ) 
def ratint ( f , x , ** flags ) : if ( type ( f ) is not tuple ) : ( p , q ) = f . as numer denom ( ) else : ( p , q ) = f ( p , q ) = (  Poly  ( p , x , composite =  False  , field =  True  ) ,  Poly  ( q , x , composite =  False  , field =  True  ) ) ( coeff , p , q ) = p . cancel ( q ) ( poly , p ) = p . div ( q ) result = poly . integrate ( x ) . as expr ( ) if p . is zero : return ( coeff * result ) ( g , h ) = ratint ratpart ( p , q , x ) ( P , Q ) = h . as numer denom ( ) P =  Poly  ( P , x ) Q =  Poly  ( Q , x ) ( q , r ) = P . div ( Q ) result += ( g + q . integrate ( x ) . as expr ( ) ) if ( not r . is zero ) : symbol = flags . get ( 'symbol' , 't' ) if ( not isinstance ( symbol ,  Symbol  ) ) : t =  Dummy  ( symbol ) else : t = symbol . as dummy ( ) L = ratint logpart ( r , Q , x , t ) real = flags . get ( 'real' ) if ( real is  None  ) : if ( type ( f ) is not tuple ) : atoms = f . atoms ( ) else : ( p , q ) = f atoms = ( p . atoms ( ) | q . atoms ( ) ) for elt in ( atoms - { x } ) : if ( not elt . is real ) : real =  False  break else : real =  True  eps = S ( 0 ) if ( not real ) : for ( h , q ) in L : (   , h ) = h . primitive ( ) eps +=  Root  Sum  ( q ,  Lambda  ( t , ( t * log ( h . as expr ( ) ) ) ) , quadratic =  True  ) else : for ( h , q ) in L : (   , h ) = h . primitive ( ) R = log to real ( h , q , x , t ) if ( R is not  None  ) : eps += R else : eps +=  Root  Sum  ( q ,  Lambda  ( t , ( t * log ( h . as expr ( ) ) ) ) , quadratic =  True  ) result += eps return ( coeff * result ) 
def get change column query ( f , new ) : desc = frappe . db . sql ( ( u'desc  `tab%s`' % f [ u'parent' ] ) ) for d in desc : if ( d [ 0 ] == f [ u'fieldname' ] ) : return ( u'alter  table  `tab%s`  change  `%s`  `%s`  %s' % ( f [ u'parent' ] , f [ u'fieldname' ] , new , d [ 1 ] ) ) 
def small testing registry ( ) : from . . description import  Widget  Description  ,  Category  Description  from . . import  Widget  Registry  registry =  Widget  Registry  ( ) data desc =  Category  Description  . from package ( ' Orange .widgets.data' ) file desc =  Widget  Description  . from module ( ' Orange .widgets.data.owfile' ) discretize desc =  Widget  Description  . from module ( ' Orange .widgets.data.owdiscretize' ) classify desc =  Category  Description  . from package ( ' Orange .widgets.classify' ) bayes desc =  Widget  Description  . from module ( ' Orange .widgets.classify.ownaivebayes' ) registry . register category ( data desc ) registry . register category ( classify desc ) registry . register widget ( file desc ) registry . register widget ( discretize desc ) registry . register widget ( bayes desc ) return registry 
def  jittered backoff ( attempt , max retry delay ) : return min ( ( random . random ( ) * ( 2 ** attempt ) ) , max retry delay ) 
def libvlc media player get hwnd ( p mi ) : f = (   Cfunctions  . get ( 'libvlc media player get hwnd' ,  None  ) or   Cfunction  ( 'libvlc media player get hwnd' , ( ( 1 , ) , ) ,  None  , ctypes . c void p ,  Media  Player  ) ) return f ( p mi ) 
def send notification ( device name ) : current time = datetime . now ( ) sender = 'sender@twb-tech.com' recipient = 'recipient@twb-tech.com' subject = ' Device   {0}  was  modified' . format ( device name ) message = '\n The   running  configuration  of  {0}  was  modified.    \n\n This   change  was  detected  at:  {1}\n\n' . format ( device name , current time ) if send mail ( recipient , subject , message , sender ) : print ' Email   notification  sent  to  {}' . format ( recipient ) return  True  
def inport ( port name = '' , props = [ ] , mac name =  None  ) : return   create port dict ( 'in' , port name , mac name , props ) 
def reset ( no Gamma  =  True  ) : OK = init ( ) if ( no Gamma  and OK ) : set Video  Mode  ( NOGAMMACORRECT ) 
def make Failure  ( ) : try : ( 1 / 0 ) except  Zero  Division  Error  : f = failure .  Failure  ( ) return f 
def  format info ( data ) : return { 'gid' : data . pw gid , 'groups' : list groups ( data . pw name ) , 'home' : data . pw dir , 'name' : data . pw name , 'shell' : data . pw shell , 'uid' : data . pw uid , 'fullname' : data . pw gecos } 
def patch collection 2d to 3d ( col , zs = 0 , zdir = u'z' , depthshade =  True  ) : if isinstance ( col ,  Path  Collection  ) : col .   class   =  Path 3D Collection  elif isinstance ( col ,  Patch  Collection  ) : col .   class   =  Patch 3D Collection  col .  depthshade = depthshade col . set 3d properties ( zs , zdir ) 
def geo apps ( namespace =  True  , runtests =  False  ) : from django . db import connection from django . contrib . gis . geos import GEOS PREPARE from django . contrib . gis . gdal import HAS GDAL apps = [ 'geoapp' , 'relatedapp' ] if ( not connection . ops . mysql ) : apps . append ( 'distapp' ) if ( connection . ops . postgis and connection . ops . geography ) : apps . append ( 'geogapp' ) if HAS GDAL : apps . extend ( [ 'geoadmin' , 'layermap' , 'inspectapp' ] ) if ( connection . ops . postgis and GEOS PREPARE ) : apps . append ( 'geo3d' ) if runtests : return [ ( 'django.contrib.gis.tests' , app ) for app in apps ] elif namespace : return [ ( 'django.contrib.gis.tests.%s' % app ) for app in apps ] else : return apps 
@ datastore rpc .  positional ( 0 ) def fetch ( start time =  None  , end time =  None  , offset =  None  , minimum log level =  None  , include incomplete =  False  , include app logs =  False  , module versions =  None  , version ids =  None  , request ids =  None  , ** kwargs ) : args diff = ( set ( kwargs ) -  FETCH KWARGS ) if args diff : raise  Invalid  Argument  Error  ( ( ' Invalid   arguments:  %s' % ',  ' . join ( args diff ) ) ) request = log service pb .  Log  Read  Request  ( ) request . set app id ( os . environ [ 'APPLICATION ID' ] ) if ( start time is not  None  ) : if ( not isinstance ( start time , ( float , int , long ) ) ) : raise  Invalid  Argument  Error  ( 'start time  must  be  a  float  or  integer' ) request . set start time ( long ( ( start time * 1000000 ) ) ) if ( end time is not  None  ) : if ( not isinstance ( end time , ( float , int , long ) ) ) : raise  Invalid  Argument  Error  ( 'end time  must  be  a  float  or  integer' ) request . set end time ( long ( ( end time * 1000000 ) ) ) if ( offset is not  None  ) : try : request . mutable offset ( ) .  Parse  From  String  ( offset ) except (  Type  Error  ,  Protocol  Buffer  .  Protocol  Buffer  Decode  Error  ) : raise  Invalid  Argument  Error  ( 'offset  must  be  a  string  or  read-only  buffer' ) if ( minimum log level is not  None  ) : if ( not isinstance ( minimum log level , int ) ) : raise  Invalid  Argument  Error  ( 'minimum log level  must  be  an  int' ) if ( minimum log level not in logsutil . LOG LEVELS ) : raise  Invalid  Argument  Error  ( ( 'minimum log level  must  be  one  of  %s' % repr ( logsutil . LOG LEVELS ) ) ) request . set minimum log level ( minimum log level ) if ( not isinstance ( include incomplete , bool ) ) : raise  Invalid  Argument  Error  ( 'include incomplete  must  be  a  boolean' ) request . set include incomplete ( include incomplete ) if ( not isinstance ( include app logs , bool ) ) : raise  Invalid  Argument  Error  ( 'include app logs  must  be  a  boolean' ) request . set include app logs ( include app logs ) if ( version ids and module versions ) : raise  Invalid  Argument  Error  ( 'version ids  and  module versions  may  not  be  used  at  the  same  time.' ) if ( ( version ids is  None  ) and ( module versions is  None  ) ) : module version = request . add module version ( ) if ( os . environ [ 'CURRENT MODULE ID' ] != 'default' ) : module version . set module id ( os . environ [ 'CURRENT MODULE ID' ] ) module version . set version id ( os . environ [ 'CURRENT VERSION ID' ] . split ( '.' ) [ 0 ] ) if module versions : if ( not isinstance ( module versions , list ) ) : raise  Invalid  Argument  Error  ( 'module versions  must  be  a  list' ) req module versions = set ( ) for entry in module versions : if ( not isinstance ( entry , ( list , tuple ) ) ) : raise  Invalid  Argument  Error  ( 'module versions  list  entries  must  all  be  tuples  or  lists.' ) if ( len ( entry ) != 2 ) : raise  Invalid  Argument  Error  ( 'module versions  list  entries  must  all  be  of  length  2.' ) req module versions . add ( ( entry [ 0 ] , entry [ 1 ] ) ) for ( module , version ) in sorted ( req module versions ) : req module version = request . add module version ( ) if ( module != 'default' ) : req module version . set module id ( module ) req module version . set version id ( version ) if version ids : if ( not isinstance ( version ids , list ) ) : raise  Invalid  Argument  Error  ( 'version ids  must  be  a  list' ) for version id in version ids : if ( not  MAJOR VERSION ID RE . match ( version id ) ) : raise  Invalid  Argument  Error  ( 'version ids  must  only  contain  valid  major  version  identifiers' ) request . add module version ( ) . set version id ( version id ) if ( request ids is not  None  ) : if ( not isinstance ( request ids , list ) ) : raise  Invalid  Argument  Error  ( 'request ids  must  be  a  list' ) if ( not request ids ) : raise  Invalid  Argument  Error  ( 'request ids  must  not  be  empty' ) if ( len ( request ids ) != len ( set ( request ids ) ) ) : raise  Invalid  Argument  Error  ( 'request ids  must  not  contain  duplicates' ) for request id in request ids : if ( not  REQUEST ID RE . match ( request id ) ) : raise  Invalid  Argument  Error  ( ( '%s  is  not  a  valid  request  log  id' % request id ) ) request . request id list ( ) [ : ] = request ids prototype request = kwargs . get ( 'prototype request' ) if prototype request : if ( not isinstance ( prototype request , log service pb .  Log  Read  Request  ) ) : raise  Invalid  Argument  Error  ( 'prototype request  must  be  a   Log  Read  Request ' ) request .  Merge  From  ( prototype request ) timeout = kwargs . get ( 'timeout' ) if ( timeout is not  None  ) : if ( not isinstance ( timeout , ( float , int , long ) ) ) : raise  Invalid  Argument  Error  ( 'timeout  must  be  a  float  or  integer' ) batch size = kwargs . get ( 'batch size' ) if ( batch size is not  None  ) : if ( not isinstance ( batch size , ( int , long ) ) ) : raise  Invalid  Argument  Error  ( 'batch size  must  be  an  integer' ) if ( batch size < 1 ) : raise  Invalid  Argument  Error  ( 'batch size  must  be  greater  than  zero' ) if ( batch size > MAX ITEMS PER FETCH ) : raise  Invalid  Argument  Error  ( 'batch size  specified  is  too  large' ) request . set count ( batch size ) return   Log  Query  Result  ( request , timeout = timeout ) 
def test nm fit invalid ratio ( ) : ratio = ( 1.0 / 10000.0 ) nm =  Near  Miss  ( ratio = ratio , random state = RND SEED ) assert raises (  Runtime  Error  , nm . fit , X , Y ) 
def is archive file ( name ) : ext = splitext ( name ) [ 1 ] . lower ( ) if ( ext in ARCHIVE EXTENSIONS ) : return  True  return  False  
def flow from clientsecrets ( filename , scope , message =  None  ) : try : ( client type , client info ) = clientsecrets . loadfile ( filename ) if ( client type in [ clientsecrets . TYPE WEB , clientsecrets . TYPE INSTALLED ] ) : return O Auth 2 Web  Server  Flow  ( client info [ 'client id' ] , client info [ 'client secret' ] , scope ,  None  , client info [ 'auth uri' ] , client info [ 'token uri' ] ) except clientsecrets .  Invalid  Client  Secrets  Error  : if message : sys . exit ( message ) else : raise else : raise  Unknown  Client  Secrets  Flow  Error  ( ( ' This   O Auth   2.0  flow  is  unsupported:  "%s"' * client type ) ) 
def send ( tag , data =  None  , preload =  None  , with env =  False  , with grains =  False  , with pillar =  False  , with env opts =  False  , ** kwargs ) : data dict = { } if with env : if isinstance ( with env , list ) : data dict [ 'environ' ] =  dict subset ( with env , dict ( os . environ ) ) else : data dict [ 'environ' ] = dict ( os . environ ) if with grains : if isinstance ( with grains , list ) : data dict [ 'grains' ] =  dict subset ( with grains ,   grains   ) else : data dict [ 'grains' ] =   grains   if with pillar : if isinstance ( with pillar , list ) : data dict [ 'pillar' ] =  dict subset ( with pillar ,   pillar   ) else : data dict [ 'pillar' ] =   pillar   if with env opts : data dict [ 'saltenv' ] =   opts   . get ( 'environment' , 'base' ) data dict [ 'pillarenv' ] =   opts   . get ( 'pillarenv' ) if kwargs : data dict . update ( kwargs ) if isinstance ( data , collections .  Mapping  ) : data dict . update ( data ) if (   opts   . get ( 'local' ) or (   opts   . get ( 'file client' ) == 'local' ) or (   opts   . get ( 'master type' ) == 'disable' ) ) : return fire ( data dict , tag ) else : return fire master ( data dict , tag , preload = preload ) 
def get Changes  ( request , options =  None  ) : def first Or  Nothing  ( value ) : '\n                 Small   helper  function  to  return  the  first  value  (if  value  is  a  list)\n                or  return  the  whole  thing  otherwise\n                ' if isinstance ( value , type ( [ ] ) ) : return value [ 0 ] else : return value args = request . args files =  None  if args . get ( 'files' ) : files = json . loads ( args . get ( 'files' ) [ 0 ] ) else : files = [ ] properties =  None  if args . get ( 'properties' ) : properties = json . loads ( args . get ( 'properties' ) [ 0 ] ) else : properties = { } revision = first Or  Nothing  ( args . get ( 'revision' ) ) when = first Or  Nothing  ( args . get ( 'when' ) ) if ( when is not  None  ) : when = float ( when ) author = first Or  Nothing  ( args . get ( 'author' ) ) if ( not author ) : author = first Or  Nothing  ( args . get ( 'who' ) ) comments = first Or  Nothing  ( args . get ( 'comments' ) ) if isinstance ( comments , bytes ) : comments = comments . decode ( 'utf-8' ) branch = first Or  Nothing  ( args . get ( 'branch' ) ) category = first Or  Nothing  ( args . get ( 'category' ) ) revlink = first Or  Nothing  ( args . get ( 'revlink' ) ) repository = first Or  Nothing  ( args . get ( 'repository' ) ) project = first Or  Nothing  ( args . get ( 'project' ) ) codebase = first Or  Nothing  ( args . get ( 'codebase' ) ) chdict = dict ( author = author , files = files , comments = comments , revision = revision , when = when , branch = branch , category = category , revlink = revlink , properties = properties , repository = repository , project = project , codebase = codebase ) return ( [ chdict ] ,  None  ) 
def gitignore templates ( ) : return gh . gitignore templates ( ) 
def enable pretty logging ( options =  None  , logger =  None  ) : if ( options is  None  ) : from tornado . options import options if ( ( options . logging is  None  ) or ( options . logging . lower ( ) == 'none' ) ) : return if ( logger is  None  ) : logger = logging . get Logger  ( ) logger . set Level  ( getattr ( logging , options . logging . upper ( ) ) ) if options . log file prefix : channel = logging . handlers .  Rotating  File  Handler  ( filename = options . log file prefix , max Bytes  = options . log file max size , backup Count  = options . log file num backups ) channel . set Formatter  (  Log  Formatter  ( color =  False  ) ) logger . add Handler  ( channel ) if ( options . log to stderr or ( ( options . log to stderr is  None  ) and ( not logger . handlers ) ) ) : channel = logging .  Stream  Handler  ( ) channel . set Formatter  (  Log  Formatter  ( ) ) logger . add Handler  ( channel ) 
def run stdout ( name , cmd , no start =  False  , preserve state =  True  , stdin =  None  , python shell =  True  , output loglevel = 'debug' , use vt =  False  , ignore retcode =  False  , keep env =  None  ) : return  run ( name , cmd , output = 'stdout' , no start = no start , preserve state = preserve state , stdin = stdin , python shell = python shell , output loglevel = output loglevel , use vt = use vt , ignore retcode = ignore retcode , keep env = keep env ) 
def  https verify certificates ( enable =  True  ) : global  create default https context if enable :  create default https context = create default context else :  create default https context =  create unverified context 
def sub ( matlist1 , matlist2 , K ) : return add ( matlist1 , negate ( matlist2 , K ) , K ) 
def  Print  Categories  ( ) : sys . stderr . write ( '' . join ( ( ( '    %s\n' % cat ) for cat in  ERROR CATEGORIES ) ) ) sys . exit ( 0 ) 
def   virtual   ( ) : if salt . utils . is windows ( ) : return (  False  , 'dnsmasq  execution  module  cannot  be  loaded:  only  works  on  non- Windows   systems.' ) return  True  
def submit rescore problem for all students ( request , usage key , only if higher =  False  ) : check arguments for rescoring ( usage key ) task type = ( 'rescore problem if higher' if only if higher else 'rescore problem' ) task class = rescore problem ( task input , task key ) = encode problem and student input ( usage key ) task input . update ( { 'only if higher' : only if higher } ) return submit task ( request , task type , task class , usage key . course key , task input , task key ) 
def   Send  Recv  ( ) : port = int ( os . getenv ( DEVSHELL ENV , 0 ) ) if ( port == 0 ) : raise  No  Devshell  Server  ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL INFO REQUEST JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall (  helpers .  to bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if ( '\n' not in header ) : raise  Communication  Error  ( 'saw  no  newline  in  the  first  6  bytes' ) ( len str , json str ) = header . split ( '\n' , 1 ) to read = ( int ( len str ) - len ( json str ) ) if ( to read > 0 ) : json str += sock . recv ( to read , socket . MSG WAITALL ) . decode ( ) return  Credential  Info  Response  ( json str ) 
@ public def LM ( f , * gens , ** args ) : options . allowed flags ( args , [ 'polys' ] ) try : ( F , opt ) = poly from expr ( f , * gens , ** args ) except  Polification  Failed  as exc : raise  Computation  Failed  ( 'LM' , 1 , exc ) monom = F . LM ( order = opt . order ) return monom . as expr ( ) 
def ckron ( * arrays ) : return reduce ( np . kron , arrays ) 
def try for ( fn , timeout = 5.0 , delay = 0.1 ) : until = ( time . time ( ) + timeout ) v = fn ( ) while ( ( not v ) and ( time . time ( ) < until ) ) : time . sleep ( delay ) v = fn ( ) return v 
@ anonymous csrf @ mobile template ( 'questions/{mobile/}marketplace category.html' ) def marketplace category ( request , category slug , template =  None  ) : try : category name = MARKETPLACE CATEGORIES [ category slug ] except  Key  Error  : raise  Http 404 error message =  None  if ( request . method == 'GET' ) : form =  Marketplace  Aaq  Form  ( request . user ) else : form =  Marketplace  Aaq  Form  ( request . user , request . POST ) if form . is valid ( ) : try : form . submit ticket ( ) return  Http  Response  Redirect  ( reverse ( 'questions.marketplace aaq success' ) ) except  Zendesk  Error  : error message = ZENDESK ERROR MESSAGE return render ( request , template , { 'category' : category name , 'category slug' : category slug , 'categories' : MARKETPLACE CATEGORIES , 'form' : form , 'error message' : error message } ) 
def list triggers ( name , location = '\\' ) : pythoncom .  Co  Initialize  ( ) task service = win32com . client .  Dispatch  ( ' Schedule . Service ' ) task service .  Connect  ( ) task folder = task service .  Get  Folder  ( location ) task definition = task folder .  Get  Task  ( name ) .  Definition  triggers = task definition .  Triggers  ret = [ ] for trigger in triggers : ret . append ( trigger .  Id  ) return ret 
@ transaction . non atomic requests @ csrf exempt def request certificate ( request ) : if ( request . method == 'POST' ) : if request . user . is authenticated ( ) : username = request . user . username student =  User  . objects . get ( username = username ) course key =  Slash  Separated  Course  Key  . from deprecated string ( request . POST . get ( 'course id' ) ) course = modulestore ( ) . get course ( course key , depth = 2 ) status = certificate status for student ( student , course key ) [ 'status' ] if ( status in [  Certificate  Statuses  . unavailable ,  Certificate  Statuses  . notpassing ,  Certificate  Statuses  . error ] ) : log msg = u' Grading   and  certification  requested  for  user  %s  in  course  %s  via  /request certificate  call' log . info ( log msg , username , course key ) status = generate user certificates ( student , course key , course = course ) return  Http  Response  ( json . dumps ( { 'add status' : status } ) , content type = 'application/json' ) return  Http  Response  ( json . dumps ( { 'add status' : 'ERRORANONYMOUSUSER' } ) , content type = 'application/json' ) 
def ip address validators ( protocol , unpack ipv4 ) : if ( ( protocol != 'both' ) and unpack ipv4 ) : raise  Value  Error  ( " You   can  only  use  `unpack ipv4`  if  `protocol`  is  set  to  'both'" ) try : return ip address validator map [ protocol . lower ( ) ] except  Key  Error  : raise  Value  Error  ( ( " The   protocol  '%s'  is  unknown.   Supported :  %s" % ( protocol , ip address validator map . keys ( ) ) ) ) 
def check server ( protocol =  None  , service address =  None  , server address =  None  , ** kwargs ) : cmd = '{0}' . format (  build cmd ( protocol = protocol , service address = service address , server address = server address , ** kwargs ) ) if ( not kwargs ) : cmd += '  ' all rules = get rules ( ) out = all rules . find ( cmd ) if ( out != ( - 1 ) ) : ret =  True  else : ret = ' Error :  server  not  exists' return ret 
def t one observation ( x , sample , tails = 'two-sided' , exp diff = 0 ) : try : sample mean = mean ( sample ) sample std = std ( sample , ddof = 1 ) if ( sample std == 0 ) : return ( nan , nan ) else : n = len ( sample ) t = ( ( ( ( x - sample mean ) - exp diff ) / sample std ) / sqrt ( ( ( n + 1 ) / n ) ) ) prob = tprob ( t , ( n - 1 ) , tails ) return ( float ( t ) , prob ) except (  Zero  Division  Error  ,  Value  Error  ,  Attribute  Error  ,  Type  Error  ,  Floating  Point  Error  ) : return ( nan , nan ) 
def switch backend ( newbackend ) : close ( u'all' ) global  backend mod , new figure manager , draw if interactive ,  show matplotlib . use ( newbackend , warn =  False  , force =  True  ) from matplotlib . backends import pylab setup (  backend mod , new figure manager , draw if interactive ,  show ) = pylab setup ( ) 
def get network metadata ( network info , use ipv6 =  None  ) : if ( not network info ) : return if ( use ipv6 is  None  ) : use ipv6 = CONF . use ipv6 nets = [ ] links = [ ] services = [ ] ifc num = ( - 1 ) net num = ( - 1 ) for vif in network info : if ( ( not vif . get ( 'network' ) ) or ( not vif [ 'network' ] . get ( 'subnets' ) ) ) : continue network = vif [ 'network' ] subnet v4 =  get first network ( network , 4 ) subnet v6 =  get first network ( network , 6 ) ifc num += 1 link =  None  if ( subnet v4 or subnet v6 ) : link =  get eth link ( vif , ifc num ) links . append ( link ) if ( subnet v4 and subnet v4 . get ( 'ips' ) ) : net num += 1 nets . append (  get nets ( vif , subnet v4 , 4 , net num , link [ 'id' ] ) ) services += [ dns for dns in  get dns services ( subnet v4 ) if ( dns not in services ) ] if ( ( use ipv6 and subnet v6 ) and subnet v6 . get ( 'ips' ) ) : net num += 1 nets . append (  get nets ( vif , subnet v6 , 6 , net num , link [ 'id' ] ) ) services += [ dns for dns in  get dns services ( subnet v6 ) if ( dns not in services ) ] return { 'links' : links , 'networks' : nets , 'services' : services } 
def  Deconstruct  Timestamp  Asset  Id  ( id prefix , asset id , reverse ts =  True  ) : assert  Id  Prefix  .  Is  Valid  ( id prefix ) , id prefix assert ( asset id [ 0 ] == id prefix ) , asset id byte str = base64hex . B64 Hex  Decode  ( asset id [ 1 : ] , padding =  False  ) ( timestamp , ) = struct . unpack ( '>I' , byte str [ : 4 ] ) if reverse ts : timestamp = ( ( ( 1 L << 32 ) - timestamp ) - 1 ) ( device id , num bytes ) = util .  Decode  Var  Length  Number  ( byte str [ 4 : ] ) uniquifier =   Decode  Uniquifier  ( byte str [ ( 4 + num bytes ) : ] ) return ( timestamp , device id , uniquifier ) 
def fetch google ( uuid ) : fetch url = constants .  Urlfetch  Test  Identifiers  . GOOGLE URL return fetch ( fetch url ) 
def pip install ( package ) : try : pip . main ( [ 'install' , '--upgrade' , package ] ) return  True  except : print ( ' Unable   to  install  %s  using  pip.   Please   read  the  instructions  for                  manual  installation..   Exiting ' % package ) print ( ' Error :  %s:  %s' % ( exc info ( ) [ 0 ] , exc info ( ) [ 1 ] ) ) return  False  
def run CSS ( input path , out path , output CSS statistics ) : if ( not output CSS statistics ) : command args = [ ( '-i  %s  -o  %s' % ( input path , out path ) ) ] else : command args = [ ( '-i  %s  -o  %s  -s  %s' % ( input path , out path , output CSS statistics ) ) ] rsl = R Executor  (  Tmp  Dir  = get qiime temp dir ( ) ) app result = rsl ( command args = command args , script name = 'CSS.r' ) return app result 
@ gof . local optimizer ( [ scan op .  Scan  ] ) def remove constants and unused inputs scan ( node ) : if ( not isinstance ( node . op , scan op .  Scan  ) ) : return  False  op = node . op st = op . n seqs st += int ( sum ( [ len ( x ) for x in op . tap array [ : ( op . n mit mot + op . n mit sot ) ] ] ) ) st += op . n sit sot st += op . n shared outs op ins = op . inputs op outs = op . outputs out stuff inner = op ins [ op . n seqs : st ] non seqs = op ins [ st : ] st = ( ( ( ( ( ( op . n seqs + op . n mit mot ) + op . n mit sot ) + op . n sit sot ) + op . n nit sot ) + op . n shared outs ) + 1 ) outer non seqs = node . inputs [ st : ] out stuff outer = node . inputs [ ( 1 + op . n seqs ) : st ] givens =  Ordered  Dict  ( ) nw inner = [ ] nw outer = [ node . inputs [ 0 ] ] all ins = gof . graph . inputs ( op outs ) for idx in xrange ( op . n seqs ) : node inp = node . inputs [ ( idx + 1 ) ] if ( isinstance ( node inp , tensor .  Tensor  Constant  ) and ( node inp . tag . unique value is not  None  ) ) : try : givens [ op ins [ idx ] ] = node inp . clone ( ) [ 0 ] except  Type  Error  : pass elif ( op ins [ idx ] in all ins ) : identical seqs = [ x for x in nw outer if scan utils . equal computations ( [ x ] , [ node inp ] ) ] if identical seqs : index = ( node . inputs . index ( identical seqs [ 0 ] ) - 1 ) givens [ op ins [ idx ] ] = op ins [ index ] else : nw inner . append ( op ins [ idx ] ) nw outer . append ( node inp ) nw n seqs = len ( nw inner ) nw inner += out stuff inner nw outer += out stuff outer nw inner nonseq = [ ] nw outer nonseq = [ ] for ( idx , ( nw in , nw out ) ) in enumerate ( zip ( non seqs , outer non seqs ) ) : if isinstance ( nw out , tensor .  Constant  ) : givens [ nw in ] = nw out . clone ( ) elif ( nw in in all ins ) : identical nonseq idx = [ i for ( i , x ) in enumerate ( nw outer nonseq ) if scan utils . equal computations ( [ x ] , [ nw out ] ) ] if identical nonseq idx : givens [ nw in ] = nw inner nonseq [ identical nonseq idx [ 0 ] ] else : nw inner nonseq . append ( nw in ) nw outer nonseq . append ( nw out ) nw inner . extend ( nw inner nonseq ) nw outer . extend ( nw outer nonseq ) if ( len ( nw inner ) != len ( op ins ) ) : op outs = scan utils . clone ( op outs , replace = givens ) nw info = copy . deepcopy ( op . info ) nw info [ 'n seqs' ] = nw n seqs nw Scan  = scan op .  Scan  ( nw inner , op outs , nw info ) nw outs = nw Scan  ( * nw outer , ** dict ( return list =  True  ) ) return  Ordered  Dict  ( ( [ ( 'remove' , [ node ] ) ] + list ( zip ( node . outputs , nw outs ) ) ) ) else : return  False  
def versions from parentdir ( parentdir prefix , root , verbose ) : dirname = os . path . basename ( root ) if ( not dirname . startswith ( parentdir prefix ) ) : if verbose : print ( "guessing  rootdir  is  '%s',  but  '%s'  doesn't  start  with  prefix  '%s'" % ( root , dirname , parentdir prefix ) ) raise  Not  This  Method  ( "rootdir  doesn't  start  with  parentdir prefix" ) return { 'version' : dirname [ len ( parentdir prefix ) : ] , 'full-revisionid' :  None  , 'dirty' :  False  , 'error' :  None  } 
def truncated cube graph ( create using =  None  ) : description = [ 'adjacencylist' , ' Truncated    Cube    Graph ' , 24 , [ [ 2 , 3 , 5 ] , [ 12 , 15 ] , [ 4 , 5 ] , [ 7 , 9 ] , [ 6 ] , [ 17 , 19 ] , [ 8 , 9 ] , [ 11 , 13 ] , [ 10 ] , [ 18 , 21 ] , [ 12 , 13 ] , [ 15 ] , [ 14 ] , [ 22 , 23 ] , [ 16 ] , [ 20 , 24 ] , [ 18 , 19 ] , [ 21 ] , [ 20 ] , [ 24 ] , [ 22 ] , [ 23 ] , [ 24 ] , [ ] ] ] G = make small undirected graph ( description , create using ) return G 
def edge dfs ( G , source =  None  , orientation = 'original' ) : nodes = list ( G . nbunch iter ( source ) ) if ( not nodes ) : raise  Stop  Iteration  kwds = { 'data' :  False  } if G . is multigraph ( ) : kwds [ 'keys' ] =  True  ( out edges , key , tailhead ) = helper funcs ( G , orientation ) visited edges = set ( ) visited nodes = set ( ) edges = { } for start node in nodes : stack = [ start node ] while stack : current node = stack [ ( - 1 ) ] if ( current node not in visited nodes ) : edges [ current node ] = out edges ( current node , ** kwds ) visited nodes . add ( current node ) try : edge = next ( edges [ current node ] ) except  Stop  Iteration  : stack . pop ( ) else : edge key = key ( edge ) if ( edge key not in visited edges ) : visited edges . add ( edge key ) stack . append ( tailhead ( edge ) [ 1 ] ) ( yield edge ) 
def dumb property dict ( style ) : return dict ( [ ( x . strip ( ) , y . strip ( ) ) for ( x , y ) in [ z . split ( ':' , 1 ) for z in style . split ( ';' ) if ( ':' in z ) ] ] ) 
def unicode2local ( p ) : return p 
def test solve polynomial cv 1a ( ) : assert ( solveset real ( ( sqrt ( x ) - 1 ) , x ) ==  Finite  Set  ( 1 ) ) assert ( solveset real ( ( sqrt ( x ) - 2 ) , x ) ==  Finite  Set  ( 4 ) ) assert ( solveset real ( ( ( x **  Rational  ( 1 , 4 ) ) - 2 ) , x ) ==  Finite  Set  ( 16 ) ) assert ( solveset real ( ( ( x **  Rational  ( 1 , 3 ) ) - 3 ) , x ) ==  Finite  Set  ( 27 ) ) assert ( solveset real ( ( x * ( ( x ** ( S ( 1 ) / 3 ) ) - 3 ) ) , x ) ==  Finite  Set  ( S ( 0 ) , S ( 27 ) ) ) 
def setup module ( ) : import shutil if ( IMAGE1 FILE not in os . listdir ( '.' ) ) : shutil . copyfile ( os . path . join ( os . path . pardir , IMAGE1 FILE ) , IMAGE1 FILE ) testnewdocument ( ) 
def walk skip hidden ( top , onerror =  None  , followlinks =  False  ) : for ( root , dirs , files ) in os . walk ( top , topdown =  True  , onerror = onerror , followlinks = followlinks ) : dirs [ : ] = [ d for d in dirs if ( not is path hidden ( d ) ) ] files [ : ] = [ f for f in files if ( not is path hidden ( f ) ) ] ( yield ( root , dirs , files ) ) 
def user passes test ( test func , login url =  None  , redirect field name = REDIRECT FIELD NAME ) : def decorator ( view func ) : @ wraps ( view func , assigned = available attrs ( view func ) ) def  wrapped view ( request , * args , ** kwargs ) : if test func ( request . user ) : return view func ( request , * args , ** kwargs ) path = request . build absolute uri ( ) ( login scheme , login netloc ) = urlparse . urlparse ( ( login url or settings . LOGIN URL ) ) [ : 2 ] ( current scheme , current netloc ) = urlparse . urlparse ( path ) [ : 2 ] if ( ( ( not login scheme ) or ( login scheme == current scheme ) ) and ( ( not login netloc ) or ( login netloc == current netloc ) ) ) : path = request . get full path ( ) from django . contrib . auth . views import redirect to login return redirect to login ( path , login url , redirect field name ) return  wrapped view return decorator 
def touch ( ui , repo , basedir ) : do touch ( ui , repo , basedir ) 
def reflow text ( text , quoting =  False  , target width = 65 ) : if quoting : target width -= 2 inlines = text . splitlines ( ) outlines = [ ] def line length ( l , word ) : return ( ( sum ( ( len ( w ) for w in l ) ) + len ( l ) ) + len ( word ) ) while inlines : thisline = inlines . pop ( 0 ) if ( re . match ( REFLOW PROSE START , thisline ) and ( not thisline . endswith ( '    ' ) ) and ( len ( thisline ) > ( target width - 10 ) ) ) : para = thisline . strip ( ) . split ( ) while ( inlines and ( not inlines [ 0 ] . endswith ( '    ' ) ) and re . match ( REFLOW PROSE START , inlines [ 0 ] ) ) : para += inlines . pop ( 0 ) . strip ( ) . split ( ) paralines = [ [ ] ] for word in para : if ( line length ( paralines [ ( - 1 ) ] , word ) <= target width ) : paralines [ ( - 1 ) ] . append ( word ) elif ( 0 == len ( paralines [ ( - 1 ) ] ) ) : paralines [ ( - 1 ) ] . append ( word ) else : paralines . append ( [ word ] ) outlines . extend ( [ '  ' . join ( l ) for l in paralines ] ) else : outlines . append ( thisline ) return '\n' . join ( outlines ) 
def pseudo Scatter  ( data , spacing =  None  , shuffle =  True  , bidir =  False  ) : inds = np . arange ( len ( data ) ) if shuffle : np . random . shuffle ( inds ) data = data [ inds ] if ( spacing is  None  ) : spacing = ( ( 2.0 * np . std ( data ) ) / ( len ( data ) ** 0.5 ) ) s2 = ( spacing ** 2 ) yvals = np . empty ( len ( data ) ) if ( len ( data ) == 0 ) : return yvals yvals [ 0 ] = 0 for i in range ( 1 , len ( data ) ) : x = data [ i ] x0 = data [ : i ] y0 = yvals [ : i ] y = 0 dx = ( ( x0 - x ) ** 2 ) xmask = ( dx < s2 ) if ( xmask . sum ( ) > 0 ) : if bidir : dirs = [ ( - 1 ) , 1 ] else : dirs = [ 1 ] yopts = [ ] for direction in dirs : y = 0 dx2 = dx [ xmask ] dy = ( ( s2 - dx2 ) ** 0.5 ) limits = np . empty ( ( 2 , len ( dy ) ) ) limits [ 0 ] = ( y0 [ xmask ] - dy ) limits [ 1 ] = ( y0 [ xmask ] + dy ) while  True  : if ( direction > 0 ) : mask = ( limits [ 1 ] >= y ) else : mask = ( limits [ 0 ] <= y ) limits2 = limits [ : , mask ] mask = ( ( limits2 [ 0 ] < y ) & ( limits2 [ 1 ] > y ) ) if ( mask . sum ( ) == 0 ) : break if ( direction > 0 ) : y = limits2 [ : , mask ] . max ( ) else : y = limits2 [ : , mask ] . min ( ) yopts . append ( y ) if bidir : y = ( yopts [ 0 ] if ( ( - yopts [ 0 ] ) < yopts [ 1 ] ) else yopts [ 1 ] ) else : y = yopts [ 0 ] yvals [ i ] = y return yvals [ np . argsort ( inds ) ] 
@ py3compat . doctest refactor print def sys info ( ) : return pprint . pformat ( get sys info ( ) ) 
def process token or pass ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : encoded token = request . args . get ( 'token' ) if encoded token : handler =  Token  Handler  . from string ( encoded token ) try : res = handler . to response ( ) except  Token  Handler  Not  Found  as e : raise HTTP Error  ( http . BAD REQUEST , data = { 'message short' : ' Invalid    Token ' , 'message long' : ' No   token  handler  for  action:  {}  found' . format ( e . action ) } ) if res : return res return func ( * args , ** kwargs ) return wrapper 
def user passes test ( test func , login url =  None  , redirect field name = REDIRECT FIELD NAME ) : def decorator ( view func ) : @ wraps ( view func , assigned = available attrs ( view func ) ) def  wrapped view ( request , * args , ** kwargs ) : if test func ( request . user ) : return view func ( request , * args , ** kwargs ) path = request . build absolute uri ( ) resolved login url = force str ( resolve url ( ( login url or settings . LOGIN URL ) ) ) ( login scheme , login netloc ) = urlparse ( resolved login url ) [ : 2 ] ( current scheme , current netloc ) = urlparse ( path ) [ : 2 ] if ( ( ( not login scheme ) or ( login scheme == current scheme ) ) and ( ( not login netloc ) or ( login netloc == current netloc ) ) ) : path = request . get full path ( ) from django . contrib . auth . views import redirect to login return redirect to login ( path , resolved login url , redirect field name ) return  wrapped view return decorator 
def  Close  Buffers  For  Filename  ( filename ) : buffer number =  Get  Buffer  Number  For  Filename  ( filename ,  False  ) while ( buffer number != ( - 1 ) ) : vim . command ( u'silent!  bwipeout!  {0}' . format ( buffer number ) ) new buffer number =  Get  Buffer  Number  For  Filename  ( filename ,  False  ) if ( buffer number == new buffer number ) : raise  Runtime  Error  ( u" Buffer   {0}  for  filename  '{1}'  should  already  be  wiped  out." . format ( buffer number , filename ) ) buffer number = new buffer number 
def unescape all ( url ) : if isinstance ( url , bytes ) : func2use =  unescape bytes keys2use =  bytes keys else : func2use =  unescape str keys2use =  str keys clean url = func2use ( url ) not done = [ ( clean url . count ( key ) > 0 ) for key in keys2use ] if (  True  in not done ) : return unescape all ( clean url ) else : return clean url 
def  print absolute limits ( limits ) : class  Limit  ( object , ) : def   init   ( self , name , used , max , other ) : self . name = name self . used = used self . max = max self . other = other limit map = { 'max Server  Meta ' : { 'name' : ' Server    Meta ' , 'type' : 'max' } , 'max Personality ' : { 'name' : ' Personality ' , 'type' : 'max' } , 'max Personality  Size ' : { 'name' : ' Personality    Size ' , 'type' : 'max' } , 'max Image  Meta ' : { 'name' : ' Image  Meta ' , 'type' : 'max' } , 'max Total  Keypairs ' : { 'name' : ' Keypairs ' , 'type' : 'max' } , 'total Cores  Used ' : { 'name' : ' Cores ' , 'type' : 'used' } , 'max Total  Cores ' : { 'name' : ' Cores ' , 'type' : 'max' } , 'totalRAM Used ' : { 'name' : 'RAM' , 'type' : 'used' } , 'max Total RAM Size ' : { 'name' : 'RAM' , 'type' : 'max' } , 'total Instances  Used ' : { 'name' : ' Instances ' , 'type' : 'used' } , 'max Total  Instances ' : { 'name' : ' Instances ' , 'type' : 'max' } , 'total Floating  Ips  Used ' : { 'name' : ' Floating  Ips ' , 'type' : 'used' } , 'max Total  Floating  Ips ' : { 'name' : ' Floating  Ips ' , 'type' : 'max' } , 'total Security  Groups  Used ' : { 'name' : ' Security  Groups ' , 'type' : 'used' } , 'max Security  Groups ' : { 'name' : ' Security  Groups ' , 'type' : 'max' } , 'max Security  Group  Rules ' : { 'name' : ' Security  Group  Rules ' , 'type' : 'max' } , 'max Server  Groups ' : { 'name' : ' Server  Groups ' , 'type' : 'max' } , 'total Server  Groups  Used ' : { 'name' : ' Server  Groups ' , 'type' : 'used' } , 'max Server  Group  Members ' : { 'name' : ' Server  Group  Members ' , 'type' : 'max' } } max = { } used = { } other = { } limit names = [ ] columns = [ ' Name ' , ' Used ' , ' Max ' ] for l in limits : map = limit map . get ( l . name , { 'name' : l . name , 'type' : 'other' } ) name = map [ 'name' ] if ( map [ 'type' ] == 'max' ) : max [ name ] = l . value elif ( map [ 'type' ] == 'used' ) : used [ name ] = l . value else : other [ name ] = l . value columns . append ( ' Other ' ) if ( name not in limit names ) : limit names . append ( name ) limit names . sort ( ) limit list = [ ] for name in limit names : l =  Limit  ( name , used . get ( name , '-' ) , max . get ( name , '-' ) , other . get ( name , '-' ) ) limit list . append ( l ) utils . print list ( limit list , columns ) 
def parse repository tag ( repo path ) : tag separator = u':' digest separator = u'@' if ( digest separator in repo path ) : ( repo , tag ) = repo path . rsplit ( digest separator , 1 ) return ( repo , tag , digest separator ) ( repo , tag ) = ( repo path , u'' ) if ( tag separator in repo path ) : ( repo , tag ) = repo path . rsplit ( tag separator , 1 ) if ( u'/' in tag ) : ( repo , tag ) = ( repo path , u'' ) return ( repo , tag , tag separator ) 
def visstd ( a , s = 0.1 ) : return ( ( ( ( a - a . mean ( ) ) / max ( a . std ( ) , 0.0001 ) ) * s ) + 0.5 ) 
@ staff member required def edit ( request ) : model = apps . get model ( request . POST [ u'app' ] , request . POST [ u'model' ] ) obj = model . objects . get ( id = request . POST [ u'id' ] ) form = get edit form ( obj , request . POST [ u'fields' ] , data = request . POST , files = request . FILES ) if ( not ( is editable ( obj , request ) and has site permission ( request . user ) ) ) : response =   ( u' Permission   denied' ) elif form . is valid ( ) : form . save ( ) model admin =  Model  Admin  ( model , admin . site ) message = model admin . construct change message ( request , form ,  None  ) model admin . log change ( request , obj , message ) response = u'' else : response = list ( form . errors . values ( ) ) [ 0 ] [ 0 ] return  Http  Response  ( response ) 
def re render content for management command ( message ) : assert  Message  . need to render content ( message . rendered content , message . rendered content version , bugdown . version ) rendered content = render markdown ( message , message . content ) message . rendered content = rendered content message . rendered content version = bugdown . version message . save rendered content ( ) 
def print column headers ( results ) : print ( ' Column    Headers :' ) headers = results . get ( 'column Headers ' ) for header in headers : print ( ( ' DCTB %s  name:        =  %s' % ( header . get ( 'column Type ' ) . title ( ) , header . get ( 'name' ) ) ) ) print ( ( ' DCTB  Column    Type   =  %s' % header . get ( 'column Type ' ) ) ) print ( ( ' DCTB  Data    Type       =  %s' % header . get ( 'data Type ' ) ) ) print ( ) 
def get start end ( sequence , skiplist = ( '-' , '?' ) ) : length = len ( sequence ) if ( length == 0 ) : return (  None  ,  None  ) end = ( length - 1 ) while ( ( end >= 0 ) and ( sequence [ end ] in skiplist ) ) : end -= 1 start = 0 while ( ( start < length ) and ( sequence [ start ] in skiplist ) ) : start += 1 if ( ( start == length ) and ( end == ( - 1 ) ) ) : return ( ( - 1 ) , ( - 1 ) ) else : return ( start , end ) 
def setup platform ( hass , config , add callback devices , discovery info =  None  ) : if ( discovery info is  None  ) : return homematic = get component ( 'homematic' ) return homematic . setup hmdevice discovery helper ( hass , HM Sensor  , discovery info , add callback devices ) 
def get device by name or pk ( name ) : if re . match ( DEVICE BY PK RE , name ) : pk = name . strip ( '{}' ) device =  Device  . objects . get ( pk = pk ) else : device =  Device  . objects . get ( name = name ) return device 
def pagerank numpy ( G , alpha = 0.85 , personalization =  None  , weight = 'weight' , dangling =  None  ) : import numpy as np if ( len ( G ) == 0 ) : return { } M = google matrix ( G , alpha , personalization = personalization , weight = weight , dangling = dangling ) ( eigenvalues , eigenvectors ) = np . linalg . eig ( M . T ) ind = np . argmax ( eigenvalues ) largest = np . array ( eigenvectors [ : , ind ] ) . flatten ( ) . real norm = float ( largest . sum ( ) ) return dict ( zip ( G , map ( float , ( largest / norm ) ) ) ) 
def get path names ( ) : return  SCHEME KEYS 
def parse qiime config file ( qiime config file ) : result = { } for line in qiime config file : line = line . strip ( ) if ( ( not line ) or line . startswith ( '#' ) ) : continue fields = line . split ( ) param id = fields [ 0 ] param value = ( expandvars ( '  ' . join ( fields [ 1 : ] ) ) or  None  ) result [ param id ] = param value return result 
def parse Dict  ( parent ) : parent . count = 0 while  True  : key =  Object  ( parent , 'key[]' ) ( yield key ) if ( key [ 'bytecode' ] . value == '0' ) : break ( yield  Object  ( parent , 'value[]' ) ) parent . count += 1 
def  prepare mne browse raw ( params , title , bgcolor , color , bad color , inds , n channels ) : import matplotlib . pyplot as plt import matplotlib as mpl size = get config ( 'MNE BROWSE RAW SIZE' ) if ( size is not  None  ) : size = size . split ( ',' ) size = tuple ( [ float ( s ) for s in size ] ) fig = figure nobar ( facecolor = bgcolor , figsize = size ) fig . canvas . set window title ( 'mne browse raw' ) ax = plt . subplot2grid ( ( 10 , 10 ) , ( 0 , 1 ) , colspan = 8 , rowspan = 9 ) ax . set title ( title , fontsize = 12 ) ax hscroll = plt . subplot2grid ( ( 10 , 10 ) , ( 9 , 1 ) , colspan = 8 ) ax hscroll . get yaxis ( ) . set visible (  False  ) ax hscroll . set xlabel ( ' Time   (s)' ) ax vscroll = plt . subplot2grid ( ( 10 , 10 ) , ( 0 , 9 ) , rowspan = 9 ) ax vscroll . set axis off ( ) ax help button = plt . subplot2grid ( ( 10 , 10 ) , ( 0 , 0 ) , colspan = 1 ) help button = mpl . widgets .  Button  ( ax help button , ' Help ' ) help button . on clicked ( partial (  onclick help , params = params ) ) params [ 'fig' ] = fig params [ 'ax' ] = ax params [ 'ax hscroll' ] = ax hscroll params [ 'ax vscroll' ] = ax vscroll params [ 'ax help button' ] = ax help button params [ 'help button' ] = help button info = params [ 'info' ] n ch = len ( inds ) if ( 'fig selection' in params ) : selections = params [ 'selections' ] labels = [ l .  text for l in params [ 'fig selection' ] . radio . labels ] cis = [ item for sublist in [ selections [ l ] for l in labels ] for item in sublist ] for ( idx , ci ) in enumerate ( cis ) : this color = ( bad color if ( info [ 'ch names' ] [ ci ] in info [ 'bads' ] ) else color ) if isinstance ( this color , dict ) : this color = this color [ params [ 'types' ] [ ci ] ] ax vscroll . add patch ( mpl . patches .  Rectangle  ( ( 0 , idx ) , 1 , 1 , facecolor = this color , edgecolor = this color ) ) ax vscroll . set ylim ( len ( cis ) , 0 ) n channels = max ( [ len ( selections [ labels [ 0 ] ] ) , n channels ] ) else : for ci in range ( len ( inds ) ) : this color = ( bad color if ( info [ 'ch names' ] [ inds [ ci ] ] in info [ 'bads' ] ) else color ) if isinstance ( this color , dict ) : this color = this color [ params [ 'types' ] [ inds [ ci ] ] ] ax vscroll . add patch ( mpl . patches .  Rectangle  ( ( 0 , ci ) , 1 , 1 , facecolor = this color , edgecolor = this color ) ) ax vscroll . set ylim ( n ch , 0 ) vsel patch = mpl . patches .  Rectangle  ( ( 0 , 0 ) , 1 , n channels , alpha = 0.5 , facecolor = 'w' , edgecolor = 'w' ) ax vscroll . add patch ( vsel patch ) params [ 'vsel patch' ] = vsel patch hsel patch = mpl . patches .  Rectangle  ( ( params [ 't start' ] , 0 ) , params [ 'duration' ] , 1 , edgecolor = 'k' , facecolor = ( 0.75 , 0.75 , 0.75 ) , alpha = 0.25 , linewidth = 1 , clip on =  False  ) ax hscroll . add patch ( hsel patch ) params [ 'hsel patch' ] = hsel patch ax hscroll . set xlim ( params [ 'first time' ] , ( params [ 'first time' ] + ( params [ 'n times' ] / float ( info [ 'sfreq' ] ) ) ) ) ax vscroll . set title ( ' Ch .' ) vertline color = ( 0.0 , 0.75 , 0.0 ) params [ 'ax vertline' ] = ax . plot ( [ 0 , 0 ] , ax . get ylim ( ) , color = vertline color , zorder = ( - 1 ) ) [ 0 ] params [ 'ax vertline' ] . ch name = '' params [ 'vertline t' ] = ax hscroll . text ( params [ 'first time' ] , 1 , '' , color = vertline color , va = 'bottom' , ha = 'right' ) params [ 'ax hscroll vertline' ] = ax hscroll . plot ( [ 0 , 0 ] , [ 0 , 1 ] , color = vertline color , zorder = 2 ) [ 0 ]  setup browser offsets ( params , n channels ) ax . set xlim ( params [ 't start' ] , ( params [ 't start' ] + params [ 'duration' ] ) ,  False  ) params [ 'lines' ] = [ ax . plot ( [ np . nan ] , antialiased =  False  , linewidth = 0.5 ) [ 0 ] for   in range ( n ch ) ] ax . set yticklabels ( [ ( 'X' * max ( [ len ( ch ) for ch in info [ 'ch names' ] ] ) ) ] ) 
def test epochs bad baseline ( ) : ( raw , events ) =  get data ( ) [ : 2 ] assert raises (  Value  Error  ,  Epochs  , raw , events ,  None  , ( - 0.1 ) , 0.3 , ( ( - 0.2 ) , 0 ) ) assert raises (  Value  Error  ,  Epochs  , raw , events ,  None  , ( - 0.1 ) , 0.3 , ( 0 , 0.4 ) ) assert raises (  Value  Error  ,  Epochs  , raw , events ,  None  , ( - 0.1 ) , 0.3 , ( 0.1 , 0 ) ) assert raises (  Value  Error  ,  Epochs  , raw , events ,  None  , 0.1 , 0.3 , (  None  , 0 ) ) assert raises (  Value  Error  ,  Epochs  , raw , events ,  None  , ( - 0.3 ) , ( - 0.1 ) , ( 0 ,  None  ) ) epochs =  Epochs  ( raw , events ,  None  , 0.1 , 0.3 , baseline =  None  ) assert raises (  Runtime  Error  , epochs . apply baseline , ( 0.1 , 0.2 ) ) epochs . load data ( ) assert raises (  Value  Error  , epochs . apply baseline , (  None  , 0 ) ) assert raises (  Value  Error  , epochs . apply baseline , ( 0 ,  None  ) ) data = np . arange ( 100 , dtype = float ) assert raises (  Value  Error  , rescale , data , times = data , baseline = ( ( - 2 ) , ( - 1 ) ) ) rescale ( data . copy ( ) , times = data , baseline = ( 2 , 2 ) ) assert raises (  Value  Error  , rescale , data , times = data , baseline = ( 2 , 1 ) ) assert raises (  Value  Error  , rescale , data , times = data , baseline = ( 100 , 101 ) ) 
def after nearest workday ( dt ) : return next workday ( nearest workday ( dt ) ) 
def safe str ( obj ) : try : return str ( obj ) except  Unicode  Encode  Error  : return unicode ( obj ) . encode ( 'unicode escape' ) 
def add time units ( time , unit , amount ) : args = { } if ( unit == 'hour' ) : args [ 'hours' ] = amount elif ( unit == 'day' ) : args [ 'days' ] = amount elif ( unit == 'week' ) : args [ 'days' ] = ( amount * 7 ) elif ( unit == 'month' ) : args [ 'months' ] = amount elif ( unit == 'quarter' ) : args [ 'months' ] = ( amount * 3 ) elif ( unit == 'year' ) : args [ 'years' ] = amount else : raise  Argument  Error  ( ' Unknown   unit  %s  for  subtraction.' ) return ( time + relativedelta ( ** args ) ) 
def ping ( host , timeout =  False  , return boolean =  False  ) : if timeout : if (   grains   [ 'kernel' ] == ' Sun OS' ) : cmd = 'ping  -c  4  {1}  {0}' . format ( timeout , salt . utils . network . sanitize host ( host ) ) else : cmd = 'ping  -W  {0}  -c  4  {1}' . format ( timeout , salt . utils . network . sanitize host ( host ) ) else : cmd = 'ping  -c  4  {0}' . format ( salt . utils . network . sanitize host ( host ) ) if return boolean : ret =   salt   [ 'cmd.run all' ] ( cmd ) if ( ret [ 'retcode' ] != 0 ) : return  False  else : return  True  else : return   salt   [ 'cmd.run' ] ( cmd ) 
def dup rshift ( f , n , K ) : return f [ : ( - n ) ] 
def get sr path ( session , sr ref =  None  ) : if ( sr ref is  None  ) : sr ref = safe find sr ( session ) pbd rec = session . call xenapi ( 'PBD.get all records where' , ( 'field  "host"="%s"  and  field  "SR"="%s"' % ( session . host ref , sr ref ) ) ) pbd ref = list ( pbd rec . keys ( ) ) [ 0 ] device config = pbd rec [ pbd ref ] [ 'device config' ] if ( 'path' in device config ) : return device config [ 'path' ] sr rec = session . call xenapi ( 'SR.get record' , sr ref ) sr uuid = sr rec [ 'uuid' ] if ( sr rec [ 'type' ] not in [ 'ext' , 'nfs' ] ) : raise exception .  Nova  Exception  ( (   ( ' Only   file-based  S Rs   (ext/NFS)  are  supported  by  this  feature.    SR  %(uuid)s  is  of  type  %(type)s' ) % { 'uuid' : sr uuid , 'type' : sr rec [ 'type' ] } ) ) return os . path . join ( CONF . xenserver . sr base path , sr uuid ) 
def get profile visibility ( user profile , user , configuration =  None  ) : if user profile . requires parental consent ( ) : return PRIVATE VISIBILITY if ( not configuration ) : configuration = settings . ACCOUNT VISIBILITY CONFIGURATION profile privacy =  User  Preference  . get value ( user , ACCOUNT VISIBILITY PREF KEY ) return ( profile privacy if profile privacy else configuration . get ( 'default visibility' ) ) 
def organisation update affiliations ( record ) : if ( record . deleted and record . deleted fk ) : try : fk = json . loads ( record . deleted fk ) branch id = fk [ 'branch id' ] except : return else : branch id = record . branch id from pr import OU BRANCHES = ' Branches ' db = current . db s3db = current . s3db otable = s3db . org organisation btable = otable . with alias ( 'branch' ) ltable = db . org organisation branch etable = s3db . pr pentity rtable = db . pr role atable = db . pr affiliation o = otable .  tablename b = btable .  tablename r = rtable .  tablename query = ( ( ltable . branch id == branch id ) & ( ltable . deleted !=  True  ) ) left = [ otable . on ( ( ltable . organisation id == otable . id ) ) , btable . on ( ( ltable . branch id == btable . id ) ) ] rows = db ( query ) . select ( otable . pe id , btable . pe id , left = left ) current memberships = [ ( row [ o ] . pe id , row [ b ] . pe id ) for row in rows ] query = ( ( ( ( ( ( ( ( rtable . deleted !=  True  ) & ( rtable . role == BRANCHES ) ) & ( rtable . pe id == etable . pe id ) ) & ( etable . instance type == o ) ) & ( atable . deleted !=  True  ) ) & ( atable . role id == rtable . id ) ) & ( atable . pe id == btable . pe id ) ) & ( btable . id == branch id ) ) rows = db ( query ) . select ( rtable . pe id , btable . pe id ) current affiliations = [ ( row [ r ] . pe id , row [ b ] . pe id ) for row in rows ] remove affiliation = s3db . pr remove affiliation for a in current affiliations : ( org , branch ) = a if ( a not in current memberships ) : remove affiliation ( org , branch , role = BRANCHES ) else : current memberships . remove ( a ) add affiliation = s3db . pr add affiliation for m in current memberships : ( org , branch ) = m add affiliation ( org , branch , role = BRANCHES , role type = OU ) 
def interpolate cache slug ( string ) : cache slug = utils . get asset dir prefix ( ) return parse string ( string , { 'cache slug' : cache slug } ) 
def compute a ( n ) : a = [ ( mp . sqrt ( 2 ) / 2 ) ] for k in range ( 1 , n ) : ak = ( a [ ( - 1 ) ] / k ) for j in range ( 1 , len ( a ) ) : ak -= ( ( a [ j ] * a [ ( - j ) ] ) / ( j + 1 ) ) ak /= ( a [ 0 ] * ( 1 + ( mp . mpf ( 1 ) / ( k + 1 ) ) ) ) a . append ( ak ) return a 
def  numpy zeros ( m , n , ** options ) : dtype = options . get ( 'dtype' , 'float64' ) if ( not np ) : raise  Import  Error  return np . zeros ( ( m , n ) , dtype = dtype ) 
def translate ( pat ) : ( i , n ) = ( 0 , len ( pat ) ) res = '' while ( i < n ) : c = pat [ i ] i = ( i + 1 ) if ( c == '*' ) : res = ( res + '.*' ) elif ( c == '?' ) : res = ( res + '.' ) elif ( c == '[' ) : j = i if ( ( j < n ) and ( pat [ j ] == '!' ) ) : j = ( j + 1 ) if ( ( j < n ) and ( pat [ j ] == ']' ) ) : j = ( j + 1 ) while ( ( j < n ) and ( pat [ j ] != ']' ) ) : j = ( j + 1 ) if ( j >= n ) : res = ( res + '\\[' ) else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = ( j + 1 ) if ( stuff [ 0 ] == '!' ) : stuff = ( '^' + stuff [ 1 : ] ) elif ( stuff [ 0 ] == '^' ) : stuff = ( '\\' + stuff ) res = ( '%s[%s]' % ( res , stuff ) ) else : res = ( res + re . escape ( c ) ) return ( res + '$' ) 
def axis ( * v , ** kwargs ) : return gca ( ) . axis ( * v , ** kwargs ) 
def add variables to context ( generator ) : context = generator . context context [ 'relpath to site' ] = relpath to site context [ 'main siteurl' ] =  MAIN SITEURL context [ 'main lang' ] =  MAIN LANG context [ 'lang siteurls' ] =  SITE DB current lang = generator . settings [ 'DEFAULT LANG' ] extra siteurls =  SITE DB . copy ( ) extra siteurls . pop ( current lang ) context [ 'extra siteurls' ] = extra siteurls 
def  Numpy  As  Parameter  ( numpy array ) : assert ( numpy array . dtype == 'float32' ) , ' Saved   arrays  should  be  float32.' return numpy array . tostring ( ) 
def release Complete  Ms  To  Net  (  Cause  presence = 0 ,  Facility  presence = 0 ,  User  User  presence = 0 ,  Ss  Version  Indicator  presence = 0 ) : a =  Tp  Pd  ( pd = 3 ) b =  Message  Type  ( mes Type  = 42 ) packet = ( a / b ) if (  Cause  presence is 1 ) : c =  Cause  Hdr  ( ieiC = 8 , eight Bit C = 0 ) packet = ( packet / c ) if (  Facility  presence is 1 ) : d =  Facility  Hdr  ( ieiF = 28 , eight Bit F = 0 ) packet = ( packet / d ) if (  User  User  presence is 1 ) : e =  User  User  Hdr  ( ieiUU = 126 , eight Bit UU = 0 ) packet = ( packet / e ) if (  Ss  Version  Indicator  presence is 1 ) : f =  Ss  Version  Indicator  Hdr  ( ieiSVI = 127 , eight Bit SVI = 0 ) packet = ( packet / f ) return packet 
@ utils . arg ( 'host' , metavar = '<host>' , help =   ( ' Name   of  host.' ) ) @ utils . arg ( 'action' , metavar = '<action>' , choices = [ 'set' , 'delete' ] , help =   ( " Actions :  'set'  or  'delete'" ) ) @ utils . arg ( 'metadata' , metavar = '<key=value>' , nargs = '+' , action = 'append' , default = [ ] , help =   ( ' Metadata   to  set  or  delete  (only  key  is  necessary  on  delete)' ) ) def do host meta ( cs , args ) : hypervisors = cs . hypervisors . search ( args . host , servers =  True  ) for hyper in hypervisors : metadata =  extract metadata ( args ) if hasattr ( hyper , 'servers' ) : for server in hyper . servers : if ( args . action == 'set' ) : cs . servers . set meta ( server [ 'uuid' ] , metadata ) elif ( args . action == 'delete' ) : cs . servers . delete meta ( server [ 'uuid' ] , metadata . keys ( ) ) 
def get dependencies ( ) : return config . check driver dependencies (   virtualname   , { 'azurearm' : HAS LIBS } ) 
def show driver ( devname ) : try : module = ethtool . get module ( devname ) except IO Error  : log . error ( ' Driver   information  not  implemented  on  {0}' . format ( devname ) ) return ' Not   implemented' try : businfo = ethtool . get businfo ( devname ) except IO Error  : log . error ( ' Bus   information  no  available  on  {0}' . format ( devname ) ) return ' Not   available' ret = { 'driver' : module , 'bus info' : businfo } return ret 
def  handle broken tcl tk ( ) : if ( is win and is venv ) : basedir = os . path . join ( base prefix , 'tcl' ) files = os . listdir ( basedir ) for f in files : abs path = os . path . join ( basedir , f ) if ( f . startswith ( 'tcl' ) and os . path . isdir ( abs path ) ) : os . environ [ 'TCL LIBRARY' ] = abs path elif ( f . startswith ( 'tk' ) and os . path . isdir ( abs path ) ) : os . environ [ 'TK LIBRARY' ] = abs path elif ( f . startswith ( 'tix' ) and os . path . isdir ( abs path ) ) : os . environ [ 'TIX LIBRARY' ] = abs path 
@ transaction . non atomic requests @ require POST @ ensure csrf cookie @ cache control ( no cache =  True  , no store =  True  , must revalidate =  True  ) @ require level ( 'staff' ) def calculate grades csv ( request , course id ) : course key =  Slash  Separated  Course  Key  . from deprecated string ( course id ) try : lms . djangoapps . instructor task . api . submit calculate grades csv ( request , course key ) success status =   ( ' The   grade  report  is  being  created.   To   view  the  status  of  the  report,  see   Pending    Tasks   below.' ) return  Json  Response  ( { 'status' : success status } ) except  Already  Running  Error  : already running status =   ( ' The   grade  report  is  currently  being  created.   To   view  the  status  of  the  report,  see   Pending    Tasks   below.   You   will  be  able  to  download  the  report  when  it  is  complete.' ) return  Json  Response  ( { 'status' : already running status } ) 
@ register . tag def ssi ( parser , token ) : import warnings warnings . warn ( ' The   syntax  for  the  ssi  template  tag  is  changing.   Load   the  `ssi`  tag  from  the  `future`  tag  library  to  start  using  the  new  behavior.' , category =  Deprecation  Warning  ) bits = token . contents . split ( ) parsed =  False  if ( len ( bits ) not in ( 2 , 3 ) ) : raise  Template  Syntax  Error  ( "'ssi'  tag  takes  one  argument:  the  path  to  the  file  to  be  included" ) if ( len ( bits ) == 3 ) : if ( bits [ 2 ] == 'parsed' ) : parsed =  True  else : raise  Template  Syntax  Error  ( ( " Second   (optional)  argument  to  %s  tag  must  be  'parsed'" % bits [ 0 ] ) ) return  Ssi  Node  ( bits [ 1 ] , parsed , legacy filepath =  True  ) 
def schemata ( schema ) : return dict ( ( ( n ,  Schema  . from attribute ( s ) ) for ( n , s ) in schema . items ( ) ) ) 
def  plot timeseries unified ( bn , ch idx , tmin , tmax , vmin , vmax , ylim , data , color , times , vline =  None  , x label =  None  , y label =  None  , colorbar =  False  , hline =  None  ) : import matplotlib . pyplot as plt if ( not ( ylim and ( not any ( ( ( v is  None  ) for v in ylim ) ) ) ) ) : ylim = np . array ( [ np . min ( data ) , np . max ( data ) ] )  compute scalings ( bn , ( tmin , tmax ) , ylim ) pos = bn . pos data lines = bn . data lines ax = bn . ax for ( data  , color  ) in zip ( data , color ) : data lines . append ( ax . plot ( ( bn . x t + ( bn . x s * times ) ) , ( bn . y t + ( bn . y s * data  [ ch idx ] ) ) , color  , clip on =  True  , clip box = pos ) [ 0 ] ) if vline : vline = ( ( np . array ( vline ) * bn . x s ) + bn . x t ) ax . vlines ( vline , pos [ 1 ] , ( pos [ 1 ] + pos [ 3 ] ) , color = 'w' , linewidth = 0.5 ) if hline : hline = ( ( np . array ( hline ) * bn . y s ) + bn . y t ) ax . hlines ( hline , pos [ 0 ] , ( pos [ 0 ] + pos [ 2 ] ) , color = 'w' , linewidth = 0.5 ) if ( x label is not  None  ) : ax . text ( ( pos [ 0 ] + ( pos [ 2 ] / 2.0 ) ) , pos [ 1 ] , x label , horizontalalignment = 'center' , verticalalignment = 'top' ) if ( y label is not  None  ) : y label = ( y label [ ch idx ] if isinstance ( y label , list ) else y label ) ax . text ( pos [ 0 ] , ( pos [ 1 ] + ( pos [ 3 ] / 2.0 ) ) , y label , horizontalignment = 'right' , verticalalignment = 'middle' , rotation = 90 ) if colorbar : plt . colorbar ( ) 
def upgrade available ( name , ** kwargs ) : saltenv = kwargs . get ( u'saltenv' , u'base' ) refresh = salt . utils . is true ( kwargs . get ( u'refresh' ,  True  ) ) return ( latest version ( name , saltenv = saltenv , refresh = refresh ) != u'' ) 
def http connect ( ipaddr , port , device , partition , method , path , headers =  None  , query string =  None  , ssl =  False  ) : if isinstance ( path , six . text type ) : try : path = path . encode ( 'utf-8' ) except  Unicode  Error  as e : logging . exception (   ( ' Error   encoding  to  UTF-8:  %s' ) , str ( e ) ) if isinstance ( device , six . text type ) : try : device = device . encode ( 'utf-8' ) except  Unicode  Error  as e : logging . exception (   ( ' Error   encoding  to  UTF-8:  %s' ) , str ( e ) ) path = quote ( ( ( ( ( '/' + device ) + '/' ) + str ( partition ) ) + path ) ) return http connect raw ( ipaddr , port , method , path , headers , query string , ssl ) 
def test sort locations file not find link ( data ) : finder =  Package  Finder  ( [ ] , [ ] , session =  Pip  Session  ( ) ) ( files , urls ) = finder .  sort locations ( [ data . index url ( 'empty with pkg' ) ] ) assert ( urls and ( not files ) ) , 'urls,  but  not  files  should  have  been  found' 
def apps ( ) : response = salt . utils . http . query ( '{0}/v2/apps' . format (  base url ( ) ) , decode type = 'json' , decode =  True  ) return { 'apps' : [ app [ 'id' ] for app in response [ 'dict' ] [ 'apps' ] ] } 
def async subscribe ( hass , topic , callback , qos = DEFAULT QOS ) : @ asyncio . coroutine def mqtt topic subscriber ( event ) : ' Match   subscribed  MQTT  topic.' if ( not  match topic ( topic , event . data [ ATTR TOPIC ] ) ) : return hass . async run job ( callback , event . data [ ATTR TOPIC ] , event . data [ ATTR PAYLOAD ] , event . data [ ATTR QOS ] ) async remove = hass . bus . async listen ( EVENT MQTT MESSAGE RECEIVED , mqtt topic subscriber ) MQTT CLIENT . subscribe ( topic , qos ) return async remove 
def demo simple grid ( fig ) : grid =  Image  Grid  ( fig , 141 , nrows ncols = ( 2 , 2 ) , axes pad = 0.05 , label mode = '1' ) ( Z , extent ) = get demo image ( ) for i in range ( 4 ) : im = grid [ i ] . imshow ( Z , extent = extent , interpolation = 'nearest' ) grid . axes llc . set xticks ( [ ( - 2 ) , 0 , 2 ] ) grid . axes llc . set yticks ( [ ( - 2 ) , 0 , 2 ] ) 
def is maximal matching ( G , matching ) : if isinstance ( matching , dict ) : matching = matching dict to set ( matching ) if ( not is matching ( G , matching ) ) : return  False  all edges = set ( map ( frozenset , G . edges ( ) ) ) matched edges = set ( map ( frozenset , matching ) ) unmatched edges = ( all edges - matched edges ) return all ( ( ( not is matching ( G , ( matching | { e } ) ) ) for e in unmatched edges ) ) 
def bytestring path ( path ) : if isinstance ( path , str ) : return path if ( ( os . path .   name   == 'ntpath' ) and path . startswith ( WINDOWS MAGIC PREFIX ) ) : path = path [ len ( WINDOWS MAGIC PREFIX ) : ] try : return path . encode (  fsencoding ( ) ) except (  Unicode  Error  ,  Lookup  Error  ) : return path . encode ( 'utf8' ) 
def run services ( services , registry path = DEFAULT REGISTRY PATH ) : mappings = service mapping ( services , registry path = registry path ) application = webapp . WSGI Application  ( mappings ) webapp util . run wsgi app ( application ) 
def compute nb ( X , y , Z ) : labels = [ int ( t ) for t in y ] ptrain = [ X [ i ] for i in range ( len ( labels ) ) if ( labels [ i ] == 0 ) ] ntrain = [ X [ i ] for i in range ( len ( labels ) ) if ( labels [ i ] == 1 ) ] poscounts = nbsvm . build dict ( ptrain , [ 1 , 2 ] ) negcounts = nbsvm . build dict ( ntrain , [ 1 , 2 ] ) ( dic , r ) = nbsvm . compute ratio ( poscounts , negcounts ) trainX = nbsvm . process text ( X , dic , r , [ 1 , 2 ] ) devX = nbsvm . process text ( Z , dic , r , [ 1 , 2 ] ) return ( trainX , devX ) 
def dt to decimal ( utc ) : decimal . getcontext ( ) . prec = 30 return ( decimal .  Decimal  ( str ( calendar . timegm ( utc . utctimetuple ( ) ) ) ) + ( decimal .  Decimal  ( str ( utc . microsecond ) ) / decimal .  Decimal  ( '1000000.0' ) ) ) 
def find file iter ( filename , env vars = ( ) , searchpath = ( ) , file names =  None  , url =  None  , verbose =  False  , finding dir =  False  ) : file names = ( [ filename ] + ( file names or [ ] ) ) assert isinstance ( filename , compat . string types ) assert ( not isinstance ( file names , compat . string types ) ) assert ( not isinstance ( searchpath , compat . string types ) ) if isinstance ( env vars , compat . string types ) : env vars = env vars . split ( ) yielded =  False  for alternative in file names : path to file = os . path . join ( filename , alternative ) if os . path . isfile ( path to file ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , path to file ) ) ) yielded =  True  ( yield path to file ) if os . path . isfile ( alternative ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , alternative ) ) ) yielded =  True  ( yield alternative ) path to file = os . path . join ( filename , 'file' , alternative ) if os . path . isfile ( path to file ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , path to file ) ) ) yielded =  True  ( yield path to file ) for env var in env vars : if ( env var in os . environ ) : if finding dir : yielded =  True  ( yield os . environ [ env var ] ) for env dir in os . environ [ env var ] . split ( os . pathsep ) : if os . path . isfile ( env dir ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , env dir ) ) ) yielded =  True  ( yield env dir ) for alternative in file names : path to file = os . path . join ( env dir , alternative ) if os . path . isfile ( path to file ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , path to file ) ) ) yielded =  True  ( yield path to file ) path to file = os . path . join ( env dir , 'bin' , alternative ) if os . path . isfile ( path to file ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , path to file ) ) ) yielded =  True  ( yield path to file ) for directory in searchpath : for alternative in file names : path to file = os . path . join ( directory , alternative ) if os . path . isfile ( path to file ) : yielded =  True  ( yield path to file ) if ( os . name == 'posix' ) : for alternative in file names : try : p = subprocess .  Popen  ( [ 'which' , alternative ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) path =  decode stdoutdata ( stdout ) . strip ( ) if ( path . endswith ( alternative ) and os . path . exists ( path ) ) : if verbose : print ( ( '[ Found   %s:  %s]' % ( filename , path ) ) ) yielded =  True  ( yield path ) except (  Keyboard  Interrupt  ,  System  Exit  , OS Error  ) : raise except : pass if ( not yielded ) : msg = ( 'NLTK  was  unable  to  find  the  %s  file!\n Use   software  specific  configuration  paramaters' % filename ) if env vars : msg += ( '  or  set  the  %s  environment  variable' % env vars [ 0 ] ) msg += '.' if searchpath : msg += '\n\n     Searched   in:' msg += '' . join ( ( ( '\n        -  %s' % d ) for d in searchpath ) ) if url : msg += ( '\n\n     For   more  information  on  %s,  see:\n        <%s>' % ( filename , url ) ) div = ( '=' * 75 ) raise  Lookup  Error  ( ( '\n\n%s\n%s\n%s' % ( div , msg , div ) ) ) 
def order angles golden ratio ( theta ) : interval = 180 def angle distance ( a , b ) : difference = ( a - b ) return min ( abs ( ( difference % interval ) ) , abs ( ( difference % ( - interval ) ) ) ) remaining = list ( np . argsort ( theta ) ) index = remaining . pop ( 0 ) angle = theta [ index ] ( yield index ) angle increment = ( interval * ( 1 - ( ( np . sqrt ( 5 ) - 1 ) / 2 ) ) ) while remaining : angle = ( ( angle + angle increment ) % interval ) insert point = np . searchsorted ( theta [ remaining ] , angle ) index below = ( insert point - 1 ) index above = ( 0 if ( insert point == len ( remaining ) ) else insert point ) distance below = angle distance ( angle , theta [ remaining [ index below ] ] ) distance above = angle distance ( angle , theta [ remaining [ index above ] ] ) if ( distance below < distance above ) : ( yield remaining . pop ( index below ) ) else : ( yield remaining . pop ( index above ) ) 
def mathml ( expr , ** settings ) : return  Math ML Printer  ( settings ) . doprint ( expr ) 
def start Dtmf  Acknowledge  ( ) : a =  Tp  Pd  ( pd = 3 ) b =  Message  Type  ( mes Type  = 50 ) c =  Keypad  Facility  Hdr  ( ieiKF = 44 , eight Bit KF = 0 ) packet = ( ( a / b ) / c ) return packet 
def store item ( context , builder , arrayty , val , ptr ) : align = (  None  if arrayty . aligned else 1 ) return context . pack value ( builder , arrayty . dtype , val , ptr , align = align ) 
def deploy script ( host , port = 22 , timeout = 900 , username = 'root' , password =  None  , key filename =  None  , script =  None  , name =  None  , sock dir =  None  , provider =  None  , conf file =  None  , start action =  None  , make master =  False  , master pub =  None  , master pem =  None  , master conf =  None  , minion pub =  None  , minion pem =  None  , minion conf =  None  , keep tmp =  False  , script args =  None  , script env =  None  , ssh timeout = 15 , maxtries = 15 , make syndic =  False  , make minion =  True  , display ssh output =  True  , preseed minion keys =  None  , parallel =  False  , sudo password =  None  , sudo =  False  , tty =  None  , deploy command = '/tmp/.saltcloud/deploy.sh' , opts =  None  , tmp dir = '/tmp/.saltcloud' , file map =  None  , master sign pub file =  None  , ** kwargs ) : if ( not isinstance ( opts , dict ) ) : opts = { } tmp dir = '{0}-{1}' . format ( tmp dir . rstrip ( '/' ) , uuid . uuid4 ( ) ) deploy command = os . path . join ( tmp dir , 'deploy.sh' ) if ( ( key filename is not  None  ) and ( not os . path . isfile ( key filename ) ) ) : raise  Salt  Cloud  Config  Error  ( " The   defined  key filename  '{0}'  does  not  exist" . format ( key filename ) ) gateway =  None  if ( 'gateway' in kwargs ) : gateway = kwargs [ 'gateway' ] starttime = time . mktime ( time . localtime ( ) ) log . debug ( ' Deploying   {0}  at  {1}' . format ( host , starttime ) ) known hosts file = kwargs . get ( 'known hosts file' , '/dev/null' ) hard timeout = opts . get ( 'hard timeout' ,  None  ) if wait for port ( host = host , port = port , gateway = gateway ) : log . debug ( 'SSH  port  {0}  on  {1}  is  available' . format ( port , host ) ) if wait for passwd ( host , port = port , username = username , password = password , key filename = key filename , ssh timeout = ssh timeout , display ssh output = display ssh output , gateway = gateway , known hosts file = known hosts file , maxtries = maxtries , hard timeout = hard timeout ) : log . debug ( ' Logging   into  {0}:{1}  as  {2}' . format ( host , port , username ) ) ssh kwargs = { 'hostname' : host , 'port' : port , 'username' : username , 'timeout' : ssh timeout , 'display ssh output' : display ssh output , 'sudo password' : sudo password , 'sftp' : opts . get ( 'use sftp' ,  False  ) } if gateway : ssh kwargs [ 'ssh gateway' ] = gateway [ 'ssh gateway' ] ssh kwargs [ 'ssh gateway key' ] = gateway [ 'ssh gateway key' ] ssh kwargs [ 'ssh gateway user' ] = gateway [ 'ssh gateway user' ] if key filename : log . debug ( ' Using   {0}  as  the  key filename' . format ( key filename ) ) ssh kwargs [ 'key filename' ] = key filename elif ( password and ( kwargs . get ( 'has ssh agent' ,  False  ) is  False  ) ) : ssh kwargs [ 'password' ] = password if root cmd ( "test  -e  '{0}'" . format ( tmp dir ) , tty , sudo , allow failure =  True  , ** ssh kwargs ) : ret = root cmd ( 'sh  -c  "(  mkdir  -p  -m  700  \'{0}\'  )"' . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( " Can 't  create  temporary  directory  in  {0}  !" . format ( tmp dir ) ) if sudo : comps = tmp dir . lstrip ( '/' ) . rstrip ( '/' ) . split ( '/' ) if ( len ( comps ) > 0 ) : if ( ( len ( comps ) > 1 ) or ( comps [ 0 ] != 'tmp' ) ) : ret = root cmd ( 'chown  {0}  "{1}"' . format ( username , tmp dir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( ' Cant   set  {0}  ownership  on  {1}' . format ( username , tmp dir ) ) if ( not isinstance ( file map , dict ) ) : file map = { } remote dirs = [ ] file map success = [ ] file map fail = [ ] for map item in file map : local file = map item remote file = file map [ map item ] if ( not os . path . exists ( map item ) ) : log . error ( ' The   local  file  "{0}"  does  not  exist,  and  will  not  be  copied  to  "{1}"  on  the  target  system' . format ( local file , remote file ) ) file map fail . append ( { local file : remote file } ) continue if os . path . isdir ( local file ) : dir name = os . path . basename ( local file ) remote dir = os . path . join ( os . path . dirname ( remote file ) , dir name ) else : remote dir = os . path . dirname ( remote file ) if ( remote dir not in remote dirs ) : root cmd ( "mkdir  -p  '{0}'" . format ( remote dir ) , tty , sudo , ** ssh kwargs ) if ( ssh kwargs [ 'username' ] != 'root' ) : root cmd ( "chown  {0}  '{1}'" . format ( ssh kwargs [ 'username' ] , remote dir ) , tty , sudo , ** ssh kwargs ) remote dirs . append ( remote dir ) ssh file ( opts , remote file , kwargs = ssh kwargs , local file = local file ) file map success . append ( { local file : remote file } ) if minion pem : ssh file ( opts , '{0}/minion.pem' . format ( tmp dir ) , minion pem , ssh kwargs ) ret = root cmd ( "chmod  600  '{0}/minion.pem'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( " Can 't  set  perms  on  {0}/minion.pem" . format ( tmp dir ) ) if minion pub : ssh file ( opts , '{0}/minion.pub' . format ( tmp dir ) , minion pub , ssh kwargs ) if master sign pub file : ssh file ( opts , '{0}/master sign.pub' . format ( tmp dir ) , kwargs = ssh kwargs , local file = master sign pub file ) if minion conf : if ( not isinstance ( minion conf , dict ) ) : raise  Deprecation  Warning  ( "`salt.utils.cloud.deploy script  now  only  accepts  dictionaries  for  it's  `minion conf`  parameter.   Loading   YAML..." ) minion grains = minion conf . pop ( 'grains' , { } ) if minion grains : ssh file ( opts , '{0}/grains' . format ( tmp dir ) , salt config to yaml ( minion grains ) , ssh kwargs ) ssh file ( opts , '{0}/minion' . format ( tmp dir ) , salt config to yaml ( minion conf ) , ssh kwargs ) if master pem : ssh file ( opts , '{0}/master.pem' . format ( tmp dir ) , master pem , ssh kwargs ) ret = root cmd ( "chmod  600  '{0}/master.pem'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( ' Cant   set  perms  on  {0}/master.pem' . format ( tmp dir ) ) if master pub : ssh file ( opts , '{0}/master.pub' . format ( tmp dir ) , master pub , ssh kwargs ) if master conf : if ( not isinstance ( master conf , dict ) ) : raise  Deprecation  Warning  ( "`salt.utils.cloud.deploy script  now  only  accepts  dictionaries  for  it's  `master conf`  parameter.   Loading   from  YAML  ..." ) ssh file ( opts , '{0}/master' . format ( tmp dir ) , salt config to yaml ( master conf ) , ssh kwargs ) preseed minion keys tempdir = '{0}/preseed-minion-keys' . format ( tmp dir ) if ( preseed minion keys is not  None  ) : ret = root cmd ( "mkdir  '{0}'" . format ( preseed minion keys tempdir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( ' Cant   create  {0}' . format ( preseed minion keys tempdir ) ) ret = root cmd ( "chmod  700  '{0}'" . format ( preseed minion keys tempdir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( " Can 't  set  perms  on  {0}" . format ( preseed minion keys tempdir ) ) if ( ssh kwargs [ 'username' ] != 'root' ) : root cmd ( "chown  {0}  '{1}'" . format ( ssh kwargs [ 'username' ] , preseed minion keys tempdir ) , tty , sudo , ** ssh kwargs ) for ( minion id , minion key ) in six . iteritems ( preseed minion keys ) : rpath = os . path . join ( preseed minion keys tempdir , minion id ) ssh file ( opts , rpath , minion key , ssh kwargs ) if ( ssh kwargs [ 'username' ] != 'root' ) : root cmd ( "chown  -R  root  '{0}'" . format ( preseed minion keys tempdir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( " Can 't  set  ownership  for  {0}" . format ( preseed minion keys tempdir ) ) if script : ssh file ( opts , '{0}/deploy.sh' . format ( tmp dir ) , script , ssh kwargs ) ret = root cmd ( 'sh  -c  "(  chmod  +x  \'{0}/deploy.sh\'  )";exit  $?' . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) if ret : raise  Salt  Cloud  System  Exit  ( " Can 't  set  perms  on  {0}/deploy.sh" . format ( tmp dir ) ) newtimeout = ( timeout - ( time . mktime ( time . localtime ( ) ) - starttime ) ) queue =  None  process =  None  if ( start action and ( not parallel ) ) : queue = multiprocessing .  Queue  ( ) process = multiprocessing .  Process  ( target = check auth , kwargs = dict ( name = name , sock dir = sock dir , timeout = newtimeout , queue = queue ) ) log . debug ( ' Starting   new  process  to  wait  for  salt-minion' ) process . start ( ) if script : if ( 'bootstrap-salt' in script ) : deploy command += "  -c  '{0}'" . format ( tmp dir ) if ( make syndic is  True  ) : deploy command += '  -S' if ( make master is  True  ) : deploy command += '  -M' if ( make minion is  False  ) : deploy command += '  -N' if ( keep tmp is  True  ) : deploy command += '  -K' if ( preseed minion keys is not  None  ) : deploy command += "  -k  '{0}'" . format ( preseed minion keys tempdir ) if script args : deploy command += '  {0}' . format ( script args ) if script env : if ( not isinstance ( script env , dict ) ) : raise  Salt  Cloud  System  Exit  ( " The   'script env'  configuration  setting  NEEDS  to  be  a  dictionary  not  a  {0}" . format ( type ( script env ) ) ) environ script contents = [ '#!/bin/sh' ] for ( key , value ) in six . iteritems ( script env ) : environ script contents . append ( "setenv  {0}  '{1}'  >/dev/null  2>&1  ||  export  {0}='{1}'" . format ( key , value ) ) environ script contents . append ( deploy command ) ssh file ( opts , '{0}/environ-deploy-wrapper.sh' . format ( tmp dir ) , '\n' . join ( environ script contents ) , ssh kwargs ) root cmd ( "chmod  +x  '{0}/environ-deploy-wrapper.sh'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) deploy command = "'{0}/environ-deploy-wrapper.sh'" . format ( tmp dir ) if ( root cmd ( deploy command , tty , sudo , ** ssh kwargs ) != 0 ) : raise  Salt  Cloud  System  Exit  ( " Executing   the  command  '{0}'  failed" . format ( deploy command ) ) log . debug ( " Executed   command  '{0}'" . format ( deploy command ) ) if ( not keep tmp ) : root cmd ( "rm  -f  '{0}/deploy.sh'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/deploy.sh' . format ( tmp dir ) ) if script env : root cmd ( "rm  -f  '{0}/environ-deploy-wrapper.sh'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/environ-deploy-wrapper.sh' . format ( tmp dir ) ) if keep tmp : log . debug ( ' Not   removing  deployment  files  from  {0}/' . format ( tmp dir ) ) else : if minion pub : root cmd ( "rm  -f  '{0}/minion.pub'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/minion.pub' . format ( tmp dir ) ) if minion pem : root cmd ( "rm  -f  '{0}/minion.pem'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/minion.pem' . format ( tmp dir ) ) if minion conf : root cmd ( "rm  -f  '{0}/grains'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/grains' . format ( tmp dir ) ) root cmd ( "rm  -f  '{0}/minion'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/minion' . format ( tmp dir ) ) if master sign pub file : root cmd ( 'rm  -f  {0}/master sign.pub' . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/master sign.pub' . format ( tmp dir ) ) if master pub : root cmd ( "rm  -f  '{0}/master.pub'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/master.pub' . format ( tmp dir ) ) if master pem : root cmd ( "rm  -f  '{0}/master.pem'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/master.pem' . format ( tmp dir ) ) if master conf : root cmd ( "rm  -f  '{0}/master'" . format ( tmp dir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}/master' . format ( tmp dir ) ) if ( preseed minion keys is not  None  ) : root cmd ( "rm  -rf  '{0}'" . format ( preseed minion keys tempdir ) , tty , sudo , ** ssh kwargs ) log . debug ( ' Removed   {0}' . format ( preseed minion keys tempdir ) ) if ( start action and ( not parallel ) ) : queuereturn = queue . get ( ) process . join ( ) if ( queuereturn and start action ) : log . info ( ' Executing   {0}  on  the  salt-minion' . format ( start action ) ) root cmd ( 'salt-call  {0}' . format ( start action ) , tty , sudo , ** ssh kwargs ) log . info ( ' Finished   executing  {0}  on  the  salt-minion' . format ( start action ) ) fire event ( 'event' , '{0}  has  been  deployed  at  {1}' . format ( name , host ) , 'salt/cloud/{0}/deploy script' . format ( name ) , args = { 'name' : name , 'host' : host } , sock dir = opts . get ( 'sock dir' , os . path . join (   opts   [ 'sock dir' ] , 'master' ) ) , transport = opts . get ( 'transport' , 'zeromq' ) ) if ( file map fail or file map success ) : return { ' File    Upload    Success ' : file map success , ' File    Upload    Failure ' : file map fail } return  True  return  False  
def getnode ( ) : global  node if (  node is not  None  ) : return  node import sys if ( sys . platform == 'win32' ) : getters = [  windll getnode ,  netbios getnode ,  ipconfig getnode ] else : getters = [  unixdll getnode ,  ifconfig getnode ,  arp getnode ,  lanscan getnode ,  netstat getnode ] for getter in ( getters + [  random getnode ] ) : try :  node = getter ( ) except : continue if (  node is not  None  ) : return  node 
def main ( client id , client secret , scopes ) : flow = client . O Auth 2 Web  Server  Flow  ( client id = client id , client secret = client secret , scope = scopes , user agent = ' Ads    Python    Client    Library ' , redirect uri = 'urn:ietf:wg:oauth:2.0:oob' ) authorize url = flow . step1 get authorize url ( ) print ( ' Log   into  the   Google    Account   you  use  to  access  your  DFP  accountand  go  to  the  following  URL:  \n%s\n' % authorize url ) print ' After   approving  the  token  enter  the  verification  code  (if  specified).' code = raw input ( ' Code :  ' ) . strip ( ) try : credential = flow . step2 exchange ( code ) except client .  Flow  Exchange  Error  as e : print ( ' Authentication   has  failed:  %s' % e ) sys . exit ( 1 ) else : print ( 'O Auth 2  authorization  successful!\n\n Your   access  token  is:\n  %s\n\n Your   refresh  token  is:\n  %s' % ( credential . access token , credential . refresh token ) ) 
def insert into file ( fileobj , data , start , end ) : buffer =  String IO ( ) fileobj . seek ( end ) copyfileobj ( fileobj , buffer , ( - 1 ) ) buffer . flush ( ) buffer . seek ( 0 ) fileobj . seek ( start ) fileobj . write ( data ) fileobj . flush ( ) fileobj . truncate ( ) delta = ( fileobj . tell ( ) - end ) copyfileobj ( buffer , fileobj , ( - 1 ) ) fileobj . flush ( ) buffer . close ( ) return delta 
def  bytes iterator py3 ( bytes  ) : for b in bytes  : ( yield bytes ( [ b ] ) ) 
def unique resolved paths ( paths ) : rps = (  slashappend or add error ( resolve parent ( p ) , 'unique resolved paths' ) for p in paths ) return frozenset ( ( x for x in rps if ( x is not  None  ) ) ) 
def libvlc get version ( ) : f = (   Cfunctions  . get ( 'libvlc get version' ,  None  ) or   Cfunction  ( 'libvlc get version' , ( ) ,  None  , ctypes . c char p ) ) return f ( ) 
def run paired t ( bt , s1 , s2 ) : test stats = [ ] pvals = [ ] s1 indices = [ bt . index ( i , axis = 'sample' ) for i in s1 ] s2 indices = [ bt . index ( i , axis = 'sample' ) for i in s2 ] for data in bt . iter data ( axis = 'observation' ) : ( test stat , pval ) = t paired ( data . take ( s1 indices ) , data . take ( s2 indices ) ) test stats . append ( test stat ) pvals . append ( pval ) return ( test stats , pvals ) 
def create release branch ( version , base branch ) : try : base branch . checkout ( b = ( 'release/flocker-' + version ) ) except  Git  Command  Error  : raise  Branch  Exists  ( ) 
def new Der  Object  Id  ( dottedstring ) : der =  Der  Object  Id  ( dottedstring ) return der 
def remove items ( lib , query , album , delete ) : ( items , albums ) =  do query ( lib , query , album ) print  ( ) if delete : fmt = u'$path  -  $title' prompt = ( ' Really   DELETE  %i  files  (y/n)?' % len ( items ) ) else : fmt =  None  prompt = ( ' Really   remove  %i  items  from  the  library  (y/n)?' % len ( items ) ) for item in items : ui . print obj ( item , lib , fmt ) if ( not ui . input yn ( prompt ,  True  ) ) : return with lib . transaction ( ) : for obj in ( albums if album else items ) : obj . remove ( delete ) 
def get doc url ( page , anchor = '' ) : if ( '-dev' in VERSION ) : version = 'latest' else : version = ( 'weblate-%s' % VERSION ) url = ( 'https://docs.weblate.org/en/%s/%s.html' % ( version , page ) ) if ( anchor != '' ) : url += ( '#%s' % anchor ) return url 
@ utils . arg ( '--all-tenants' , dest = 'all tenants' , metavar = '<0|1>' , nargs = '?' , type = int , const = 1 , default = 0 , help = ' Display   information  from  all  tenants  ( Admin   only).' ) @ utils . arg ( '--all tenants' , nargs = '?' , type = int , const = 1 , help = argparse . SUPPRESS ) @ utils . arg ( '--display-name' , metavar = '<display-name>' , default =  None  , help = ' Filter   results  by  display-name' ) @ utils . arg ( '--status' , metavar = '<status>' , default =  None  , help = ' Filter   results  by  status' ) @ utils . arg ( '--monitor-id' , metavar = '<monitor-id>' , default =  None  , help = ' Filter   results  by  monitor-id' ) @ utils . service type ( 'monitor' ) def do snapshot list ( cs , args ) : all tenants = int ( os . environ . get ( 'ALL TENANTS' , args . all tenants ) ) search opts = { 'all tenants' : all tenants , 'display name' : args . display name , 'status' : args . status , 'monitor id' : args . monitor id } snapshots = cs . monitor snapshots . list ( search opts = search opts )  translate monitor snapshot keys ( snapshots ) utils . print list ( snapshots , [ 'ID' , ' Service  Manage   ID' , ' Status ' , ' Display    Name ' , ' Size ' ] ) 
def update index ( sender , instance , created , ** kwargs ) : if ( isinstance ( instance ,  Object  ) and instance . is searchable ( ) ) : search item = instance . get search item ( ) ix = index . open dir ( settings . WHOOSH INDEX ) try : writer = ix . writer ( ) try : if created : writer . add document ( id = search item [ 'id' ] , name = search item [ 'name' ] , type = search item [ 'type' ] , content = search item [ 'content' ] , url = unicode ( search item [ 'url' ] ) ) writer . commit ( ) else : writer . update document ( id = search item [ 'id' ] , name = search item [ 'name' ] , type = search item [ 'type' ] , content = search item [ 'content' ] , url = search item [ 'url' ] ) writer . commit ( ) except : writer . cancel ( ) except : pass 
def chem correction ( melting temp , DMSO = 0 , fmd = 0 , DMS Ofactor  = 0.75 , fmdfactor = 0.65 , fmdmethod = 1 , GC =  None  ) : if DMSO : melting temp -= ( DMS Ofactor  * DMSO ) if fmd : if ( fmdmethod == 1 ) : melting temp -= ( fmdfactor * fmd ) if ( fmdmethod == 2 ) : if ( ( GC is  None  ) or ( GC < 0 ) ) : raise  Value  Error  ( "'GC'  is  missing  or  negative" ) melting temp += ( ( ( 0.453 * ( GC / 100.0 ) ) - 2.88 ) * fmd ) if ( fmdmethod not in ( 1 , 2 ) ) : raise  Value  Error  ( "'fmdmethod'  must  be  1  or  2" ) return melting temp 
def display properties ( options ) : properties = api . properties ( options ) if options . json : if options . values : print ( json . dumps ( properties , cls =  Guessit  Encoder  , ensure ascii =  False  ) ) else : print ( json . dumps ( list ( properties . keys ( ) ) , cls =  Guessit  Encoder  , ensure ascii =  False  ) ) elif options . yaml : import yaml from guessit import yamlutils if options . values : print ( yaml . dump ( properties ,  Dumper  = yamlutils .  Custom  Dumper  , default flow style =  False  , allow unicode =  True  ) ) else : print ( yaml . dump ( list ( properties . keys ( ) ) ,  Dumper  = yamlutils .  Custom  Dumper  , default flow style =  False  , allow unicode =  True  ) ) else : print ( ' Guess  It   properties:' ) properties list = list ( sorted ( properties . keys ( ) ) ) for property name in properties list : property values = properties . get ( property name ) print ( ( ( 2 * '  ' ) + ( '[+]  %s' % ( property name , ) ) ) ) if ( property values and options . values ) : for property value in property values : print ( ( ( 4 * '  ' ) + ( '[!]  %s' % ( property value , ) ) ) ) 
def unhold ( queue id ) : ret = { 'message' : '' , 'result' :  True  } if ( not queue id ) : log . error ( ' Require   argument  queue id' ) if ( not ( queue id == 'ALL' ) ) : queue = show queue ( )  message =  None  for item in queue : if ( item [ 'queue id' ] == queue id ) :  message = item if ( not  message ) : ret [ 'message' ] = ' No   message  in  queue  with  ID  {0}' . format ( queue id ) ret [ 'result' ] =  False  return ret cmd = 'postsuper  -H  {0}' . format ( queue id ) result =   salt   [ 'cmd.run all' ] ( cmd ) if ( result [ 'retcode' ] == 0 ) : if ( queue id == 'ALL' ) : ret [ 'message' ] = ' Successfully   set  all  message  as  unheld' else : ret [ 'message' ] = ' Successfully   set  message  as  unheld  with  queue  id  {0}' . format ( queue id ) elif ( queue id == 'ALL' ) : ret [ 'message' ] = ' Unable   to  set  all  message  as  unheld.' else : ret [ 'message' ] = ' Unable   to  set  message  as  unheld  with  queue  id  {0}:  {1}' . format ( queue id , result [ 'stderr' ] ) return ret 
def format taxa summary ( taxa summary ) : result = ( ( ' Taxon  DCTB ' + ' DCTB ' . join ( taxa summary [ 0 ] ) ) + '\n' ) for ( taxon , row ) in zip ( taxa summary [ 1 ] , taxa summary [ 2 ] ) : row = map ( str , row ) result += ( ( ( '%s DCTB ' % taxon ) + ' DCTB ' . join ( row ) ) + '\n' ) return result 
def write chunk ( outfile , tag , data = '' ) : outfile . write ( struct . pack ( '!I' , len ( data ) ) ) outfile . write ( tag ) outfile . write ( data ) checksum = zlib . crc32 ( tag ) checksum = zlib . crc32 ( data , checksum ) outfile . write ( struct . pack ( '!i' , checksum ) ) 
def ttest power ( effect size , nobs , alpha , df =  None  , alternative = 'two-sided' ) : d = effect size if ( df is  None  ) : df = ( nobs - 1 ) if ( alternative in [ 'two-sided' , '2s' ] ) : alpha  = ( alpha / 2.0 ) elif ( alternative in [ 'smaller' , 'larger' ] ) : alpha  = alpha else : raise  Value  Error  ( ( "alternative  has  to  be  'two-sided',  'larger'  " + "or  'smaller'" ) ) pow  = 0 if ( alternative in [ 'two-sided' , '2s' , 'larger' ] ) : crit upp = stats . t . isf ( alpha  , df ) if np . any ( np . isnan ( crit upp ) ) : pow  = np . nan else : pow  = stats . nct .  sf ( crit upp , df , ( d * np . sqrt ( nobs ) ) ) if ( alternative in [ 'two-sided' , '2s' , 'smaller' ] ) : crit low = stats . t . ppf ( alpha  , df ) if np . any ( np . isnan ( crit low ) ) : pow  = np . nan else : pow  += stats . nct .  cdf ( crit low , df , ( d * np . sqrt ( nobs ) ) ) return pow  
def org facility rheader ( r , tabs = [ ] ) : T = current . T s3db = current . s3db ( tablename , record ) = s3 rheader resource ( r ) r . record = record r . table = s3db [ tablename ] tabs = [ ( T ( ' Details ' ) ,  None  ) ] try : tabs = ( tabs + s3db . req tabs ( r ) ) except : pass try : tabs = ( tabs + s3db . inv tabs ( r ) ) except : pass rheader fields = [ [ 'name' ] , [ 'location id' ] ] rheader = S3 Resource  Header  ( rheader fields , tabs ) ( r ) return rheader 
def get plot commands ( ) : import inspect exclude = { u'colormaps' , u'colors' , u'connect' , u'disconnect' , u'get plot commands' , u'get current fig manager' , u'ginput' , u'plotting' , u'waitforbuttonpress' } exclude |= set ( colormaps ( ) ) this module = inspect . getmodule ( get plot commands ) commands = set ( ) for ( name , obj ) in list ( six . iteritems ( globals ( ) ) ) : if ( name . startswith ( u' ' ) or ( name in exclude ) ) : continue if ( inspect . isfunction ( obj ) and ( inspect . getmodule ( obj ) is this module ) ) : commands . add ( name ) return sorted ( commands ) 
def diagflat ( v , k = 0 ) : if isinstance ( v , cupy . ndarray ) : return cupy . diag ( v . ravel ( ) , k ) else : return cupy . diag ( numpy . ndarray ( v ) . ravel ( ) , k ) 
def validate options ( options ) : kwcase = options . get ( 'keyword case' ) if ( kwcase not in [  None  , 'upper' , 'lower' , 'capitalize' ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  keyword case:  {0!r}' . format ( kwcase ) ) idcase = options . get ( 'identifier case' ) if ( idcase not in [  None  , 'upper' , 'lower' , 'capitalize' ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  identifier case:  {0!r}' . format ( idcase ) ) ofrmt = options . get ( 'output format' ) if ( ofrmt not in [  None  , 'sql' , 'python' , 'php' ] ) : raise SQL Parse  Error  ( ' Unknown   output  format:  {0!r}' . format ( ofrmt ) ) strip comments = options . get ( 'strip comments' ,  False  ) if ( strip comments not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  strip comments:  {0!r}' . format ( strip comments ) ) space around operators = options . get ( 'use space around operators' ,  False  ) if ( space around operators not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  use space around operators:  {0!r}' . format ( space around operators ) ) strip ws = options . get ( 'strip whitespace' ,  False  ) if ( strip ws not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  strip whitespace:  {0!r}' . format ( strip ws ) ) truncate strings = options . get ( 'truncate strings' ) if ( truncate strings is not  None  ) : try : truncate strings = int ( truncate strings ) except (  Value  Error  ,  Type  Error  ) : raise SQL Parse  Error  ( ' Invalid   value  for  truncate strings:  {0!r}' . format ( truncate strings ) ) if ( truncate strings <= 1 ) : raise SQL Parse  Error  ( ' Invalid   value  for  truncate strings:  {0!r}' . format ( truncate strings ) ) options [ 'truncate strings' ] = truncate strings options [ 'truncate char' ] = options . get ( 'truncate char' , '[...]' ) reindent = options . get ( 'reindent' ,  False  ) if ( reindent not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  reindent:  {0!r}' . format ( reindent ) ) elif reindent : options [ 'strip whitespace' ] =  True  reindent aligned = options . get ( 'reindent aligned' ,  False  ) if ( reindent aligned not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  reindent aligned:  {0!r}' . format ( reindent ) ) elif reindent aligned : options [ 'strip whitespace' ] =  True  indent tabs = options . get ( 'indent tabs' ,  False  ) if ( indent tabs not in [  True  ,  False  ] ) : raise SQL Parse  Error  ( ' Invalid   value  for  indent tabs:  {0!r}' . format ( indent tabs ) ) elif indent tabs : options [ 'indent char' ] = ' DCTB ' else : options [ 'indent char' ] = '  ' indent width = options . get ( 'indent width' , 2 ) try : indent width = int ( indent width ) except (  Type  Error  ,  Value  Error  ) : raise SQL Parse  Error  ( 'indent width  requires  an  integer' ) if ( indent width < 1 ) : raise SQL Parse  Error  ( 'indent width  requires  a  positive  integer' ) options [ 'indent width' ] = indent width wrap after = options . get ( 'wrap after' , 0 ) try : wrap after = int ( wrap after ) except (  Type  Error  ,  Value  Error  ) : raise SQL Parse  Error  ( 'wrap after  requires  an  integer' ) if ( wrap after < 0 ) : raise SQL Parse  Error  ( 'wrap after  requires  a  positive  integer' ) options [ 'wrap after' ] = wrap after right margin = options . get ( 'right margin' ) if ( right margin is not  None  ) : try : right margin = int ( right margin ) except (  Type  Error  ,  Value  Error  ) : raise SQL Parse  Error  ( 'right margin  requires  an  integer' ) if ( right margin < 10 ) : raise SQL Parse  Error  ( 'right margin  requires  an  integer  >  10' ) options [ 'right margin' ] = right margin return options 
def  get dvs portgroup ( dvs , portgroup name ) : for portgroup in dvs . portgroup : if ( portgroup . name == portgroup name ) : return portgroup return  None  
def instance get all by host ( context , host , columns to join =  None  ) : return IMPL . instance get all by host ( context , host , columns to join ) 
def test gemm opt0 ( ) : ( X , Y , Z , a , b ) = XY Zab  ( ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( T . dot ( X , Y ) * a ) + ( Z * b ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( a * T . dot ( X , Y ) ) + ( b * Z ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( b * Z ) + ( a * T . dot ( X , Y ) ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( T . dot ( X , Y ) * a ) - ( Z * b ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( a * T . dot ( X , Y ) ) - ( b * Z ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( b * Z ) - ( a * T . dot ( X , Y ) ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( b * Z . T ) - ( a * T . dot ( Y . T , X . T ) ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( b * Z . T ) + ( ( a * b ) * T . dot ( X , Y ) . T ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( b * Z ) + ( a * T . dot ( X , Y ) . T ) ) ] , ishapes = [ ( 5 , 3 ) , ( 3 , 4 ) , ( 4 , 5 ) , ( ) , ( ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( ( ( b * b ) * Z ) * a ) + ( ( ( a * a ) * T . dot ( X , Y ) ) * b ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( Z + T . dot ( X , Y ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( Z * b ) + T . dot ( X , Y ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( Z + ( ( ( a * b ) * a ) * T . dot ( X , Y ) ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( ( ( b * b ) * Z ) * a ) - ( ( ( a * a ) * T . dot ( X , Y ) ) * b ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( Z - T . dot ( X , Y ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( ( Z * b ) - T . dot ( X , Y ) ) ] ) just gemm ( [ X , Y , Z , a , b ] , [ ( Z - ( ( ( a * b ) * a ) * T . dot ( X , Y ) ) ) ] ) 
def join domain ( domain , username =  None  , password =  None  , account ou =  None  , account exists =  False  , restart =  False  ) : status = get domain workgroup ( ) if ( ' Domain ' in status ) : if ( status [ ' Domain ' ] == domain ) : return ' Already   joined  to  {0}' . format ( domain ) if ( username and ( '\\' not in username ) and ( '@' not in username ) ) : username = '{0}@{1}' . format ( username , domain ) if ( username and ( password is  None  ) ) : return ' Must   specify  a  password  if  you  pass  a  username' if isinstance ( account ou , str ) : account ou = account ou . split ( '\\' ) account ou = '' . join ( account ou ) NETSETUP JOIN DOMAIN = 1 NETSETUP ACCOUNT CREATE = 2 NETSETUP DOMAIN JOIN IF JOINED = 32 NETSETUP JOIN WITH NEW NAME = 1024 join options = 0 join options |= NETSETUP JOIN DOMAIN join options |= NETSETUP DOMAIN JOIN IF JOINED join options |= NETSETUP JOIN WITH NEW NAME if ( not account exists ) : join options |= NETSETUP ACCOUNT CREATE pythoncom .  Co  Initialize  ( ) conn = wmi . WMI ( ) comp = conn .  Win 32  Computer  System  ( ) [ 0 ] err = comp .  Join  Domain  Or  Workgroup  (  Name  = domain ,  Password  = password ,  User  Name  = username ,  Account OU = account ou , F Join  Options  = join options ) if ( not err [ 0 ] ) : ret = { ' Domain ' : domain , ' Restart ' :  False  } if restart : ret [ ' Restart ' ] = reboot ( ) return ret log . error (  lookup error ( err [ 0 ] ) ) return  False  
def client config ( path , env var = 'SALT CLIENT CONFIG' , defaults =  None  ) : if ( defaults is  None  ) : defaults = DEFAULT MASTER OPTS xdg dir = salt . utils . xdg . xdg config dir ( ) if os . path . isdir ( xdg dir ) : client config dir = xdg dir saltrc config file = 'saltrc' else : client config dir = os . path . expanduser ( '~' ) saltrc config file = '.saltrc' opts = { 'token file' : defaults . get ( 'token file' , os . path . join ( client config dir , 'salt token' ) ) } opts . update ( master config ( path , defaults = defaults ) ) saltrc config = os . path . join ( client config dir , saltrc config file ) opts . update ( load config ( saltrc config , env var , saltrc config ) ) if ( 'token file' in opts ) : opts [ 'token file' ] = os . path . abspath ( os . path . expanduser ( opts [ 'token file' ] ) ) if os . path . isfile ( opts [ 'token file' ] ) : expire = opts . get ( 'token expire' , 43200 ) if ( ( os . stat ( opts [ 'token file' ] ) . st mtime + expire ) > time . mktime ( time . localtime ( ) ) ) : with salt . utils . fopen ( opts [ 'token file' ] ) as fp  : opts [ 'token' ] = fp  . read ( ) . strip ( ) if ( opts [ 'interface' ] == '0.0.0.0' ) : opts [ 'interface' ] = '127.0.0.1' if ( 'master uri' not in opts ) : opts [ 'master uri' ] = 'tcp://{ip}:{port}' . format ( ip = salt . utils . ip bracket ( opts [ 'interface' ] ) , port = opts [ 'ret port' ] )  validate opts ( opts ) return opts 
def dir ( * obj ) : if ( not obj ) : return sorted ( inspect . stack ( ) [ 1 ] [ 0 ] . f locals . keys ( ) ) if ( not ( len ( obj ) == 1 ) ) : raise  Type  Error  ( ( 'dir  expected  at  most  1  arguments,  got  %d' % ( len ( obj ) , ) ) ) obj = obj [ 0 ] if orig isinstance ( obj ,  Net  Proxy  ) : return  get conn ( obj ) . modules .   builtin   . dir ( obj ) else : return orig dir ( obj ) 
def subscription check ( ) : subscriptions =  Subscription  . objects . all ( ) for subscription in subscriptions : subscription . check status ( ) 
@ transaction . atomic def add plugin ( placeholder , plugin type , language , position = 'last-child' , target =  None  , ** data ) : assert isinstance ( placeholder ,  Placeholder  ) ( plugin model , plugin type ) =  verify plugin type ( plugin type ) if target : if ( position == 'last-child' ) : if CMS Plugin  . node order by : position = 'sorted-child' new pos = CMS Plugin  . objects . filter ( parent = target ) . count ( ) parent id = target . pk elif ( position == 'first-child' ) : new pos = 0 if CMS Plugin  . node order by : position = 'sorted-child' parent id = target . pk elif ( position == 'left' ) : new pos = target . position if CMS Plugin  . node order by : position = 'sorted-sibling' parent id = target . parent id elif ( position == 'right' ) : new pos = ( target . position + 1 ) if CMS Plugin  . node order by : position = 'sorted-sibling' parent id = target . parent id else : raise  Exception  ( ( 'position  not  supported:  %s' % position ) ) if ( ( position == 'last-child' ) or ( position == 'first-child' ) ) : qs = CMS Plugin  . objects . filter ( language = language , parent = target , position  gte = new pos , placeholder = placeholder ) else : qs = CMS Plugin  . objects . filter ( language = language , parent = target . parent id , position  gte = new pos , placeholder = placeholder ) for pl in qs : pl . position += 1 pl . save ( ) else : if ( position == 'last-child' ) : new pos = CMS Plugin  . objects . filter ( language = language , parent  isnull =  True  , placeholder = placeholder ) . count ( ) else : new pos = 0 for pl in CMS Plugin  . objects . filter ( language = language , parent  isnull =  True  , position  gte = new pos , placeholder = placeholder ) : pl . position += 1 pl . save ( ) parent id =  None  plugin base = CMS Plugin  ( plugin type = plugin type , placeholder = placeholder , position = new pos , language = language , parent id = parent id ) plugin base = plugin base . add root ( instance = plugin base ) if target : plugin base = plugin base . move ( target , pos = position ) plugin = plugin model ( ** data ) plugin base . set base attr ( plugin ) plugin . save ( ) return plugin 
def hash napiprojekt ( video path ) : readsize = ( ( 1024 * 1024 ) * 10 ) with open ( video path , 'rb' ) as f : data = f . read ( readsize ) return hashlib . md5 ( data ) . hexdigest ( ) 
def  get gcp ansible credentials ( module ) : service account email = module . params . get ( 'service account email' ,  None  ) credentials file = ( module . params . get ( 'pem file' ,  None  ) or module . params . get ( 'credentials file' ,  None  ) ) project id = module . params . get ( 'project id' ,  None  ) return ( service account email , credentials file , project id ) 
def handle ( text , mic , profile ) : try : msgs = fetch Unread  Emails  ( profile , limit = 5 ) if isinstance ( msgs , int ) : response = ( ' You   have  %d  unread  emails.' % msgs ) mic . say ( response ) return senders = [ get Sender  ( e ) for e in msgs ] except imaplib . IMAP4 . error : mic . say ( "I'm  sorry.  I'm  not  authenticated  to  work  with  your   Gmail ." ) return if ( not senders ) : mic . say ( ' You   have  no  unread  emails.' ) elif ( len ( senders ) == 1 ) : mic . say ( ( ( ' You   have  one  unread  email  from  ' + senders [ 0 ] ) + '.' ) ) else : response = ( ' You   have  %d  unread  emails' % len ( senders ) ) unique senders = list ( set ( senders ) ) if ( len ( unique senders ) > 1 ) : unique senders [ ( - 1 ) ] = ( 'and  ' + unique senders [ ( - 1 ) ] ) response += '.   Senders   include:  ' response += '...' . join ( senders ) else : response += ( '  from  ' + unique senders [ 0 ] ) mic . say ( response ) 
def find pylintrc ( ) : if exists ( 'pylintrc' ) : return abspath ( 'pylintrc' ) if isfile ( '  init  .py' ) : curdir = abspath ( os . getcwd ( ) ) while isfile ( join ( curdir , '  init  .py' ) ) : curdir = abspath ( join ( curdir , '..' ) ) if isfile ( join ( curdir , 'pylintrc' ) ) : return join ( curdir , 'pylintrc' ) if ( ( 'PYLINTRC' in os . environ ) and exists ( os . environ [ 'PYLINTRC' ] ) ) : pylintrc = os . environ [ 'PYLINTRC' ] else : user home = expanduser ( '~' ) if ( ( user home == '~' ) or ( user home == '/root' ) ) : pylintrc = '.pylintrc' else : pylintrc = join ( user home , '.pylintrc' ) if ( not isfile ( pylintrc ) ) : pylintrc = join ( user home , '.config' , 'pylintrc' ) if ( not isfile ( pylintrc ) ) : if isfile ( '/etc/pylintrc' ) : pylintrc = '/etc/pylintrc' else : pylintrc =  None  return pylintrc 
@ skipif ( ( not is installed ( 'networkx' ) ) ) def test ncut stable subgraph ( ) : img = np . zeros ( ( 100 , 100 , 3 ) , dtype = 'uint8' ) labels = np . zeros ( ( 100 , 100 ) , dtype = 'uint8' ) labels [ ... ] = 0 labels [ : 50 , : 50 ] = 1 labels [ : 50 , 50 : ] = 2 rag = graph . rag mean color ( img , labels , mode = 'similarity' ) new labels = graph . cut normalized ( labels , rag , in place =  False  ) ( new labels ,   ,   ) = segmentation . relabel sequential ( new labels ) assert ( new labels . max ( ) == 0 ) 
def get version ( connection ) : if hasattr ( connection , 'server version' ) : return connection . server version else : cursor = connection . cursor ( ) cursor . execute ( 'SELECT  version()' ) return  parse version ( cursor . fetchone ( ) [ 0 ] ) 
def test labeller ( ) : assert ( labeller ( 2 ) == [ 'q 1' , 'q 0' ] ) assert ( labeller ( 3 , 'j' ) == [ 'j 2' , 'j 1' , 'j 0' ] ) 
def create lexer ( base class ) : def get tokens unprocessed ( self , text , statestack ) : pos = 0 tokendefs = self .  tokens statetokens = tokendefs [ statestack [ ( - 1 ) ] ] while  True  : for ( rexmatch , action , new state ) in statetokens : m = rexmatch ( text , pos ) if ( m is not  None  ) : if ( action is not  None  ) : if ( type ( action ) is   Token  Type  ) : ( yield ( pos , action , m . group ( ) ) ) else : for item in action ( self , m ) : ( yield item ) pos = m . end ( ) if ( new state is not  None  ) : if isinstance ( new state , tuple ) : for state in new state : if ( state == u'#pop' ) : statestack . pop ( ) elif ( state == u'#push' ) : statestack . append ( statestack [ ( - 1 ) ] ) else : statestack . append ( state ) elif isinstance ( new state , int ) : del statestack [ new state : ] elif ( new state == u'#push' ) : statestack . append ( statestack [ ( - 1 ) ] ) else : assert  False  , ( u'wrong  state  def:  %r' % new state ) statetokens = tokendefs [ statestack [ ( - 1 ) ] ] break else : try : if ( text [ pos ] == u'\n' ) : statestack [ : ] = [ u'root' ] break ( yield ( pos ,  Error  , text [ pos ] ) ) pos += 1 except  Index  Error  : break def lex a line ( self , state , text , i , formats map , user data ) : u'   Get   formats  for  a  single  block  (line)  ' statestack = ( list ( state . pygments stack ) if ( state . pygments stack is not  None  ) else [ u'root' ] ) formats = [ ] if ( i > 0 ) : state . pygments stack =  None  return [ ( ( len ( text ) - i ) , formats map (  Error  ) ) ] try : for ( pos , token , txt ) in self . get tokens unprocessed ( ( text + u'\n' ) , statestack ) : if ( txt not in ( u'\n' , u'' ) ) : formats . append ( ( len ( txt ) , formats map ( token ) ) ) except  Exception  : import traceback traceback . print exc ( ) state . pygments stack =  None  return [ ( ( len ( text ) - i ) , formats map (  Error  ) ) ] state . pygments stack = statestack return formats return type ( str ( ( u' Qt ' + base class .   name   ) ) , ( base class , ) , { u'get tokens unprocessed' : get tokens unprocessed , u'lex a line' : lex a line } ) 
def session info ( consul url =  None  , session =  None  , ** kwargs ) : ret = { } if ( not consul url ) : consul url =  get config ( ) if ( not consul url ) : log . error ( ' No    Consul   URL  found.' ) ret [ 'message' ] = ' No    Consul   URL  found.' ret [ 'res' ] =  False  return ret if ( not session ) : raise  Salt  Invocation  Error  ( ' Required   argument  "session"  is  missing.' ) query params = { } if ( 'dc' in kwargs ) : query params [ 'dc' ] = kwargs [ 'dc' ] function = 'session/info/{0}' . format ( session ) ret =  query ( consul url = consul url , function = function , query params = query params ) return ret 
def formatargspec ( function , args , varargs =  None  , varkw =  None  , defaults =  None  , kwonlyargs = ( ) , kwonlydefaults = { } , annotations = { } ) : def format arg with annotation ( name ) : if ( name in annotations ) : return ( '%s:  %s' % ( name , format annotation ( get annotation ( name ) ) ) ) return name def get annotation ( name ) : value = annotations [ name ] if isinstance ( value , string types ) : return introspected hints . get ( name , value ) else : return value introspected hints = ( typing . get type hints ( function ) if ( typing and hasattr ( function , '  code  ' ) ) else { } ) fd =  String IO ( ) fd . write ( '(' ) formatted = [ ] defaults start = ( ( len ( args ) - len ( defaults ) ) if defaults else len ( args ) ) for ( i , arg ) in enumerate ( args ) : arg fd =  String IO ( ) arg fd . write ( format arg with annotation ( arg ) ) if ( defaults and ( i >= defaults start ) ) : arg fd . write ( ( '  =  ' if ( arg in annotations ) else '=' ) ) arg fd . write ( object description ( defaults [ ( i - defaults start ) ] ) ) formatted . append ( arg fd . getvalue ( ) ) if varargs : formatted . append ( ( '*' + format arg with annotation ( varargs ) ) ) if kwonlyargs : if ( not varargs ) : formatted . append ( '*' ) for kwarg in kwonlyargs : arg fd =  String IO ( ) arg fd . write ( format arg with annotation ( kwarg ) ) if ( kwonlydefaults and ( kwarg in kwonlydefaults ) ) : arg fd . write ( ( '  =  ' if ( kwarg in annotations ) else '=' ) ) arg fd . write ( object description ( kwonlydefaults [ kwarg ] ) ) formatted . append ( arg fd . getvalue ( ) ) if varkw : formatted . append ( ( '**' + format arg with annotation ( varkw ) ) ) fd . write ( ',  ' . join ( formatted ) ) fd . write ( ')' ) if ( 'return' in annotations ) : fd . write ( '  ->  ' ) fd . write ( format annotation ( get annotation ( 'return' ) ) ) return fd . getvalue ( ) 
def direct put object ( node , part , account , container , name , contents , content length =  None  , etag =  None  , content type =  None  , headers =  None  , conn timeout = 5 , response timeout = 15 , resp chunk size =  None  ) : path = ( '/%s/%s/%s' % ( account , container , name ) ) if ( headers is  None  ) : headers = { } if etag : headers [ 'E Tag ' ] = etag . strip ( '"' ) if ( content length is not  None  ) : headers [ ' Content - Length ' ] = str ( content length ) if ( content type is not  None  ) : headers [ ' Content - Type ' ] = content type else : headers [ ' Content - Type ' ] = 'application/octet-stream' if ( not contents ) : headers [ ' Content - Length ' ] = '0' if isinstance ( contents , basestring ) : contents = [ contents ] headers [ 'X- Timestamp ' ] = normalize timestamp ( time ( ) ) with  Timeout  ( conn timeout ) : conn = http connect ( node [ 'ip' ] , node [ 'port' ] , node [ 'device' ] , part , 'PUT' , path , headers = headers ) for chunk in contents : conn . send ( chunk ) with  Timeout  ( response timeout ) : resp = conn . getresponse ( ) resp . read ( ) if ( not is success ( resp . status ) ) : raise  Client  Exception  ( ( ' Object   server  %s:%s  direct  PUT  %s  gave  status  %s' % ( node [ 'ip' ] , node [ 'port' ] , repr ( ( '/%s/%s%s' % ( node [ 'device' ] , part , path ) ) ) , resp . status ) ) , http host = node [ 'ip' ] , http port = node [ 'port' ] , http device = node [ 'device' ] , http status = resp . status , http reason = resp . reason ) return resp . getheader ( 'etag' ) . strip ( '"' ) 
def scan ( opts ) : ret = { } for ( root , dirs , files ) in os . walk ( opts [ 'root' ] ) : for fn  in files : full = os . path . join ( root , fn  ) if full . endswith ( '.py' ) : ret . update ( mod data ( opts , full ) ) return ret 
def cache tip names ( tree ) : if hasattr ( tree , ' tip names' ) : return else : for n in tree . postorder ( ) : if n . is Tip  ( ) : n .  tip names = [ n .  Name  ] else : n .  tip names = reduce ( add , [ c .  tip names for c in n .  Children  ] ) 
def test uniform mode ( ) : img = data . coins ( ) keypoints = corner peaks ( corner harris ( img ) , min distance = 5 , threshold abs = 0 , threshold rel = 0.1 ) extractor = BRIEF ( descriptor size = 8 , sigma = 2 , mode = 'uniform' ) extractor . extract ( img , keypoints [ : 8 ] ) expected = np . array ( [ [  False  ,  False  ,  False  ,  True  ,  True  ,  True  ,  False  ,  False  ] , [  True  ,  True  ,  True  ,  False  ,  True  ,  False  ,  False  ,  True  ] , [  True  ,  True  ,  True  ,  False  ,  True  ,  True  ,  False  ,  True  ] , [  True  ,  True  ,  True  ,  True  ,  False  ,  True  ,  False  ,  True  ] , [  True  ,  True  ,  True  ,  True  ,  True  ,  True  ,  False  ,  False  ] , [  True  ,  True  ,  True  ,  True  ,  True  ,  True  ,  True  ,  True  ] , [  False  ,  False  ,  False  ,  True  ,  True  ,  True  ,  True  ,  True  ] , [  False  ,  True  ,  False  ,  True  ,  False  ,  True  ,  True  ,  True  ] ] , dtype = bool ) assert array equal ( extractor . descriptors , expected ) 
def is valid dot atom ( value ) : return ( isinstance ( value , six . string types ) and ( not ( value [ 0 ] == '.' ) ) and ( not ( value [ ( - 1 ) ] == '.' ) ) and set ( value ) . issubset ( valid dot atom characters ) ) 
def test show with files from wheel ( script , data ) : wheel file = data . packages . join ( 'simple.dist-0.1-py2.py3-none-any.whl' ) script . pip ( 'install' , '--no-index' , wheel file ) result = script . pip ( 'show' , '-f' , 'simple.dist' ) lines = result . stdout . splitlines ( ) assert ( ' Name :  simple.dist' in lines ) assert ( ' Cannot   locate  installed-files.txt' not in lines [ 6 ] ) , lines [ 6 ] assert re . search ( ' Files :\\n(    .+\\n)+' , result . stdout ) 
def  b64encode ( s ) : return b2a base64 ( s ) . strip ( ) 
def filesystem absent ( name , force =  False  , recursive =  False  ) : return  absent ( name , 'filesystem' , force , recursive ) 
def  get home ( ) : path = '' try : path = os . path . expanduser ( '~' ) except : pass if ( not os . path . isdir ( path ) ) : for evar in ( 'HOME' , 'USERPROFILE' , 'TMP' ) : try : path = os . environ [ evar ] if os . path . isdir ( path ) : break except : pass if path : return path else : raise  Runtime  Error  ( 'please  define  environment  variable  $HOME' ) 
def drain consumer ( consumer , limit = 1 , timeout =  None  , callbacks =  None  ) : acc = deque ( ) def on message ( body , message ) : acc . append ( ( body , message ) ) consumer . callbacks = ( [ on message ] + ( callbacks or [ ] ) ) with consumer : for   in eventloop ( consumer . channel . connection . client , limit = limit , timeout = timeout , ignore timeouts =  True  ) : try : ( yield acc . popleft ( ) ) except  Index  Error  : pass 
def  Process  File  ( filename , vlevel , extra check functions =  None  ) :   Set  Verbose  Level  ( vlevel )   Backup  Filters  ( ) if ( not  Process  Config  Overrides  ( filename ) ) :   Restore  Filters  ( ) return lf lines = [ ] crlf lines = [ ] try : if ( filename == '-' ) : lines = codecs .  Stream  Reader  Writer  ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) for linenum in range ( ( len ( lines ) - 1 ) ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf lines . append ( ( linenum + 1 ) ) else : lf lines . append ( ( linenum + 1 ) ) except IO Error  :  cpplint state .  Print  Error  ( ( " Skipping   input  '%s':   Can 't  open  for  reading\n" % filename ) )   Restore  Filters  ( ) return file extension = filename [ ( filename . rfind ( '.' ) + 1 ) : ] if ( ( filename != '-' ) and ( file extension not in  Get  All  Extensions  ( ) ) ) :  cpplint state .  Print  Error  ( ( ' Ignoring   %s;  not  a  valid  file  name  (%s)\n' % ( filename , ',  ' . join (  Get  All  Extensions  ( ) ) ) ) ) else :  Process  File  Data  ( filename , file extension , lines ,  Error  , extra check functions ) if ( lf lines and crlf lines ) : for linenum in crlf lines :  Error  ( filename , linenum , 'whitespace/newline' , 1 , ' Unexpected   \\r  (^M)  found;  better  to  use  only  \\n' )  cpplint state .  Print  Info  ( ( ' Done   processing  %s\n' % filename ) )   Restore  Filters  ( ) 
def color palette ( palette =  None  , n colors =  None  , desat =  None  ) : if ( palette is  None  ) : palette = get color cycle ( ) if ( n colors is  None  ) : n colors = len ( palette ) elif ( not isinstance ( palette , string types ) ) : palette = palette if ( n colors is  None  ) : n colors = len ( palette ) else : if ( n colors is  None  ) : n colors = 6 if ( palette == 'hls' ) : palette = hls palette ( n colors ) elif ( palette == 'husl' ) : palette = husl palette ( n colors ) elif ( palette . lower ( ) == 'jet' ) : raise  Value  Error  ( ' No .' ) elif ( palette in SEABORN PALETTES ) : palette = SEABORN PALETTES [ palette ] elif ( palette in dir ( mpl . cm ) ) : palette = mpl palette ( palette , n colors ) elif ( palette [ : ( - 2 ) ] in dir ( mpl . cm ) ) : palette = mpl palette ( palette , n colors ) else : raise  Value  Error  ( ( '%s  is  not  a  valid  palette  name' % palette ) ) if ( desat is not  None  ) : palette = [ desaturate ( c , desat ) for c in palette ] pal cycle = cycle ( palette ) palette = [ next ( pal cycle ) for   in range ( n colors ) ] try : palette = map ( mpl . colors . color Converter  . to rgb , palette ) palette =   Color  Palette  ( palette ) except  Value  Error  : raise  Value  Error  ( ( ' Could   not  generate  a  palette  for  %s' % str ( palette ) ) ) return palette 
def  find rteq ( a , l , x ) : i = bisect right ( a , x , lo = l ) if ( ( i != ( len ( a ) + 1 ) ) and ( a [ ( i - 1 ) ] == x ) ) : return ( i - 1 ) raise  Value  Error  
def parse ( tmpl str ) : validate template limit ( six . text type ( tmpl str ) ) tpl = simple parse ( tmpl str ) if ( not ( ( ' Heat  Template  Format  Version ' in tpl ) or ( 'heat template version' in tpl ) or ( 'AWS Template  Format  Version ' in tpl ) ) ) : raise  Value  Error  (   ( ' Template   format  version  not  found.' ) ) return tpl 
def get subsequent neighbors ( nearest neighbors , data , curr ) : left neigh = nearest neighbors [ 1 ] . split ( '  ' ) right neigh = nearest neighbors [ 2 ] . split ( '  ' ) left = left neigh [ 0 ] right = right neigh [ 0 ] left dist = ( - 1 ) right dist = ( - 1 ) at four left =  False  at four right =  False  if ( len ( left neigh ) > 1 ) : left dist = left neigh [ 1 ] else : left = '  ' if ( len ( right neigh ) > 1 ) : right dist = right neigh [ 1 ] else : right = '  ' other neighbors = [ ] while ( ( left != '  ' ) or ( right != '  ' ) ) : if ( left == '' ) : left = '  ' if ( left != '  ' ) : if ( data [ left ] [ 'related subtopics' ] [ 1 ] != '  ' ) : if at four left : new dist = 4 at four left =  True  else : try : if ( data [ curr ] [ 'related subtopics' ] [ 1 ] . split ( '  ' ) [ 1 ] == '4' ) : at four left =  True  new dist = 4 elif ( data [ left ] [ 'related subtopics' ] [ 1 ] . split ( '  ' ) [ 1 ] == '4' ) : at four left =  True  new dist = 4 else : new dist = 1 except  Index  Error  : new dist = 1 other neighbors . append ( ( ( data [ left ] [ 'related subtopics' ] [ 1 ] . split ( '  ' ) [ 0 ] + '  ' ) + str ( new dist ) ) ) left = data [ left ] [ 'related subtopics' ] [ 1 ] . split ( '  ' ) [ 0 ] if ( right == '' ) : right = '  ' if ( right != '  ' ) : if ( data [ right ] [ 'related subtopics' ] [ 2 ] != '  ' ) : if at four right : new dist = 4 at four right =  True  elif ( data [ curr ] [ 'related subtopics' ] [ 2 ] . split ( '  ' ) [ 1 ] == '4' ) : new dist = 4 elif ( data [ right ] [ 'related subtopics' ] [ 2 ] . split ( '  ' ) [ 1 ] == '4' ) : new dist = 4 else : new dist = 1 if ( new dist == 4 ) : at four right =  True  other neighbors . append ( ( ( data [ right ] [ 'related subtopics' ] [ 2 ] . split ( '  ' ) [ 0 ] + '  ' ) + str ( new dist ) ) ) right = data [ right ] [ 'related subtopics' ] [ 2 ] . split ( '  ' ) [ 0 ] return other neighbors 
def metric cleanup ( ) : global conn conn . close ( ) pass 
def apply provider facts ( facts , provider facts ) : if ( not provider facts ) : return facts common vars = [ ( 'hostname' , 'ip' ) , ( 'public hostname' , 'public ip' ) ] for ( h var , ip var ) in common vars : ip value = provider facts [ 'network' ] . get ( ip var ) if ip value : facts [ 'common' ] [ ip var ] = ip value facts [ 'common' ] [ h var ] = choose hostname ( [ provider facts [ 'network' ] . get ( h var ) ] , facts [ 'common' ] [ h var ] ) facts [ 'provider' ] = provider facts return facts 
def apply units ( string , units , inter =  None  , final = float , blank reg =  BLANK RE , value reg =  VALUE RE ) : if ( inter is  None  ) : inter = final fstring =  BLANK RE . sub ( '' , string ) if ( not ( fstring and  VALIDATION RE . match ( fstring ) ) ) : raise  Value  Error  ( ( ' Invalid   unit  string:  %r.' % string ) ) values = [ ] for match in value reg . finditer ( fstring ) : dic = match . groupdict ( ) ( lit , unit ) = ( dic [ 'value' ] , dic . get ( 'unit' ) ) value = inter ( lit ) if ( unit is not  None  ) : try : value *= units [ unit . lower ( ) ] except  Key  Error  : raise  Key  Error  ( ( 'invalid  unit  %s.  valid  units  are  %s' % ( unit , units . keys ( ) ) ) ) values . append ( value ) return final ( sum ( values ) ) 
def inject search after save ( output ) : if ( 'form' in output ) : id = 'search after save' label = LABEL ( ( '%s:' % T ( ' Search    After    Save ?' ) ) ,  for = 'msg twitter search' ) widget = INPUT (  name = 'search after save' ,  type = 'checkbox' , value = 'on' ,  id = id ,  class = 'boolean' ) comment = '' if ( s3 formstyle == 'bootstrap' ) :  controls = DIV ( widget , comment ,  class = 'controls' ) row = DIV ( label ,  controls ,  class = 'control-group' ,  id = ( '%s  row' % id ) ) elif callable ( s3 formstyle ) : row = s3 formstyle ( id , label , widget , comment ) else : raise output [ 'form' ] [ 0 ] [ ( - 2 ) ] . append ( row ) 
def move Host  ( host , old Switch  , new Switch  , new Port  =  None  ) : ( hintf , sintf ) = host . connections To  ( old Switch  ) [ 0 ] old Switch  . move Intf  ( sintf , new Switch  , port = new Port  ) return ( hintf , sintf ) 
def get scanner ( hass , config ) : return  Tomato  Device  Scanner  ( config [ DOMAIN ] ) 
def setup platform ( hass , config , add devices , discovery info =  None  ) : if ( discovery info is  None  ) : return gateways = hass . data . get ( mysensors . MYSENSORS GATEWAYS ) if ( not gateways ) : return for gateway in gateways : pres = gateway . const .  Presentation  set req = gateway . const .  Set  Req  map sv types = { pres . S TEMP : [ set req . V TEMP ] , pres . S HUM : [ set req . V HUM ] , pres . S BARO : [ set req . V PRESSURE , set req . V FORECAST ] , pres . S WIND : [ set req . V WIND , set req . V GUST ] , pres . S RAIN : [ set req . V RAIN , set req . V RAINRATE ] , pres . S UV : [ set req . V UV ] , pres . S WEIGHT : [ set req . V WEIGHT , set req . V IMPEDANCE ] , pres . S POWER : [ set req . V WATT , set req . V KWH ] , pres . S DISTANCE : [ set req . V DISTANCE ] , pres . S LIGHT LEVEL : [ set req . V LIGHT LEVEL ] , pres . S IR : [ set req . V IR RECEIVE ] , pres . S WATER : [ set req . V FLOW , set req . V VOLUME ] , pres . S CUSTOM : [ set req . V VAR1 , set req . V VAR2 , set req . V VAR3 , set req . V VAR4 , set req . V VAR5 ] , pres . S SCENE CONTROLLER : [ set req . V SCENE ON , set req . V SCENE OFF ] } if ( float ( gateway . protocol version ) < 1.5 ) : map sv types . update ( { pres . S AIR QUALITY : [ set req . V DUST LEVEL ] , pres . S DUST : [ set req . V DUST LEVEL ] } ) if ( float ( gateway . protocol version ) >= 1.5 ) : map sv types . update ( { pres . S COLOR SENSOR : [ set req . V RGB ] , pres . S MULTIMETER : [ set req . V VOLTAGE , set req . V CURRENT , set req . V IMPEDANCE ] , pres . S SOUND : [ set req . V LEVEL ] , pres . S VIBRATION : [ set req . V LEVEL ] , pres . S MOISTURE : [ set req . V LEVEL ] , pres . S AIR QUALITY : [ set req . V LEVEL ] , pres . S DUST : [ set req . V LEVEL ] } ) map sv types [ pres . S LIGHT LEVEL ] . append ( set req . V LEVEL ) if ( float ( gateway . protocol version ) >= 2.0 ) : map sv types . update ( { pres . S INFO : [ set req . V TEXT ] , pres . S GAS : [ set req . V FLOW , set req . V VOLUME ] , pres . S GPS : [ set req . V POSITION ] , pres . S WATER QUALITY : [ set req . V TEMP , set req . V PH , set req . V ORP , set req . V EC ] } ) map sv types [ pres . S CUSTOM ] . append ( set req . V CUSTOM ) map sv types [ pres . S POWER ] . extend ( [ set req . V VAR , set req . V VA , set req . V POWER FACTOR ] ) devices = { } gateway . platform callbacks . append ( mysensors . pf callback factory ( map sv types , devices ,  My  Sensors  Sensor  , add devices ) ) 
def inject ( ** k ) : return  Injection  Factory  ( k ) 
def bool or str ( * text ) : def bool or value ( obj ) : if ( obj in text ) : return obj else : return asbool ( obj ) return bool or value 
def track from url ( url , timeout = DEFAULT ASYNC TIMEOUT ) : param dict = dict ( url = url ) return  upload ( param dict , timeout , data =  None  ) 
def commit manually ( using =  None  ) : def inner commit manually ( func , db =  None  ) : def  commit manually ( * args , ** kw ) : try : enter transaction management ( using = db ) managed (  True  , using = db ) return func ( * args , ** kw ) finally : leave transaction management ( using = db ) return wraps ( func ) (  commit manually ) if ( using is  None  ) : using = DEFAULT DB ALIAS if callable ( using ) : return inner commit manually ( using , DEFAULT DB ALIAS ) return ( lambda func : inner commit manually ( func , using ) ) 
def morsel to cookie ( morsel ) : c = create cookie ( name = morsel . key , value = morsel . value , version = ( morsel [ 'version' ] or 0 ) , port =  None  , port specified =  False  , domain = morsel [ 'domain' ] , domain specified = bool ( morsel [ 'domain' ] ) , domain initial dot = morsel [ 'domain' ] . startswith ( '.' ) , path = morsel [ 'path' ] , path specified = bool ( morsel [ 'path' ] ) , secure = bool ( morsel [ 'secure' ] ) , expires = ( morsel [ 'max-age' ] or morsel [ 'expires' ] ) , discard =  False  , comment = morsel [ 'comment' ] , comment url = bool ( morsel [ 'comment' ] ) , rest = { ' Http  Only ' : morsel [ 'httponly' ] } , rfc2109 =  False  ) return c 
def add service protocol ( service , protocol ) : cmd = '--permanent  --service={0}  --add-protocol={1}' . format ( service , protocol ) return   firewall cmd ( cmd ) 
def subnets6 ( ) : return salt . utils . network . subnets6 ( ) 
def is unresponsive ( url ) : host = urlparse ( url ) . hostname return ( host in unresponsive hosts ) 
def make linkcode resolve ( package , url fmt ) : revision =  get git revision ( ) return partial (  linkcode resolve , revision = revision , package = package , url fmt = url fmt ) 
def energy ( H , q , p ) : return ( H . pot . energy ( p ) - H . logp ( q ) ) 
def squeeze ( x ) : view = x . dimshuffle ( [ i for i in range ( x . ndim ) if ( not x . broadcastable [ i ] ) ] ) return view 
def show quickpanel ( captions , entries , show cancel =  False  ) : if show cancel :  Quickpanel  =  Cancel  Entries  Quickpanel  else :  Quickpanel  =  Entries  Quickpanel   Quickpanel  ( captions , entries ) . show quickpanel ( ) 
def load pandas ( ) : data =  get data ( ) return du . process recarray pandas ( data , endog idx = 0 , dtype = float ) 
def decode ( stream , strict =  False  , logger =  None  , timezone offset =  None  ) : if ( not isinstance ( stream , util .  Buffered  Byte  Stream  ) ) : stream = util .  Buffered  Byte  Stream  ( stream ) if logger : logger . debug ( 'remoting.decode  start' ) msg =  Envelope  ( ) msg . amf Version  = stream . read ushort ( ) if ( msg . amf Version  > 9 ) : raise pyamf .  Decode  Error  ( ( ' Malformed   stream  (amf Version =%d)' % msg . amf Version  ) ) decoder = pyamf . get decoder ( pyamf . AMF0 , stream , strict = strict , timezone offset = timezone offset ) context = decoder . context decoder . use amf3 = ( msg . amf Version  == pyamf . AMF3 ) header count = stream . read ushort ( ) for i in xrange ( header count ) : ( name , required , data ) =  read header ( stream , decoder , strict ) msg . headers [ name ] = data if required : msg . headers . set required ( name ) body count = stream . read short ( ) for i in xrange ( body count ) : context . clear ( ) ( target , payload ) =  read body ( stream , decoder , strict , logger ) msg [ target ] = payload if ( strict and ( stream . remaining ( ) > 0 ) ) : raise  Runtime  Error  ( ' Unable   to  fully  consume  the  buffer' ) if logger : logger . debug ( 'remoting.decode  end' ) return msg 
def setup ( hass , config ) : conf = config [ DOMAIN ] token = conf . get ( CONF TOKEN ) le wh = '{}{}' . format ( DEFAULT HOST , token ) def logentries event listener ( event ) : ' Listen   for  new  messages  on  the  bus  and  sends  them  to   Logentries .' state = event . data . get ( 'new state' ) if ( state is  None  ) : return try :  state = state helper . state as number ( state ) except  Value  Error  :  state = state . state json body = [ { 'domain' : state . domain , 'entity id' : state . object id , 'attributes' : dict ( state . attributes ) , 'time' : str ( event . time fired ) , 'value' :  state } ] try : payload = { 'host' : le wh , 'event' : json body } requests . post ( le wh , data = json . dumps ( payload ) , timeout = 10 ) except requests . exceptions .  Request  Exception  as error :  LOGGER . exception ( ' Error   sending  to   Logentries :  %s' , error ) hass . bus . listen ( EVENT STATE CHANGED , logentries event listener ) return  True  
def null formatter ( view , value ) : return  Markup  ( '<i>NULL</i>' ) 
def change password ( username , password , uid =  None  , host =  None  , admin username =  None  , admin password =  None  , module =  None  ) : if ( len ( password ) > 20 ) : raise  Command  Execution  Error  ( ' Supplied   password  should  be  20  characters  or  less' ) if ( uid is  None  ) : user = list users ( host = host , admin username = admin username , admin password = admin password , module = module ) uid = user [ username ] [ 'index' ] if uid : return   execute cmd ( 'config  -g  cfg User  Admin   -o  cfg User  Admin  Password   -i  {0}  {1}' . format ( uid , password ) , host = host , admin username = admin username , admin password = admin password , module = module ) else : log . warning ( "'{0}'  does  not  exist" . format ( username ) ) return  False  
def get multiple collections by id ( collection ids , strict =  True  ) : collection ids = set ( collection ids ) result = { } uncached = [ ] memcache keys = [  get collection memcache key ( i ) for i in collection ids ] cache result = memcache services . get multi ( memcache keys ) for collection obj in cache result . itervalues ( ) : result [ collection obj . id ] = collection obj for  id in collection ids : if (  id not in result ) : uncached . append (  id ) db collection models = collection models .  Collection  Model  . get multi ( uncached ) db results dict = { } not found = [ ] for ( index , cid ) in enumerate ( uncached ) : model = db collection models [ index ] if model : collection = get collection from model ( model ) db results dict [ cid ] = collection else : logging . info ( ( ' Tried   to  fetch  collection  with  id  %s,  but  no  such  collection  exists  in  the  datastore' % cid ) ) not found . append ( cid ) if ( strict and not found ) : raise  Value  Error  ( ( " Couldn 't  find  collections  with  the  following  ids:\n%s" % '\n' . join ( not found ) ) ) cache update = { cid : db results dict [ cid ] for cid in db results dict . iterkeys ( ) if ( db results dict [ cid ] is not  None  ) } if cache update : memcache services . set multi ( cache update ) result . update ( db results dict ) return result 
def bin2long ( text , endian ) : assert ( endian in ( LITTLE ENDIAN , BIG ENDIAN ) ) bits = [ ( ord ( character ) - ord ( '0' ) ) for character in text if ( character in '01' ) ] if ( endian is not BIG ENDIAN ) : bits = bits [ : : ( - 1 ) ] size = len ( bits ) assert ( 0 < size ) value = 0 for bit in bits : value *= 2 value += bit return value 
def  infer fill value ( val ) : if ( not is list like ( val ) ) : val = [ val ] val = np . array ( val , copy =  False  ) if is datetimelike ( val ) : return np . array ( ' Na T' , dtype = val . dtype ) elif is object dtype ( val . dtype ) : dtype = lib . infer dtype (  ensure object ( val ) ) if ( dtype in [ 'datetime' , 'datetime64' ] ) : return np . array ( ' Na T' , dtype =  NS DTYPE ) elif ( dtype in [ 'timedelta' , 'timedelta64' ] ) : return np . array ( ' Na T' , dtype =  TD DTYPE ) return np . nan 
def command ( method Name  , cmd Class  =  Wrapper  Command  ) : def wrapper ( obj , journal , * args , ** kwargs ) : return journal . execute Command  ( cmd Class  ( method Name  , obj , args , kwargs ) ) return wrapper 
def register ( linter ) : linter . register checker (  Type  Checker  ( linter ) ) 
def  Get  Campaign  Feeds  ( client , feed , placeholder type ) : campaign feed service = client .  Get  Service  ( ' Campaign  Feed  Service ' , 'v201605' ) campaign feeds = [ ] more pages =  True  selector = { 'fields' : [ ' Campaign  Id ' , ' Matching  Function ' , ' Placeholder  Types ' ] , 'predicates' : [ { 'field' : ' Status ' , 'operator' : 'EQUALS' , 'values' : [ 'ENABLED' ] } , { 'field' : ' Feed  Id ' , 'operator' : 'EQUALS' , 'values' : [ feed [ 'id' ] ] } , { 'field' : ' Placeholder  Types ' , 'operator' : 'CONTAINS ANY' , 'values' : [ placeholder type ] } ] , 'paging' : { 'start Index ' : 0 , 'number Results ' : PAGE SIZE } } while more pages : page = campaign feed service . get ( selector ) if ( 'entries' in page ) : campaign feeds . extend ( page [ 'entries' ] ) selector [ 'paging' ] [ 'start Index ' ] += PAGE SIZE more pages = ( selector [ 'paging' ] [ 'start Index ' ] < int ( page [ 'total Num  Entries ' ] ) ) return campaign feeds 
def   Check  Facet  Depth  ( depth ) : if ( depth is  None  ) : return  None  else : return   Check  Integer  ( depth , 'depth' , zero ok =  False  , upper bound = MAXIMUM DEPTH FOR FACETED SEARCH ) 
def get Mobile  Network  Type  ( ) : ( info , fast ) = ( ' Error !' ,  False  ) try : m Context  = autoclass ( 'android.content. Context ' ) python Activity  = autoclass ( 'org.renpy.android. Python  Service ' ) connectivity Manager  = autoclass ( 'android.net. Connectivity  Manager ' ) telephony Manager  = autoclass ( 'android.telephony. Telephony  Manager ' ) c Manager  = cast ( 'android.net. Connectivity  Manager ' , python Activity  . m Service  . get System  Service  ( m Context  . CONNECTIVITY SERVICE ) ) active Network  Info  = c Manager  . get Active  Network  Info  ( ) c Type  = active Network  Info  . get Type  ( ) c Sub  Type  = active Network  Info  . get Subtype  ( ) if ( c Type  != connectivity Manager  . TYPE MOBILE ) : return  None  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE 1xRTT ) : info = '1xRTT:  50-100  kbps' fast =  False  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE CDMA ) : info = 'CDMA:  14-64  kbps' fast =  False  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE EDGE ) : info = 'EDGE:  50-100  kbps' fast =  False  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE EVDO 0 ) : info = 'EVDO 0:  400-1000  kbps' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE EVDO A ) : info = 'EVDO A:  600-1400  kbps' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE GPRS ) : info = 'GPRS:  100  kbps' fast =  False  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE HSDPA ) : info = 'HSDPA:  2-14   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE HSPA ) : info = 'HSPA:  700-1700  kbps' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE HSUPA ) : info = 'HSUPA:  1-23   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE UMTS ) : info = 'UMTS:  400-7000  kbps' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE EHRPD ) : info = 'EHRPD:  1-2   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE EVDO B ) : info = 'EVDO B:  5   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE HSPAP ) : info = 'HSPAP:  10-20   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE IDEN ) : info = 'IDEN:  25  kbps' fast =  False  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE LTE ) : info = 'LTE:  10+   Mbps ' fast =  True  if ( c Sub  Type  == telephony Manager  . NETWORK TYPE UNKNOWN ) : info = 'UNKNOWN:  ?' fast =  None  return { 'info' : info , 'fast' : fast } except  Exception  as e : return { 'info' : info , 'fast' : fast } 
@ status ( ' Getting   the  list  of  files  that  have  been  added/changed' , info = ( lambda x : n files str ( len ( x ) ) ) ) def changed files ( ) : if os . path . isdir ( os . path . join ( SRCDIR , '.hg' ) ) : cmd = 'hg  status  --added  --modified  --no-status' if mq patches applied ( ) : cmd += '  --rev  qparent' with subprocess .  Popen  ( cmd . split ( ) , stdout = subprocess . PIPE ) as st : return [ x . decode ( ) . rstrip ( ) for x in st . stdout ] elif os . path . isdir ( os . path . join ( SRCDIR , '.git' ) ) : cmd = 'git  status  --porcelain' filenames = [ ] with subprocess .  Popen  ( cmd . split ( ) , stdout = subprocess . PIPE ) as st : for line in st . stdout : line = line . decode ( ) . rstrip ( ) status = set ( line [ : 2 ] ) if ( not status . intersection ( 'MAU' ) ) : continue filename = line [ 3 : ] if ( '  ->  ' in filename ) : filename = filename . split ( '  ->  ' , 2 ) [ 1 ] . strip ( ) filenames . append ( filename ) return filenames else : sys . exit ( 'need  a   Mercurial   or  git  checkout  to  get  modified  files' ) 
def  Get  Owner I Ds  ( region ) : ec2 =   Connect  ( region ) return [ g . owner id for g in ec2 . get all security groups ( ) ] 
def update patch log ( patchmodule ) : frappe . get doc ( { u'doctype' : u' Patch    Log ' , u'patch' : patchmodule } ) . insert ( ) 
def true ( * args , ** kwargs ) : return  True  
@  replace by ( ' tifffile.decode lzw' ) def decode lzw ( encoded ) : len encoded = len ( encoded ) bitcount max = ( len encoded * 8 ) unpack = struct . unpack if ( sys . version [ 0 ] == '2' ) : newtable = [ chr ( i ) for i in range ( 256 ) ] else : newtable = [ bytes ( [ i ] ) for i in range ( 256 ) ] newtable . extend ( ( 0 , 0 ) ) def next code ( ) : " Return   integer  of  'bitw'  bits  at  'bitcount'  position  in  encoded." start = ( bitcount // 8 ) s = encoded [ start : ( start + 4 ) ] try : code = unpack ( '>I' , s ) [ 0 ] except  Exception  : code = unpack ( '>I' , ( s + ( '\x00' * ( 4 - len ( s ) ) ) ) ) [ 0 ] code <<= ( bitcount % 8 ) code &= mask return ( code >> shr ) switchbitch = { 255 : ( 9 , 23 , int ( ( ( 9 * '1' ) + ( '0' * 23 ) ) , 2 ) ) , 511 : ( 10 , 22 , int ( ( ( 10 * '1' ) + ( '0' * 22 ) ) , 2 ) ) , 1023 : ( 11 , 21 , int ( ( ( 11 * '1' ) + ( '0' * 21 ) ) , 2 ) ) , 2047 : ( 12 , 20 , int ( ( ( 12 * '1' ) + ( '0' * 20 ) ) , 2 ) ) } ( bitw , shr , mask ) = switchbitch [ 255 ] bitcount = 0 if ( len encoded < 4 ) : raise  Value  Error  ( 'strip  must  be  at  least  4  characters  long' ) if ( next code ( ) != 256 ) : raise  Value  Error  ( 'strip  must  begin  with  CLEAR  code' ) code = 0 oldcode = 0 result = [ ] result append = result . append while  True  : code = next code ( ) bitcount += bitw if ( ( code == 257 ) or ( bitcount >= bitcount max ) ) : break if ( code == 256 ) : table = newtable [ : ] table append = table . append lentable = 258 ( bitw , shr , mask ) = switchbitch [ 255 ] code = next code ( ) bitcount += bitw if ( code == 257 ) : break result append ( table [ code ] ) else : if ( code < lentable ) : decoded = table [ code ] newcode = ( table [ oldcode ] + decoded [ : 1 ] ) else : newcode = table [ oldcode ] newcode += newcode [ : 1 ] decoded = newcode result append ( decoded ) table append ( newcode ) lentable += 1 oldcode = code if ( lentable in switchbitch ) : ( bitw , shr , mask ) = switchbitch [ lentable ] if ( code != 257 ) : warnings . warn ( ( 'unexpected  end  of  lzw  stream  (code  %i)' % code ) ) return '' . join ( result ) 
def print name status ( changes ) : for change in changes : if ( not change ) : continue if ( type ( change ) is list ) : change = change [ 0 ] if ( change . type == CHANGE ADD ) : path1 = change . new . path path2 = '' kind = 'A' elif ( change . type == CHANGE DELETE ) : path1 = change . old . path path2 = '' kind = 'D' elif ( change . type == CHANGE MODIFY ) : path1 = change . new . path path2 = '' kind = 'M' elif ( change . type in RENAME CHANGE TYPES ) : path1 = change . old . path path2 = change . new . path if ( change . type == CHANGE RENAME ) : kind = 'R' elif ( change . type == CHANGE COPY ) : kind = 'C' ( yield ( '%-8s%-20s%-20s' % ( kind , path1 , path2 ) ) ) 
def add page if missing ( request ) : try : return { 'feincms page' :  Page  . objects . for request ( request , best match =  True  ) } except  Page  .  Does  Not  Exist  : return { } 
def jointplot ( x , y , data =  None  , kind = 'scatter' , stat func = stats . pearsonr , color =  None  , size = 6 , ratio = 5 , space = 0.2 , dropna =  True  , xlim =  None  , ylim =  None  , joint kws =  None  , marginal kws =  None  , annot kws =  None  , ** kwargs ) : if ( joint kws is  None  ) : joint kws = { } joint kws . update ( kwargs ) if ( marginal kws is  None  ) : marginal kws = { } if ( annot kws is  None  ) : annot kws = { } if ( color is  None  ) : color = color palette ( ) [ 0 ] color rgb = mpl . colors . color Converter  . to rgb ( color ) colors = [ set hls values ( color rgb , l = l ) for l in np . linspace ( 1 , 0 , 12 ) ] cmap = blend palette ( colors , as cmap =  True  ) grid =  Joint  Grid  ( x , y , data , dropna = dropna , size = size , ratio = ratio , space = space , xlim = xlim , ylim = ylim ) if ( kind == 'scatter' ) : joint kws . setdefault ( 'color' , color ) grid . plot joint ( plt . scatter , ** joint kws ) marginal kws . setdefault ( 'kde' ,  False  ) marginal kws . setdefault ( 'color' , color ) grid . plot marginals ( distplot , ** marginal kws ) elif kind . startswith ( 'hex' ) : x bins =  freedman diaconis bins ( grid . x ) y bins =  freedman diaconis bins ( grid . y ) gridsize = int ( np . mean ( [ x bins , y bins ] ) ) joint kws . setdefault ( 'gridsize' , gridsize ) joint kws . setdefault ( 'cmap' , cmap ) grid . plot joint ( plt . hexbin , ** joint kws ) marginal kws . setdefault ( 'kde' ,  False  ) marginal kws . setdefault ( 'color' , color ) grid . plot marginals ( distplot , ** marginal kws ) elif kind . startswith ( 'kde' ) : joint kws . setdefault ( 'shade' ,  True  ) joint kws . setdefault ( 'cmap' , cmap ) grid . plot joint ( kdeplot , ** joint kws ) marginal kws . setdefault ( 'shade' ,  True  ) marginal kws . setdefault ( 'color' , color ) grid . plot marginals ( kdeplot , ** marginal kws ) elif kind . startswith ( 'reg' ) : from . linearmodels import regplot marginal kws . setdefault ( 'color' , color ) grid . plot marginals ( distplot , ** marginal kws ) joint kws . setdefault ( 'color' , color ) grid . plot joint ( regplot , ** joint kws ) elif kind . startswith ( 'resid' ) : from . linearmodels import residplot joint kws . setdefault ( 'color' , color ) grid . plot joint ( residplot , ** joint kws ) ( x , y ) = grid . ax joint . collections [ 0 ] . get offsets ( ) . T marginal kws . setdefault ( 'color' , color ) marginal kws . setdefault ( 'kde' ,  False  ) distplot ( x , ax = grid . ax marg x , ** marginal kws ) distplot ( y , vertical =  True  , fit = stats . norm , ax = grid . ax marg y , ** marginal kws ) stat func =  None  else : msg = "kind  must  be  either  'scatter',  'reg',  'resid',  'kde',  or  'hex'" raise  Value  Error  ( msg ) if ( stat func is not  None  ) : grid . annotate ( stat func , ** annot kws ) return grid 
def  Friendly  Exception  Dlg  ( message ) : class w3af message dialog ( gtk .  Message  Dialog  , ) : def dialog response cb ( self , widget , response id ) : '\n                        http://faq.pygtk.org/index.py?req=show&file=faq10.017.htp\n                        ' self . destroy ( ) def dialog run ( self ) : '\n                        http://faq.pygtk.org/index.py?req=show&file=faq10.017.htp\n                        ' if ( not self . modal ) : self . set modal (  True  ) self . connect ( 'response' , self . dialog response cb ) self . show ( ) dlg = w3af message dialog (  None  , gtk . DIALOG MODAL , gtk . MESSAGE WARNING , gtk . BUTTONS OK , message ) dlg . set icon from file ( W3AF ICON ) dlg . set title ( ' Error ' ) dlg . dialog run ( ) return 
def user has language set ( request ) : if ( hasattr ( request , u'session' ) and ( request . session . get ( LANGUAGE SESSION KEY ) is not  None  ) ) : return  True  if ( LANGUAGE COOKIE NAME in request . COOKIES ) : return  True  return  False  
def symbols ( names , ** args ) : result = [ ] if isinstance ( names , string types ) : marker = 0 literals = [ '\\,' , '\\:' , '\\  ' ] for i in range ( len ( literals ) ) : lit = literals . pop ( 0 ) if ( lit in names ) : while ( chr ( marker ) in names ) : marker += 1 lit char = chr ( marker ) marker += 1 names = names . replace ( lit , lit char ) literals . append ( ( lit char , lit [ 1 : ] ) ) def literal ( s ) : if literals : for ( c , l ) in literals : s = s . replace ( c , l ) return s names = names . strip ( ) as seq = names . endswith ( ',' ) if as seq : names = names [ : ( - 1 ) ] . rstrip ( ) if ( not names ) : raise  Value  Error  ( 'no  symbols  given' ) names = [ n . strip ( ) for n in names . split ( ',' ) ] if ( not all ( ( n for n in names ) ) ) : raise  Value  Error  ( 'missing  symbol  between  commas' ) for i in range ( ( len ( names ) - 1 ) , ( - 1 ) , ( - 1 ) ) : names [ i : ( i + 1 ) ] = names [ i ] . split ( ) cls = args . pop ( 'cls' ,  Symbol  ) seq = args . pop ( 'seq' , as seq ) for name in names : if ( not name ) : raise  Value  Error  ( 'missing  symbol' ) if ( ':' not in name ) : symbol = cls ( literal ( name ) , ** args ) result . append ( symbol ) continue split =  range . split ( name ) for i in range ( ( len ( split ) - 1 ) ) : if ( i and ( ':' in split [ i ] ) and ( split [ i ] != ':' ) and split [ ( i - 1 ) ] . endswith ( '(' ) and split [ ( i + 1 ) ] . startswith ( ')' ) ) : split [ ( i - 1 ) ] = split [ ( i - 1 ) ] [ : ( - 1 ) ] split [ ( i + 1 ) ] = split [ ( i + 1 ) ] [ 1 : ] for ( i , s ) in enumerate ( split ) : if ( ':' in s ) : if s [ ( - 1 ) ] . endswith ( ':' ) : raise  Value  Error  ( 'missing  end  range' ) ( a , b ) = s . split ( ':' ) if ( b [ ( - 1 ) ] in string . digits ) : a = ( 0 if ( not a ) else int ( a ) ) b = int ( b ) split [ i ] = [ str ( c ) for c in range ( a , b ) ] else : a = ( a or 'a' ) split [ i ] = [ string . ascii letters [ c ] for c in range ( string . ascii letters . index ( a ) , ( string . ascii letters . index ( b ) + 1 ) ) ] if ( not split [ i ] ) : break else : split [ i ] = [ s ] else : seq =  True  if ( len ( split ) == 1 ) : names = split [ 0 ] else : names = [ '' . join ( s ) for s in cartes ( * split ) ] if literals : result . extend ( [ cls ( literal ( s ) , ** args ) for s in names ] ) else : result . extend ( [ cls ( s , ** args ) for s in names ] ) if ( ( not seq ) and ( len ( result ) <= 1 ) ) : if ( not result ) : return ( ) return result [ 0 ] return tuple ( result ) else : for name in names : result . append ( symbols ( name , ** args ) ) return type ( names ) ( result ) 
@ pytest . mark . network def test download vcs link ( script ) : result = script . pip ( 'download' , '-d' , '.' , 'git+git://github.com/pypa/pip-test-package.git' ) assert ( (  Path  ( 'scratch' ) / 'pip-test-package-0.1.1.zip' ) in result . files created ) assert ( ( script . site packages / 'piptestpackage' ) not in result . files created ) 
def common dtype ( cols ) : np types = ( np . bool  , np . object  , np . number , np . character , np . void ) uniq types = set ( ( tuple ( ( issubclass ( col . dtype . type , np type ) for np type in np types ) ) for col in cols ) ) if ( len ( uniq types ) > 1 ) : incompat types = [ col . dtype . name for col in cols ] tme =  Table  Merge  Error  ( u' Columns   have  incompatible  types  {0}' . format ( incompat types ) ) tme .  incompat types = incompat types raise tme arrs = [ np . empty ( 1 , dtype = col . dtype ) for col in cols ] for arr in arrs : if ( arr . dtype . kind in ( u'S' , u'U' ) ) : arr [ 0 ] = ( u'0' * arr . itemsize ) arr common = np . array ( [ arr [ 0 ] for arr in arrs ] ) return arr common . dtype . str 
def stripascii ( string ) : ord  = ( ord if ( sys . version info [ 0 ] < 3 ) else ( lambda x : x ) ) i = len ( string ) while i : i -= 1 if ( 8 < ord  ( string [ i ] ) < 127 ) : break else : i = ( - 1 ) return string [ : ( i + 1 ) ] 
def   Add  Properties  For  Extensions  ( descriptor , cls ) : extension dict = descriptor . extensions by name for ( extension name , extension field ) in extension dict . iteritems ( ) : constant name = ( extension name . upper ( ) + ' FIELD NUMBER' ) setattr ( cls , constant name , extension field . number ) 
def get time zone offset ( time zone , date time =  None  ) : date time = ( datetime . now ( utc ) if ( date time is  None  ) else date time ) return  format time zone string ( time zone , date time , '%z' ) 
def write Output  ( file Name  = '' ) : file Name  = fabmetheus interpret . get First  Translator  File  Name  Unmodified  ( file Name  ) if ( file Name  == '' ) : return export Repository  =  Export  Repository  ( ) settings . get Read  Repository  ( export Repository  ) start Time  = time . time ( ) print ( ( ' File   ' + archive . get Summarized  File  Name  ( file Name  ) ) + '  is  being  chain  exported.' ) suffix File  Name  = ( ( file Name  [ : file Name  . rfind ( '.' ) ] + '.' ) + export Repository  . file Extension  . value ) gcode Text  = gcodec . get Gcode  File  Text  ( file Name  , '' ) procedures = skeinforge craft . get Procedures  ( 'export' , gcode Text  ) gcode Text  = skeinforge craft . get Chain  Text  From  Procedures  ( file Name  , procedures [ : ( - 1 ) ] , gcode Text  ) if ( gcode Text  == '' ) : return skeinforge analyze . write Output  ( file Name  , suffix File  Name  , gcode Text  ) if export Repository  . save Penultimate  Gcode  . value : penultimate File  Name  = ( file Name  [ : file Name  . rfind ( '.' ) ] + ' penultimate.gcode' ) archive . write File  Text  ( penultimate File  Name  , gcode Text  ) print ( ' The   penultimate  file  is  saved  as  ' + archive . get Summarized  File  Name  ( penultimate File  Name  ) ) export Chain  Gcode  = get Crafted  Text  From  Text  ( gcode Text  , export Repository  ) replaceable Export  Chain  Gcode  =  None  selected Plugin  Module  = get Selected  Plugin  Module  ( export Repository  . export Plugins  ) if ( selected Plugin  Module  ==  None  ) : replaceable Export  Chain  Gcode  = export Chain  Gcode  elif selected Plugin  Module  . global Is  Replaceable  : replaceable Export  Chain  Gcode  = selected Plugin  Module  . get Output  ( export Chain  Gcode  ) else : selected Plugin  Module  . write Output  ( suffix File  Name  , export Chain  Gcode  ) if ( replaceable Export  Chain  Gcode  !=  None  ) : replaceable Export  Chain  Gcode  = get Replaced  ( replaceable Export  Chain  Gcode  ) archive . write File  Text  ( suffix File  Name  , replaceable Export  Chain  Gcode  ) print ( ' The   exported  file  is  saved  as  ' + archive . get Summarized  File  Name  ( suffix File  Name  ) ) if ( export Repository  . also Send  Output  To  . value != '' ) : if ( replaceable Export  Chain  Gcode  ==  None  ) : replaceable Export  Chain  Gcode  = selected Plugin  Module  . get Output  ( export Chain  Gcode  ) exec ( ( 'print  >>  ' + export Repository  . also Send  Output  To  . value ) + ',  replaceable Export  Chain  Gcode ' ) print ( ' It   took  %s  to  export  the  file.' % euclidean . get Duration  String  ( ( time . time ( ) - start Time  ) ) ) 
def get era names ( width = 'wide' , locale = LC TIME ) : return  Locale  . parse ( locale ) . eras [ width ] 
def python 2 unicode compatible ( klass ) : if PY2 : if ( '  str  ' not in klass .   dict   ) : raise  Value  Error  ( ( "@python 2 unicode compatible  cannot  be  applied  to  %s  because  it  doesn't  define    str  ()." % klass .   name   ) ) klass .   unicode   = klass .   str   klass .   str   = ( lambda self : self .   unicode   ( ) . encode ( 'utf-8' ) ) return klass 
def dcc Describe  ( data ) : orig data = data data = data . split ( ) if ( len ( data ) < 4 ) : return orig data ( dcctype , arg , address , port ) = data [ : 4 ] if ( '.' in address ) : pass else : try : address = int ( address ) except  Value  Error  : pass else : address = ( ( ( address >> 24 ) & 255 ) , ( ( address >> 16 ) & 255 ) , ( ( address >> 8 ) & 255 ) , ( address & 255 ) ) address = '.' . join ( map ( str , address ) ) if ( dcctype == 'SEND' ) : filename = arg size txt = '' if ( len ( data ) >= 5 ) : try : size = int ( data [ 4 ] ) size txt = ( '  of  size  %d  bytes' % ( size , ) ) except  Value  Error  : pass dcc text = ( "SEND  for  file  '%s'%s  at  host  %s,  port  %s" % ( filename , size txt , address , port ) ) elif ( dcctype == 'CHAT' ) : dcc text = ( 'CHAT  for  host  %s,  port  %s' % ( address , port ) ) else : dcc text = orig data return dcc text 
def add Value  To  Output  ( depth , key Input  , output , value ) : depth Start  = ( '    ' * depth ) output . write ( ( '%s%s:' % ( depth Start  , key Input  ) ) ) if ( value .   class   == dict ) : output . write ( '\n' ) keys = value . keys ( ) keys . sort ( ) for key in keys : add Value  To  Output  ( ( depth + 1 ) , key , output , value [ key ] ) return if ( value .   class   == list ) : output . write ( '\n' ) for ( element Index  , element ) in enumerate ( value ) : add Value  To  Output  ( ( depth + 1 ) , element Index  , output , element ) return output . write ( ( '  %s\n' % value ) ) 
def base64 len ( s ) : ( groups of 3 , leftover ) = divmod ( len ( s ) , 3 ) n = ( groups of 3 * 4 ) if leftover : n += 4 return n 
def zmap ( scores , compare , axis = 0 , ddof = 0 ) : ( scores , compare ) = map ( np . asanyarray , [ scores , compare ] ) mns = compare . mean ( axis = axis ) sstd = compare . std ( axis = axis , ddof = ddof ) if ( axis and ( mns . ndim < compare . ndim ) ) : return ( ( scores - np . expand dims ( mns , axis = axis ) ) / np . expand dims ( sstd , axis = axis ) ) else : return ( ( scores - mns ) / sstd ) 
def  typecheck ( name , value , * types ) : if ( not types ) : types = ( unicode , ) if ( not isinstance ( value , types ) ) : raise  Type  Error  ( 'expected  {}  for  {},  got  {}' . format ( '  or  ' . join ( [ t .   name   for t in types ] ) , name , repr ( value ) ) ) return value 
def settings ( * args , ** kwargs ) : managers = list ( args ) if kwargs : managers . append (  setenv ( kwargs ) ) return nested ( * managers ) 
def occurrence ( request , event id , template name = 'schedule/occurrence.html' , * args , ** kwargs ) : ( event , occurrence ) = get occurrence ( event id , * args , ** kwargs ) back url = request . META . get ( 'HTTP REFERER' ,  None  ) return render to response ( template name , { 'event' : event , 'occurrence' : occurrence , 'back url' : back url } , context instance =  Request  Context  ( request ) ) 
def gen column names ( n ) : col names = list ( DEFAULT COLUMN NAMES ) if ( n < len ( col names ) ) : return list ( take ( n , col names ) ) else : n left = ( n - len ( col names ) ) labels = [ '' . join ( item ) for item in take ( n left , itertools . product ( DEFAULT COLUMN NAMES , DEFAULT COLUMN NAMES ) ) ] col names . extend ( labels ) return col names 
def  maybe set current user vars ( method , api info =  None  , request =  None  ) : if  is auth info available ( ) : return os . environ [  ENV AUTH EMAIL ] = '' os . environ [  ENV AUTH DOMAIN ] = '' try : api info = ( api info or method . im self . api info ) except  Attribute  Error  : logging . warning ( ' Attribute  Error   when  accessing  %s.im self.     An   unbound  method  was  probably  passed  as  an  endpoints  handler.' , method .   name   ) scopes = method . method info . scopes audiences = method . method info . audiences allowed client ids = method . method info . allowed client ids else : scopes = ( method . method info . scopes if ( method . method info . scopes is not  None  ) else api info . scopes ) audiences = ( method . method info . audiences if ( method . method info . audiences is not  None  ) else api info . audiences ) allowed client ids = ( method . method info . allowed client ids if ( method . method info . allowed client ids is not  None  ) else api info . allowed client ids ) if ( ( not scopes ) and ( not audiences ) and ( not allowed client ids ) ) : return token =  get token ( request ) if ( not token ) : return  None  if ( ( ( scopes == [  EMAIL SCOPE ] ) or ( scopes == (  EMAIL SCOPE , ) ) ) and allowed client ids ) : logging . info ( ' Checking   for  id token.' ) time now = long ( time . time ( ) ) user =  get id token user ( token , audiences , allowed client ids , time now , memcache ) if user : os . environ [  ENV AUTH EMAIL ] = user . email ( ) os . environ [  ENV AUTH DOMAIN ] = user . auth domain ( ) return if scopes : logging . info ( ' Checking   for  oauth  token.' ) result = urlfetch . fetch ( ( '%s?%s' % (  TOKENINFO URL , urllib . urlencode ( { 'access token' : token } ) ) ) ) if ( result . status code == 200 ) : token info = json . loads ( result . content )  set oauth user vars ( token info , audiences , allowed client ids , scopes ,  is local dev ( ) ) 
def raises  Invalid  ( function ) : def call and assert ( * args , ** kwargs ) : nose . tools . assert raises ( df .  Invalid  , function , * args , ** kwargs ) return call and assert 
@ pytest . mark . django db def test reject suggestion with comment ( client , request users ) :  Comment  = get comment model ( ) unit =  Unit  . objects . filter ( suggestion  state = 'pending' , state = UNTRANSLATED ) [ 0 ] sugg =  Suggestion  . objects . filter ( unit = unit , state = 'pending' ) [ 0 ] comment = ' This   is  a  comment!' user = request users [ 'user' ] if ( user . username != 'nobody' ) : client . login ( username = user . username , password = request users [ 'password' ] ) url = ( '/xhr/units/%d/suggestions/%d/' % ( unit . id , sugg . id ) ) response = client . delete ( url , ( 'comment=%s' % comment ) , HTTP X REQUESTED WITH = 'XML Http  Request ' ) can reject = ( check permission ( 'review' , response . wsgi request ) or ( sugg . user . id == user . id ) ) if can reject : assert ( response . status code == 200 ) rejected suggestion =  Suggestion  . objects . get ( id = sugg . id ) assert ( rejected suggestion . state == 'rejected' ) assert (  Comment  . objects . for model ( rejected suggestion ) . get ( ) . comment == comment ) else : assert ( response . status code == 403 ) 
def validate ( ** vkargs ) : depr ( ' Use   route  wildcard  filters  instead.' ) def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kargs ) : for ( key , value ) in vkargs . iteritems ( ) : if ( key not in kargs ) : abort ( 403 , ( ' Missing   parameter:  %s' % key ) ) try : kargs [ key ] = value ( kargs [ key ] ) except  Value  Error  : abort ( 403 , ( ' Wrong   parameter  format  for:  %s' % key ) ) return func ( * args , ** kargs ) return wrapper return decorator 
def unpack Open  direct tcpip ( data ) : ( conn Host  , rest ) = common . getNS ( data ) if (  PY3 and isinstance ( conn Host  , bytes ) ) : conn Host  = conn Host  . decode ( 'utf-8' ) conn Port  = int ( struct . unpack ( '>L' , rest [ : 4 ] ) [ 0 ] ) ( orig Host  , rest ) = common . getNS ( rest [ 4 : ] ) if (  PY3 and isinstance ( orig Host  , bytes ) ) : orig Host  = orig Host  . decode ( 'utf-8' ) orig Port  = int ( struct . unpack ( '>L' , rest [ : 4 ] ) [ 0 ] ) return ( ( conn Host  , conn Port  ) , ( orig Host  , orig Port  ) ) 
def evaluate deltas ( e ) : accepted functions = (  Add  , ) if isinstance ( e , accepted functions ) : return e . func ( * [ evaluate deltas ( arg ) for arg in e . args ] ) elif isinstance ( e ,  Mul  ) : deltas = [ ] indices = { } for i in e . args : for s in i . free symbols : if ( s in indices ) : indices [ s ] += 1 else : indices [ s ] = 0 if isinstance ( i ,  Kronecker  Delta  ) : deltas . append ( i ) for d in deltas : if indices [ d . killable index ] : e = e . subs ( d . killable index , d . preferred index ) if ( len ( deltas ) > 1 ) : return evaluate deltas ( e ) elif ( indices [ d . preferred index ] and d . indices contain equal information ) : e = e . subs ( d . preferred index , d . killable index ) if ( len ( deltas ) > 1 ) : return evaluate deltas ( e ) else : pass return e else : return e 
@ not implemented for ( 'directed' ) def generalized degree ( G , nodes =  None  ) : if ( nodes in G ) : return next (  triangles and degree iter ( G , nodes ) ) [ 3 ] return { v : gd for ( v , d , t , gd ) in  triangles and degree iter ( G , nodes ) } 
@ webob . dec . wsgify @ util . check accept ( 'application/json' ) def list usages ( req ) : context = req . environ [ 'placement.context' ] uuid = util . wsgi path item ( req . environ , 'uuid' ) try : resource provider = objects .  Resource  Provider  . get by uuid ( context , uuid ) except exception .  Not  Found  as exc : raise webob . exc . HTTP Not  Found  ( (   ( ' No   resource  provider  with  uuid  %(uuid)s  found:  %(error)s' ) % { 'uuid' : uuid , 'error' : exc } ) , json formatter = util . json error formatter ) usage = objects .  Usage  List  . get all by resource provider uuid ( context , uuid ) response = req . response response . body = encodeutils . to utf8 ( jsonutils . dumps (  serialize usages ( resource provider , usage ) ) ) req . response . content type = 'application/json' return req . response 
def skip Unless  ( condition , reason ) : if ( not condition ) : return skip ( reason ) return  id 
def numpy cupy array almost equal nulp ( nulp = 1 , name = 'xp' , type check =  True  , accept error =  False  ) : def check func ( x , y ) : array . assert array almost equal nulp ( x , y , nulp ) return  make decorator ( check func , name , type check , accept error ) 
def  Check  For  Question  Pending  ( task ) : vm = task . info . entity if ( ( vm is not  None  ) and isinstance ( vm , vim .  Virtual  Machine  ) ) : qst = vm . runtime . question if ( qst is not  None  ) : raise  Task  Blocked  ( ' Task   blocked,   User    Intervention   required' ) 
def verify signed data ( token , data ) : if data . startswith ( MAC MARKER ) : try : data = data [ len ( MAC MARKER ) : ] mac data = json . loads ( base64 . b64decode ( data ) ) mac = compute mac ( token , mac data [ 'serialized data' ] ) if ( mac != mac data [ 'mac' ] ) : raise  Invalid  Mac  Error  ( ( 'invalid  MAC;  expect=%s,  actual=%s' % ( mac data [ 'mac' ] , mac ) ) ) return json . loads ( mac data [ 'serialized data' ] ) except : raise  Invalid  Mac  Error  ( 'invalid  MAC;  data  appeared  to  be  corrupted' ) else : return data 
def  read single hdf ( path , key , start = 0 , stop =  None  , columns =  None  , chunksize = int ( 1000000.0 ) , sorted index =  False  , lock =  None  , mode = 'a' ) : def get keys stops divisions ( path , key , stop , sorted index ) : '\n                 Get   the  "keys"  or  group  identifiers  which  match  the  given  key,  which\n                can  contain  wildcards.   This   uses  the  hdf  file  identified  by  the\n                given  path.   Also   get  the  index  of  the  last  row  of  data  for  each  matched\n                key.\n                ' with pd . HDF Store  ( path , mode = mode ) as hdf : keys = [ k for k in hdf . keys ( ) if fnmatch ( k , key ) ] stops = [ ] divisions = [ ] for k in keys : storer = hdf . get storer ( k ) if ( storer . format type != 'table' ) : raise  Type  Error  ( dont use fixed error message ) if ( stop is  None  ) : stops . append ( storer . nrows ) elif ( stop > storer . nrows ) : raise  Value  Error  ( ' Stop   keyword  exceeds  dataset  number  of  rows  ({})' . format ( storer . nrows ) ) else : stops . append ( stop ) if sorted index : division start = storer . read column ( 'index' , start = 0 , stop = 1 ) [ 0 ] division end = storer . read column ( 'index' , start = ( storer . nrows - 1 ) , stop = storer . nrows ) [ 0 ] divisions . append ( [ division start , division end ] ) else : divisions . append (  None  ) return ( keys , stops , divisions ) def one path one key ( path , key , start , stop , columns , chunksize , division , lock ) : '\n                 Get   the  data  frame  corresponding  to  one  path  and  one  key  (which  should\n                not  contain  any  wildcards).\n                ' empty = pd . read hdf ( path , key , mode = mode , stop = 0 ) if ( columns is not  None  ) : empty = empty [ columns ] token = tokenize ( ( path , os . path . getmtime ( path ) , key , start , stop , empty , chunksize , division ) ) name = ( 'read-hdf-' + token ) if ( empty . ndim == 1 ) : base = { 'name' : empty . name , 'mode' : mode } else : base = { 'columns' : empty . columns , 'mode' : mode } if ( start >= stop ) : raise  Value  Error  ( ' Start   row  number  ({})  is  above  or  equal  to  stop  row  number  ({})' . format ( start , stop ) ) if division : dsk = { ( name , 0 ) : (  pd read hdf , path , key , lock , base ) } divisions = division else : def update ( s ) : new = base . copy ( ) new . update ( { 'start' : s , 'stop' : ( s + chunksize ) } ) return new dsk = dict ( ( ( ( name , i ) , (  pd read hdf , path , key , lock , update ( s ) ) ) for ( i , s ) in enumerate ( range ( start , stop , chunksize ) ) ) ) divisions = ( [  None  ] * ( len ( dsk ) + 1 ) ) return new dd object ( dsk , name , empty , divisions ) ( keys , stops , divisions ) = get keys stops divisions ( path , key , stop , sorted index ) if ( ( ( start != 0 ) or ( stop is not  None  ) ) and ( len ( keys ) > 1 ) ) : raise  Not  Implemented  Error  ( read hdf error msg ) from . . multi import concat return concat ( [ one path one key ( path , k , start , s , columns , chunksize , d , lock ) for ( k , s , d ) in zip ( keys , stops , divisions ) ] ) 
@ testing . requires testing data def test pick channels mixin ( ) : raw = read raw fif ( fif fname , preload =  True  ) ch names = raw . ch names [ : 3 ] ch names orig = raw . ch names dummy = raw . copy ( ) . pick channels ( ch names ) assert equal ( ch names , dummy . ch names ) assert equal ( ch names orig , raw . ch names ) assert equal ( len ( ch names orig ) , raw .  data . shape [ 0 ] ) raw . pick channels ( ch names ) assert equal ( ch names , raw . ch names ) assert equal ( len ( ch names ) , len ( raw .  cals ) ) assert equal ( len ( ch names ) , raw .  data . shape [ 0 ] ) assert raises (  Value  Error  , raw . pick channels , ch names [ 0 ] ) raw = read raw fif ( fif fname , preload =  False  ) assert raises (  Runtime  Error  , raw . pick channels , ch names ) assert raises (  Runtime  Error  , raw . drop channels , ch names ) 
@ plugins . notify info yielded ( u'albuminfo received' ) def album candidates ( items , artist , album , va likely ) : if ( artist and album ) : try : for candidate in mb . match album ( artist , album , len ( items ) ) : ( yield candidate ) except mb .  Music  Brainz API Error  as exc : exc . log ( log ) if ( va likely and album ) : try : for candidate in mb . match album (  None  , album , len ( items ) ) : ( yield candidate ) except mb .  Music  Brainz API Error  as exc : exc . log ( log ) for candidate in plugins . candidates ( items , artist , album , va likely ) : ( yield candidate ) 
def  paint path ( fill , stroke ) : if stroke : if fill : return  Op  . fill stroke else : return  Op  . stroke elif fill : return  Op  . fill else : return  Op  . endpath 
def list networks ( profile =  None  ) : conn =  auth ( profile ) return conn . list networks ( ) 
def colormaps ( ) : pass 
@ synchronized ( IO LOCK ) def remove data (  id , path ) : path = os . path . join ( path ,  id ) try : if os . path . exists ( path ) : os . remove ( path ) logging . info ( '%s  removed' , path ) except : logging . debug ( ' Failed   to  remove  %s' , path ) 
@ verbose def read inverse operator ( fname , verbose =  None  ) : check fname ( fname , 'inverse  operator' , ( '-inv.fif' , '-inv.fif.gz' ) ) logger . info ( ( ' Reading   inverse  operator  decomposition  from  %s...' % fname ) ) ( f , tree ,   ) = fiff open ( fname , preload =  True  ) with f as fid : invs = dir tree find ( tree , FIFF . FIFFB MNE INVERSE SOLUTION ) if ( ( invs is  None  ) or ( len ( invs ) < 1 ) ) : raise  Exception  ( ( ' No   inverse  solutions  in  %s' % fname ) ) invs = invs [ 0 ] parent mri = dir tree find ( tree , FIFF . FIFFB MNE PARENT MRI FILE ) if ( len ( parent mri ) == 0 ) : raise  Exception  ( ( ' No   parent  MRI  information  in  %s' % fname ) ) parent mri = parent mri [ 0 ] logger . info ( '         Reading   inverse  operator  info...' ) tag = find tag ( fid , invs , FIFF . FIFF MNE INCLUDED METHODS ) if ( tag is  None  ) : raise  Exception  ( ' Modalities   not  found' ) inv = dict ( ) inv [ 'methods' ] = int ( tag . data ) tag = find tag ( fid , invs , FIFF . FIFF MNE SOURCE ORIENTATION ) if ( tag is  None  ) : raise  Exception  ( ' Source   orientation  constraints  not  found' ) inv [ 'source ori' ] = int ( tag . data ) tag = find tag ( fid , invs , FIFF . FIFF MNE SOURCE SPACE NPOINTS ) if ( tag is  None  ) : raise  Exception  ( ' Number   of  sources  not  found' ) inv [ 'nsource' ] = int ( tag . data ) inv [ 'nchan' ] = 0 tag = find tag ( fid , invs , FIFF . FIFF MNE COORD FRAME ) if ( tag is  None  ) : raise  Exception  ( ' Coordinate   frame  tag  not  found' ) inv [ 'coord frame' ] = tag . data tag = find tag ( fid , invs , FIFF . FIFF MNE INVERSE SOURCE UNIT ) unit dict = { FIFF . FIFF UNIT AM : ' Am ' , FIFF . FIFF UNIT AM M2 : ' Am /m^2' , FIFF . FIFF UNIT AM M3 : ' Am /m^3' } inv [ 'units' ] = unit dict . get ( int ( getattr ( tag , 'data' , ( - 1 ) ) ) ,  None  ) tag = find tag ( fid , invs , FIFF . FIFF MNE INVERSE SOURCE ORIENTATIONS ) if ( tag is  None  ) : raise  Exception  ( ' Source   orientation  information  not  found' ) inv [ 'source nn' ] = tag . data logger . info ( '        [done]' ) logger . info ( '         Reading   inverse  operator  decomposition...' ) tag = find tag ( fid , invs , FIFF . FIFF MNE INVERSE SING ) if ( tag is  None  ) : raise  Exception  ( ' Singular   values  not  found' ) inv [ 'sing' ] = tag . data inv [ 'nchan' ] = len ( inv [ 'sing' ] ) inv [ 'eigen leads weighted' ] =  False  inv [ 'eigen leads' ] =  read named matrix ( fid , invs , FIFF . FIFF MNE INVERSE LEADS , transpose =  True  ) if ( inv [ 'eigen leads' ] is  None  ) : inv [ 'eigen leads weighted' ] =  True  inv [ 'eigen leads' ] =  read named matrix ( fid , invs , FIFF . FIFF MNE INVERSE LEADS WEIGHTED , transpose =  True  ) if ( inv [ 'eigen leads' ] is  None  ) : raise  Value  Error  ( ' Eigen   leads  not  found  in  inverse  operator.' ) inv [ 'eigen fields' ] =  read named matrix ( fid , invs , FIFF . FIFF MNE INVERSE FIELDS ) logger . info ( '        [done]' ) inv [ 'noise cov' ] =  Covariance  ( **  read cov ( fid , invs , FIFF . FIFFV MNE NOISE COV , limited =  True  ) ) logger . info ( '         Noise   covariance  matrix  read.' ) inv [ 'source cov' ] =  read cov ( fid , invs , FIFF . FIFFV MNE SOURCE COV ) logger . info ( '         Source   covariance  matrix  read.' ) inv [ 'orient prior' ] =  read cov ( fid , invs , FIFF . FIFFV MNE ORIENT PRIOR COV ) if ( inv [ 'orient prior' ] is not  None  ) : logger . info ( '         Orientation   priors  read.' ) inv [ 'depth prior' ] =  read cov ( fid , invs , FIFF . FIFFV MNE DEPTH PRIOR COV ) if ( inv [ 'depth prior' ] is not  None  ) : logger . info ( '         Depth   priors  read.' ) inv [ 'fmri prior' ] =  read cov ( fid , invs , FIFF . FIFFV MNE FMRI PRIOR COV ) if ( inv [ 'fmri prior' ] is not  None  ) : logger . info ( '        fMRI  priors  read.' ) inv [ 'src' ] =  read source spaces from tree ( fid , tree , patch stats =  False  ) for s in inv [ 'src' ] : s [ 'id' ] = find source space hemi ( s ) tag = find tag ( fid , parent mri , FIFF . FIFF COORD TRANS ) if ( tag is  None  ) : raise  Exception  ( 'MRI/head  coordinate  transformation  not  found' ) mri head t =  ensure trans ( tag . data , 'mri' , 'head' ) inv [ 'mri head t' ] = mri head t inv [ 'info' ] =  read forward meas info ( tree , fid ) if ( inv [ 'coord frame' ] not in ( FIFF . FIFFV COORD MRI , FIFF . FIFFV COORD HEAD ) ) : raise  Exception  ( ' Only   inverse  solutions  computed  in  MRI  or  head  coordinates  are  acceptable' ) inv [ 'nave' ] = 1 inv [ 'projs' ] =  read proj ( fid , tree ) inv [ 'proj' ] = [ ] inv [ 'whitener' ] = [ ] inv [ 'reginv' ] = [ ] inv [ 'noisenorm' ] = [ ] nuse = 0 for k in range ( len ( inv [ 'src' ] ) ) : try : inv [ 'src' ] [ k ] = transform surface to ( inv [ 'src' ] [ k ] , inv [ 'coord frame' ] , mri head t ) except  Exception  as inst : raise  Exception  ( ( ' Could   not  transform  source  space  (%s)' % inst ) ) nuse += inv [ 'src' ] [ k ] [ 'nuse' ] logger . info ( '         Source   spaces  transformed  to  the  inverse  solution  coordinate  frame' ) return  Inverse  Operator  ( inv ) 
def floored twelfth of a 360 day year ( date ) : timetuple = date . timetuple ( ) year = timetuple . tm year day of year = timetuple . tm yday month0 = floor ( ( ( day of year / 360 ) * 12 ) ) return ( ( ( ( year - start year ) * 12 ) + month0 ) - start month 0 indexed ) 
def installed ( name , categories =  None  , skips =  None  , retries = 10 ) : ret = { 'name' : name , 'result' :  True  , 'changes' : { } , 'comment' : '' } if ( not categories ) : categories = [ name ] log . debug ( 'categories  to  search  for  are:  {0}' . format ( categories ) ) win updater =  Py  Win  Updater  ( ) win updater .  Set  Categories  ( categories ) win updater .  Set  Skips  ( skips ) ( comment , passed , retries ) =  search ( win updater , retries ) ret [ 'comment' ] += comment if ( not passed ) : ret [ 'result' ] =  False  return ret ( comment , passed , retries ) =  download ( win updater , retries ) ret [ 'comment' ] += comment if ( not passed ) : ret [ 'result' ] =  False  return ret ( comment , passed , retries ) =  install ( win updater , retries ) ret [ 'comment' ] += comment if ( not passed ) : ret [ 'result' ] =  False  return ret try : ret [ 'changes' ] = win updater .  Get  Installation  Results  ( ) except  Exception  : ret [ 'comment' ] += 'could  not  get  results,  but  updates  were  installed.' return ret 
def  Path  Collection  ( mode = 'agg' , * args , ** kwargs ) : if ( mode == 'raw' ) : return  Raw  Path  Collection  ( * args , ** kwargs ) elif ( mode == 'agg+' ) : return  Agg  Path  Collection  ( * args , ** kwargs ) return  Agg  Fast  Path  Collection  ( * args , ** kwargs ) 
def  add p tags ( raw body ) : return '<p>{raw body}</p>' . format ( raw body = raw body ) 
def list frameworks ( ) : sys . stdout . write ( ( ' Testable   frameworks:  %s\n\n Note   that  membership  in  this  list  means  the  framework  can  be  tested  with\n Py  Mongo ,  not  necessarily  that  it  is  officially  supported.\n' % ',  ' . join ( sorted ( FRAMEWORKS ) ) ) ) 
def build job ( name =  None  , parameters =  None  ) : if ( not name ) : raise  Salt  Invocation  Error  ( ' Required   parameter  `name`  is  missing.' ) server =  connect ( ) if ( not job exists ( name ) ) : raise  Salt  Invocation  Error  ( ' Job   `{0}`  does  not  exist.' . format ( name ) ) try : server . build job ( name , parameters ) except jenkins .  Jenkins  Exception  as err : raise  Salt  Invocation  Error  ( ' Something   went  wrong  {0}.' . format ( err ) ) return  True  
def features contains ( value ) : return var contains ( 'FEATURES' , value ) 
def get cache mtime ( ) : if os . path . exists ( APT UPDATE SUCCESS STAMP PATH ) : return os . stat ( APT UPDATE SUCCESS STAMP PATH ) . st mtime elif os . path . exists ( APT LISTS PATH ) : return os . stat ( APT LISTS PATH ) . st mtime else : return 0 
def  Get  Local  Time  Zone  ( ) : tz Reg  Key  = 'SYSTEM\\ Current  Control  Set \\ Control \\ Time  Zone  Information ' key =  winreg .  Open  Key  Ex  (  winreg . HKEY LOCAL MACHINE , tz Reg  Key  ) local =   Reg  Key  Dict  ( key ) fix Standard  Time  = ( ( local [ ' Standard  Name ' ] == local [ ' Daylight  Name ' ] ) and ( local [ ' Standard  Bias ' ] == local [ ' Daylight  Bias ' ] ) ) key Name  = [ ' Standard  Name ' , ' Time  Zone  Key  Name ' ] [ ( sys . getwindowsversion ( ) >= ( 6 , ) ) ] standard Name  = local [ key Name  ] standard Name  =    Time  Zone  Key  Name  Workaround  ( standard Name  ) return  Time  Zone  Info  ( standard Name  , fix Standard  Time  ) 
def get ( quality name ) : found components = { } for part in quality name . lower ( ) . split ( ) : component =  registry . get ( part ) if ( not component ) : raise  Value  Error  ( ( u'`%s`  is  not  a  valid  quality  string' % part ) ) if ( component . type in found components ) : raise  Value  Error  ( ( u'`%s`  cannot  be  defined  twice  in  a  quality' % component . type ) ) found components [ component . type ] = component if ( not found components ) : raise  Value  Error  ( u' No   quality  specified' ) result =  Quality  ( ) for ( type , component ) in found components . items ( ) : setattr ( result , type , component ) return result 
def  compare epochs infos ( info1 , info2 , ind ) : info1 .  check consistency ( ) info2 .  check consistency ( ) if ( info1 [ 'nchan' ] != info2 [ 'nchan' ] ) : raise  Value  Error  ( ( "epochs[%d]['info']['nchan']  must  match" % ind ) ) if ( info1 [ 'bads' ] != info2 [ 'bads' ] ) : raise  Value  Error  ( ( "epochs[%d]['info']['bads']  must  match" % ind ) ) if ( info1 [ 'sfreq' ] != info2 [ 'sfreq' ] ) : raise  Value  Error  ( ( "epochs[%d]['info']['sfreq']  must  match" % ind ) ) if ( set ( info1 [ 'ch names' ] ) != set ( info2 [ 'ch names' ] ) ) : raise  Value  Error  ( ( "epochs[%d]['info']['ch names']  must  match" % ind ) ) if ( len ( info2 [ 'projs' ] ) != len ( info1 [ 'projs' ] ) ) : raise  Value  Error  ( 'SSP  projectors  in  epochs  files  must  be  the  same' ) if any ( ( ( not  proj equal ( p1 , p2 ) ) for ( p1 , p2 ) in zip ( info2 [ 'projs' ] , info1 [ 'projs' ] ) ) ) : raise  Value  Error  ( 'SSP  projectors  in  epochs  files  must  be  the  same' ) if ( ( ( info1 [ 'dev head t' ] is  None  ) != ( info2 [ 'dev head t' ] is  None  ) ) or ( ( info1 [ 'dev head t' ] is not  None  ) and ( not np . allclose ( info1 [ 'dev head t' ] [ 'trans' ] , info2 [ 'dev head t' ] [ 'trans' ] , rtol = 1e-06 ) ) ) ) : raise  Value  Error  ( ( "epochs[%d]['info']['dev head t']  must  match.   The   epochs  probably  come  from  different  runs,  and  are  therefore  associated  with  different  head  positions.   Manually   change  info['dev head t']  to  avoid  this  message  but  beware  that  this  means  the  MEG  sensors  will  not  be  properly  spatially  aligned.   See   mne.preprocessing.maxwell filter  to  realign  the  runs  to  a  common  head  position." % ind ) ) 
@ pytest . mark . parametrize ( u'icoo' , icrs coords ) def test icrs gcrs ( icoo ) : gcrscoo = icoo . transform to ( gcrs frames [ 0 ] ) icoo2 = gcrscoo . transform to ( ICRS ) assert allclose ( icoo . distance , icoo2 . distance ) assert allclose ( icoo . ra , icoo2 . ra ) assert allclose ( icoo . dec , icoo2 . dec ) assert isinstance ( icoo2 . data , icoo . data .   class   ) gcrscoo2 = icoo . transform to ( gcrs frames [ 1 ] ) assert ( not allclose ( gcrscoo . ra , gcrscoo2 . ra , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) assert ( not allclose ( gcrscoo . dec , gcrscoo2 . dec , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) gcrscoo3 = gcrscoo . transform to ( gcrs frames [ 0 ] ) assert allclose ( gcrscoo . ra , gcrscoo3 . ra ) assert allclose ( gcrscoo . dec , gcrscoo3 . dec ) gcrscoo4 = gcrscoo . transform to ( gcrs frames [ 1 ] ) assert ( not allclose ( gcrscoo4 . ra , gcrscoo . ra , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) assert ( not allclose ( gcrscoo4 . dec , gcrscoo . dec , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) gcrscoo5 = gcrscoo4 . transform to ( gcrs frames [ 0 ] ) assert allclose ( gcrscoo . ra , gcrscoo5 . ra , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) assert allclose ( gcrscoo . dec , gcrscoo5 . dec , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) gframe3 = GCRS ( obsgeoloc = ( [ 385000.0 , 0 , 0 ] * u . km ) , obsgeovel = ( ( [ 1 , 0 , 0 ] * u . km ) / u . s ) ) gcrscoo6 = icoo . transform to ( gframe3 ) assert ( not allclose ( gcrscoo . ra , gcrscoo6 . ra , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) assert ( not allclose ( gcrscoo . dec , gcrscoo6 . dec , rtol = 1e-08 , atol = ( 1e-10 * u . deg ) ) ) icooviag3 = gcrscoo6 . transform to ( ICRS ) assert allclose ( icoo . ra , icooviag3 . ra ) assert allclose ( icoo . dec , icooviag3 . dec ) 
def objtag ( accessing obj , accessed obj , * args , ** kwargs ) : return accessed obj . tags . get ( * args ) 
def save vocab ( count = [ ] , name = 'vocab.txt' ) : pwd = os . getcwd ( ) vocabulary size = len ( count ) with open ( os . path . join ( pwd , name ) , 'w' ) as f : for i in xrange ( vocabulary size ) : f . write ( ( '%s  %d\n' % ( tf . compat . as text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) ) print ( '%d  vocab  saved  to  %s  in  %s' % ( vocabulary size , name , pwd ) ) 
def dumps ( obj , skipkeys =  False  , ensure ascii =  True  , check circular =  True  , allow nan =  True  , cls =  None  , indent =  None  , separators =  None  , ** kw ) : if ( cls is  None  ) : cls = JSON Encoder  return cls ( skipkeys = skipkeys , ensure ascii = ensure ascii , check circular = check circular , allow nan = allow nan , indent = indent , separators = separators , ** kw ) . encode ( obj ) 
def parse color setting ( config string ) : if ( not config string ) : return PALETTES [ DEFAULT PALETTE ] parts = config string . lower ( ) . split ( ';' ) palette = PALETTES [ NOCOLOR PALETTE ] . copy ( ) for part in parts : if ( part in PALETTES ) : palette . update ( PALETTES [ part ] ) elif ( '=' in part ) : definition = { } ( role , instructions ) = part . split ( '=' ) role = role . upper ( ) styles = instructions . split ( ',' ) styles . reverse ( ) colors = styles . pop ( ) . split ( '/' ) colors . reverse ( ) fg = colors . pop ( ) if ( fg in color names ) : definition [ 'fg' ] = fg if ( colors and ( colors [ ( - 1 ) ] in color names ) ) : definition [ 'bg' ] = colors [ ( - 1 ) ] opts = tuple ( ( s for s in styles if ( s in opt dict . keys ( ) ) ) ) if opts : definition [ 'opts' ] = opts if ( ( role in PALETTES [ NOCOLOR PALETTE ] ) and definition ) : palette [ role ] = definition if ( palette == PALETTES [ NOCOLOR PALETTE ] ) : return  None  return palette 
def  check is not multitable ( values , model ) : used models = set ( ) for field in values : if isinstance ( field , sqlalchemy . orm . attributes .  Instrumented  Attribute  ) : used models . add ( field . class  ) elif isinstance ( field , six . string types ) : used models . add ( model ) else : raise exception .  Programming  Error  ( reason = 'DB   Conditional   update  -   Unknown   field  type,  must  be  string  or  ORM  field.' ) if ( len ( used models ) > 1 ) : raise exception .  Programming  Error  ( reason = 'DB   Conditional   update  -   Error   in  query,  multitable  updates  are  not  supported.' ) 
def tabs obsolete ( physical line ) : indent = indent match ( physical line ) . group ( 1 ) if indent . count ( ' DCTB ' ) : return ( indent . index ( ' DCTB ' ) , 'W191  indentation  contains  tabs' ) 
def commit message path ( ) : path = git . git path ( u'GIT COLA MSG' ) if core . exists ( path ) : return path return  None  
def  find ( root , thread count = 10 , relative =  False  , follow =  False  ) : threads = [ ] results = { } errors = { } done = threading .  Event  ( ) work = queue .  Queue  ( ) work . put ( ( os . path . abspath ( root ) , [ ] ) ) if ( not relative ) : root =  None  args = ( root , follow , done , work , results , errors ) for i in range ( thread count ) : t = threading .  Thread  ( target =  find worker , args = args ) t . daemon =  True  t . start ( ) threads . append ( t ) work . join ( ) done . set ( ) for t in threads : t . join ( ) return ( results , errors ) 
@ requires segment info def mode ( pl , segment info , override = { u'vicmd' : u'COMMND' , u'viins' : u'INSERT' } , default =  None  ) : mode = segment info . get ( u'mode' ,  None  ) if ( not mode ) : pl . debug ( u' No   mode  specified' ) return  None  default = ( default or segment info . get ( u'default mode' ,  None  ) ) if ( mode == default ) : return  None  try : return override [ mode ] except  Key  Error  : return mode . upper ( ) 
def  convert write result ( operation , command , result ) : affected = result . get ( 'n' , 0 ) res = { 'ok' : 1 , 'n' : affected } errmsg = result . get ( 'errmsg' , result . get ( 'err' , '' ) ) if errmsg : if result . get ( 'wtimeout' ) : res [ 'write Concern  Error ' ] = { 'errmsg' : errmsg , 'code' : 64 , 'err Info ' : { 'wtimeout' :  True  } } else : error = { 'index' : 0 , 'code' : result . get ( 'code' , 8 ) , 'errmsg' : errmsg } if ( 'err Info ' in result ) : error [ 'err Info ' ] = result [ 'err Info ' ] res [ 'write Errors ' ] = [ error ] return res if ( operation == 'insert' ) : res [ 'n' ] = len ( command [ 'documents' ] ) elif ( operation == 'update' ) : if ( 'upserted' in result ) : res [ 'upserted' ] = [ { 'index' : 0 , ' id' : result [ 'upserted' ] } ] elif ( ( result . get ( 'updated Existing ' ) is  False  ) and ( affected == 1 ) ) : update = command [ 'updates' ] [ 0 ]  id = update [ 'u' ] . get ( ' id' , update [ 'q' ] . get ( ' id' ) ) res [ 'upserted' ] = [ { 'index' : 0 , ' id' :  id } ] return res 
def   Create  Config  Dir  ( ) : directory = sqlcmd . DEFAULT CONFIG DIR if ( not os . access ( directory , ( ( os . R OK | os . W OK ) | os . X OK ) ) ) : old umask = os . umask ( 63 ) os . makedirs ( sqlcmd . DEFAULT CONFIG DIR ) os . umask ( old umask ) 
def get closest dir ( workdir ) : closest dir = '' for wdi in path split all ( workdir ) : if os . path . isdir ( os . path . join ( closest dir , wdi ) ) : closest dir = os . path . join ( closest dir , wdi ) else : break assert ( closest dir != workdir ) return ( closest dir , wdi ) 
def  get read region ( read name ) : return int ( read name [ 8 ] ) 
def zpad ( x , l ) : return ( ( '\x00' * max ( 0 , ( l - len ( x ) ) ) ) + x ) 
@ login required def preview handler ( request , usage key string , handler , suffix = '' ) : usage key =  Usage  Key  . from string ( usage key string ) if isinstance ( usage key , (  Aside  Usage  Key V1 ,  Aside  Usage  Key V2 ) ) : descriptor = modulestore ( ) . get item ( usage key . usage key ) for aside in descriptor . runtime . get asides ( descriptor ) : if ( aside . scope ids . block type == usage key . aside type ) : asides = [ aside ] instance = aside break else : descriptor = modulestore ( ) . get item ( usage key ) instance =  load preview module ( request , descriptor ) asides = [ ] req = django to webob request ( request ) try : resp = instance . handle ( handler , req , suffix ) except  No  Such  Handler  Error  : log . exception ( 'X Block   %s  attempted  to  access  missing  handler  %r' , instance , handler ) raise  Http 404 except  Not  Found  Error  : log . exception ( " Module   indicating  to  user  that  request  doesn't  exist" ) raise  Http 404 except  Processing  Error  : log . warning ( ' Module   raised  an  error  while  processing  AJAX  request' , exc info =  True  ) return  Http  Response  Bad  Request  ( ) except  Exception  : log . exception ( 'error  processing  ajax  call' ) raise modulestore ( ) . update item ( descriptor , request . user . id , asides = asides ) return webob to django response ( resp ) 
def escape ( text ) : def fixup ( m ) : ch = m . group ( 0 ) return ( ( '&#' + str ( ord ( ch ) ) ) + ';' ) text = re . sub ( '[^  -~]|[&"]' , fixup , text ) return ( text if isinstance ( text , str ) else str ( text ) ) 
@ jsonify def write record ( self , arg dict ) : cmd = [ 'xenstore-write' , ( '/local/domain/%(dom id)s/%(path)s' % arg dict ) , arg dict [ 'value' ] ]  run command ( cmd ) return arg dict [ 'value' ] 
def get yaml basepath ( ) : if (  handler dir is  None  ) : set builtins dir ( DEFAULT DIR ) return  handler dir 
def dbg ( x ) : return '' 
def load processed files ( path ) : processed files = { } with open ( path ) as input file : for line in input file . readlines ( ) : line = line . strip ( ) if ( not line ) : continue if ( '  ' not in line ) : raise  Type  Error  ( ( ' Malformed   line:  %s' % line ) ) ( path , timestamp ) = line . rsplit ( '  ' , 1 ) if ( not os . path . isabs ( path ) ) : raise  Type  Error  ( ( "'%s'  is  not  an  absolute  path" % path ) ) elif ( not timestamp . isdigit ( ) ) : raise  Type  Error  ( ( "'%s'  is  not  an  integer  timestamp" % timestamp ) ) processed files [ path ] = int ( timestamp ) return processed files 
def check for language ( lang code ) : for path in all locale paths ( ) : if ( gettext module . find ( u'django' , path , [ to locale ( lang code ) ] ) is not  None  ) : return  True  return  False  
def volume create ( name , size = 100 , snapshot =  None  , voltype =  None  , profile =  None  ) : conn =  auth ( profile ) return conn . volume create ( name , size , snapshot , voltype ) 
def require map reduce ( conn ) : try : import spidermonkey except  Base  Exception  : try : from ming import mim if ( hasattr ( conn , 'conn' ) and isinstance ( conn . conn , mim .  Connection  ) ) : import testtools raise testtools . testcase .  Test  Skipped  ( 'requires  spidermonkey' ) except  Import  Error  : import testtools raise testtools . testcase .  Test  Skipped  ( 'requires  mim' ) 
def parse date time ( d , t , network , date Only  =  False  ) : if ( not network dict ) : load network dict ( ) parsed time = time regex . search ( t ) network tz = get network timezone ( network ) hr = 0 m = 0 if parsed time : hr = try Int  ( parsed time . group ( u'hour' ) ) m = try Int  ( parsed time . group ( u'minute' ) ) ap = parsed time . group ( u'meridiem' ) ap = ( ap [ 0 ] . lower ( ) if ap else u'' ) if ( ( ap == u'a' ) and ( hr == 12 ) ) : hr -= 12 elif ( ( ap == u'p' ) and ( hr != 12 ) ) : hr += 12 hr = ( hr if ( 0 <= hr <= 23 ) else 0 ) m = ( m if ( 0 <= m <= 59 ) else 0 ) result = datetime . fromordinal ( max ( try Int  ( d ) , 1 ) ) return ( result . replace ( hour = hr , minute = m , tzinfo = network tz ) if ( not date Only  ) else result . replace ( tzinfo = network tz ) ) 
def facility Net  To  Ms  ( ) : a =  Tp  Pd  ( pd = 3 ) b =  Message  Type  ( mes Type  = 58 ) c =  Facility  ( ) packet = ( ( a / b ) / c ) return packet 
def validate labels ( db session , added categories , removed categories ) : add = { c . name for c in added categories if c . name } add all = ( 'all' in add ) add trash = ( 'trash' in add ) add spam = ( 'spam' in add ) if ( ( add all and ( add trash or add spam ) ) or ( add trash and add spam ) ) : raise  Input  Error  ( ' Only   one  of  "all",  "trash"  or  "spam"  can  be  added' ) remove = { c . name for c in removed categories if c . name } remove all = ( 'all' in remove ) remove trash = ( 'trash' in remove ) remove spam = ( 'spam' in remove ) if ( remove all and remove trash and remove spam ) : raise  Input  Error  ( '"all",  "trash"  and  "spam"  cannot  all  be  removed' ) 
def parse keywords ( strings = [ ] ) : keywords = { } for string in strings : if ( ':' in string ) : ( funcname , indices ) = string . split ( ':' ) else : ( funcname , indices ) = ( string ,  None  ) if ( funcname not in keywords ) : if indices : indices = tuple ( [ int ( x ) for x in indices . split ( ',' ) ] ) keywords [ funcname ] = indices return keywords 
def report ( tracking id , client id , requestable , extra info =  None  , extra headers =  None  ) : return [  request ( data , extra headers ) for ( data , extra headers ) in payloads ( tracking id , client id , requestable , extra info , extra headers ) ] 
def authorize ( ) : service = get oauth service ( ) ( request token , request token secret ) = service . get request token ( ) subprocess . call ( [ 'open' , service . get authorize url ( request token , ** constants . AUTHORIZE DATA ) ] ) return ( request token , request token secret ) 
def ext pillar ( minion id , pillar , conf ) : comps = conf . split ( ) profile =  None  if comps [ 0 ] : profile = comps [ 0 ] client = salt . utils . etcd util . get conn (   opts   , profile ) path = '/' if ( ( len ( comps ) > 1 ) and comps [ 1 ] . startswith ( 'root=' ) ) : path = comps [ 1 ] . replace ( 'root=' , '' ) path %= { 'minion id' : minion id } try : pillar = salt . utils . etcd util . tree ( client , path ) except  Key  Error  : log . error ( ' No   such  key  in  etcd  profile  {0}:  {1}' . format ( profile , path ) ) pillar = { } return pillar 
def remove Element  From  Dictionary  ( dictionary , key ) : if ( key in dictionary ) : del dictionary [ key ] 
def has progress ( toppath ) : state = progress read ( ) return ( toppath in state ) 
def s block start ( name , group =  None  , encoder =  None  , dep =  None  , dep value =  None  , dep values = [ ] , dep compare = '==' ) : block = blocks . block ( name , blocks . CURRENT , group , encoder , dep , dep value , dep values , dep compare ) blocks . CURRENT . push ( block ) return  True  
def ec2 credentials get ( user id =  None  , name =  None  , access =  None  , profile =  None  , ** connection args ) : kstone = auth ( profile , ** connection args ) ret = { } if name : for user in kstone . users . list ( ) : if ( user . name == name ) : user id = user . id break if ( not user id ) : return { ' Error ' : ' Unable   to  resolve  user  id' } if ( not access ) : return { ' Error ' : ' Access   key  is  required' } ec2 credentials = kstone . ec2 . get ( user id = user id , access = access , profile = profile , ** connection args ) ret [ ec2 credentials . user id ] = { 'user id' : ec2 credentials . user id , 'tenant' : ec2 credentials . tenant id , 'access' : ec2 credentials . access , 'secret' : ec2 credentials . secret } return ret 
def int power impl ( context , builder , sig , args ) : is integer = isinstance ( sig . args [ 0 ] , types .  Integer  ) tp = sig . return type zerodiv return =  get power zerodiv return ( context , tp ) def int power ( a , b ) : r = tp ( 1 ) a = tp ( a ) if ( b < 0 ) : invert =  True  exp = ( - b ) if ( exp < 0 ) : raise  Overflow  Error  if is integer : if ( a == 0 ) : if zerodiv return : return zerodiv return else : raise  Zero  Division  Error  ( '0  cannot  be  raised  to  a  negative  power' ) if ( ( a != 1 ) and ( a != ( - 1 ) ) ) : return 0 else : invert =  False  exp = b if ( exp > 65536 ) : return math . pow ( a , float ( b ) ) while ( exp != 0 ) : if ( exp & 1 ) : r *= a exp >>= 1 a *= a return ( ( 1.0 / r ) if invert else r ) res = context . compile internal ( builder , int power , sig , args ) return impl ret untracked ( context , builder , sig . return type , res ) 
def test vstack bytes ( ) : t = table .  Table  ( [ [ 'a' ] ] , names = [ 'a' ] ) assert ( t [ 'a' ] . itemsize == 1 ) t2 = table . vstack ( [ t , t ] ) assert ( len ( t2 ) == 2 ) assert ( t2 [ 'a' ] . itemsize == 1 ) 
def get Network  Country  Iso  ( ) : try : m Context  = autoclass ( 'android.content. Context ' ) python Activity  = autoclass ( 'org.renpy.android. Python  Service ' ) telephony Manager  = cast ( 'android.telephony. Telephony  Manager ' , python Activity  . m Service  . get System  Service  ( m Context  . TELEPHONY SERVICE ) ) network Country  Iso  = telephony Manager  . get Network  Country  Iso  ( ) return network Country  Iso  except  Exception  as e : return  None  
def read float ( fid ) : return  unpack simple ( fid , '>f4' , np . float32 ) 
def keygen ( keyfile =  None  ) : b = libnacl . secret .  Secret  Box  ( ) key = b . sk key = base64 . b64encode ( key ) if keyfile : if os . path . isfile ( keyfile ) : raise  Exception  ( 'file  already  found:  {0}' . format ( keyfile ) ) with salt . utils . fopen ( keyfile , 'w' ) as keyf : keyf . write ( key ) return 'saved:  {0}' . format ( keyfile ) return key 
def  modify tagids ( source , add =  True  ) : output = [ ] tagcount = 0 if ( not isinstance ( source ,  Html  Page  ) ) : source =  Html  Page  ( body = source ) for element in source . parsed body : if  must add tagid ( element ) : if add : element . attributes [ TAGID ] = str ( tagcount ) tagcount += 1 else : element . attributes . pop ( TAGID ,  None  ) output . append ( serialize tag ( element ) ) else : output . append ( source . body [ element . start : element . end ] ) return u'' . join ( output ) 
def safeimport ( path , forceload = 0 , cache = { } ) : try : if ( forceload and ( path in sys . modules ) ) : if ( path not in sys . builtin module names ) : subs = [ m for m in sys . modules if m . startswith ( ( path + '.' ) ) ] for key in ( [ path ] + subs ) : cache [ key ] = sys . modules [ key ] del sys . modules [ key ] module =   import   ( path ) except : ( exc , value , tb ) = info = sys . exc info ( ) if ( path in sys . modules ) : raise  Error  During  Import  ( sys . modules [ path ] .   file   , info ) elif ( exc is  Syntax  Error  ) : raise  Error  During  Import  ( value . filename , info ) elif ( ( exc is  Import  Error  ) and ( split ( lower ( str ( value ) ) ) [ : 2 ] == [ 'no' , 'module' ] ) ) : return  None  else : raise  Error  During  Import  ( path , sys . exc info ( ) ) for part in split ( path , '.' ) [ 1 : ] : try : module = getattr ( module , part ) except  Attribute  Error  : return  None  return module 
def find selected ( nodes ) : for node in nodes : if hasattr ( node , 'selected' ) : return node elif hasattr ( node , 'ancestor' ) : result = find selected ( node . children ) if result : return result 
def  subs ( value ) : subs = { 'group Of  Names ' : [ 'keystone Tenant ' , 'keystone Role ' , 'keystone Tenant  Role ' ] } if ( value in subs ) : return ( [ value ] + subs [ value ] ) return [ value ] 
def extract description ( texts ) : document = [ ] for text in texts : try : document . append ( text [ 'description' ] ) locale = text [ 'locale' ] break except  Key  Error  as e : logging . error ( ( ' Key  Error :  %s\n%s' % ( e , text ) ) ) return ( locale , '  ' . join ( document ) ) 
def service ( description , factory , reactor =  None  ) : if ( reactor is  None  ) : from twisted . internet import reactor svc =  Stream  Server  Endpoint  Service  ( endpoints . server From  String  ( reactor , description ) , factory ) svc .  raise Synchronously  =  True  return svc 
def get os args ( ) : if ( PY2 and WIN and (  initial argv hash ==  hash py argv ( ) ) ) : return  get windows argv ( ) return sys . argv [ 1 : ] 
def test coffee ( ) : data . coffee ( ) 
def parse boundary stream ( stream , max header size ) : chunk = stream . read ( max header size ) header end = chunk . find ( '\r\n\r\n' ) def  parse header ( line ) : ( main value pair , params ) = parse header ( line ) try : ( name , value ) = main value pair . split ( u':' , 1 ) except : raise  Value  Error  ( ( u' Invalid   header:  %r' % line ) ) return ( name , ( value , params ) ) if ( header end == ( - 1 ) ) : stream . unget ( chunk ) return ( RAW , { } , stream ) header = chunk [ : header end ] stream . unget ( chunk [ ( header end + 4 ) : ] ) TYPE = RAW outdict = { } for line in header . split ( '\r\n' ) : try : ( name , ( value , params ) ) =  parse header ( line ) except : continue if ( name == u'content-disposition' ) : TYPE = FIELD if params . get ( u'filename' ) : TYPE = FILE outdict [ name ] = ( value , params ) if ( TYPE == RAW ) : stream . unget ( chunk ) return ( TYPE , outdict , stream ) 
@ service ( DOMAIN , SERVICE FLASH ) def flash service ( hass , call ) : if ( not TARGET ID ) : return if core . is on ( hass , TARGET ID ) : core . turn off ( hass , TARGET ID ) time . sleep ( 10 ) core . turn on ( hass , TARGET ID ) else : core . turn on ( hass , TARGET ID ) time . sleep ( 10 ) core . turn off ( hass , TARGET ID ) 
def  cmp by local pref ( path1 , path2 ) : lp1 = path1 . get pattr ( BGP ATTR TYPE LOCAL PREF ) lp2 = path2 . get pattr ( BGP ATTR TYPE LOCAL PREF ) if ( not ( lp1 and lp2 ) ) : return  None  lp1 = lp1 . value lp2 = lp2 . value if ( lp1 > lp2 ) : return path1 elif ( lp2 > lp1 ) : return path2 else : return  None  
def  model oper ( oper , ** kwargs ) : return ( lambda left , right :   Compound  Model  Meta  .  from operator ( oper , left , right , ** kwargs ) ) 
@ pytest . fixture ( ) def filepath ( ) : def make filepath ( filename ) : return os . path . join ( FILES DIR , filename ) return make filepath 
def compute Chunk  Height  Map  ( materials , blocks ,  Height  Map  =  None  ) : light Absorption  = materials . light Absorption  [ blocks ] heights = extract Heights  ( light Absorption  ) heights = heights . swapaxes ( 0 , 1 ) if (  Height  Map  is  None  ) : return heights . astype ( 'uint8' ) else :  Height  Map  [ : ] = heights return  Height  Map  
def set transparent torification ( new transparent torification ) : global transparent torification stay open = new transparent torification 
@ command ( 'c\\s?(\\d{1,4})' ) def comments ( number ) : if ( g . browse mode == 'normal' ) : item = g . model [ ( int ( number ) - 1 ) ] fetch comments ( item ) else : g . content = generate songlist display ( ) g . message = ' Comments   only  available  for  video  items' 
def create group ( group name , path =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : if ( not path ) : path = '/' if get group ( group name , region = region , key = key , keyid = keyid , profile = profile ) : return  True  conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) try : conn . create group ( group name , path ) log . info ( ' Created   group  :  {0}.' . format ( group name ) ) return  True  except boto . exception .  Boto  Server  Error  as e : log . debug ( e ) msg = ' Failed   to  create  group  {0}.' log . error ( msg . format ( group name ) ) return  False  
def full2sparse clipped ( vec , topn , eps = 1e-09 ) : if ( topn <= 0 ) : return [ ] vec = np . asarray ( vec , dtype = float ) nnz = np . nonzero ( ( abs ( vec ) > eps ) ) [ 0 ] biggest = nnz . take ( argsort ( abs ( vec ) . take ( nnz ) , topn , reverse =  True  ) ) return list ( zip ( biggest , vec . take ( biggest ) ) ) 
def list vms ( name =  None  , info =  False  , all =  False  , args =  None  , runas =  None  , template =  False  ) : if ( args is  None  ) : args = [ ] else : args =  normalize args ( args ) if name : args . extend ( [ name ] ) if info : args . append ( '--info' ) if all : args . append ( '--all' ) if template : args . append ( '--template' ) return prlctl ( 'list' , args , runas = runas ) 
def make Stack  ( element ) : layers = [ ] for child in element . child Nodes  : if ( child . node Type  == child . ELEMENT NODE ) : if ( child . tag Name  == 'stack' ) : stack = make Stack  ( child ) layers . append ( stack ) elif ( child . tag Name  == 'layer' ) : layer = make Layer  ( child ) layers . append ( layer ) else : raise  Exception  ( ( ' Unknown   element  "%s"' % child . tag Name  ) ) print >> sys . stderr , ( ' Making   a  stack  with  %d  layers' % len ( layers ) ) return  Stack  ( layers ) 
def plot snr estimate ( evoked , inv , show =  True  ) : import matplotlib . pyplot as plt from . . minimum norm import estimate snr ( snr , snr est ) = estimate snr ( evoked , inv , verbose =  True  ) ( fig , ax ) = plt . subplots ( 1 , 1 ) lims = np . concatenate ( [ evoked . times [ [ 0 , ( - 1 ) ] ] , [ ( - 1 ) , snr est . max ( ) ] ] ) ax . plot ( [ 0 , 0 ] , lims [ 2 : ] , 'k:' ) ax . plot ( lims [ : 2 ] , [ 0 , 0 ] , 'k:' ) ax . plot ( evoked . times , snr est , color = [ 0.0 , 0.6 , 0.5 ] ) ax . plot ( evoked . times , snr , color = [ 0.8 , 0.4 , 0.0 ] ) ax . set xlim ( lims [ : 2 ] ) ax . set ylim ( lims [ 2 : ] ) ax . set ylabel ( 'SNR' ) ax . set xlabel ( ' Time   (sec)' ) if ( evoked . comment is not  None  ) : ax . set title ( evoked . comment ) plt . draw ( ) plt show ( show ) return fig 
def deploy ( config , model fn , args =  None  , kwargs =  None  , optimizer =  None  , summarize gradients =  False  ) : summaries = set ( tf . get collection ( tf .  Graph  Keys  . SUMMARIES ) ) clones = create clones ( config , model fn , args , kwargs ) first clone = clones [ 0 ] update ops = tf . get collection ( tf .  Graph  Keys  . UPDATE OPS , first clone . scope ) train op =  None  total loss =  None  with tf . device ( config . optimizer device ( ) ) : if optimizer : with tf . device ( config . variables device ( ) ) : global step = slim . get or create global step ( ) ( total loss , clones gradients ) = optimize clones ( clones , optimizer ) if clones gradients : if summarize gradients : summaries |= set (  add gradients summaries ( clones gradients ) ) grad updates = optimizer . apply gradients ( clones gradients , global step = global step ) update ops . append ( grad updates ) update op = tf . group ( * update ops ) train op = control flow ops . with dependencies ( [ update op ] , total loss , name = 'train op' ) else : clones losses = [ ] regularization losses = tf . get collection ( tf .  Graph  Keys  . REGULARIZATION LOSSES ) for clone in clones : with tf . name scope ( clone . scope ) : clone loss =  gather clone loss ( clone , len ( clones ) , regularization losses ) if ( clone loss is not  None  ) : clones losses . append ( clone loss ) regularization losses =  None  if clones losses : total loss = tf . add n ( clones losses , name = 'total loss' ) summaries |= set ( tf . get collection ( tf .  Graph  Keys  . SUMMARIES , first clone . scope ) ) if ( total loss is not  None  ) : summaries . add ( tf . scalar summary ( 'total loss' , total loss , name = 'total loss' ) ) if summaries : summary op = tf . merge summary ( list ( summaries ) , name = 'summary op' ) else : summary op =  None  return  Deployed  Model  ( train op , summary op , total loss , clones ) 
@ sopel . module . event ( events . RPL WELCOME , events . RPL LUSERCLIENT ) @ sopel . module . rule ( u'.*' ) @ sopel . module . thread (  False  ) @ sopel . module . unblockable def startup ( bot , trigger ) : if bot . connection registered : return bot . connection registered =  True  auth after register ( bot ) modes = bot . config . core . modes bot . write ( ( u'MODE  ' , ( u'%s  +%s' % ( bot . nick , modes ) ) ) ) bot . memory [ u'retry join' ] = dict ( ) if bot . config . core . throttle join : throttle rate = int ( bot . config . core . throttle join ) channels joined = 0 for channel in bot . config . core . channels : channels joined += 1 if ( not ( channels joined % throttle rate ) ) : time . sleep ( 1 ) bot . join ( channel ) else : for channel in bot . config . core . channels : bot . join ( channel ) if ( ( not bot . config . core . owner account ) and ( u'account-tag' in bot . enabled capabilities ) and ( u'@' not in bot . config . core . owner ) ) : msg = u' This   network  supports  using  network  services  to  identify  you  as  my  owner,  rather  than  just  matching  your  nickname.   This   is  much  more  secure.   If   you\'d  like  to  do  this,  make  sure  you\'re  logged  in  and  reply  with  "{}useserviceauth"' . format ( bot . config . core . help prefix ) bot . msg ( bot . config . core . owner , msg ) 
def haddr to str ( addr ) : if ( addr is  None  ) : return ' None ' try : return addrconv . mac . bin to text ( addr ) except : raise  Assertion  Error  
def avail images ( call =  None  ) : if ( call == 'action' ) : raise  Salt  Cloud  System  Exit  ( ' The   avail images  function  must  be  called  with  -f  or  --function,  or  with  the  --list-images  option' ) ( server , user , password ) =  get xml rpc ( ) auth = ':' . join ( [ user , password ] ) image pool = server . one . imagepool . info ( auth , ( - 2 ) , ( - 1 ) , ( - 1 ) ) [ 1 ] images = { } for image in  get xml ( image pool ) : images [ image . find ( 'NAME' ) . text ] =  xml to dict ( image ) return images 
def replace all ( text , terms ) : for (  from ,  to ) in terms . items ( ) : text = text . replace (  from ,  to ) return text 
def test mro super ( ) : class F : def meth ( self ) : return 'F' class G : pass def gmeth ( self ) : return 'G' class A ( object , ) : def meth ( self ) : if hasattr ( super ( A , self ) , 'meth' ) : return ( 'A' + super ( A , self ) . meth ( ) ) else : return 'A' class B ( A , ) : def   init   ( self ) : self .   super = super ( B , self ) super ( B , self ) .   init   ( ) def meth ( self ) : return ( 'B' + self .   super . meth ( ) ) class C ( A , ) : def   init   ( self ) : self .   super = super ( C , self ) super ( C , self ) .   init   ( ) def meth ( self ) : return ( 'C' + self .   super . meth ( ) ) class D ( C , B , ) : def meth ( self ) : return ( 'D' + super ( D , self ) . meth ( ) )  Are  Equal  ( D ( ) . meth ( ) , 'DCBA' ) class D ( C , F , B , ) : def meth ( self ) : return ( 'D' + super ( D , self ) . meth ( ) )  Are  Equal  ( D .   mro   , ( D , C , F , B , A , object ) )  Are  Equal  ( D ( ) . meth ( ) , 'DCF' ) class D ( C , B , F , ) : def meth ( self ) : return ( 'D' + super ( D , self ) . meth ( ) )  Are  Equal  ( D .   mro   , ( D , C , B , A , object , F ) )  Are  Equal  ( D ( ) . meth ( ) , 'DCBAF' ) class D ( C , B , G , ) : def meth ( self ) : return ( 'D' + super ( D , self ) . meth ( ) ) d = D ( ) d . meth = type ( F . meth ) ( gmeth , d , G )  Are  Equal  ( d . meth ( ) , 'G' ) 
def test preprocessor simple ( ) : obj = macroexpand ( tokenize ( '(test  "one"  "two")' ) [ 0 ] ,  Hy AST Compiler  (   name   ) ) assert ( obj ==  Hy  List  ( [ 'one' , 'two' ] ) ) assert ( type ( obj ) ==  Hy  List  ) 
def set Privacy  List  ( disp , list ) : resp = disp .  Send  And  Wait  For  Response  (  Iq  ( 'set' , NS PRIVACY , payload = [ list ] ) ) if is Result  Node  ( resp ) : return 1 
def test bc sk estimator ( ) : check estimator (  Balance  Cascade  ) 
def to progress instance ( progress ) : if callable ( progress ) : return  Callable  Remote  Progress  ( progress ) elif ( progress is  None  ) : return  Remote  Progress  ( ) else : return progress 
def make with init tests ( record type , kwargs , expected defaults =  None  ) : if ( expected defaults is  None  ) : expected defaults = { } unknown defaults = ( set ( expected defaults . keys ( ) ) - set ( kwargs . keys ( ) ) ) if unknown defaults : raise  Type  Error  ( 'expected defaults  contained  the  following  unrecognized  keys:  {}' . format ( tuple ( unknown defaults ) ) ) required kwargs = kwargs . copy ( ) for k in expected defaults . keys ( ) : required kwargs . pop ( k ) class  With  Init  Tests  (  Test  Case  , ) : '\n                 Tests   for  classes  decorated  with  ``with init``.\n                ' def test init ( self ) : '\n                         The   record  type  accepts  keyword  arguments  which  are  exposed  as\n                        public  attributes.\n                        ' record = record type ( ** kwargs ) self . assert Equal  ( kwargs . values ( ) , [ getattr ( record , key ) for key in kwargs . keys ( ) ] ) def test optional arguments ( self ) : '\n                         The   record  type  initialiser  has  arguments  which  may  be  omitted.\n                        ' try : record = record type ( ** required kwargs ) except  Value  Error  as e : self . fail ( ( ' One   of  the  following  arguments  was  expected  to  be  optional  but  appears  to  be  required:  %r.   Error   was:  %r' % ( expected defaults . keys ( ) , e ) ) ) self . assert Equal  ( required kwargs . values ( ) , [ getattr ( record , key ) for key in required kwargs . keys ( ) ] ) def test optional defaults ( self ) : '\n                         The   optional  arguments  have  the  expected  defaults.\n                        ' try : record = record type ( ** required kwargs ) except  Value  Error  as e : self . fail ( ( ' One   of  the  following  arguments  was  expected  to  be  optional  but  appears  to  be  required:  %r.   Error   was:  %r' % ( expected defaults . keys ( ) , e ) ) ) self . assert Equal  ( expected defaults . values ( ) , [ getattr ( record , key ) for key in expected defaults . keys ( ) ] ) return  With  Init  Tests  
def robust scale ( X , axis = 0 , with centering =  True  , with scaling =  True  , quantile range = ( 25.0 , 75.0 ) , copy =  True  ) : s =  Robust  Scaler  ( with centering = with centering , with scaling = with scaling , quantile range = quantile range , copy = copy ) if ( axis == 0 ) : return s . fit transform ( X ) else : return s . fit transform ( X . T ) . T 
def filter section ( context , section ) : return  False  
def get Twist  Precision  ( element Node  ) : return get Cascade  Float  Without  Self  ( 5.0 , element Node  , 'twist Precision ' ) 
def test elemwise4 ( ) : shape = ( 3 , 4 ) a = tcn . shared constructor ( theano .  asarray ( numpy . random . rand ( * shape ) , dtype = 'float32' ) , 'a' ) b = tensor . fvector ( ) c = tensor . fvector ( ) f = pfunc ( [ b , c ] , [ ] , updates = [ ( a , ( a + ( b . dimshuffle ( 'x' , 0 ) * c . dimshuffle ( 0 , 'x' ) ) ) ) ] , mode = mode with gpu ) has elemwise =  False  for ( i , node ) in enumerate ( f . maker . fgraph . toposort ( ) ) : has elemwise = ( has elemwise or isinstance ( node . op , tensor .  Elemwise  ) ) assert ( not has elemwise ) f ( theano .  asarray ( numpy . random . rand ( 4 ) , dtype = 'float32' ) , theano .  asarray ( numpy . random . rand ( 3 ) , dtype = 'float32' ) ) 
def test fnpickling many ( tmpdir ) : from ... . tests . helper import pytest fn = str ( tmpdir . join ( 'test3.pickle' ) ) obj3 = 328.3432 obj4 = 'blahblahfoo' fnpickle ( obj3 , fn ) fnpickle ( obj4 , fn , append =  True  ) res = fnunpickle ( fn , number = ( - 1 ) ) assert ( len ( res ) == 2 ) assert ( res [ 0 ] == obj3 ) assert ( res [ 1 ] == obj4 ) fnpickle ( obj4 , fn , append =  True  ) res = fnunpickle ( fn , number = 2 ) assert ( len ( res ) == 2 ) with pytest . raises ( EOF Error  ) : fnunpickle ( fn , number = 5 ) 
def  mark every path ( markevery , tpath , affine , ax transform ) : ( codes , verts ) = ( tpath . codes , tpath . vertices ) def  slice or none ( in v , slc ) : u'\n                 Helper   function  to  cope  with  `codes`  being  an\n                ndarray  or  ` None `\n                ' if ( in v is  None  ) : return  None  return in v [ slc ] if isinstance ( markevery , float ) : markevery = ( 0.0 , markevery ) elif isinstance ( markevery , int ) : markevery = ( 0 , markevery ) if isinstance ( markevery , tuple ) : if ( len ( markevery ) != 2 ) : raise  Value  Error  ( ( u'`markevery`  is  a  tuple  but  its  len  is  not  2;  markevery=%s' % ( markevery , ) ) ) ( start , step ) = markevery if isinstance ( step , int ) : if ( not isinstance ( start , int ) ) : raise  Value  Error  ( ( u'`markevery`  is  a  tuple  with  len  2  and  second  element  is  an  int,  but  the  first  element  is  not  an  int;  markevery=%s' % ( markevery , ) ) ) return  Path  ( verts [ slice ( start ,  None  , step ) ] ,  slice or none ( codes , slice ( start ,  None  , step ) ) ) elif isinstance ( step , float ) : if ( not ( isinstance ( start , int ) or isinstance ( start , float ) ) ) : raise  Value  Error  ( ( u'`markevery`  is  a  tuple  with  len  2  and  second  element  is  a  float,  but  the  first  element  is  not  a  float  or  an  int;  markevery=%s' % ( markevery , ) ) ) disp coords = affine . transform ( tpath . vertices ) delta = np . empty ( ( len ( disp coords ) , 2 ) , dtype = float ) delta [ 0 , : ] = 0.0 delta [ 1 : , : ] = ( disp coords [ 1 : , : ] - disp coords [ : ( - 1 ) , : ] ) delta = np . sum ( ( delta ** 2 ) , axis = 1 ) delta = np . sqrt ( delta ) delta = np . cumsum ( delta ) scale = ax transform . transform ( np . array ( [ [ 0 , 0 ] , [ 1 , 1 ] ] ) ) scale = np . diff ( scale , axis = 0 ) scale = np . sum ( ( scale ** 2 ) ) scale = np . sqrt ( scale ) marker delta = np . arange ( ( start * scale ) , delta [ ( - 1 ) ] , ( step * scale ) ) inds = np . abs ( ( delta [ np . newaxis , : ] - marker delta [ : , np . newaxis ] ) ) inds = inds . argmin ( axis = 1 ) inds = np . unique ( inds ) return  Path  ( verts [ inds ] ,  slice or none ( codes , inds ) ) else : raise  Value  Error  ( ( u'`markevery`  is  a  tuple  with  len  2,  but  its  second  element  is  not  an  int  or  a  float;  markevery=%s' % ( markevery , ) ) ) elif isinstance ( markevery , slice ) : return  Path  ( verts [ markevery ] ,  slice or none ( codes , markevery ) ) elif iterable ( markevery ) : try : return  Path  ( verts [ markevery ] ,  slice or none ( codes , markevery ) ) except (  Value  Error  ,  Index  Error  ) : raise  Value  Error  ( ( u'`markevery`  is  iterable  but  not  a  valid  form  of  numpy  fancy  indexing;  markevery=%s' % ( markevery , ) ) ) else : raise  Value  Error  ( ( u' Value   of  `markevery`  is  not  recognized;  markevery=%s' % ( markevery , ) ) ) 
def  clip warp output ( input image , output image , order , mode , cval , clip ) : if ( clip and ( order != 0 ) ) : min val = input image . min ( ) max val = input image . max ( ) preserve cval = ( ( mode == 'constant' ) and ( not ( min val <= cval <= max val ) ) ) if preserve cval : cval mask = ( output image == cval ) np . clip ( output image , min val , max val , out = output image ) if preserve cval : output image [ cval mask ] = cval 
def worker ( options ) : worker Pid  = os . getpid ( ) if ( not options . noaffinity ) : p = psutil .  Process  ( worker Pid  ) print 'affinity  [before]' , p . cpu affinity ( ) p . cpu affinity ( [ options . cpuid ] ) print 'affinity  [after]' , p . cpu affinity ( ) factory =  Echo  Server  Factory  ( options . wsuri ) reactor . adopt Stream  Port  ( options . fd , AF INET , factory ) if ( not options . silence ) : print ( ' Worker   started  on  PID  %s  using  factory  %s  and  protocol  %s' % ( worker Pid  , factory , factory . protocol ) ) if options . profile : statprof . reset ( PROFILER FREQ ) statprof . start ( ) if ( not options . silence ) : def stat ( ) : if options . profile : statprof . stop ( ) output =  String IO .  String IO ( ) output . write ( ( ( '-' * 80 ) + '\n' ) ) output . write ( ( ' Worker    Statistics   (PID  %s)\n\n%s' % ( worker Pid  , factory . stats . stats ( ) ) ) ) if options . profile : output . write ( '\n' ) statprof . display ( output ) output . write ( ( ( '-' * 80 ) + '\n\n' ) ) sys . stdout . write ( output . getvalue ( ) ) if options . profile : statprof . reset ( PROFILER FREQ ) statprof . start ( ) reactor . call Later  ( options . interval , stat ) reactor . call Later  ( options . interval , stat ) if  False  : import c Profile  print 'RUNNING  c Profile ' c Profile  . run ( 'reactor.run()' ) else : reactor . run ( ) 
@ app . route ( '/sms/receive' , methods = [ 'POST' ] ) def receive sms ( ) : sender = request . values . get ( ' From ' ) body = request . values . get ( ' Body ' ) message = ' Hello ,  {},  you  said:  {}' . format ( sender , body ) response = twiml .  Response  ( ) response . message ( message ) return ( str ( response ) , 200 , { ' Content - Type ' : 'application/xml' } ) 
def diop quadratic ( eq , param = symbols ( 't' , integer =  True  ) ) : ( var , coeff , diop type ) = classify diop ( eq ,  dict =  False  ) if ( diop type == 'binary quadratic' ) : return  diop quadratic ( var , coeff , param ) 
def  render login template ( login url , continue url , email , admin ) : if email : login message = ' Logged   in' else : login message = ' Not   logged  in' email = 'test@example.com' admin checked = ( 'checked' if admin else '' ) template dict = { 'email' : cgi . escape ( email , quote =  True  ) , 'admin checked' : admin checked , 'login message' : login message , 'login url' : cgi . escape ( login url , quote =  True  ) , 'continue url' : cgi . escape ( continue url , quote =  True  ) } return (  LOGIN TEMPLATE % template dict ) 
def create imagefile ( options , filename , latlon , ground width , path objs , mission obj , fence obj , width = 600 , height = 600 ) : mt = mp tile . MP Tile  ( service = options . service ) map img = mt . area to image ( latlon [ 0 ] , latlon [ 1 ] , width , height , ground width ) while ( mt . tiles pending ( ) > 0 ) : print ( ' Waiting   on  %u  tiles' % mt . tiles pending ( ) ) time . sleep ( 1 ) map img = mt . area to image ( latlon [ 0 ] , latlon [ 1 ] , width , height , ground width ) pixmapper = functools . partial ( pixel coords , ground width = ground width , mt = mt , topleft = latlon , width = width ) for path obj in path objs : path obj . draw ( map img , pixmapper ,  None  ) if ( mission obj is not  None  ) : for m in mission obj : m . draw ( map img , pixmapper ,  None  ) if ( fence obj is not  None  ) : fence obj . draw ( map img , pixmapper ,  None  ) map img = cv2 . cvt Color  ( map img , cv2 . COLOR BGR2RGB ) cv2 . imwrite ( filename , map img ) 
def createoutputdirs ( outputs ) : for output in list ( outputs . values ( ) ) : dirname = os . path . dirname ( output ) if ( not os . path . isdir ( dirname ) ) : os . makedirs ( dirname ) 
def flocker volume options ( cls ) : original parameters = getattr ( cls , 'opt Parameters ' , [ ] ) cls . opt Parameters  = ( original parameters + [ [ 'config' ,  None  , DEFAULT CONFIG PATH . path , ' The   path  to  the   Flocker   volume  configuration  file,  containing  the  node  ID  of  the   Flocker   volume  service  on  this  node.   This   file  will  be  created  if  it  does  not  already  exist.' ] , [ 'pool' ,  None  , FLOCKER POOL , ' The   ZFS  pool  to  use  for  volumes.' ] , [ 'mountpoint' ,  None  , FLOCKER MOUNTPOINT . path , ' The   path  where  ZFS  filesystems  will  be  mounted.' ] ] ) original post Options  = cls . post Options  def post Options  ( self ) : self [ 'config' ] =  File  Path  ( self [ 'config' ] ) original post Options  ( self ) cls . post Options  = post Options  return cls 
def encode ( data , version = 'a' ) : assert isinstance ( version , str ) if ( version == 'a' ) : return ( 'a' + '' . join (  a encode mapping [ type ( data ) ] ( data ,  a encode mapping ) ) ) raise  Value  Error  ( ' Unknown   encode  version' ) 
def is dyad ( frac ) : if ( isinstance ( frac , numbers .  Integral  ) and ( frac >= 0 ) ) : return  True  elif ( isinstance ( frac ,  Fraction  ) and ( frac >= 0 ) and is power2 ( frac . denominator ) ) : return  True  else : return  False  
@ world . absorb def verify setting entry ( setting , display name , value , explicitly set ) : label element = setting . find by css ( '.setting-label' ) [ 0 ] assert equal ( display name , label element . html . strip ( ) ) label for = label element [ 'for' ] if ( setting . has class ( 'metadata-list-enum' ) or setting . has class ( 'metadata-dict' ) or setting . has class ( 'metadata-video-translations' ) ) : list value = ',  ' . join ( ( ele . value for ele in setting . find by css ( '.list-settings-item' ) ) ) assert equal ( value , list value ) elif setting . has class ( 'metadata-videolist-enum' ) : list value = ',  ' . join ( ( ele . find by css ( 'input' ) [ 0 ] . value for ele in setting . find by css ( '.videolist-settings-item' ) ) ) assert equal ( value , list value ) else : assert equal ( value , setting . find by id ( label for ) . value ) if ( not setting . has class ( 'metadata-videolist-enum' ) ) : setting Clear  Button  = setting . find by css ( '.setting-clear' ) [ 0 ] assert equal ( explicitly set , setting Clear  Button  . has class ( 'active' ) ) assert equal ( ( not explicitly set ) , setting Clear  Button  . has class ( 'inactive' ) ) 
def get profile model ( ) : if ( not getattr ( settings , u'ACCOUNTS PROFILE MODEL' ,  None  ) ) : raise  Profile  Not  Configured  try : return apps . get model ( settings . ACCOUNTS PROFILE MODEL ) except  Value  Error  : raise  Improperly  Configured  ( u"ACCOUNTS PROFILE MODEL  must  be  of  the  form  'app label.model name'" ) except  Lookup  Error  : raise  Improperly  Configured  ( ( u"ACCOUNTS PROFILE MODEL  refers  to  model  '%s'  that  has  not  been  installed" % settings . ACCOUNTS PROFILE MODEL ) ) 
def changed ( * a , ** kw ) : item = a [ 0 ] if ( type ( item ) != dict ) : raise errors .  Ansible  Filter  Error  ( '|changed  expects  a  dictionary' ) if ( not ( 'changed' in item ) ) : changed =  False  if ( ( 'results' in item ) and ( type ( item [ 'results' ] ) == list ) and ( type ( item [ 'results' ] [ 0 ] ) == dict ) ) : for result in item [ 'results' ] : changed = ( changed or result . get ( 'changed' ,  False  ) ) else : changed = item . get ( 'changed' ,  False  ) return changed 
def jsonify ( obj , builtin types = ( numbers .  Real  , string t ) , key =  None  , keyfilter =  None  , unknown type filter =  None  ) : from kombu . abstract import  Object  as  Kombu  Dict  Type   jsonify = partial ( jsonify , builtin types = builtin types , key = key , keyfilter = keyfilter , unknown type filter = unknown type filter ) if isinstance ( obj ,  Kombu  Dict  Type  ) : obj = obj . as dict ( recurse =  True  ) if ( ( obj is  None  ) or isinstance ( obj , builtin types ) ) : return obj elif isinstance ( obj , ( tuple , list ) ) : return [  jsonify ( v ) for v in obj ] elif isinstance ( obj , dict ) : return { k :  jsonify ( v , key = k ) for ( k , v ) in items ( obj ) if ( keyfilter ( k ) if keyfilter else 1 ) } elif isinstance ( obj , ( datetime . date , datetime . time ) ) : return  datetime to json ( obj ) elif isinstance ( obj , datetime . timedelta ) : return str ( obj ) else : if ( unknown type filter is  None  ) : raise  Value  Error  ( u' Unsupported   type:  {0!r}  {1!r}  (parent:  {2})' . format ( type ( obj ) , obj , key ) ) return unknown type filter ( obj ) 
def present ( dbname , name , owner =  None  , db user =  None  , db password =  None  , db host =  None  , db port =  None  ) : ret = { 'dbname' : dbname , 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : ' Schema   {0}  is  already  present  in  database  {1}' . format ( name , dbname ) } db args = { 'db user' : db user , 'db password' : db password , 'db host' : db host , 'db port' : db port } schema attr =   salt   [ 'postgres.schema get' ] ( dbname , name , ** db args ) cret =  None  if ( schema attr is  None  ) : cret =   salt   [ 'postgres.schema create' ] ( dbname , name , owner = owner , ** db args ) else : msg = ' Schema   {0}  already  exists  in  database  {1}' cret =  None  if cret : msg = ' Schema   {0}  has  been  created  in  database  {1}' ret [ 'result' ] =  True  ret [ 'changes' ] [ name ] = ' Present ' elif ( cret is not  None  ) : msg = ' Failed   to  create  schema  {0}  in  database  {1}' ret [ 'result' ] =  False  else : msg = ' Schema   {0}  already  exists  in  database  {1}' ret [ 'result' ] =  True  ret [ 'comment' ] = msg . format ( name , dbname ) return ret 
def degrees ( B , nodes , weight =  None  ) : bottom = set ( nodes ) top = ( set ( B ) - bottom ) return ( B . degree ( top , weight ) , B . degree ( bottom , weight ) ) 
def test smote bad ratio ( ) : ratio = ( - 1.0 ) smote = SMOTE ( ratio = ratio , random state = RND SEED ) assert raises (  Value  Error  , smote . fit , X , Y ) ratio = 100.0 smote = SMOTE ( ratio = ratio , random state = RND SEED ) assert raises (  Value  Error  , smote . fit , X , Y ) ratio = 'rnd' smote = SMOTE ( ratio = ratio , random state = RND SEED ) assert raises (  Value  Error  , smote . fit , X , Y ) ratio = [ 0.5 , 0.5 ] smote = SMOTE ( ratio = ratio , random state = RND SEED ) assert raises (  Value  Error  , smote . fit , X , Y ) 
def reshape axes ( axes , shape , newshape ) : if ( len ( axes ) != len ( shape ) ) : raise  Value  Error  ( 'axes  do  not  match  shape' ) if ( product ( shape ) != product ( newshape ) ) : raise  Value  Error  ( ( 'can  not  reshape  %s  to  %s' % ( shape , newshape ) ) ) if ( ( not axes ) or ( not newshape ) ) : return '' lendiff = max ( 0 , ( len ( shape ) - len ( newshape ) ) ) if lendiff : newshape = ( newshape + ( ( 1 , ) * lendiff ) ) i = ( len ( shape ) - 1 ) prodns = 1 prods = 1 result = [ ] for ns in newshape [ : : ( - 1 ) ] : prodns *= ns while ( ( i > 0 ) and ( shape [ i ] == 1 ) and ( ns != 1 ) ) : i -= 1 if ( ( ns == shape [ i ] ) and ( prodns == ( prods * shape [ i ] ) ) ) : prods *= shape [ i ] result . append ( axes [ i ] ) i -= 1 else : result . append ( 'Q' ) return '' . join ( reversed ( result [ lendiff : ] ) ) 
def list keys ( user =  None  , gnupghome =  None  ) :  keys = [ ] for  key in  list keys ( user , gnupghome ) : tmp = { 'keyid' :  key [ 'keyid' ] , 'fingerprint' :  key [ 'fingerprint' ] , 'uids' :  key [ 'uids' ] } expires =  key . get ( 'expires' ,  None  ) date =  key . get ( 'date' ,  None  ) length =  key . get ( 'length' ,  None  ) owner trust =  key . get ( 'ownertrust' ,  None  ) trust =  key . get ( 'trust' ,  None  ) if expires : tmp [ 'expires' ] = time . strftime ( '%Y-%m-%d' , time . localtime ( float (  key [ 'expires' ] ) ) ) if date : tmp [ 'created' ] = time . strftime ( '%Y-%m-%d' , time . localtime ( float (  key [ 'date' ] ) ) ) if length : tmp [ 'key Length ' ] =  key [ 'length' ] if owner trust : tmp [ 'owner Trust ' ] = LETTER TRUST DICT [  key [ 'ownertrust' ] ] if trust : tmp [ 'trust' ] = LETTER TRUST DICT [  key [ 'trust' ] ]  keys . append ( tmp ) return  keys 
def set log format ( log format , server =  DEFAULT SERVER ) : setting = ' Log  Plugin  Clsid ' log format types = get log format types ( ) format id = log format types . get ( log format ,  None  ) if ( not format id ) : message = " Invalid   log  format  '{0}'  specified.   Valid   formats:  {1}" . format ( log format , log format types . keys ( ) ) raise  Salt  Invocation  Error  ( message )  LOG . debug ( " Id   for  '%s'  found:  %s" , log format , format id ) current log format = get log format ( server ) if ( log format == current log format ) :  LOG . debug ( '%s  already  contains  the  provided  format.' , setting ) return  True   set wmi setting ( 'I Is  Smtp  Server  Setting ' , setting , format id , server ) new log format = get log format ( server ) ret = ( log format == new log format ) if ret :  LOG . debug ( ' Setting   %s  configured  successfully:  %s' , setting , log format ) else :  LOG . error ( ' Unable   to  configure  %s  with  value:  %s' , setting , log format ) return ret 
def  fit binary ( estimator , X , y , classes =  None  ) : unique y = np . unique ( y ) if ( len ( unique y ) == 1 ) : if ( classes is not  None  ) : if ( y [ 0 ] == ( - 1 ) ) : c = 0 else : c = y [ 0 ] warnings . warn ( ( ' Label   %s  is  present  in  all  training  examples.' % str ( classes [ c ] ) ) ) estimator =   Constant  Predictor  ( ) . fit ( X , unique y ) else : estimator = clone ( estimator ) estimator . fit ( X , y ) return estimator 
@ bp . route ( '/user/<int:uid>' , methods = [ 'GET' , 'POST' ] ) @ require admin def user ( uid ) : user =  Account  . query . get or 404 ( uid ) form =  User  Form  ( obj = user ) if form . validate on submit ( ) : form . populate obj ( user ) user . save ( ) return redirect ( url for ( '.user' , uid = uid ) ) return render template ( 'admin/user.html' , form = form , user = user ) 
def nova not in ( logical line ) : split line = logical line . split ( ) if ( ( len ( split line ) == 5 ) and ( split line [ 0 ] == 'if' ) and ( split line [ 1 ] == 'not' ) and ( split line [ 3 ] == 'in' ) and ( not split line [ 2 ] . startswith ( '(' ) ) ) : ( yield ( logical line . find ( 'not' ) , "N902:   Use   the  'not  in'  operator  for  collection  membership  evaluation" ) ) 
@ pytest . mark . network def test require file from url ( ) : from fabtools . require import file as require file try : require file ( url = 'http://www.google.com/robots.txt' ) assert is file ( 'robots.txt' ) finally : run ( 'rm  -f  robots.txt' ) 
def event source mapping absent ( name ,  Event  Source  Arn  ,  Function  Name  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : ret = { 'name' :  None  , 'result' :  True  , 'comment' : '' , 'changes' : { } } desc =   salt   [ 'boto lambda.describe event source mapping' ] (  Event  Source  Arn  =  Event  Source  Arn  ,  Function  Name  =  Function  Name  , region = region , key = key , keyid = keyid , profile = profile ) if ( 'error' in desc ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  delete  event  source  mapping:  {0}.' . format ( desc [ 'error' ] [ 'message' ] ) return ret if ( not desc . get ( 'event source mapping' ) ) : ret [ 'comment' ] = ' Event   source  mapping  does  not  exist.' return ret ret [ 'name' ] = desc [ 'event source mapping' ] [ 'UUID' ] if   opts   [ 'test' ] : ret [ 'comment' ] = ' Event   source  mapping  is  set  to  be  removed.' ret [ 'result' ] =  None  return ret r =   salt   [ 'boto lambda.delete event source mapping' ] (  Event  Source  Arn  =  Event  Source  Arn  ,  Function  Name  =  Function  Name  , region = region , key = key , keyid = keyid , profile = profile ) if ( not r [ 'deleted' ] ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  delete  event  source  mapping:  {0}.' . format ( r [ 'error' ] [ 'message' ] ) return ret ret [ 'changes' ] [ 'old' ] = desc ret [ 'changes' ] [ 'new' ] = { 'event source mapping' :  None  } ret [ 'comment' ] = ' Event   source  mapping  deleted.' return ret 
def add Bevel  Gear  ( derivation , extrude Derivation  , pitch Radius  , positives , teeth , vector3 Gear  Profile  ) : total Pitch  Radius  = ( derivation . pitch Radius  Gear  + derivation . pitch Radius  ) total Teeth  = ( derivation . teeth Pinion  + derivation . teeth Gear  ) portion Directions  = extrude . get Spaced  Portion  Directions  ( extrude Derivation  . interpolation Dictionary  ) loop Lists  = extrude . get Loop  Lists  By  Path  ( extrude Derivation  ,  None  , vector3 Gear  Profile  [ 0 ] , portion Directions  ) first Loop  List  = loop Lists  [ 0 ] gear Over  Pinion  = ( float ( ( total Teeth  - teeth ) ) / float ( teeth ) ) third Layer  Thickness  = ( 0.33333333333 * evaluate . get Layer  Thickness  ( derivation . xml Element  ) ) pitch Radian  = math . atan ( ( math . sin ( derivation . operating Radian  ) / ( gear Over  Pinion  + math . cos ( derivation . operating Radian  ) ) ) ) cone Distance  = ( pitch Radius  / math . sin ( pitch Radian  ) ) apex =  Vector 3 ( 0.0 , 0.0 , math . sqrt ( ( ( cone Distance  * cone Distance  ) - ( pitch Radius  * pitch Radius  ) ) ) ) cos Pitch  = ( apex . z / cone Distance  ) sin Pitch  = math . sin ( pitch Radian  ) for loop in first Loop  List  : for point in loop : along Way  = ( point . z / cone Distance  ) one Minus  Along  Way  = ( 1.0 - along Way  ) point Complex  = point . drop Axis  ( ) point Complex  Length  = abs ( point Complex  ) delta Radius  = ( point Complex  Length  - pitch Radius  ) cos Delta  Radius  = ( cos Pitch  * delta Radius  ) sin Delta  Radius  = ( sin Pitch  * delta Radius  ) point Complex  *= ( ( cos Delta  Radius  + pitch Radius  ) / point Complex  Length  ) point . x = point Complex  . real point . y = point Complex  . imag point . z += sin Delta  Radius  point . x *= one Minus  Along  Way  point . y *= one Minus  Along  Way  add Bottom  Loop  ( ( - third Layer  Thickness  ) , first Loop  List  ) top Loop  = first Loop  List  [ ( - 1 ) ] top Addition  = [ ] topZ = ( euclidean . get Top  Path  ( top Loop  ) + third Layer  Thickness  ) old Index  = top Loop  [ ( - 1 ) ] . index for point in top Loop  : old Index  += 1 top Addition  . append (  Vector 3 Index  ( old Index  , ( 0.8 * point . x ) , ( 0.8 * point . y ) , topZ ) ) first Loop  List  . append ( top Addition  ) translation =  Vector 3 ( 0.0 , 0.0 , ( - euclidean . get Bottom  Paths  ( first Loop  List  ) ) ) euclidean . translate Vector 3 Paths  ( first Loop  List  , translation ) geometry Output  = trianglemesh . get Pillars  Output  ( loop Lists  ) positives . append ( geometry Output  ) 
@ utils . arg ( 'image' , metavar = '<image>' , help =   ( ' Name   or  ID  of  image.' ) ) @ utils . arg ( 'action' , metavar = '<action>' , choices = [ 'set' , 'delete' ] , help =   ( " Actions :  'set'  or  'delete'." ) ) @ utils . arg ( 'metadata' , metavar = '<key=value>' , nargs = '+' , action = 'append' , default = [ ] , help =   ( ' Metadata   to  add/update  or  delete  (only  key  is  necessary  on  delete).' ) ) @ deprecated image def do image meta ( cs , args ) : image =  find image ( cs , args . image ) metadata =  extract metadata ( args ) if ( args . action == 'set' ) : cs . images . set meta ( image , metadata ) elif ( args . action == 'delete' ) : cs . images . delete meta ( image , metadata . keys ( ) ) 
def cpu times percent ( interval =  None  , percpu =  False  ) : global  last cpu times 2 global  last per cpu times 2 blocking = ( ( interval is not  None  ) and ( interval > 0.0 ) ) if ( ( interval is not  None  ) and ( interval < 0 ) ) : raise  Value  Error  ( ( 'interval  is  not  positive  (got  %r)' % interval ) ) def calculate ( t1 , t2 ) : nums = [ ] all delta = (  cpu tot time ( t2 ) -  cpu tot time ( t1 ) ) for field in t1 .  fields : field delta = ( getattr ( t2 , field ) - getattr ( t1 , field ) ) try : field perc = ( ( 100 * field delta ) / all delta ) except  Zero  Division  Error  : field perc = 0.0 field perc = round ( field perc , 1 ) if ( field perc > 100.0 ) : field perc = 100.0 elif ( field perc <= 0.0 ) : field perc = 0.0 nums . append ( field perc ) return  psplatform . scputimes ( * nums ) if ( not percpu ) : if blocking : t1 = cpu times ( ) time . sleep ( interval ) else : t1 =  last cpu times 2 if ( t1 is  None  ) : t1 = cpu times ( )  last cpu times 2 = cpu times ( ) return calculate ( t1 ,  last cpu times 2 ) else : ret = [ ] if blocking : tot1 = cpu times ( percpu =  True  ) time . sleep ( interval ) else : tot1 =  last per cpu times 2 if ( tot1 is  None  ) : tot1 = cpu times ( percpu =  True  )  last per cpu times 2 = cpu times ( percpu =  True  ) for ( t1 , t2 ) in zip ( tot1 ,  last per cpu times 2 ) : ret . append ( calculate ( t1 , t2 ) ) return ret 
def numpy to sympy ( m , ** options ) : return  Matrix  ( m ) 
def create ( ) : use app ( call reuse =  False  ) return default app . create ( ) 
def test ast valid let ( ) : can compile ( u'(let  [a  b])' ) can compile ( u'(let  [a  1])' ) can compile ( u'(let  [a  1  b   None ])' ) 
def test hsl to rgb part 9 ( ) : assert ( hsl to rgb ( 120 , 20 , 50 ) == ( 102 , 153 , 102 ) ) assert ( hsl to rgb ( 120 , 60 , 50 ) == ( 51 , 204 , 51 ) ) assert ( hsl to rgb ( 120 , 100 , 50 ) == ( 0 , 255 , 0 ) ) 
def get Instance  Details  ( api , server ) : instance = { 'id' : server [ 'LINODEID' ] , 'name' : server [ 'LABEL' ] , 'public' : [ ] , 'private' : [ ] } for ip in api . linode ip list (  Linode  Id  = server [ 'LINODEID' ] ) : if ( ip [ 'ISPUBLIC' ] and ( 'ipv4' not in instance ) ) : instance [ 'ipv4' ] = ip [ 'IPADDRESS' ] instance [ 'fqdn' ] = ip [ 'RDNS NAME' ] if ip [ 'ISPUBLIC' ] : instance [ 'public' ] . append ( { 'ipv4' : ip [ 'IPADDRESS' ] , 'fqdn' : ip [ 'RDNS NAME' ] , 'ip id' : ip [ 'IPADDRESSID' ] } ) else : instance [ 'private' ] . append ( { 'ipv4' : ip [ 'IPADDRESS' ] , 'fqdn' : ip [ 'RDNS NAME' ] , 'ip id' : ip [ 'IPADDRESSID' ] } ) return instance 
def copy to master ( registry , xml parent , data ) : cm = XML .  Sub  Element  ( xml parent , 'com.michelin.cio.hudson.plugins.copytoslave. Copy  To  Master  Notifier ' ) cm . set ( 'plugin' , 'copy-to-slave' ) XML .  Sub  Element  ( cm , 'includes' ) . text = ',' . join ( data . get ( 'includes' , [ '' ] ) ) XML .  Sub  Element  ( cm , 'excludes' ) . text = ',' . join ( data . get ( 'excludes' , [ '' ] ) ) mappings = [ ( 'run-after-result' , 'run After  Result  Finalised ' ,  True  ) , ( 'destination' , 'destination Folder ' , '' ) ] helpers . convert mapping to xml ( cm , data , mappings , fail required =  True  ) if data . get ( 'destination' , '' ) : XML .  Sub  Element  ( cm , 'override Destination  Folder ' ) . text = 'true' 
def  format issue ( issue ) : ret = { 'id' : issue . get ( 'id' ) , 'issue number' : issue . get ( 'number' ) , 'state' : issue . get ( 'state' ) , 'title' : issue . get ( 'title' ) , 'user' : issue . get ( 'user' ) . get ( 'login' ) , 'html url' : issue . get ( 'html url' ) } assignee = issue . get ( 'assignee' ) if assignee : assignee = assignee . get ( 'login' ) labels = issue . get ( 'labels' ) label names = [ ] for label in labels : label names . append ( label . get ( 'name' ) ) milestone = issue . get ( 'milestone' ) if milestone : milestone = milestone . get ( 'title' ) ret [ 'assignee' ] = assignee ret [ 'labels' ] = label names ret [ 'milestone' ] = milestone return ret 
def get data files ( dname , ignore =  None  , parent =  None  ) : parent = ( parent or '.' ) ignore = ( ignore or [ ] ) result = [ ] for ( directory , subdirectories , filenames ) in os . walk ( dname ) : resultfiles = [ ] for exname in EXCLUDE NAMES : if ( exname in subdirectories ) : subdirectories . remove ( exname ) for ig in ignore : if ( ig in subdirectories ) : subdirectories . remove ( ig ) for filename in  filter names ( filenames ) : resultfiles . append ( filename ) if resultfiles : for filename in resultfiles : file path = os . path . join ( directory , filename ) if parent : file path = file path . replace ( ( parent + os . sep ) , '' ) result . append ( file path ) return result 
def  normalizeargs ( sequence , output =  None  ) : if ( output is  None  ) : output = [ ] cls = sequence .   class   if ( (  Interface  Class  in cls .   mro   ) or (  Implements  in cls .   mro   ) ) : output . append ( sequence ) else : for v in sequence :  normalizeargs ( v , output ) return output 
def  plot topo ( info , times , show func , click func =  None  , layout =  None  , vmin =  None  , vmax =  None  , ylim =  None  , colorbar =  None  , border = 'none' , axis facecolor = 'k' , fig facecolor = 'k' , cmap = ' Rd  Bu  r' , layout scale =  None  , title =  None  , x label =  None  , y label =  None  , font color = 'w' , unified =  False  , img =  False  ) : import matplotlib . pyplot as plt ( tmin , tmax ) = times [ [ 0 , ( - 1 ) ] ] click func = ( show func if ( click func is  None  ) else click func ) on pick = partial ( click func , tmin = tmin , tmax = tmax , vmin = vmin , vmax = vmax , ylim = ylim , x label = x label , y label = y label , colorbar = colorbar ) fig = plt . figure ( ) if colorbar : sm = plt . cm .  Scalar  Mappable  ( cmap = cmap , norm = plt .  Normalize  ( vmin , vmax ) ) sm . set array ( np . linspace ( vmin , vmax ) ) ax = plt . axes ( [ 0.015 , 0.025 , 1.05 , 0.8 ] , axisbg = fig facecolor ) cb = fig . colorbar ( sm , ax = ax ) cb yticks = plt . getp ( cb . ax . axes , 'yticklabels' ) plt . setp ( cb yticks , color = font color ) ax . axis ( 'off' ) my topo plot =  iter topography ( info , layout = layout , on pick = on pick , fig = fig , layout scale = layout scale , axis spinecolor = border , axis facecolor = axis facecolor , fig facecolor = fig facecolor , unified = unified , img = img ) for ( ax , ch idx ) in my topo plot : if ( ( layout . kind == ' Vectorview -all' ) and ( ylim is not  None  ) ) : this type = { 'mag' : 0 , 'grad' : 1 } [ channel type ( info , ch idx ) ] ylim  = [ ( v [ this type ] if  check vlim ( v ) else v ) for v in ylim ] else : ylim  = ylim show func ( ax , ch idx , tmin = tmin , tmax = tmax , vmin = vmin , vmax = vmax , ylim = ylim  ) if ( title is not  None  ) : plt . figtext ( 0.03 , 0.9 , title , color = font color , fontsize = 19 ) return fig 
def local open ( url ) : ( scheme , server , path , param , query , frag ) = urlparse . urlparse ( url ) filename = urllib2 . url2pathname ( path ) if os . path . isfile ( filename ) : return urllib2 . urlopen ( url ) elif ( path . endswith ( '/' ) and os . path . isdir ( filename ) ) : files = [ ] for f in os . listdir ( filename ) : if ( f == 'index.html' ) : body = open ( os . path . join ( filename , f ) , 'rb' ) . read ( ) break elif os . path . isdir ( os . path . join ( filename , f ) ) : f += '/' files . append ( ( '<a  href=%r>%s</a>' % ( f , f ) ) ) else : body = ( ( '<html><head><title>%s</title>' % url ) + ( '</head><body>%s</body></html>' % '\n' . join ( files ) ) ) ( status , message ) = ( 200 , 'OK' ) else : ( status , message , body ) = ( 404 , ' Path   not  found' , ' Not   found' ) return urllib2 . HTTP Error  ( url , status , message , { 'content-type' : 'text/html' } , c String IO .  String IO ( body ) ) 
def identify names ( code ) : finder =  Name  Finder  ( ) finder . visit ( ast . parse ( code ) ) example code obj = { } for ( name , full name ) in finder . get mapping ( ) : splitted = full name . rsplit ( '.' , 1 ) if ( len ( splitted ) == 1 ) : continue ( module , attribute ) = splitted module short = get short module name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module short' : module short } example code obj [ name ] = cobj return example code obj 
def black out ( x , t , W , samples ) : batch size = x . shape [ 0 ] neg emb = embed id . embed id ( samples , W ) neg y = matmul . batch matmul ( neg emb , x ) neg y = reshape . reshape ( neg y , neg y . shape [ : ( - 1 ) ] ) pos emb = expand dims . expand dims ( embed id . embed id ( t , W ) , 1 ) pos y = matmul . batch matmul ( pos emb , x ) pos y = reshape . reshape ( pos y , pos y . shape [ : ( - 1 ) ] ) logz = logsumexp . logsumexp ( concat . concat ( [ pos y , neg y ] ) , axis = 1 ) ( blogz , bneg y ) = broadcast . broadcast ( reshape . reshape ( logz , ( batch size , 1 ) ) , neg y ) ny = exponential . log ( ( 1 - exponential . exp ( ( bneg y - blogz ) ) ) ) py = reshape . reshape ( pos y , ( batch size , ) ) loss = ( ( py - logz ) +  sum . sum ( ny , axis = 1 ) ) return ( ( -  sum . sum ( loss ) ) / batch size ) 
@ utils . arg ( 'monitor id' , metavar = '<monitor-id>' , help = 'ID  of  the  monitor  to  upload  to  an  image' ) @ utils . arg ( '--force' , metavar = '< True | False >' , help = " Optional   flag  to  indicate  whether  to  upload  a  monitor  even  if  it's  attached  to  an  instance.  ( Default = False )" , default =  False  ) @ utils . arg ( '--container-format' , metavar = '<container-format>' , help = ' Optional   type  for  container  format  ( Default =bare)' , default = 'bare' ) @ utils . arg ( '--disk-format' , metavar = '<disk-format>' , help = ' Optional   type  for  disk  format  ( Default =raw)' , default = 'raw' ) @ utils . arg ( 'image name' , metavar = '<image-name>' , help = ' Name   for  created  image' ) @ utils . service type ( 'monitor' ) def do upload to image ( cs , args ) : monitor =  find monitor ( cs , args . monitor id ) monitor . upload to image ( args . force , args . image name , args . container format , args . disk format ) 
def returner argspec ( module = '' ) : returners  = salt . loader . returners (   opts   , [ ] ) return salt . utils . argspec report ( returners  , module ) 
def create regex extractor ( pattern ) : ereg = re . compile ( pattern , re . S ) def  extractor ( txt , htmlpage =  None  ) : if ( txt is  None  ) : return m = ereg . search ( txt ) if m : return htmlregion ( u'' . join ( [ g for g in ( m . groups ( ) or m . group ( ) ) if g ] ) )  extractor .   name   = ( ' Regex :  %s' % pattern . encode ( 'utf-8' ) ) return  extractor 
@ treeio login required @ handle response format def currency edit ( request , currency id , response format = 'html' ) : currency = get object or 404 (  Currency  , pk = currency id ) if ( ( not request . user . profile . has permission ( currency , mode = 'w' ) ) and ( not request . user . profile . is admin ( 'treeio finance' ) ) ) : return user denied ( request , " You   don't  have  access  to  this   Currency " , response format ) if request . POST : if ( 'cancel' not in request . POST ) : form =  Currency  Form  ( request . user . profile , request . POST , instance = currency ) if form . is valid ( ) : currency = form . save ( ) return  Http  Response  Redirect  ( reverse ( 'finance currency view' , args = [ currency . id ] ) ) else : return  Http  Response  Redirect  ( reverse ( 'finance currency view' , args = [ currency . id ] ) ) else : form =  Currency  Form  ( request . user . profile , instance = currency ) return render to response ( 'finance/currency edit' , { 'form' : form , 'currency' : currency } , context instance =  Request  Context  ( request ) , response format = response format ) 
def format blocks ( blocks ) : assert ( isinstance ( blocks , tuple ) and all ( ( isinstance ( x , int ) for x in blocks ) ) ) return   Pretty  Blocks  ( blocks ) 
def  load config ( ) : fname =  get config fname ( ) if ( ( fname is  None  ) or ( not op . isfile ( fname ) ) ) : return dict ( ) with open ( fname , 'r' ) as fid : config = json . load ( fid ) return config 
def replace dots in field names ( document ) : for ( key , value ) in list ( document . items ( ) ) : if isinstance ( value , dict ) : value = replace dots in field names ( value ) if ( isinstance ( key , string types ) and ( key . find ( '.' ) != ( - 1 ) ) ) : del document [ key ] document [ key . replace ( '.' , ' ' ) ] = value return document 
def get logger ( name , handlers ) : l = logbook .  Logger  ( name ) for h in handlers : if isinstance ( h , list ) : l . handlers += h else : l . handlers . append ( h ) return l 
def enter transaction management ( managed =  True  , using =  None  ) : if ( using is  None  ) : using = DEFAULT DB ALIAS connection = connections [ using ] connection . enter transaction management ( managed ) 
def get tests from fs ( parent dir , control pattern , add noncompliant =  False  ) : tests = { } profilers =  False  if ( 'client/profilers' in parent dir ) : profilers =  True  for dir in [ parent dir ] : files = recursive walk ( dir , control pattern ) for file in files : if ( ( '  init  .py' in file ) or ( '.svn' in file ) ) : continue if ( not profilers ) : if ( not add noncompliant ) : try : found test = control data . parse control ( file , raise warnings =  True  ) tests [ file ] = found test except control data .  Control  Variable  Exception  as e : logging . warn ( ' Skipping   %s\n%s' , file , e ) except  Exception  as e : logging . error ( ' Bad   %s\n%s' , file , e ) else : found test = control data . parse control ( file ) tests [ file ] = found test else : tests [ file ] = compiler . parse File  ( file ) . doc return tests 
@ requires sklearn def test ica rank reduction ( ) : raw = read raw fif ( raw fname ) . crop ( 0.5 , stop ) . load data ( ) picks = pick types ( raw . info , meg =  True  , stim =  False  , ecg =  False  , eog =  False  , exclude = 'bads' ) [ : 10 ] n components = 5 max pca components = len ( picks ) for n pca components in [ 6 , 10 ] : with warnings . catch warnings ( record =  True  ) : warnings . simplefilter ( 'always' ) ica = ICA ( n components = n components , max pca components = max pca components , n pca components = n pca components , method = 'fastica' , max iter = 1 ) . fit ( raw , picks = picks ) rank before = raw . estimate rank ( picks = picks ) assert equal ( rank before , len ( picks ) ) raw clean = ica . apply ( raw . copy ( ) ) rank after = raw clean . estimate rank ( picks = picks ) assert true ( ( n components < n pca components <= rank after <= rank before ) ) 
def method params ( doc ) : doclines = doc . splitlines ( ) if ( ' Args :' in doclines ) : begin = doclines . index ( ' Args :' ) if ( ' Returns :' in doclines [ ( begin + 1 ) : ] ) : end = doclines . index ( ' Returns :' , begin ) args = doclines [ ( begin + 1 ) : end ] else : args = doclines [ ( begin + 1 ) : ] parameters = [ ] for line in args : m = re . search ( '^\\s+([a-zA-Z0-9 ]+):  (.*)' , line ) if ( m is  None  ) : continue pname = m . group ( 1 ) desc = m . group ( 2 ) if ( '(required)' not in desc ) : pname = ( pname + '= None ' ) parameters . append ( pname ) parameters = ',  ' . join ( parameters ) else : parameters = '' return parameters 
def iter multipart mime documents ( wsgi input , boundary , read chunk size = 4096 ) : boundary = ( '--' + boundary ) blen = ( len ( boundary ) + 2 ) try : got = wsgi input . readline ( blen ) while ( got == '\r\n' ) : got = wsgi input . readline ( blen ) except ( IO Error  ,  Value  Error  ) as e : raise swift . common . exceptions .  Chunk  Read  Error  ( str ( e ) ) if ( got . strip ( ) != boundary ) : raise swift . common . exceptions .  Mime  Invalid  ( 'invalid  starting  boundary:  wanted  %r,  got  %r' , ( boundary , got ) ) boundary = ( '\r\n' + boundary ) input buffer = '' done =  False  while ( not done ) : it =   Multipart  Mime  File  Like  Object  ( wsgi input , boundary , input buffer , read chunk size ) ( yield it ) done = it . no more files input buffer = it . input buffer 
def test dnn tag ( ) : x = T . ftensor4 ( ) old = theano . config . on opt error theano . config . on opt error = 'raise' sio =  String IO ( ) handler = logging .  Stream  Handler  ( sio ) logging . get Logger  ( 'theano.compile.tests.test dnn' ) . add Handler  ( handler ) logging . get Logger  ( 'theano' ) . remove Handler  ( theano . logging default handler ) raised =  False  try : f = theano . function ( [ x ] , pool 2d ( x , ws = ( 2 , 2 ) , ignore border =  True  ) , mode = mode with gpu . including ( 'cudnn' ) ) except (  Assertion  Error  ,  Runtime  Error  ) : assert ( not cuda . dnn . dnn available ( ) ) raised =  True  finally : theano . config . on opt error = old logging . get Logger  ( 'theano.compile.tests.test dnn' ) . remove Handler  ( handler ) logging . get Logger  ( 'theano' ) . add Handler  ( theano . logging default handler ) if ( not raised ) : assert cuda . dnn . dnn available ( ) assert any ( [ isinstance ( n . op , cuda . dnn .  Gpu  Dnn  Pool  ) for n in f . maker . fgraph . toposort ( ) ] ) 
def  create entry ( entry body ) : updated str = datetime . utcnow ( ) . isoformat ( ) if ( datetime . utcnow ( ) . utcoffset ( ) is  None  ) : updated str += '+00:00' entry start = '<?xml  version="1.0"  encoding="utf-8"  standalone="yes"?>\n<entry  xmlns:d="http://schemas.microsoft.com/ado/2007/08/dataservices"  xmlns:m="http://schemas.microsoft.com/ado/2007/08/dataservices/metadata"  xmlns="http://www.w3.org/2005/ Atom "  >\n<title  /><updated>{updated}</updated><author><name  /></author><id  />\n<content  type="application/xml">\n        {body}</content></entry>' return entry start . format ( updated = updated str , body = entry body ) 
def xml Doc  from html ( response ) : utf8body = ( body as utf8 ( response ) or '  ' ) try : lxdoc = libxml2 . html Read  Doc  ( utf8body , response . url , 'utf-8' , html parser options ) except  Type  Error  : lxdoc = libxml2 . html Read  Doc  ( utf8body . replace ( '\x00' , '' ) , response . url , 'utf-8' , html parser options ) return lxdoc 
def main ( args = sys . argv ) : try : tarball = ( '--no-tarball' not in args ) logging . basic Config  ( level = logging . WARN ) work dir = args [ 1 ] assert os . path . exists ( work dir ) , ' First   argument  to  sge runner.py  must  be  a  directory  that  exists' project dir = args [ 2 ] sys . path . append ( project dir )  do work on compute node ( work dir , tarball ) except  Exception  as e : print e raise 
def delete user policy ( user name , policy name , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) if ( not conn ) : return  False   policy = get user policy ( user name , policy name , region , key , keyid , profile ) if ( not  policy ) : return  True  try : conn . delete user policy ( user name , policy name ) msg = ' Successfully   deleted  {0}  policy  for  user  {1}.' log . info ( msg . format ( policy name , user name ) ) return  True  except boto . exception .  Boto  Server  Error  as e : log . debug ( e ) msg = ' Failed   to  delete  {0}  policy  for  user  {1}.' log . error ( msg . format ( policy name , user name ) ) return  False  
def  dont fail not exist ( error ) : if isinstance ( error ,  Azure  Missing  Resource  Http  Error  ) : return  False  else : raise error 
def dmp pquo ( f , g , u , K ) : return dmp pdiv ( f , g , u , K ) [ 0 ] 
def base64image ( src ) : return base64 . b64encode (  get file ( src ) ) 
def normalize time ( timestamp ) : offset = timestamp . utcoffset ( ) if ( offset is  None  ) : return timestamp return ( timestamp . replace ( tzinfo =  None  ) - offset ) 
def infer shape ( outs , inputs , input shapes ) : for ( inp , inp shp ) in izip ( inputs , input shapes ) : if ( ( inp shp is not  None  ) and ( len ( inp shp ) != inp . ndim ) ) : assert ( len ( inp shp ) == inp . ndim ) shape feature = tensor . opt .  Shape  Feature  ( ) shape feature . on attach ( theano . gof .  Function  Graph  ( [ ] , [ ] ) ) for ( inp , inp shp ) in izip ( inputs , input shapes ) : shape feature . set shape ( inp , inp shp ) def local traverse ( out ) : '\n                 Go   back  in  the  graph,  from  out,  adding  computable  shapes  to  shape of.\n\n                ' if ( out in shape feature . shape of ) : return elif ( out . owner is  None  ) : shape feature . init r ( out ) else : for inp in out . owner . inputs : if ( not ( inp in shape feature . shape of ) ) : local traverse ( inp ) dummy fgraph =  None  shape feature . on import ( dummy fgraph , out . owner , reason = 'dummy' ) ret = [ ] for o in outs : local traverse ( o ) ret . append ( shape feature . shape of [ o ] ) return ret 
def scour Coordinates  ( data , options , force Comma  Wsp  =  False  ) : if ( data !=  None  ) : new Data  = [ ] c = 0 previous Coord  = '' for coord in data : scoured Coord  = scour Unitless  Length  ( coord , needs Renderer  Workaround  = options . renderer workaround ) if ( ( c > 0 ) and ( force Comma  Wsp  or scoured Coord  [ 0 ] . isdigit ( ) or ( ( scoured Coord  [ 0 ] == '.' ) and ( not ( ( '.' in previous Coord  ) or ( 'e' in previous Coord  ) ) ) ) ) ) : new Data  . append ( '  ' ) new Data  . append ( scoured Coord  ) previous Coord  = scoured Coord  c += 1 if options . renderer workaround : if ( len ( new Data  ) > 0 ) : for i in xrange ( 1 , len ( new Data  ) ) : if ( ( new Data  [ i ] [ 0 ] == '-' ) and ( 'e' in new Data  [ ( i - 1 ) ] ) ) : new Data  [ ( i - 1 ) ] += '  ' return '' . join ( new Data  ) else : return '' . join ( new Data  ) return '' 
@ pytest . mark . django db def test verify user without existing email ( trans member ) : member = trans member with pytest . raises (  Email  Address  .  Does  Not  Exist  ) :  Email  Address  . objects . get ( user = member ) member . email = 'member@this.test' accounts . utils . verify user ( member )  Email  Address  . objects . get ( user = member , email = 'member@this.test' , primary =  True  , verified =  True  ) assert ( get user model ( ) . objects . get ( pk = member . pk ) . email == '' ) 
def   virtual   ( ) : if dns support : return 'ddns' return (  False  , ' The   ddns  execution  module  cannot  be  loaded:  dnspython  not  installed.' ) 
def  coo to sparse series ( A , dense index =  False  ) : s =  Series  ( A . data ,  Multi  Index  . from arrays ( ( A . row , A . col ) ) ) s = s . sort index ( ) s = s . to sparse ( ) if dense index : i = range ( A . shape [ 0 ] ) j = range ( A . shape [ 1 ] ) ind =  Multi  Index  . from product ( [ i , j ] ) s = s . reindex axis ( ind ) return s 
def  formatinfo ( format ) : size = struct . calcsize ( format ) return ( size , len ( struct . unpack ( format , ( '\x00' * size ) ) ) ) 
def pperm above ( accessing obj , accessed obj , * args , ** kwargs ) : return perm above (  to player ( accessing obj ) , accessed obj , * args , ** kwargs ) 
def vararg callback ( option , opt str , value , parser ) : value = [ value ] def floatable ( str ) : try : float ( str ) return  True  except  Value  Error  : return  False  for arg in parser . rargs : if ( ( arg [ : 2 ] == '--' ) and ( len ( arg ) > 2 ) ) : break if ( ( arg [ : 1 ] == '-' ) and ( len ( arg ) > 1 ) and ( not floatable ( arg ) ) ) : break value . append ( arg ) del parser . rargs [ : ( len ( value ) - 1 ) ] setattr ( parser . values , option . dest , value ) 
def get New  Derivation  ( element Node  ) : return  Sponge  Slice  Derivation  ( element Node  ) 
def get Intermediate  Location  ( along Way  , begin , end ) : return ( ( begin * ( 1.0 - along Way  ) ) + ( end * along Way  ) ) 
def test oss sample wrong X ( ) : oss =  One  Sided  Selection  ( random state = RND SEED ) oss . fit ( X , Y ) assert raises (  Runtime  Error  , oss . sample , np . random . random ( ( 100 , 40 ) ) , np . array ( ( ( [ 0 ] * 50 ) + ( [ 1 ] * 50 ) ) ) ) 
def get cohorted commentables ( course key ) : course cohort settings = get course cohort settings ( course key ) if ( not course cohort settings . is cohorted ) : ans = set ( ) else : ans = set ( course cohort settings . cohorted discussions ) return ans 
def discretize linear 1D ( model , x range ) : x = np . arange ( ( x range [ 0 ] - 0.5 ) , ( x range [ 1 ] + 0.5 ) ) values intermediate grid = model ( x ) return ( 0.5 * ( values intermediate grid [ 1 : ] + values intermediate grid [ : ( - 1 ) ] ) ) 
def  ar transparams ( params ) : newparams = ( ( 1 - np . exp ( ( - params ) ) ) / ( 1 + np . exp ( ( - params ) ) ) ) . copy ( ) tmp = ( ( 1 - np . exp ( ( - params ) ) ) / ( 1 + np . exp ( ( - params ) ) ) ) . copy ( ) for j in range ( 1 , len ( params ) ) : a = newparams [ j ] for kiter in range ( j ) : tmp [ kiter ] -= ( a * newparams [ ( ( j - kiter ) - 1 ) ] ) newparams [ : j ] = tmp [ : j ] return newparams 
def pluralize ( word , pos = NOUN , custom = { } ) : if ( word in custom ) : return custom [ word ] w = word . lower ( ) if ( len ( w ) < 3 ) : return w if ( w in plural irregular ) : return plural irregular [ w ] if ( w . endswith ( ( 'cia' , 'gia' ) ) and ( len ( w ) > 4 ) and ( not is vowel ( w [ ( - 4 ) ] ) ) ) : return ( w [ : ( - 2 ) ] + 'e' ) if w . endswith ( ( 'ca' , 'ga' ) ) : return ( w [ : ( - 2 ) ] + 'he' ) if w . endswith ( 'a' ) : return ( w [ : ( - 1 ) ] + 'e' ) if w . endswith ( 'e' ) : return ( w [ : ( - 1 ) ] + 'i' ) if w . endswith ( 'io' ) : return ( w [ : ( - 2 ) ] + 'i' ) if ( w in plural co chi ) : return ( w [ : ( - 2 ) ] + 'chi' ) if ( w in plural co chi ) : return ( w [ : ( - 2 ) ] + 'ghi' ) if w . endswith ( 'o' ) : return ( w [ : ( - 1 ) ] + 'i' ) return w 
@ aborts def test abort ( ) : abort ( ' Test ' ) 
def invert tree ( root ) : if ( root ==  None  ) : return  None  temp = root . right root . right = root . left root . left = temp invert tree ( root . left ) invert tree ( root . right ) return root 
def arccos ( x ) : return  Arccos  ( ) ( x ) 
def get Window  Given  Text  Repository  ( file Name  , gcode Text  , repository ) : skein =  Skeiniso  Skein  ( ) skein . parse Gcode  ( file Name  , gcode Text  , repository ) return  Skein  Window  ( repository , skein ) 
def iflatten ( x ) : for el in x : if is listlike ( el ) : for el  in iflatten ( el ) : ( yield el  ) else : ( yield el ) 
def corner shi tomasi ( image , sigma = 1 ) : (  Axx  ,  Axy  ,  Ayy  ) = structure tensor ( image , sigma ) response = ( ( (  Axx  +  Ayy  ) - np . sqrt ( ( ( (  Axx  -  Ayy  ) ** 2 ) + ( 4 * (  Axy  ** 2 ) ) ) ) ) / 2 ) return response 
def setup platform ( hass , config , add callback devices , discovery info =  None  ) : netatmo = get component ( 'netatmo' ) device = config . get ( CONF RELAY ) import lnetatmo try : data =  Thermostat  Data  ( netatmo . NETATMO AUTH , device ) for module name in data . get module names ( ) : if ( CONF THERMOSTAT in config ) : if ( ( config [ CONF THERMOSTAT ] != [ ] ) and ( module name not in config [ CONF THERMOSTAT ] ) ) : continue add callback devices ( [  Netatmo  Thermostat  ( data , module name ) ] ) except lnetatmo .  No  Device  : return  None  
def save comment ( request , unit ) : unit . commented by = request . user unit . commented on = timezone . now ( ) . replace ( microsecond = 0 ) language = request . translation project . language form = unit comment form factory ( language ) ( request . POST , instance = unit , request = request ) if form . is valid ( ) : form . save ( ) user = request . user directory = unit . store . parent ctx = { 'unit' : unit , 'language' : language , 'cantranslate' : check user permission ( user , 'translate' , directory ) , 'cansuggest' : check user permission ( user , 'suggest' , directory ) } t = loader . get template ( 'editor/units/xhr comment.html' ) return  Json  Response  ( { 'comment' : t . render ( context = ctx , request = request ) } ) return  Json  Response  Bad  Request  ( { 'msg' :   ( ' Comment   submission  failed.' ) } ) 
def delete ( path , attribute ) : cmd = 'xattr  -d  "{0}"  "{1}"' . format ( attribute , path ) try : salt . utils . mac utils . execute return success ( cmd ) except  Command  Execution  Error  as exc : if ( ' No   such  file' in exc . strerror ) : raise  Command  Execution  Error  ( ' File   not  found:  {0}' . format ( path ) ) if ( ' No   such  xattr' in exc . strerror ) : raise  Command  Execution  Error  ( ' Attribute   not  found:  {0}' . format ( attribute ) ) raise  Command  Execution  Error  ( ' Unknown    Error :  {0}' . format ( exc . strerror ) ) return ( attribute not in list  ( path ) ) 
def spam dashboard recent events ( start =  None  , end =  None  ) : now = datetime . datetime . now ( ) data = { 'events generated' : now . isoformat ( '  ' ) , 'recent spam' : [ ] } if ( not end ) : end = now if ( not start ) : start = ( end - datetime . timedelta ( days = 183 ) ) recent spam =  Revision  Akismet  Submission  . objects . filter ( type = 'spam' , revision  created  gt = start , revision  created  lt = end ) . select related ( 'revision  document' ) . order by ( '-id' ) change types = { (  False  ,  False  ) : ' New    Page ' , (  True  ,  False  ) : ' Page    Edit ' , (  False  ,  True  ) : ' New    Translation ' , (  True  ,  True  ) : ' Translation    Update ' } for rs in recent spam : revision = rs . revision document = revision . document revisions = list ( document . revisions . filter ( id  gte = revision . id ) . only ( 'id' , 'created' ) . order by ( 'id' ) [ : 2 ] ) if ( len ( revisions ) == 1 ) : if document . deleted : try : entry =  Document  Deletion  Log  . objects . filter ( locale = document . locale , slug = document . slug ) . latest ( 'id' ) time active raw = ( entry . timestamp - revision . created ) time active = int ( time active raw . total seconds ( ) ) except  Document  Deletion  Log  .  Does  Not  Exist  : time active = ' Deleted ' else : time active = ' Current ' else : next rev = revisions [ 1 ] time active raw = ( next rev . created - revision . created ) time active = int ( time active raw . total seconds ( ) ) change type = change types [ ( bool ( revision . previous ) , bool ( document . parent ) ) ] data [ 'recent spam' ] . append ( { 'date' : revision . created . date ( ) , 'time active' : time active , 'revision id' : revision . id , 'revision path' : revision . get absolute url ( ) , 'change type' : change type , 'document path' : revision . document . get absolute url ( ) } ) for chunk in chunker ( data [ 'recent spam' ] , 250 ) : start date = min ( ( item [ 'date' ] for item in chunk ) ) revs = [ item [ 'revision id' ] for item in chunk ] try : views = analytics upageviews ( revs , start date ) except  Improperly  Configured  as e : data [ 'improperly configured' ] = e . message break for item in chunk : item [ 'viewers' ] = views [ item [ 'revision id' ] ] return data 
def render ( gpg data , saltenv = 'base' , sls = '' , argline = '' , ** kwargs ) : if ( not  get gpg exec ( ) ) : raise  Salt  Render  Error  ( 'GPG  unavailable' ) log . debug ( ' Reading   GPG  keys  from:  %s' ,  get key dir ( ) ) translate newlines = kwargs . get ( 'translate newlines' ,  False  ) return  decrypt object ( gpg data , translate newlines = translate newlines ) 
def get Carving  From  Parser  ( xml Parser  ) : boolean Geometry  Element  = xml Parser  . get Root  ( ) boolean Geometry  Element  . object = boolean geometry .  Boolean  Geometry  ( ) root = xml Parser  . get Root  ( ) root . xml Processor  = XML Boolean  Geometry  Processor  ( ) root . xml Processor  . process Children  ( boolean Geometry  Element  ) return boolean Geometry  Element  . object 
def  parse conf ( conf file =  None  , in mem =  False  , family = 'ipv4' ) : if (  conf ( ) and ( not conf file ) and ( not in mem ) ) : conf file =  conf ( family ) rules = '' if conf file : with salt . utils . fopen ( conf file , 'r' ) as ifile : rules = ifile . read ( ) elif in mem : cmd = '{0}-save' . format (  iptables cmd ( family ) ) rules =   salt   [ 'cmd.run' ] ( cmd ) else : raise  Salt  Exception  ( 'A  file  was  not  found  to  parse' ) ret = { } table = '' parser =  parser ( ) for line in rules . splitlines ( ) : if line . startswith ( '*' ) : table = line . replace ( '*' , '' ) ret [ table ] = { } elif line . startswith ( ':' ) : comps = line . split ( ) chain = comps [ 0 ] . replace ( ':' , '' ) ret [ table ] [ chain ] = { } ret [ table ] [ chain ] [ 'policy' ] = comps [ 1 ] counters = comps [ 2 ] . replace ( '[' , '' ) . replace ( ']' , '' ) ( pcount , bcount ) = counters . split ( ':' ) ret [ table ] [ chain ] [ 'packet  count' ] = pcount ret [ table ] [ chain ] [ 'byte  count' ] = bcount ret [ table ] [ chain ] [ 'rules' ] = [ ] ret [ table ] [ chain ] [ 'rules comment' ] = { } elif line . startswith ( '-A' ) : args = salt . utils . shlex split ( line ) index = 0 while ( ( index + 1 ) < len ( args ) ) : swap = ( ( args [ index ] == '!' ) and args [ ( index + 1 ) ] . startswith ( '-' ) ) if swap : ( args [ index ] , args [ ( index + 1 ) ] ) = ( args [ ( index + 1 ) ] , args [ index ] ) if args [ index ] . startswith ( '-' ) : index += 1 if ( args [ index ] . startswith ( '-' ) or ( ( args [ index ] == '!' ) and ( not swap ) ) ) : args . insert ( index , '' ) else : while ( ( ( index + 1 ) < len ( args ) ) and ( args [ ( index + 1 ) ] != '!' ) and ( not args [ ( index + 1 ) ] . startswith ( '-' ) ) ) : args [ index ] += '  {0}' . format ( args . pop ( ( index + 1 ) ) ) index += 1 if args [ ( - 1 ) ] . startswith ( '-' ) : args . append ( '' ) parsed args = [ ] if sys . version . startswith ( '2.6' ) : ( opts , leftover args ) = parser . parse args ( args ) parsed args = vars ( opts ) else : parsed args = vars ( parser . parse args ( args ) ) ret args = { } chain = parsed args [ 'append' ] for arg in parsed args : if ( parsed args [ arg ] and ( arg is not 'append' ) ) : ret args [ arg ] = parsed args [ arg ] if ( parsed args [ 'comment' ] is not  None  ) : comment = parsed args [ 'comment' ] [ 0 ] . strip ( '"' ) ret [ table ] [ chain [ 0 ] ] [ 'rules comment' ] [ comment ] = ret args ret [ table ] [ chain [ 0 ] ] [ 'rules' ] . append ( ret args ) return ret 
@ dispatch (  Expr  , object ) def post compute ( expr , result , scope =  None  ) : return result 
@ shared task def print unicode ( log message = u'h\xe5\xe5\xae\u0192  valmuefr\xf8' , print message = u'hi\xf6\xe4\xfc\xdf' ) : logger . warning ( log message ) print print message 
def get controller ( doctype ) : from frappe . model . document import  Document  global  classes if ( not ( doctype in  classes ) ) : ( module name , custom ) = ( frappe . db . get value ( u' Doc  Type ' , doctype , [ u'module' , u'custom' ] ) or [ u' Core ' ,  False  ] ) if custom :  class =  Document  else : module = load doctype module ( doctype , module name ) classname = doctype . replace ( u'  ' , u'' ) . replace ( u'-' , u'' ) if hasattr ( module , classname ) :  class = getattr ( module , classname ) if issubclass (  class ,  Base  Document  ) :  class = getattr ( module , classname ) else : raise  Import  Error  , doctype else : raise  Import  Error  , doctype  classes [ doctype ] =  class return  classes [ doctype ] 
def get langids ( dev ) : from usb . control import get descriptor buf = get descriptor ( dev , 254 , DESC TYPE STRING , 0 ) if ( ( len ( buf ) < 4 ) or ( buf [ 0 ] < 4 ) or ( ( buf [ 0 ] & 1 ) != 0 ) ) : return ( ) return tuple ( map ( ( lambda x , y : ( x + ( y << 8 ) ) ) , buf [ 2 : buf [ 0 ] : 2 ] , buf [ 3 : buf [ 0 ] : 2 ] ) ) 
def check map ( mapping file , barcode type = 'golay 12' , added demultiplex field =  None  ) : if ( barcode type == 0 ) : has barcodes =  False  var len barcodes =  False  elif ( barcode type == 'variable length' ) : has barcodes =  True  var len barcodes =  True  else : has barcodes =  True  var len barcodes =  False  ( header , mapping data , run description , errors , warnings ) = process id map ( mapping file , has barcodes = has barcodes , disable primer check =  True  , added demultiplex field = added demultiplex field , variable len barcodes = var len barcodes ) for warning in warnings : if ( 'differs  than  length' in warning ) : raise  Value  Error  ( ( ' Detected   variable  length  barcodes,  if  these  ' + 'are  being  used,  use  -b  variable length' ) ) if errors : raise  Value  Error  ( ( ' Errors   found  in  mapping  file,  please  check  ' + 'mapping  file  with  validate mapping file.py' ) ) return ( header , mapping data ) 
def mach o change ( path , what , value ) : def do macho ( file , bits , endian ) : ( cputype , cpusubtype , filetype , ncmds , sizeofcmds , flags ) = read data ( file , endian , 6 ) if ( bits == 64 ) : read data ( file , endian ) for n in range ( ncmds ) : where = file . tell ( ) ( cmd , cmdsize ) = read data ( file , endian , 2 ) if ( cmd == LC LOAD DYLIB ) : name offset = read data ( file , endian ) file . seek ( ( where + name offset ) , os . SEEK SET ) load = file . read ( ( cmdsize - name offset ) ) . decode ( ) load = load [ : load . index ( '\x00' ) ] if ( load == what ) : file . seek ( ( where + name offset ) , os . SEEK SET ) file . write ( ( value . encode ( ) + '\x00' . encode ( ) ) ) file . seek ( ( where + cmdsize ) , os . SEEK SET ) def do file ( file , offset = 0 , size = maxint ) : file = fileview ( file , offset , size ) magic = read data ( file , BIG ENDIAN ) if ( magic == FAT MAGIC ) : nfat arch = read data ( file , BIG ENDIAN ) for n in range ( nfat arch ) : ( cputype , cpusubtype , offset , size , align ) = read data ( file , BIG ENDIAN , 5 ) do file ( file , offset , size ) elif ( magic == MH MAGIC ) : do macho ( file , 32 , BIG ENDIAN ) elif ( magic == MH CIGAM ) : do macho ( file , 32 , LITTLE ENDIAN ) elif ( magic == MH MAGIC 64 ) : do macho ( file , 64 , BIG ENDIAN ) elif ( magic == MH CIGAM 64 ) : do macho ( file , 64 , LITTLE ENDIAN ) assert ( len ( what ) >= len ( value ) ) with open ( path , 'r+b' ) as f : do file ( f ) 
def add qos ( tenant id , qos name , qos desc ) : LOG . debug (   ( 'add qos()  called' ) ) session = db . get session ( ) try : qos = session . query ( network models v2 .  Qo S ) . filter by ( tenant id = tenant id ) . filter by ( qos name = qos name ) . one ( ) raise c exc .  Qos  Name  Already  Exists  ( qos name = qos name , tenant id = tenant id ) except exc .  No  Result  Found  : qos = network models v2 .  Qo S ( tenant id , qos name , qos desc ) session . add ( qos ) session . flush ( ) return qos 
def get id ( name =  None  , tags =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  , in states =  None  , filters =  None  ) : instance ids = find instances ( name = name , tags = tags , region = region , key = key , keyid = keyid , profile = profile , in states = in states , filters = filters ) if instance ids : log . info ( ' Instance   ids:  {0}' . format ( '  ' . join ( instance ids ) ) ) if ( len ( instance ids ) == 1 ) : return instance ids [ 0 ] else : raise  Command  Execution  Error  ( ' Found   more  than  one  instance  matching  the  criteria.' ) else : log . warning ( ' Could   not  find  instance.' ) return  None  
def find functions ( code ) : regex = ( ( '^\\s*' + re func decl ) + '\\s*{' ) funcs = [ ] while  True  : m = re . search ( regex , code , re . M ) if ( m is  None  ) : return funcs ( rtype , name , args ) = m . groups ( ) [ : 3 ] if ( ( args == 'void' ) or ( args . strip ( ) == '' ) ) : args = [ ] else : args = [ tuple ( arg . strip ( ) . split ( '  ' ) ) for arg in args . split ( ',' ) ] funcs . append ( ( name , args , rtype ) ) code = code [ m . end ( ) : ] 
def  whitelist method generator ( klass , whitelist ) : method wrapper template = 'def  %(name)s(%(sig)s)  :\n        """\n        %(doc)s\n        """\n        f  =  %(self)s.  getattr  (\'%(name)s\')\n        return  f(%(args)s)' property wrapper template = '@property\ndef  %(name)s(self)  :\n        """\n        %(doc)s\n        """\n        return  self.  getattr  (\'%(name)s\')' for name in whitelist : if hasattr (  Group  By  , name ) : continue f = getattr ( klass , name ) doc = f .   doc   doc = ( doc if ( type ( doc ) == str ) else '' ) if isinstance ( f , types .  Method  Type  ) : wrapper template = method wrapper template ( decl , args ) = make signature ( f ) args by name = [ '{0}={0}' . format ( arg ) for arg in args [ 1 : ] ] params = { 'name' : name , 'doc' : doc , 'sig' : ',' . join ( decl ) , 'self' : args [ 0 ] , 'args' : ',' . join ( args by name ) } else : wrapper template = property wrapper template params = { 'name' : name , 'doc' : doc } ( yield ( wrapper template % params ) ) 
def create node ( vm  , newid ) : newnode = { } if ( 'technology' not in vm  ) : vm  [ 'technology' ] = 'openvz' if ( vm  [ 'technology' ] not in [ 'qemu' , 'openvz' , 'lxc' ] ) : log . error ( ' Wrong   VM  type.   Valid   options  are:  qemu,  openvz  (proxmox3)  or  lxc  (proxmox4)' ) raise  Salt  Cloud  Execution  Failure  if ( 'host' not in vm  ) : vm  [ 'host' ] = config . get cloud config value ( 'default host' , get configured provider ( ) ,   opts   , search global =  False  ) if ( vm  [ 'host' ] is  None  ) : log . error ( ' No   host  given  to  create  this  VM  on' ) raise  Salt  Cloud  Execution  Failure  vmhost = vm  [ 'host' ] newnode [ 'vmid' ] = newid for prop in ( 'cpuunits' , 'description' , 'memory' , 'onboot' ) : if ( prop in vm  ) : newnode [ prop ] = vm  [ prop ] if ( vm  [ 'technology' ] == 'openvz' ) : newnode [ 'hostname' ] = vm  [ 'name' ] newnode [ 'ostemplate' ] = vm  [ 'image' ] for prop in ( 'cpus' , 'disk' , 'ip address' , 'nameserver' , 'password' , 'swap' , 'poolid' , 'storage' ) : if ( prop in vm  ) : newnode [ prop ] = vm  [ prop ] elif ( vm  [ 'technology' ] == 'lxc' ) : newnode [ 'hostname' ] = vm  [ 'name' ] newnode [ 'ostemplate' ] = vm  [ 'image' ] for prop in ( 'cpuunits' , 'description' , 'memory' , 'onboot' , 'net0' , 'password' , 'nameserver' , 'swap' , 'storage' , 'rootfs' ) : if ( prop in vm  ) : newnode [ prop ] = vm  [ prop ] if ( 'disk' in vm  ) : log . warning ( ' The   "disk"  option  is  not  supported  for  LXC  hosts  and  was  ignored' ) if ( ( 'ip address' in vm  ) and ( 'net0' not in vm  ) ) : newnode [ 'net0' ] = ( ( 'bridge=vmbr0,ip=' + vm  [ 'ip address' ] ) + '/24,name=eth0,type=veth' ) if ( 'gw' in vm  ) : newnode [ 'net0' ] = ( ( newnode [ 'net0' ] + ',gw=' ) + vm  [ 'gw' ] ) elif ( vm  [ 'technology' ] == 'qemu' ) : for prop in ( 'acpi' , 'cores' , 'cpu' , 'pool' , 'storage' , 'sata0' , 'ostype' , 'ide2' , 'net0' ) : if ( prop in vm  ) : newnode [ prop ] = vm  [ prop ]   utils   [ 'cloud.fire event' ] ( 'event' , 'requesting  instance' , 'salt/cloud/{0}/requesting' . format ( vm  [ 'name' ] ) , args = { 'kwargs' : newnode } , sock dir =   opts   [ 'sock dir' ] ) log . debug ( ' Preparing   to  generate  a  node  using  these  parameters:  {0}  ' . format ( newnode ) ) if ( ( 'clone' in vm  ) and ( vm  [ 'clone' ] is  True  ) and ( vm  [ 'technology' ] == 'qemu' ) ) : post Params  = { } post Params  [ 'newid' ] = newnode [ 'vmid' ] for prop in ( 'description' , 'format' , 'full' , 'name' ) : if ( ( 'clone ' + prop ) in vm  ) : post Params  [ prop ] = vm  [ ( 'clone ' + prop ) ] node = query ( 'post' , 'nodes/{0}/qemu/{1}/clone' . format ( vmhost , vm  [ 'clone from' ] ) , post Params  ) else : node = query ( 'post' , 'nodes/{0}/{1}' . format ( vmhost , vm  [ 'technology' ] ) , newnode ) return  parse proxmox upid ( node , vm  ) 
def macro ( name ) : def inner ( view , context , model , column ) : m = context . resolve ( name ) if ( not m ) : return m return m ( model = model , column = column ) return inner 
def autoescape ( parser , token ) : args = token . contents . split ( ) if ( len ( args ) != 2 ) : raise  Template  Syntax  Error  ( "'autoescape'  tag  requires  exactly  one  argument." ) arg = args [ 1 ] if ( arg not in ( u'on' , u'off' ) ) : raise  Template  Syntax  Error  ( "'autoescape'  argument  should  be  'on'  or  'off'" ) nodelist = parser . parse ( ( 'endautoescape' , ) ) parser . delete first token ( ) return  Auto  Escape  Control  Node  ( ( arg == 'on' ) , nodelist ) 
def main ( ) : argument spec = dict ( src = dict ( ) , force = dict ( default =  False  , type = 'bool' ) , backup = dict ( default =  False  , type = 'bool' ) , config = dict ( ) ) argument spec . update (  transitional argument spec ( ) ) mutually exclusive = [ ( 'config' , 'backup' ) , ( 'config' , 'force' ) ] module =  Local  Ansible  Module  ( argument spec = argument spec , mutually exclusive = mutually exclusive , supports check mode =  True  ) warnings = check args ( module ) result = dict ( changed =  False  , warnings = warnings ) candidate =  Network  Config  ( contents = module . params [ 'src' ] , indent = 1 ) if module . params [ 'backup' ] : result [ '  backup  ' ] = get config ( module ) if ( not module . params [ 'force' ] ) : contents = get config ( module ) configobj =  Network  Config  ( contents = contents , indent = 1 ) commands = candidate . difference ( configobj ) commands = dumps ( commands , 'commands' ) . split ( '\n' ) commands = [ str ( c ) . strip ( ) for c in commands if c ] else : commands = [ c . strip ( ) for c in str ( candidate ) . split ( '\n' ) ] if commands : load config ( module , commands , ( not module . check mode ) ) result [ 'changed' ] = ( not module . check mode ) result [ 'updates' ] = commands module . exit json ( ** result ) 
def computer desc ( name ) : name = str ( name ) ret = { 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : " Computer   description  already  set  to  '{0}'" . format ( name ) } before desc =   salt   [ 'system.get computer desc' ] ( ) if ( before desc == name ) : return ret if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = " Computer   description  will  be  changed  to  '{0}'" . format ( name ) return ret result =   salt   [ 'system.set computer desc' ] ( name ) if ( result [ ' Computer    Description ' ] == name ) : ret [ 'comment' ] = " Computer   description  successfully  changed  to  '{0}'" . format ( name ) ret [ 'changes' ] = { 'old' : before desc , 'new' : name } else : ret [ 'result' ] =  False  ret [ 'comment' ] = " Unable   to  set  computer  description  to  '{0}'" . format ( name ) return ret 
def find distributions ( path item , only =  False  ) : importer = get importer ( path item ) finder =  find adapter (  distribution finders , importer ) return finder ( importer , path item , only ) 
def log warn ( warnmsg ) : try : warnmsg = str ( warnmsg ) except  Exception  as e : warnmsg = str ( e ) for line in warnmsg . splitlines ( ) : log . msg ( ( '[WW]  %s' % line ) ) 
def reload  ( name ) : if   SYSLOG NG BINARY PATH : syslog ng ctl binary = os . path . join (   SYSLOG NG BINARY PATH , 'syslog-ng-ctl' ) command = [ syslog ng ctl binary , 'reload' ] result =   salt   [ 'cmd.run all' ] ( command , python shell =  False  ) else : command = [ 'syslog-ng-ctl' , 'reload' ] result =   salt   [ 'cmd.run all' ] ( command , python shell =  False  ) succ = (  True  if ( result [ 'retcode' ] == 0 ) else  False  ) return  format state result ( name , result = succ , comment = result [ 'stdout' ] ) 
def obj equal prims ( obj 1 , obj 2 , ignore =  None  ) : def  strip ( prim , keys ) : if isinstance ( prim , dict ) : for k in keys : prim . pop ( k ,  None  ) for v in prim . values ( ) :  strip ( v , keys ) if isinstance ( prim , list ) : for v in prim :  strip ( v , keys ) return prim if ( ignore is not  None  ) : keys = ( [ 'nova object.changes' ] + ignore ) else : keys = [ 'nova object.changes' ] prim 1 =  strip ( obj 1 . obj to primitive ( ) , keys ) prim 2 =  strip ( obj 2 . obj to primitive ( ) , keys ) return ( prim 1 == prim 2 ) 
def get root path ( import name ) : mod = sys . modules . get ( import name ) if ( ( mod is not  None  ) and hasattr ( mod , '  file  ' ) ) : return os . path . dirname ( os . path . abspath ( mod .   file   ) ) loader = pkgutil . get loader ( import name ) if ( ( loader is  None  ) or ( import name == '  main  ' ) ) : return os . getcwd ( ) if hasattr ( loader , 'get filename' ) : filepath = loader . get filename ( import name ) else :   import   ( import name ) filepath = sys . modules [ import name ] .   file   return os . path . dirname ( os . path . abspath ( filepath ) ) 
def attachment specs get ( context , attachment id ) : return IMPL . attachment specs get ( context , attachment id ) 
def  format jid instance ( jid , job ) : ret =  format job instance ( job ) ret . update ( { ' Start  Time ' : salt . utils . jid . jid to time ( jid ) } ) return ret 
def get auth params from request ( request ) : return ( request . user . username , request . user . token . id , request . user . tenant id , base . url for ( request , 'compute' ) , base . url for ( request , 'identity' ) ) 
def load translations ( directory , encoding =  None  ) : global  translations global  supported locales  translations = { } for path in os . listdir ( directory ) : if ( not path . endswith ( '.csv' ) ) : continue ( locale , extension ) = path . split ( '.' ) if ( not re . match ( '[a-z]+( [A-Z]+)?$' , locale ) ) : gen log . error ( ' Unrecognized   locale  %r  (path:  %s)' , locale , os . path . join ( directory , path ) ) continue full path = os . path . join ( directory , path ) if ( encoding is  None  ) : with open ( full path , 'rb' ) as f : data = f . read ( len ( codecs . BOM UTF16 LE ) ) if ( data in ( codecs . BOM UTF16 LE , codecs . BOM UTF16 BE ) ) : encoding = 'utf-16' else : encoding = 'utf-8-sig' if PY3 : f = open ( full path , 'r' , encoding = encoding ) else : f =  Bytes IO ( ) with codecs . open ( full path , 'r' , encoding = encoding ) as infile : f . write ( escape . utf8 ( infile . read ( ) ) ) f . seek ( 0 )  translations [ locale ] = { } for ( i , row ) in enumerate ( csv . reader ( f ) ) : if ( ( not row ) or ( len ( row ) < 2 ) ) : continue row = [ escape . to unicode ( c ) . strip ( ) for c in row ] ( english , translation ) = row [ : 2 ] if ( len ( row ) > 2 ) : plural = ( row [ 2 ] or 'unknown' ) else : plural = 'unknown' if ( plural not in ( 'plural' , 'singular' , 'unknown' ) ) : gen log . error ( ' Unrecognized   plural  indicator  %r  in  %s  line  %d' , plural , path , ( i + 1 ) ) continue  translations [ locale ] . setdefault ( plural , { } ) [ english ] = translation f . close ( )  supported locales = frozenset ( ( list (  translations . keys ( ) ) + [  default locale ] ) ) gen log . debug ( ' Supported   locales:  %s' , sorted (  supported locales ) ) 
def unwrap order by ( clause ) : cols = util . column set ( ) stack = deque ( [ clause ] ) while stack : t = stack . popleft ( ) if ( isinstance ( t ,  Column  Element  ) and ( ( not isinstance ( t ,  Unary  Expression  ) ) or ( not operators . is ordering modifier ( t . modifier ) ) ) ) : cols . add ( t ) else : for c in t . get children ( ) : stack . append ( c ) return cols 
def parse entry ( entry str , separator = '=' ) : entry = { } for line in entry str . splitlines ( ) : if ( len ( line ) == 0 ) : continue try : ( name , value ) = line . split ( separator , 1 ) except  Value  Error  : continue name = name . strip ( ) value = value . strip ( ) if ( name == 'index' ) : value = int ( value ) entry [ name ] = value return entry 
def training updates ( visible batch , model , sampler , optimizer ) : sampler updates = sampler . updates ( ) pos v = visible batch neg v = sampler . particles grads = model . ml gradients ( pos v , neg v ) ups = optimizer . updates ( gradients = grads ) safe update ( ups , sampler updates ) return ups 
def  truncate to field ( model , field name , value ) : field = model .  meta . get field ( field name ) if ( len ( value ) > field . max length ) : midpoint = ( field . max length // 2 ) len after midpoint = ( field . max length - midpoint ) first = value [ : midpoint ] sep = u'...' last = value [ ( ( len ( value ) - len after midpoint ) + len ( sep ) ) : ] value = sep . join ( [ first , last ] ) return value 
def  load config ( filename ) : if ( not os . path . isfile ( filename ) ) : return { } try : with open ( filename , 'r' ) as fdesc : inp = fdesc . read ( ) if ( not inp ) : return { } return json . loads ( inp ) except ( IO Error  ,  Value  Error  ) as error :  LOGGER . error ( ' Reading   config  file  %s  failed:  %s' , filename , error ) return  None  
def filter err ( status , application = 'app' , ticket = 'tkt' ) : routes = THREAD LOCAL . routes if ( ( status > 399 ) and routes . routes onerror ) : keys = set ( ( ( '%s/%s' % ( application , status ) ) , ( '%s/*' % application ) , ( '*/%s' % status ) , '*/*' ) ) for ( key , redir ) in routes . routes onerror : if ( key in keys ) : if ( redir == '!' ) : break elif ( '?' in redir ) : url = ( ( redir + '&' ) + ( 'code=%s&ticket=%s' % ( status , ticket ) ) ) else : url = ( ( redir + '?' ) + ( 'code=%s&ticket=%s' % ( status , ticket ) ) ) return url return status 
def float nan ( n ) : if ( n != n ) : return  None  else : return float ( n ) 
def building ( ) : ctable = s3db . gis config config = db ( ( ctable . name == ' Queens ' ) ) . select ( ctable . id , limitby = ( 0 , 1 ) ) . first ( ) if config : gis . set config ( config . id ) def prep ( r ) : if r . interactive : if ( r . method == 'map' ) : s3db . configure ( 'assess building' , marker fn = building marker fn ) elif ( r . representation == 'geojson' ) : mtable = s3db . gis marker s3db . configure ( 'assess building' , marker fn = building marker fn ) return  True  s3 . prep = prep return s3 rest controller ( rheader = s3db . assess building rheader ) 
@ pytest . mark . parametrize ( 'scorer,processor' , full scorers processors ( ) ) @ given ( data = st . data ( ) ) @ settings ( max examples = 100 ) def test only identical strings extracted ( scorer , processor , data ) : strings = data . draw ( st . lists ( st . text ( min size = 10 , max size = 100 ) , min size = 1 , max size = 50 ) ) choiceidx = data . draw ( st . integers ( min value = 0 , max value = ( len ( strings ) - 1 ) ) ) choice = strings [ choiceidx ] assume ( ( processor ( choice ) != '' ) ) result = process . extract Bests  ( choice , strings , scorer = scorer , processor = processor , score cutoff = 100 , limit =  None  ) assert ( result != [ ] ) pchoice = processor ( choice ) for r in result : assert ( pchoice == processor ( r [ 0 ] ) ) 
@ handle response format @ treeio login required @  process mass form def index owned ( request , response format = 'html' ) : query = ( Q ( object  in =  Object  . filter by request ( request ,  Object  . objects ) ) & Q ( author = request . user . profile ) ) if request . GET : query = ( query &  get filter query ( request . GET ) ) filters =  Filter  Form  ( request . user . profile , 'author' , request . GET ) else : query = ( query & Q ( status  hidden =  False  ) ) filters =  Filter  Form  ( request . user . profile , 'author' ) changesets =  Change  Set  . objects . filter ( query ) context =  get default context ( request ) context . update ( { 'filters' : filters , 'changesets' : changesets } ) return render to response ( 'changes/index owned' , context , context instance =  Request  Context  ( request ) , response format = response format ) 
def present ( name , character set =  None  , collate =  None  , ** connection args ) : ret = { 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : ' Database   {0}  is  already  present' . format ( name ) } existing =   salt   [ 'mysql.db get' ] ( name , ** connection args ) if existing : alter =  False  if ( character set and ( character set != existing . get ( 'character set' ) ) ) : LOG . debug ( 'character  set  differes  from  {0}  :  {1}' . format ( character set , existing . get ( 'character set' ) ) ) alter =  True  if ( collate and ( collate != existing . get ( 'collate' ) ) ) : LOG . debug ( 'collate  set  differes  from  {0}  :  {1}' . format ( collate , existing . get ( 'collate' ) ) ) alter =  True  if alter :   salt   [ 'mysql.alter db' ] ( name , character set = character set , collate = collate , ** connection args ) current =   salt   [ 'mysql.db get' ] ( name , ** connection args ) if ( existing . get ( 'collate' ,  None  ) != current . get ( 'collate' ,  None  ) ) : ret [ 'changes' ] . update ( { 'collate' : { 'before' : existing . get ( 'collate' ,  None  ) , 'now' : current . get ( 'collate' ,  None  ) } } ) if ( existing . get ( 'character set' ,  None  ) != current . get ( 'character set' ,  None  ) ) : ret [ 'changes' ] . update ( { 'character set' : { 'before' : existing . get ( 'character set' ,  None  ) , 'now' : current . get ( 'character set' ,  None  ) } } ) return ret else : err =  get mysql error ( ) if ( err is not  None  ) : ret [ 'comment' ] = err ret [ 'result' ] =  False  return ret if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = ' Database   {0}  is  not  present  and  needs  to  be  created' . format ( name ) return ret if   salt   [ 'mysql.db create' ] ( name , character set = character set , collate = collate , ** connection args ) : ret [ 'comment' ] = ' The   database  {0}  has  been  created' . format ( name ) ret [ 'changes' ] [ name ] = ' Present ' else : ret [ 'comment' ] = ' Failed   to  create  database  {0}' . format ( name ) err =  get mysql error ( ) if ( err is not  None  ) : ret [ 'comment' ] += '  ({0})' . format ( err ) ret [ 'result' ] =  False  return ret 
@ block user agents @ require GET def needs review ( request , tag =  None  ) : tag obj = ( ( tag and get object or 404 (  Review  Tag  , name = tag ) ) or  None  ) docs =  Document  . objects . filter for review ( locale = request . LANGUAGE CODE , tag = tag obj ) paginated docs = paginate ( request , docs , per page = DOCUMENTS PER PAGE ) context = { 'documents' : paginated docs , 'count' : docs . count ( ) , 'tag' : tag obj , 'tag name' : tag } return render ( request , 'wiki/list/needs review.html' , context ) 
def head ( bucket , path =  None  , key =  None  , keyid =  None  , service url =  None  , verify ssl =  None  , kms keyid =  None  , location =  None  , role arn =  None  , path style =  None  , https enable =  None  ) : ( key , keyid , service url , verify ssl , kms keyid , location , role arn , path style , https enable ) =  get key ( key , keyid , service url , verify ssl , kms keyid , location , role arn , path style , https enable ) return   utils   [ 's3.query' ] ( method = 'HEAD' , bucket = bucket , path = path , key = key , keyid = keyid , kms keyid = kms keyid , service url = service url , verify ssl = verify ssl , location = location , full headers =  True  , role arn = role arn , path style = path style , https enable = https enable ) 
def update org user ( userid , orgname =  None  , profile = 'grafana' , ** kwargs ) : if isinstance ( profile , string types ) : profile =   salt   [ 'config.option' ] ( profile ) if orgname : switch org ( orgname , profile ) response = requests . patch ( '{0}/api/org/users/{1}' . format ( profile [ 'grafana url' ] , userid ) , json = kwargs , auth =  get auth ( profile ) , headers =  get headers ( profile ) , timeout = profile . get ( 'grafana timeout' , 3 ) ) if ( response . status code >= 400 ) : response . raise for status ( ) return response . json ( ) 
def categorical order ( values , order =  None  ) : if ( order is  None  ) : if hasattr ( values , 'categories' ) : order = values . categories else : try : order = values . cat . categories except (  Type  Error  ,  Attribute  Error  ) : try : order = values . unique ( ) except  Attribute  Error  : order = pd . unique ( values ) try : np . asarray ( values ) . astype ( np . float ) order = np . sort ( order ) except (  Value  Error  ,  Type  Error  ) : order = order order = filter ( pd . notnull , order ) return list ( order ) 
def test require python package ( venv ) : from fabtools import require import fabtools with fabtools . python . virtualenv ( venv ) : require . python . package ( 'fabric' ) assert is file ( posixpath . join ( venv , 'bin/fab' ) ) 
def  determine scaling policies ( scaling policies , scaling policies from pillar ) : pillar scaling policies = copy . deepcopy (   salt   [ 'config.option' ] ( scaling policies from pillar , { } ) ) if ( ( not scaling policies ) and ( len ( pillar scaling policies ) > 0 ) ) : scaling policies = pillar scaling policies return scaling policies 
def  afterpoint ( string ) : if  isnumber ( string ) : if  isint ( string ) : return ( - 1 ) else : pos = string . rfind ( u'.' ) pos = ( string . lower ( ) . rfind ( u'e' ) if ( pos < 0 ) else pos ) if ( pos >= 0 ) : return ( ( len ( string ) - pos ) - 1 ) else : return ( - 1 ) else : return ( - 1 ) 
@ register . tag def firstof ( parser , token ) : bits = token . split contents ( ) [ 1 : ] if ( len ( bits ) < 1 ) : raise  Template  Syntax  Error  ( u"'firstof'  statement  requires  at  least  one  argument" ) return  First  Of  Node  ( [ parser . compile filter ( bit ) for bit in bits ] ) 
def snapshot ( name , snap name =  None  , desc =  None  , runas =  None  ) : name =  sdecode ( name ) if snap name : snap name =  sdecode ( snap name ) args = [ name ] if snap name : args . extend ( [ '--name' , snap name ] ) if desc : args . extend ( [ '--description' , desc ] ) return prlctl ( 'snapshot' , args , runas = runas ) 
def dict from expr ( expr , ** args ) : ( rep , opt ) =  dict from expr ( expr , build options ( args ) ) return ( rep , opt . gens ) 
def index groups ( request ) : group form = forms .  Group  Create  Form  ( ( request . POST or  None  ) ) if group form . is valid ( ) : group = group form . save ( ) group . curators . add ( request . user . userprofile ) group . add member ( request . user . userprofile ,  Group  Membership  . MEMBER ) return redirect ( reverse ( 'groups:group edit' , args = [ group . url ] ) ) query =  Group  . get non functional areas ( ) template = 'groups/index groups.html' context = { 'group form' : group form } return  list groups ( request , template , query , context ) 
def check verify status by course ( user , course enrollments ) : status by course = { } verifications =  Software  Secure  Photo  Verification  . objects . filter ( user = user ) has active or pending =  Software  Secure  Photo  Verification  . user has valid or pending ( user , queryset = verifications ) expiration datetime =  Software  Secure  Photo  Verification  . get expiration datetime ( user , verifications ) verification expiring soon =  Software  Secure  Photo  Verification  . is verification expiring soon ( expiration datetime ) enrolled course keys = [ enrollment . course id for enrollment in course enrollments ] course deadlines =  Verification  Deadline  . deadlines for courses ( enrolled course keys ) recent verification datetime =  None  for enrollment in course enrollments : if ( enrollment . mode in  Course  Mode  . VERIFIED MODES ) : deadline = course deadlines . get ( enrollment . course id ) relevant verification =  Software  Secure  Photo  Verification  . verification for datetime ( deadline , verifications ) if ( ( relevant verification is not  None  ) and ( relevant verification . status == 'approved' ) ) : recent verification datetime = max ( ( recent verification datetime if ( recent verification datetime is not  None  ) else relevant verification . expiration datetime ) , relevant verification . expiration datetime ) status =  None  if ( relevant verification is not  None  ) : if ( relevant verification . status == 'approved' ) : if verification expiring soon : status = VERIFY STATUS NEED TO REVERIFY else : status = VERIFY STATUS APPROVED elif ( relevant verification . status == 'submitted' ) : if verification expiring soon : status = VERIFY STATUS RESUBMITTED else : status = VERIFY STATUS SUBMITTED submitted = ( ( relevant verification is not  None  ) and ( relevant verification . status not in [ 'created' , 'ready' ] ) ) if ( ( status is  None  ) and ( not submitted ) ) : if ( ( deadline is  None  ) or ( deadline > datetime . now ( UTC ) ) ) : if  Software  Secure  Photo  Verification  . user is verified ( user ) : if verification expiring soon : status = VERIFY STATUS NEED TO REVERIFY else : status = VERIFY STATUS NEED TO VERIFY elif has active or pending : status = VERIFY STATUS APPROVED else : status = VERIFY STATUS MISSED DEADLINE if ( status is not  None  ) : days until deadline =  None  now = datetime . now ( UTC ) if ( ( deadline is not  None  ) and ( deadline > now ) ) : days until deadline = ( deadline - now ) . days status by course [ enrollment . course id ] = { 'status' : status , 'days until deadline' : days until deadline } if recent verification datetime : for ( key , value ) in status by course . iteritems ( ) : status by course [ key ] [ 'verification good until' ] = recent verification datetime . strftime ( '%m/%d/%Y' ) return status by course 
def  upgrade ( fields , sig ) : sig . update ( chord size = fields . get ( u'chord size' ) ) return sig 
def blackwhite ( clip , RGB = [ 1 , 1 , 1 ] , preserve luminosity =  True  ) : if ( RGB == 'CRT phosphor' ) : RGB = [ 0.2125 , 0.7154 , 0.0721 ] ( R , G , B ) = ( ( 1.0 * np . array ( RGB ) ) / ( sum ( RGB ) if preserve luminosity else 1 ) ) def fl ( im ) : im = ( ( ( R * im [ : , : , 0 ] ) + ( G * im [ : , : , 1 ] ) ) + ( B * im [ : , : , 2 ] ) ) return np . dstack ( ( 3 * [ im ] ) ) . astype ( 'uint8' ) return clip . fl image ( fl ) 
def test comments ( ) : feature =  Feature  . from string ( FEATURE10 ) assert equals ( feature . max length , 55 ) 
def  write file ( iface , data , folder , pattern ) : filename = os . path . join ( folder , pattern . format ( iface ) ) if ( not os . path . exists ( folder ) ) : msg = '{0}  cannot  be  written.  {1}  does  not  exist' msg = msg . format ( filename , folder ) log . error ( msg ) raise  Attribute  Error  ( msg ) with salt . utils . flopen ( filename , 'w' ) as fout : fout . write ( data ) return filename 
def dt to timestamp ( dt ) : if dt . tzinfo : td = ( dt - EPOCH AWARE ) else : td = ( dt - EPOCH NAIVE ) return total seconds ( td ) 
@ depends ( HAS ESX CLI ) def reset syslog config ( host , username , password , protocol =  None  , port =  None  , syslog config =  None  , esxi hosts =  None  ) : if ( not syslog config ) : raise  Command  Execution  Error  ( " The   'reset syslog config'  function  requires  a  'syslog config'  setting." ) valid resets = [ 'logdir' , 'loghost' , 'default-rotate' , 'default-size' , 'default-timeout' , 'logdir-unique' ] cmd = 'system  syslog  config  set  --reset=' if ( ',' in syslog config ) : resets = [ ind reset . strip ( ) for ind reset in syslog config . split ( ',' ) ] elif ( syslog config == 'all' ) : resets = valid resets else : resets = [ syslog config ] ret = { } if esxi hosts : if ( not isinstance ( esxi hosts , list ) ) : raise  Command  Execution  Error  ( "'esxi hosts'  must  be  a  list." ) for esxi host in esxi hosts : response dict =  reset syslog config params ( host , username , password , cmd , resets , valid resets , protocol = protocol , port = port , esxi host = esxi host ) ret . update ( { esxi host : response dict } ) else : response dict =  reset syslog config params ( host , username , password , cmd , resets , valid resets , protocol = protocol , port = port ) ret . update ( { host : response dict } ) return ret 
def rewrite single shorthand state decl ( data ) : for ( sid , states ) in six . iteritems ( data ) : if isinstance ( states , six . string types ) : data [ sid ] = { states : [ ] } 
@ login required @ require GET def program details ( request , program id ) : programs config =  Programs  Api  Config  . current ( ) if ( not programs config . show program details ) : raise  Http 404 try : uuid . UUID ( program id ) program data = get catalog programs ( request . user , uuid = program id ) if program data : program data = munge catalog program ( program data ) except  Value  Error  : program data = utils . get programs ( request . user , program id = program id ) if ( not program data ) : raise  Http 404 program data = utils .  Program  Data  Extender  ( program data , request . user ) . extend ( ) urls = { 'program listing url' : reverse ( 'program listing view' ) , 'track selection url' : strip course id ( reverse ( 'course modes choose' , kwargs = { 'course id' : FAKE COURSE KEY } ) ) , 'commerce api url' : reverse ( 'commerce api:v0:baskets:create' ) } context = { 'program data' : program data , 'urls' : urls , 'show program listing' : programs config . show program listing , 'nav hidden' :  True  , 'disable courseware js' :  True  , 'uses pattern library' :  True  , 'user preferences' : get user preferences ( request . user ) } return render to response ( 'learner dashboard/program details.html' , context ) 
def toURI Normal  ( xri ) : return iri To URI ( toIRI Normal  ( xri ) ) 
def  escape value ( value ) : return value . replace ( '\\' , '\\\\' ) . replace ( '\n' , '\\n' ) . replace ( ' DCTB ' , '\\t' ) . replace ( '"' , '\\"' ) 
def search Refs  ( obj , * args ) : ignore = { id ( sys .  getframe ( ) ) :  None  } gc . collect ( ) refs = gc . get referrers ( obj ) ignore [ id ( refs ) ] =  None  refs = [ r for r in refs if ( id ( r ) not in ignore ) ] for a in args : if ( type ( a ) is int ) : obj = refs [ a ] gc . collect ( ) refs = gc . get referrers ( obj ) ignore [ id ( refs ) ] =  None  refs = [ r for r in refs if ( id ( r ) not in ignore ) ] elif ( a == 't' ) : print ( list ( map ( type Str  , refs ) ) ) elif ( a == 'i' ) : print ( list ( map ( id , refs ) ) ) elif ( a == 'l' ) : def slen ( o ) : if hasattr ( o , '  len  ' ) : return len ( o ) else : return  None  print ( list ( map ( slen , refs ) ) ) elif ( a == 'o' ) : print ( obj ) elif ( a == 'ro' ) : return obj elif ( a == 'rr' ) : return refs 
def tower loss ( scope ) : ( images , labels ) = cifar10 . distorted inputs ( ) logits = cifar10 . inference ( images )   = cifar10 . loss ( logits , labels ) losses = tf . get collection ( 'losses' , scope ) total loss = tf . add n ( losses , name = 'total loss' ) for l in ( losses + [ total loss ] ) : loss name = re . sub ( ( '%s [0-9]*/' % cifar10 . TOWER NAME ) , '' , l . op . name ) tf . scalar summary ( loss name , l ) return total loss 
def create security group rule ( security group , remote group id =  None  , direction = 'ingress' , protocol =  None  , port range min =  None  , port range max =  None  , ethertype = 'I Pv 4' , profile =  None  ) : conn =  auth ( profile ) return conn . create security group rule ( security group , remote group id , direction , protocol , port range min , port range max , ethertype ) 
def tokenize ( content ) : return [ token . encode ( 'utf8' ) for token in utils . tokenize ( content , lower =  True  , errors = 'ignore' ) if ( ( 2 <= len ( token ) <= 15 ) and ( not token . startswith ( ' ' ) ) ) ] 
def date to month number ( date ) : return year month to month number ( date . year , date . month ) 
def set cover position ( hass , position , entity id =  None  ) : data = ( { ATTR ENTITY ID : entity id } if entity id else { } ) data [ ATTR POSITION ] = position hass . services . call ( DOMAIN , SERVICE SET COVER POSITION , data ) 
def set urlconf ( urlconf name ) : if urlconf name :  urlconfs . value = urlconf name elif hasattr (  urlconfs , u'value' ) : del  urlconfs . value 
@ pytest . mark . linux def test ascii locale ( request , httpbin , tmpdir , quteproc new ) : if request . config . webengine : pytest . skip ( ' Downloads   are  not  implemented  with   Qt  Web  Engine   yet' ) args = ( [ '--temp-basedir' ] +  base args ( request . config ) ) quteproc new . start ( args , env = { 'LC ALL' : 'C' } ) quteproc new . set setting ( 'storage' , 'download-directory' , str ( tmpdir ) ) quteproc new . set setting ( 'storage' , 'prompt-download-directory' , 'false' ) url = 'http://localhost:{port}/data/downloads/\xc3\xa4-issue908.bin' . format ( port = httpbin . port ) quteproc new . send cmd ( ':download  {}' . format ( url ) ) quteproc new . wait for ( category = 'downloads' , message = ' Download   ?-issue908.bin  finished' ) quteproc new . set setting ( 'storage' , 'prompt-download-directory' , 'true' ) quteproc new . send cmd ( ':download  {}' . format ( url ) ) quteproc new . send cmd ( ':prompt-open-download  "{}"  -c  pass' . format ( sys . executable ) ) quteproc new . wait for ( category = 'downloads' , message = ' Download   \xc3\xa4-issue908.bin  finished' ) quteproc new . wait for ( category = 'downloads' , message = ' Opening   *  with  [*python*]' ) assert ( len ( tmpdir . listdir ( ) ) == 1 ) assert ( tmpdir / '?-issue908.bin' ) . exists ( ) 
def safe repr ( obj , short =  False  ) :  MAX LENGTH = 80 try : result = repr ( obj ) except  Exception  : result = object .   repr   ( obj ) if ( ( not short ) or ( len ( result ) <  MAX LENGTH ) ) : return result return ( result [ :  MAX LENGTH ] + '  [truncated]...' ) 
@ task ( ) def manage ( ctx , cmd str ) : manage cmd = os . path . join ( HERE , '..' , 'manage.py' ) env = 'DJANGO SETTINGS MODULE="admin.base.settings"' cmd = '{}  python  {}  {}' . format ( env , manage cmd , cmd str ) ctx . run ( cmd , echo =  True  , pty =  True  ) 
def list networks ( service instance ) : return list objects ( service instance , vim .  Network  ) 
def write double ( fid , kind , data ) : data size = 8 data = np . array ( data , dtype = '>f8' ) . T  write ( fid , data , kind , data size , FIFF . FIFFT DOUBLE , '>f8' ) 
def get elliptic curve ( name ) : for curve in get elliptic curves ( ) : if ( curve . name == name ) : return curve raise  Value  Error  ( 'unknown  curve  name' , name ) 
def patch nsis ( ) : RE NSIS = re . compile ( '^(\\s* Lang  String \\s+\\w+\\s+\\$\\{LANG )(\\w+)\\}\\s+(".*)' , re . I ) RE NSIS = re . compile ( '^(\\s* Lang  String \\s+)(\\w+)(\\s+\\$\\{LANG )(\\w+)\\}\\s+(".*)' , re . I ) languages = [ os . path . split ( path ) [ 1 ] for path in glob . glob ( os . path . join ( MO DIR , '*' ) ) ] src = open ( NSIS , 'r' ) new = [ ] for line in src : m = RE NSIS . search ( line ) if m : leader = m . group ( 1 ) item = m . group ( 2 ) rest = m . group ( 3 ) langname = m . group ( 4 ) . upper ( ) text = m . group ( 5 ) . strip ( '"\n' ) if ( langname == 'ENGLISH' ) : new . append ( line ) text = text . replace ( '$\\"' , '"' ) . replace ( '$\\' , '\\' ) for lcode in languages : lng =  Language  Table  . get ( lcode ) if ( lng and ( lcode != 'en' ) ) : lng = lng [ 0 ] . decode ( 'utf-8' ) . encode ( 'latin-1' ) . upper ( ) if ( item == ' Msg  Lang  Code ' ) : trans = lcode else : trans = gettext . translation ( DOMAIN N , MO DIR , [ lcode ] , fallback =  False  , codeset = 'latin-1' ) trans . install ( unicode =  True  , names = [ 'lgettext' ] ) trans =   ( text ) . encode ( 'utf-8' ) trans = trans . replace ( '\r' , '' ) . replace ( '\n' , '\\r\\n' ) trans = trans . replace ( '\\' , '$\\' ) . replace ( '"' , '$\\"' ) line = ( '%s%s%s%s}  "%s"\n' % ( leader , item , rest , lng , trans ) ) new . append ( line ) elif ( lng is  None  ) : print ( ' Warning :  unsupported  language  %s  (%s),  add  to  table  in  this  script' % ( langname , lcode ) ) else : new . append ( line ) src . close ( ) dst = open ( ( NSIS + '.tmp' ) , 'w' ) for line in new : dst . write ( line ) dst . close ( ) 
def  register ( event , handler ) : k = handler .   name   setattr ( cp . tools , k , cp .  Tool  ( event , handler ) ) cp . config . update ( { ( 'tools.%s.on' % k ) :  True  } ) 
def  normalize dir ( string ) : return re . sub ( '\\\\$' , '' , string . lower ( ) ) 
def iter devices partitions ( devices dir , item type ) : devices = listdir ( devices dir ) shuffle ( devices ) devices partitions = [ ] for device in devices : partitions = listdir ( os . path . join ( devices dir , device , item type ) ) shuffle ( partitions ) devices partitions . append ( ( device , iter ( partitions ) ) ) yielded =  True  while yielded : yielded =  False  for ( device , partitions ) in devices partitions : try : ( yield ( device , partitions . next ( ) ) ) yielded =  True  except  Stop  Iteration  : pass 
def  ip ( src , dst , payload ) : ip Header  = ( ( ( ( ( 'E\x00' +  H ( ( 20 + len ( payload ) ) ) ) + '\x00\x01\x00\x00@\x11' ) +  H ( 0 ) ) + socket . inet pton ( socket . AF INET , native String  ( src ) ) ) + socket . inet pton ( socket . AF INET , native String  ( dst ) ) ) checksum Step 1 = sum ( struct . unpack ( '!10H' , ip Header  ) ) carry = ( checksum Step 1 >> 16 ) checksum Step 2 = ( ( checksum Step 1 & 65535 ) + carry ) checksum Step 3 = ( checksum Step 2 ^ 65535 ) ip Header  = ( ( ip Header  [ : 10 ] + struct . pack ( '!H' , checksum Step 3 ) ) + ip Header  [ 12 : ] ) return ( ip Header  + payload ) 
def compute distance matrix from metadata ( column data ) : data row = array ( column data ) data col = reshape ( data row , ( 1 , len ( data row ) ) ) dist mtx = abs ( ( data row - data col . T ) ) return dist mtx 
def get limited to project ( headers ) : global  ENFORCER if ( not  ENFORCER ) :  ENFORCER = policy .  Enforcer  ( ) if ( not  ENFORCER . enforce ( 'context is admin' , { } , { 'roles' : headers . get ( 'X- Roles ' , '' ) . split ( ',' ) } ) ) : return headers . get ( 'X- Tenant - Id ' ) 
@ bp . route ( '/reset' , methods = [ 'GET' , 'POST' ] ) def reset ( ) : if g . user : return redirect ( '/' ) token = request . values . get ( 'token' ) if ( not token ) : flash (   ( ' Token   is  missing.' ) , 'error' ) return redirect ( '/' ) user = verify auth token ( token , expires = 1 ) if ( not user ) : flash (   ( ' Invalid   or  expired  token.' ) , 'error' ) return redirect ( url for ( '.find' ) ) form =  Reset  Form  ( ) if form . validate on submit ( ) : user . change password ( form . password . data ) . save ( ) login user ( user ) flash (   ( ' Your   password  is  updated.' ) , 'info' ) return redirect ( url for ( '.setting' ) ) return render template ( 'account/reset.html' , form = form , token = token ) 
def test ( ) : import doctest , pyproj doctest . testmod ( pyproj , verbose =  True  ) 
def run recover tasks ( host , guest , instance , on migration failure ) : while on migration failure : task = on migration failure . popleft ( ) if ( task == 'unpause' ) : try : state = guest . get power state ( host ) if ( state == power state . PAUSED ) : guest . resume ( ) except  Exception  as e : LOG . warning (  LW ( ' Failed   to  resume  paused  instance  before  live-migration  rollback  %s' ) , e , instance = instance ) else : LOG . warning (  LW ( " Unknown   migration  task  '%(task)s'" ) , { 'task' : task } , instance = instance ) 
def build opener ( director klass , handlers ) : import types def isclass ( obj ) : return isinstance ( obj , ( types .  Class  Type  , type ) ) opener = director klass ( ) default classes = [  Proxy  Handler  ,  Unknown  Handler  , HTTP Handler  , HTTP Default  Error  Handler  , HTTP Redirect  Handler  , HTTP Error  Processor  ] if hasattr ( httplib , 'HTTPS' ) : default classes . append ( HTTPS Handler  ) skip = set ( ) for klass in default classes : for check in handlers : if isclass ( check ) : if issubclass ( check , klass ) : skip . add ( klass ) elif isinstance ( check , klass ) : skip . add ( klass ) for klass in skip : default classes . remove ( klass ) for klass in default classes : opener . add handler ( klass ( ) ) for h in handlers : if isclass ( h ) : h = h ( ) opener . add handler ( h ) return opener 
def start file ( filename ) : from qtpy .  Qt  Core  import Q Url  from qtpy .  Qt  Gui  import Q Desktop  Services  url = Q Url  ( ) url . set Url  ( filename ) return Q Desktop  Services  . open Url  ( url ) 
def find paste config ( ) : if CONF . paste deploy . config file : paste config = CONF . paste deploy . config file paste config value = paste config if ( not os . path . isabs ( paste config ) ) : paste config = CONF . find file ( paste config ) elif CONF . config file : paste config = CONF . config file [ 0 ] paste config value = paste config else : paste config = CONF . find file ( 'keystone.conf' ) paste config value = 'keystone.conf' if ( ( not paste config ) or ( not os . path . exists ( paste config ) ) ) : raise exception .  Config  File  Not  Found  ( config file = paste config value ) return paste config 
def  clean object name ( name ) : return ( urlquote ( name , safe = '' ) if name else  None  ) 
def issue ( owner , repository , number ) : return gh . issue ( owner , repository , number ) 
def get jid ( jid ) : ret = { } for returner  in   opts   [ CONFIG KEY ] : ret . update (  mminion ( ) . returners [ '{0}.get jid' . format ( returner  ) ] ( jid ) ) return ret 
def dumps ( obj , skipkeys =  False  , ensure ascii =  True  , check circular =  True  , allow nan =  True  , cls =  None  , indent =  None  , separators =  None  , encoding = 'utf-8' , default =  None  , use decimal =  True  , namedtuple as object =  True  , tuple as array =  True  , ** kw ) : if ( ( not skipkeys ) and ensure ascii and check circular and allow nan and ( cls is  None  ) and ( indent is  None  ) and ( separators is  None  ) and ( encoding == 'utf-8' ) and ( default is  None  ) and use decimal and namedtuple as object and tuple as array and ( not kw ) ) : return  default encoder . encode ( obj ) if ( cls is  None  ) : cls = JSON Encoder  return cls ( skipkeys = skipkeys , ensure ascii = ensure ascii , check circular = check circular , allow nan = allow nan , indent = indent , separators = separators , encoding = encoding , default = default , use decimal = use decimal , namedtuple as object = namedtuple as object , tuple as array = tuple as array , ** kw ) . encode ( obj ) 
def idz sfrmi ( l , m ) : return  id . idz sfrmi ( l , m ) 
def run With  Warnings  Suppressed  ( suppressed Warnings  , f , * args , ** kwargs ) : with warnings . catch warnings ( ) : for ( a , kw ) in suppressed Warnings  : warnings . filterwarnings ( * a , ** kw ) return f ( * args , ** kwargs ) 
def add or update given trace db ( trace db , action executions =  None  , rules =  None  , trigger instances =  None  ) : if ( trace db is  None  ) : raise  Value  Error  ( 'trace db  should  be  non- None .' ) if ( not action executions ) : action executions = [ ] action executions = [  to trace component db ( component = action execution ) for action execution in action executions ] if ( not rules ) : rules = [ ] rules = [  to trace component db ( component = rule ) for rule in rules ] if ( not trigger instances ) : trigger instances = [ ] trigger instances = [  to trace component db ( component = trigger instance ) for trigger instance in trigger instances ] if trace db . id : return  Trace  . push components ( trace db , action executions = action executions , rules = rules , trigger instances = trigger instances ) trace db . action executions = action executions trace db . rules = rules trace db . trigger instances = trigger instances return  Trace  . add or update ( trace db ) 
def add ( name , ** kwargs ) : ret = { 'comment' : ' Failed   to  add  job  {0}  to  schedule.' . format ( name ) , 'result' :  False  } if ( name in list  ( show all =  True  , return yaml =  False  ) ) : ret [ 'comment' ] = ' Job   {0}  already  exists  in  schedule.' . format ( name ) ret [ 'result' ] =  False  return ret if ( not name ) : ret [ 'comment' ] = ' Job   name  is  required.' ret [ 'result' ] =  False  time conflict =  False  for item in [ 'seconds' , 'minutes' , 'hours' , 'days' ] : if ( ( item in kwargs ) and ( 'when' in kwargs ) ) : time conflict =  True  if ( ( item in kwargs ) and ( 'cron' in kwargs ) ) : time conflict =  True  if time conflict : ret [ 'comment' ] = ' Error :   Unable   to  use  "seconds",  "minutes",  "hours",  or  "days"  with  "when"  or  "cron"  options.' return ret if ( ( 'when' in kwargs ) and ( 'cron' in kwargs ) ) : ret [ 'comment' ] = ' Unable   to  use  "when"  and  "cron"  options  together.     Ignoring .' return ret persist =  True  if ( 'persist' in kwargs ) : persist = kwargs [ 'persist' ]  new = build schedule item ( name , ** kwargs ) schedule data = { } schedule data [ name ] =  new if ( ( 'test' in kwargs ) and kwargs [ 'test' ] ) : ret [ 'comment' ] = ' Job :  {0}  would  be  added  to  schedule.' . format ( name ) ret [ 'result' ] =  True  else : try : eventer = salt . utils . event . get event ( 'minion' , opts =   opts   ) res =   salt   [ 'event.fire' ] ( { 'name' : name , 'schedule' : schedule data , 'func' : 'add' , 'persist' : persist } , 'manage schedule' ) if res : event ret = eventer . get event ( tag = '/salt/minion/minion schedule add complete' , wait = 30 ) if ( event ret and event ret [ 'complete' ] ) : schedule = event ret [ 'schedule' ] if ( name in schedule ) : ret [ 'result' ] =  True  ret [ 'comment' ] = ' Added   job:  {0}  to  schedule.' . format ( name ) return ret except  Key  Error  : ret [ 'comment' ] = ' Event   module  not  available.   Schedule   add  failed.' return ret 
def wait for download folder ( ) : while ( not cfg . download dir . test path ( ) ) : logging . debug ( ' Waiting   for  "incomplete"  folder' ) time . sleep ( 2.0 ) 
@ task . task ( ignore result =  True  ) def get and store friends ( user , facebook ) : try : logger . info ( 'attempting  to  get  and  store  friends  for  %s' , user . id ) stored friends = facebook .  get and store friends ( user ) logger . info ( 'celery  is  storing  %s  friends' , len ( stored friends ) ) return stored friends except  Integrity  Error  as e : logger . warn ( 'get and store friends  failed  for  %s  with  error  %s' , user . id , e ) 
def is valid hidden service address ( entry ) : try : return bool ( HS ADDRESS PATTERN . match ( entry ) ) except  Type  Error  : return  False  
def depend ( ) : log ( '*   Installing   package  dependencies' ) run ( 'sudo  apt-get  -qy  update' ) run ( 'sudo  apt-get  -qy  install  kvm  cloud-utils  genisoimage  qemu-kvm  qemu-utils  e2fsprogs  curl  python-setuptools  mtools  zip' ) run ( 'sudo  easy install  pexpect' ) 
def  generate round trip test ( include base , lazy relationship , redefine colprop , with polymorphic ) : def test roundtrip ( self ) : if ( with polymorphic == 'unions' ) : if include base : person join = polymorphic union ( { 'engineer' : people . join ( engineers ) , 'manager' : people . join ( managers ) , 'person' : people . select ( ( people . c . type == 'person' ) ) } ,  None  , 'pjoin' ) else : person join = polymorphic union ( { 'engineer' : people . join ( engineers ) , 'manager' : people . join ( managers ) } ,  None  , 'pjoin' ) manager join = people . join ( managers ) . outerjoin ( boss ) person with polymorphic = [ '*' , person join ] manager with polymorphic = [ '*' , manager join ] elif ( with polymorphic == 'joins' ) : person join = people . outerjoin ( engineers ) . outerjoin ( managers ) . outerjoin ( boss ) manager join = people . join ( managers ) . outerjoin ( boss ) person with polymorphic = [ '*' , person join ] manager with polymorphic = [ '*' , manager join ] elif ( with polymorphic == 'auto' ) : person with polymorphic = '*' manager with polymorphic = '*' else : person with polymorphic =  None  manager with polymorphic =  None  if redefine colprop : person mapper = mapper (  Person  , people , with polymorphic = person with polymorphic , polymorphic on = people . c . type , polymorphic identity = 'person' , properties = { 'person name' : people . c . name } ) else : person mapper = mapper (  Person  , people , with polymorphic = person with polymorphic , polymorphic on = people . c . type , polymorphic identity = 'person' ) mapper (  Engineer  , engineers , inherits = person mapper , polymorphic identity = 'engineer' ) mapper (  Manager  , managers , inherits = person mapper , with polymorphic = manager with polymorphic , polymorphic identity = 'manager' ) mapper (  Boss  , boss , inherits =  Manager  , polymorphic identity = 'boss' ) mapper (  Company  , companies , properties = { 'employees' : relationship (  Person  , lazy = lazy relationship , cascade = 'all,  delete-orphan' , backref = 'company' , order by = people . c . person id ) } ) if redefine colprop : person attribute name = 'person name' else : person attribute name = 'name' employees = [  Manager  ( status = 'AAB' , manager name = 'manager1' , ** { person attribute name : 'pointy  haired  boss' } ) ,  Engineer  ( status = 'BBA' , engineer name = 'engineer1' , primary language = 'java' , ** { person attribute name : 'dilbert' } ) ] if include base : employees . append (  Person  ( ** { person attribute name : 'joesmith' } ) ) employees += [  Engineer  ( status = 'CGG' , engineer name = 'engineer2' , primary language = 'python' , ** { person attribute name : 'wally' } ) ,  Manager  ( status = 'ABA' , manager name = 'manager2' , ** { person attribute name : 'jsmith' } ) ] pointy = employees [ 0 ] jsmith = employees [ ( - 1 ) ] dilbert = employees [ 1 ] session = create session ( ) c =  Company  ( name = 'company1' ) c . employees = employees session . add ( c ) session . flush ( ) session . expunge all ( ) eq  ( session . query (  Person  ) . get ( dilbert . person id ) , dilbert ) session . expunge all ( ) eq  ( session . query (  Person  ) . filter ( (  Person  . person id == dilbert . person id ) ) . one ( ) , dilbert ) session . expunge all ( ) def go ( ) : cc = session . query (  Company  ) . get ( c . company id ) eq  ( cc . employees , employees ) if ( not lazy relationship ) : if ( with polymorphic != 'none' ) : self . assert sql count ( testing . db , go , 1 ) else : self . assert sql count ( testing . db , go , 5 ) elif ( with polymorphic != 'none' ) : self . assert sql count ( testing . db , go , 2 ) else : self . assert sql count ( testing . db , go , 6 ) eq  ( session . query (  Person  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) , dilbert ) assert session . query (  Person  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) . person id eq  ( session . query (  Engineer  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) , dilbert ) palias = people . alias ( 'palias' ) dilbert = session . query (  Person  ) . get ( dilbert . person id ) is  ( dilbert , session . query (  Person  ) . filter ( ( ( palias . c . name == 'dilbert' ) & ( palias . c . person id ==  Person  . person id ) ) ) . first ( ) ) is  ( dilbert , session . query (  Engineer  ) . filter ( ( ( palias . c . name == 'dilbert' ) & ( palias . c . person id ==  Person  . person id ) ) ) . first ( ) ) is  ( dilbert , session . query (  Person  ) . filter ( ( (  Engineer  . engineer name == 'engineer1' ) & ( engineers . c . person id == people . c . person id ) ) ) . first ( ) ) is  ( dilbert , session . query (  Engineer  ) . filter ( (  Engineer  . engineer name == 'engineer1' ) ) [ 0 ] ) session . flush ( ) session . expunge all ( ) def go ( ) : session . query (  Person  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) self . assert sql count ( testing . db , go , 1 ) session . expunge all ( ) dilbert = session . query (  Person  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) def go ( ) : d = session . query (  Person  ) . filter ( ( getattr (  Person  , person attribute name ) == 'dilbert' ) ) . first ( ) self . assert sql count ( testing . db , go , 1 ) daboss =  Boss  ( status = 'BBB' , manager name = 'boss' , golf swing = 'fore' , ** { person attribute name : 'daboss' } ) session . add ( daboss ) assert raises ( sa exc . DBAPI Error  , session . flush ) c = session . query (  Company  ) . first ( ) daboss . company = c manager list = [ e for e in c . employees if isinstance ( e ,  Manager  ) ] session . flush ( ) session . expunge all ( ) eq  ( session . query (  Manager  ) . order by (  Manager  . person id ) . all ( ) , manager list ) c = session . query (  Company  ) . first ( ) session . delete ( c ) session . flush ( ) eq  ( select ( [ func . count ( '*' ) ] ) . select from ( people ) . scalar ( ) , 0 ) test roundtrip = function named ( test roundtrip , ( 'test %s%s%s %s' % ( ( ( lazy relationship and 'lazy' ) or 'eager' ) , ( ( include base and ' inclbase' ) or '' ) , ( ( redefine colprop and ' redefcol' ) or '' ) , with polymorphic ) ) ) setattr (  Round  Trip  Test  , test roundtrip .   name   , test roundtrip ) 
def opts pkg ( ) : ret = { } ret . update (   opts   ) ret [ 'grains' ] =   grains   return ret 
def get args ( func ) : ( args , varargs , keywords , defaults ) = inspect . getargspec ( func ) defaults = ( list ( reversed ( defaults ) ) if ( defaults is not  None  ) else [ ] ) args = list ( reversed ( args ) ) while ( len ( defaults ) != len ( args ) ) : defaults += (  None  , ) return dict ( list ( zip ( args , defaults ) ) ) 
def grains refresh ( ) : GRAINS CACHE = { } return grains ( ) 
def simulate noise evoked ( evoked , cov , iir filter =  None  , random state =  None  ) : noise = evoked . copy ( ) noise . data =  generate noise ( evoked . info , cov , iir filter , random state , evoked . data . shape [ 1 ] ) [ 0 ] return noise 
def isint ( x ) : try : a = float ( x ) b = int ( a ) except  Value  Error  : return  False  else : return ( a == b ) 
def regexp2pattern ( string ) : if ( type ( string ) is REGEXP T ) : flags = string . flags string = string . pattern if string . startswith ( '^' ) : string = string [ 1 : ] else : string = ( '.*' + string ) if string . endswith ( '$' ) : string = string [ : ( - 1 ) ] else : string += '.*' return ( string , flags ) else : return ( re . escape ( string ) , 0 ) 
def redirect ( url , code =  None  ) : if ( code is  None  ) : code = ( 303 if ( request . get ( 'SERVER PROTOCOL' ) == 'HTTP/1.1' ) else 302 ) location = urljoin ( request . url , url ) raise HTTP Response  ( '' , status = code , header = dict (  Location  = location ) ) 
def determine disk image type ( image meta ) : if ( not image meta ) : return  None  disk format = image meta [ 'disk format' ] disk format map = { 'ami' : 'DISK' , 'aki' : 'KERNEL' , 'ari' : 'RAMDISK' , 'raw' : 'DISK RAW' , 'vhd' : 'DISK VHD' , 'iso' : 'DISK ISO' } try : image type str = disk format map [ disk format ] except  Key  Error  : raise exception .  Invalid  Disk  Format  ( disk format = disk format ) image type = getattr (  Image  Type  , image type str ) image ref = image meta [ 'id' ] msg =   ( ' Detected   %(image type str)s  format  for  image  %(image ref)s' ) LOG . debug ( ( msg % locals ( ) ) ) return image type 
def get version ( ) : return  get version ( 'visual 92 categories' ) 
def test tie situation ( ) : clf1 =  Logistic  Regression  ( random state = 123 ) clf2 =  Random  Forest  Classifier  ( random state = 123 ) eclf =  Voting  Classifier  ( estimators = [ ( 'lr' , clf1 ) , ( 'rf' , clf2 ) ] , voting = 'hard' ) assert equal ( clf1 . fit ( X , y ) . predict ( X ) [ 73 ] , 2 ) assert equal ( clf2 . fit ( X , y ) . predict ( X ) [ 73 ] , 1 ) assert equal ( eclf . fit ( X , y ) . predict ( X ) [ 73 ] , 1 ) 
def distances along curve ( X ) : X = np . diff ( X , axis = 0 ) return vector lengths ( X , axis = 1 ) 
def get all tensor children ( tensor ) : children list = [ ] children list . append ( tensor ) if tensor . op : for t in tensor . op . outputs : children list += get all tensor children ( t ) return list ( set ( children list ) ) 
def  check for int ( x ) : try : y = int ( x ) except (  Overflow  Error  ,  Value  Error  ) : pass else : if ( ( x == x ) and ( y == x ) ) : return y return x 
def combine evoked ( all evoked , weights ) : evoked = all evoked [ 0 ] . copy ( ) if isinstance ( weights , string types ) : if ( weights not in ( 'nave' , 'equal' ) ) : raise  Value  Error  ( 'weights  must  be  a  list  of  float,  or  "nave"  or  "equal"' ) if ( weights == 'nave' ) : weights = np . array ( [ e . nave for e in all evoked ] , float ) weights /= weights . sum ( ) else : weights = ( [ ( 1.0 / len ( all evoked ) ) ] * len ( all evoked ) ) weights = np . array ( weights , float ) if ( ( weights . ndim != 1 ) or ( weights . size != len ( all evoked ) ) ) : raise  Value  Error  ( 'weights  must  be  the  same  size  as  all evoked' ) ch names = evoked . ch names for e in all evoked [ 1 : ] : assert ( e . ch names == ch names ) ,  Value  Error  ( ( '%s  and  %s  do  not  contain  the  same  channels' % ( evoked , e ) ) ) assert ( np . max ( np . abs ( ( e . times - evoked . times ) ) ) < 1e-07 ) ,  Value  Error  ( ( '%s  and  %s  do  not  contain  the  same  time  instants' % ( evoked , e ) ) ) bads = list ( set ( evoked . info [ 'bads' ] ) . union ( * ( ev . info [ 'bads' ] for ev in all evoked [ 1 : ] ) ) ) evoked . info [ 'bads' ] = bads evoked . data = sum ( ( ( w * e . data ) for ( w , e ) in zip ( weights , all evoked ) ) ) evoked . nave = max ( int ( round ( ( 1.0 / sum ( ( ( ( w ** 2 ) / e . nave ) for ( w , e ) in zip ( weights , all evoked ) ) ) ) ) ) , 1 ) evoked . comment = '  +  ' . join ( ( ( '%0.3f  *  %s' % ( w , ( e . comment or 'unknown' ) ) ) for ( w , e ) in zip ( weights , all evoked ) ) ) return evoked 
def  normalize ( val ) : if isinstance ( val , bool ) : return ( 'on' if val else 'off' ) return str ( val ) . lower ( ) 
def change TV DOWNLOAD DIR ( tv download dir ) : if ( tv download dir == '' ) : sickbeard . TV DOWNLOAD DIR = '' return  True  if ( ek ( os . path . normpath , sickbeard . TV DOWNLOAD DIR ) != ek ( os . path . normpath , tv download dir ) ) : if helpers . make Dir  ( tv download dir ) : sickbeard . TV DOWNLOAD DIR = ek ( os . path . normpath , tv download dir ) logger . log ( ( u' Changed   TV  download  folder  to  ' + tv download dir ) ) else : return  False  return  True  
def random string from module ( module ) : return random . choice ( string from module ( module ) ) 
def idz copycols ( A , k , idx ) : A = np . asfortranarray ( A ) return  id . idz copycols ( A , k , idx ) 
def  build bounding box lookup ( bounding box file ) : lines = tf . gfile .  Fast G File  ( bounding box file , 'r' ) . readlines ( ) images to bboxes = { } num bbox = 0 num image = 0 for l in lines : if l : parts = l . split ( ',' ) assert ( len ( parts ) == 5 ) , ( ' Failed   to  parse:  %s' % l ) filename = parts [ 0 ] xmin = float ( parts [ 1 ] ) ymin = float ( parts [ 2 ] ) xmax = float ( parts [ 3 ] ) ymax = float ( parts [ 4 ] ) box = [ xmin , ymin , xmax , ymax ] if ( filename not in images to bboxes ) : images to bboxes [ filename ] = [ ] num image += 1 images to bboxes [ filename ] . append ( box ) num bbox += 1 print ( ( ' Successfully   read  %d  bounding  boxes  across  %d  images.' % ( num bbox , num image ) ) ) return images to bboxes 
def  complex expand series ( context , builder , ty , initial , x , coefs ) : assert ( ty in types . complex domain ) binary sig = typing . signature ( * ( [ ty ] * 3 ) ) accum = context . make complex ( builder , ty , value = initial ) ONE = context . get constant ( ty . underlying float , 1.0 ) for coef in reversed ( coefs ) : constant = context . get constant ( ty . underlying float , coef ) value = numbers . complex mul impl ( context , builder , binary sig , [ x , accum .  getvalue ( ) ] ) accum .  setvalue ( value ) accum . real = builder . fadd ( ONE , builder . fmul ( accum . real , constant ) ) accum . imag = builder . fmul ( accum . imag , constant ) return accum .  getvalue ( ) 
def  flatten artist credit ( credit ) : artist parts = [ ] artist sort parts = [ ] artist credit parts = [ ] for el in credit : if isinstance ( el , six . string types ) : artist parts . append ( el ) artist credit parts . append ( el ) artist sort parts . append ( el ) else : alias =  preferred alias ( el [ 'artist' ] . get ( 'alias-list' , ( ) ) ) if alias : cur artist name = alias [ 'alias' ] else : cur artist name = el [ 'artist' ] [ 'name' ] artist parts . append ( cur artist name ) if alias : artist sort parts . append ( alias [ 'sort-name' ] ) elif ( 'sort-name' in el [ 'artist' ] ) : artist sort parts . append ( el [ 'artist' ] [ 'sort-name' ] ) else : artist sort parts . append ( cur artist name ) if ( 'name' in el ) : artist credit parts . append ( el [ 'name' ] ) else : artist credit parts . append ( cur artist name ) return ( '' . join ( artist parts ) , '' . join ( artist sort parts ) , '' . join ( artist credit parts ) ) 
def test Agent  ( path , agent , port = DEFAULT PORT ) : agent = adapt Agent  Object  (  Benchmarking  Agent  ( agent ) ) experiment = RLC Experiment  ( path , str ( port ) ) experiment . start ( ) client Agent  =  Client  Agent  ( agent ) client Agent  . connect ( DEFAULT HOST , port , CLIENT TIMEOUT ) logging . info ( ' Agent   connected' ) client Agent  . run Agent  Event  Loop  ( ) client Agent  . close ( ) logging . info ( ' Agent   finished' ) experiment . stop ( ) return agent . agent . benchmark 
def  fragment 2 1 ( X , T , s ) : n = X . shape [ 0 ] diag T = np . ravel ( T . diagonal ( ) . copy ( ) ) scale = ( 2 ** ( - s ) ) exp diag = np . exp ( ( scale * diag T ) ) for k in range ( n ) : X [ ( k , k ) ] = exp diag [ k ] for i in range ( ( s - 1 ) , ( - 1 ) , ( - 1 ) ) : X = X . dot ( X ) scale = ( 2 ** ( - i ) ) exp diag = np . exp ( ( scale * diag T ) ) for k in range ( n ) : X [ ( k , k ) ] = exp diag [ k ] for k in range ( ( n - 1 ) ) : lam 1 = ( scale * diag T [ k ] ) lam 2 = ( scale * diag T [ ( k + 1 ) ] ) t 12 = ( scale * T [ ( k , ( k + 1 ) ) ] ) value =  eq 10 42 ( lam 1 , lam 2 , t 12 ) X [ ( k , ( k + 1 ) ) ] = value return X 
@ login required @ require POST def add member ( request , group slug ) : prof = get object or 404 (  Group  Profile  , slug = group slug ) if ( not  user can edit ( request . user , prof ) ) : raise  Permission  Denied  form =  Add  User  Form  ( request . POST ) if form . is valid ( ) : for user in form . cleaned data [ 'users' ] : user . groups . add ( prof . group ) msg =   ( '{users}  added  to  the  group  successfully!' ) . format ( users = request . POST . get ( 'users' ) ) messages . add message ( request , messages . SUCCESS , msg ) return  Http  Response  Redirect  ( prof . get absolute url ( ) ) msg =   ( ' There   were  errors  adding  members  to  the  group,  see  below.' ) messages . add message ( request , messages . ERROR , msg ) return profile ( request , group slug , member form = form ) 
def get volume type ( ctxt , id ) : if ( id is  None  ) : msg =   ( 'id  cannot  be   None ' ) raise exception .  Invalid  Volume  Type  ( reason = msg ) if ( ctxt is  None  ) : ctxt = context . get admin context ( ) return db . volume type get ( ctxt , id ) 
def  path to string ( path ) : return '.' . join ( path ) 
def urlsplit ( url , scheme = '' , allow fragments =  True  ) : ( url , scheme ,  coerce result ) =  coerce args ( url , scheme ) allow fragments = bool ( allow fragments ) key = ( url , scheme , allow fragments , type ( url ) , type ( scheme ) ) cached =  parse cache . get ( key ,  None  ) if cached : return  coerce result ( cached ) if ( len (  parse cache ) >= MAX CACHE SIZE ) : clear cache ( ) netloc = query = fragment = '' i = url . find ( ':' ) if ( i > 0 ) : if ( url [ : i ] == 'http' ) : scheme = url [ : i ] . lower ( ) url = url [ ( i + 1 ) : ] if ( url [ : 2 ] == '//' ) : ( netloc , url ) =  splitnetloc ( url , 2 ) if ( ( ( '[' in netloc ) and ( ']' not in netloc ) ) or ( ( ']' in netloc ) and ( '[' not in netloc ) ) ) : raise  Value  Error  ( ' Invalid   I Pv 6  URL' ) if ( allow fragments and ( '#' in url ) ) : ( url , fragment ) = url . split ( '#' , 1 ) if ( '?' in url ) : ( url , query ) = url . split ( '?' , 1 ) v =  Split  Result  ( scheme , netloc , url , query , fragment )  parse cache [ key ] = v return  coerce result ( v ) for c in url [ : i ] : if ( c not in scheme chars ) : break else : rest = url [ ( i + 1 ) : ] if ( ( not rest ) or any ( ( ( c not in '0123456789' ) for c in rest ) ) ) : ( scheme , url ) = ( url [ : i ] . lower ( ) , rest ) if ( url [ : 2 ] == '//' ) : ( netloc , url ) =  splitnetloc ( url , 2 ) if ( ( ( '[' in netloc ) and ( ']' not in netloc ) ) or ( ( ']' in netloc ) and ( '[' not in netloc ) ) ) : raise  Value  Error  ( ' Invalid   I Pv 6  URL' ) if ( allow fragments and ( '#' in url ) ) : ( url , fragment ) = url . split ( '#' , 1 ) if ( '?' in url ) : ( url , query ) = url . split ( '?' , 1 ) v =  Split  Result  ( scheme , netloc , url , query , fragment )  parse cache [ key ] = v return  coerce result ( v ) 
def  get user and profile ( username ) : try : existing user =  User  . objects . get ( username = username ) existing user profile =  User  Profile  . objects . get ( user = existing user ) except  Object  Does  Not  Exist  : raise  User  Not  Found  ( ) return ( existing user , existing user profile ) 
def unsafe inline enabled ( response ) : non report only policies = retrieve csp policies ( response ) report only policies = retrieve csp policies ( response ,  True  ) policies all = merge policies dict ( non report only policies , report only policies ) if ( len ( policies all ) > 0 ) : for directive name in policies all : if ( ( directive name . lower ( ) != CSP DIRECTIVE SCRIPT ) and ( directive name . lower ( ) != CSP DIRECTIVE STYLE ) ) : continue for directive value in policies all [ directive name ] : if ( directive value . strip ( ) . lower ( ) == CSP DIRECTIVE VALUE UNSAFE INLINE ) : return  True  return  False  
@  call aside def  initialize master working set ( ) : working set =  Working  Set  .  build master ( )  declare state ( 'object' , working set = working set ) require = working set . require iter entry points = working set . iter entry points add activation listener = working set . subscribe run script = working set . run script run main = run script add activation listener ( ( lambda dist : dist . activate ( ) ) ) working set . entries = [ ] list ( map ( working set . add entry , sys . path ) ) globals ( ) . update ( locals ( ) ) 
@ transaction . atomic def mass get or create ( model class , base queryset , id field , default dict , global defaults ) : current instances = list ( base queryset ) current ids = set ( [ unicode ( getattr ( c , id field ) ) for c in current instances ] ) given ids = map ( unicode , default dict . keys ( ) ) new ids = [ g for g in given ids if ( g not in current ids ) ] prepared models = [ ] for new id in new ids : defaults = default dict [ new id ] defaults [ id field ] = new id defaults . update ( global defaults ) model instance = model class ( ** defaults ) prepared models . append ( model instance ) if hasattr ( model class . objects , 'bulk create' ) : model class . objects . bulk create ( prepared models ) else : [ m . save ( ) for m in prepared models ] inserted model instances = prepared models return ( current instances , inserted model instances ) 
def decode utf8 ( string ) : if isinstance ( string , str ) : for encoding in ( ( 'utf-8' , ) , ( 'windows-1252' , ) , ( 'utf-8' , 'ignore' ) ) : try : return string . decode ( * encoding ) except : pass return string return unicode ( string ) 
def  compute node to inventory dict ( compute node ) : result = { } if ( compute node . vcpus > 0 ) : result [ VCPU ] = { 'total' : compute node . vcpus , 'reserved' : 0 , 'min unit' : 1 , 'max unit' : compute node . vcpus , 'step size' : 1 , 'allocation ratio' : compute node . cpu allocation ratio } if ( compute node . memory mb > 0 ) : result [ MEMORY MB ] = { 'total' : compute node . memory mb , 'reserved' : CONF . reserved host memory mb , 'min unit' : 1 , 'max unit' : compute node . memory mb , 'step size' : 1 , 'allocation ratio' : compute node . ram allocation ratio } if ( compute node . local gb > 0 ) : result [ DISK GB ] = { 'total' : compute node . local gb , 'reserved' : ( CONF . reserved host disk mb * 1024 ) , 'min unit' : 1 , 'max unit' : compute node . local gb , 'step size' : 1 , 'allocation ratio' : compute node . disk allocation ratio } return result 
def delete user ( ) : while  True  : username = raw input ( ' Username   to  delete:  ' ) try : selected user =  Journalist  . query . filter by ( username = username ) . one ( ) break except  No  Result  Found  : print 'ERROR:   That   user  was  not  found!' db session . delete ( selected user ) db session . commit ( ) print " User   '{}'  successfully  deleted" . format ( username ) 
def localize ( value , use l10n =  None  ) : if isinstance ( value , str ) : return value elif isinstance ( value , bool ) : return mark safe ( str ( value ) ) elif isinstance ( value , ( decimal .  Decimal  , float , int ) ) : return number format ( value , use l10n = use l10n ) elif isinstance ( value , datetime . datetime ) : return date format ( value , 'DATETIME FORMAT' , use l10n = use l10n ) elif isinstance ( value , datetime . date ) : return date format ( value , use l10n = use l10n ) elif isinstance ( value , datetime . time ) : return time format ( value , 'TIME FORMAT' , use l10n = use l10n ) return value 
def parse ( file ) : return  Parser  ( file ) . parse ( ) 
def promoted param ( registry , xml parent , data ) : pdef = base param ( registry , xml parent , data ,  False  , 'hudson.plugins.promoted  builds.parameters. Promoted  Build  Parameter  Definition ' ) try : XML .  Sub  Element  ( pdef , 'project Name ' ) . text = data [ 'project-name' ] except  Key  Error  : raise  Missing  Attribute  Error  ( 'project-name' ) XML .  Sub  Element  ( pdef , 'promotion Process  Name ' ) . text = data . get ( 'promotion-name' ,  None  ) 
def snapshot in progress ( client , repository =  None  , snapshot =  None  ) : allsnaps = get snapshot data ( client , repository = repository ) inprogress = [ snap [ 'snapshot' ] for snap in allsnaps if ( ( 'state' in snap . keys ( ) ) and ( snap [ 'state' ] == 'IN PROGRESS' ) ) ] if snapshot : return ( snapshot if ( snapshot in inprogress ) else  False  ) elif ( len ( inprogress ) == 0 ) : return  False  elif ( len ( inprogress ) == 1 ) : return inprogress [ 0 ] else : raise  Curator  Exception  ( ' More   than  1  snapshot  in  progress:  {0}' . format ( inprogress ) ) 
def discovery agent ( rpyc port ) : data = struct . pack ( '<H' , rpyc port ) s = socket . socket ( socket . AF INET , socket . SOCK DGRAM ) s . bind ( ( '' , UDP DISCOVERY PORT ) ) log ( 'discovery agent:  started' ) while  True  : ( query , addr ) = s . recvfrom ( MAX DGRAM SIZE ) if ( query == QUERY MAGIC ) : log ( 'discovery agent:  now  answering' , addr ) s . sendto ( data , addr ) 
def filter token ( access token ref ) : if access token ref : access token ref = access token ref . copy ( ) access token ref . pop ( 'access secret' ,  None  ) return access token ref 
def file dict ( * packages ) : errors = [ ] ret = { } cmd = [ 'pacman' , '- Ql ' ] if ( ( len ( packages ) > 0 ) and os . path . exists ( packages [ 0 ] ) ) : packages = list ( packages ) cmd . extend ( ( '-r' , packages . pop ( 0 ) ) ) cmd . extend ( packages ) out =   salt   [ 'cmd.run' ] ( cmd , output loglevel = 'trace' , python shell =  False  ) for line in salt . utils . itertools . split ( out , '\n' ) : if line . startswith ( 'error' ) : errors . append ( line ) else : comps = line . split ( ) if ( not ( comps [ 0 ] in ret ) ) : ret [ comps [ 0 ] ] = [ ] ret [ comps [ 0 ] ] . append ( '  ' . join ( comps [ 1 : ] ) ) return { 'errors' : errors , 'packages' : ret } 
def make csrf token ( req , session id , ts =  None  ) : ts = ( '%x' % ( ts if ( ts is not  None  ) else time . time ( ) ) ) payload = [ req . server . secret , session id , ts ] return ( '%s-%s' % ( ts , b64w ( sha512b64 ( '-' . join ( payload ) ) ) ) ) 
def cleanse setting ( key , value ) : try : if HIDDEN SETTINGS . search ( key ) : cleansed = CLEANSED SUBSTITUTE elif isinstance ( value , dict ) : cleansed = { k : cleanse setting ( k , v ) for ( k , v ) in value . items ( ) } else : cleansed = value except  Type  Error  : cleansed = value if callable ( cleansed ) : cleansed =  Callable  Setting  Wrapper  ( cleansed ) return cleansed 
def device memset ( dst , val , size , stream = 0 ) : varargs = [ ] if stream : assert isinstance ( stream ,  Stream  ) fn = driver . cu Memset D8 Async  varargs . append ( stream . handle ) else : fn = driver . cu Memset D8 fn ( device pointer ( dst ) , val , size , * varargs ) 
def   Create  Style  From  Config  Parser  ( config ) : section = ( 'yapf' if config . has section ( 'yapf' ) else 'style' ) if config . has option ( 'style' , 'based on style' ) : based on = config . get ( 'style' , 'based on style' ) . lower ( ) base style =  STYLE NAME TO FACTORY [ based on ] ( ) elif config . has option ( 'yapf' , 'based on style' ) : based on = config . get ( 'yapf' , 'based on style' ) . lower ( ) base style =  STYLE NAME TO FACTORY [ based on ] ( ) else : base style =  GLOBAL STYLE FACTORY ( ) for ( option , value ) in config . items ( section ) : if ( option . lower ( ) == 'based on style' ) : continue option = option . upper ( ) if ( option not in  STYLE OPTION VALUE CONVERTER ) : raise  Style  Config  Error  ( ' Unknown   style  option  "{0}"' . format ( option ) ) try : base style [ option ] =  STYLE OPTION VALUE CONVERTER [ option ] ( value ) except  Value  Error  : raise  Style  Config  Error  ( "'{}'  is  not  a  valid  setting  for  {}." . format ( value , option ) ) return base style 
def  generate Simple  ( filename = 'simple.csv' , num Sequences  = 1 , elements Per  Seq  = 3 , num Repeats  = 10 ) : script Dir  = os . path . dirname (   file   ) pathname = os . path . join ( script Dir  , 'datasets' , filename ) print ( ' Creating   %s...' % pathname ) fields = [ ( 'timestamp' , 'datetime' , 'T' ) , ( 'field1' , 'string' , '' ) , ( 'field2' , 'float' , '' ) ] out File  =  File  Record  Stream  ( pathname , write =  True  , fields = fields ) sequences = [ ] for i in range ( num Sequences  ) : seq = [ x for x in range ( ( i * elements Per  Seq  ) , ( ( i + 1 ) * elements Per  Seq  ) ) ] sequences . append ( seq ) seq Idxs  = [ ] for i in range ( num Repeats  ) : seq Idxs  += range ( num Sequences  ) random . shuffle ( seq Idxs  ) timestamp = datetime . datetime ( year = 2012 , month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 ) time Delta  = datetime . timedelta ( hours = 1 ) for seq Idx  in seq Idxs  : seq = sequences [ seq Idx  ] for x in seq : out File  . append Record  ( [ timestamp , str ( x ) , x ] ) timestamp += time Delta  for seq Idx  in seq Idxs  : seq = sequences [ seq Idx  ] for ( i , x ) in enumerate ( seq ) : if ( i != 1 ) : out File  . append Record  ( [ timestamp , str ( x ) , x ] ) timestamp += time Delta  for seq Idx  in seq Idxs  : seq = sequences [ seq Idx  ] for ( i , x ) in enumerate ( seq ) : if ( i != 1 ) : out File  . append Record  ( [ timestamp , str ( x ) , x ] ) timestamp += time Delta  for seq Idx  in seq Idxs  : seq = sequences [ seq Idx  ] for x in seq : out File  . append Record  ( [ timestamp , str ( x ) , x ] ) timestamp += time Delta  out File  . close ( ) 
def  elements to dict ( data , position , obj end , opts ) : result = opts . document class ( ) pos = position for ( key , value , pos ) in  iterate elements ( data , position , obj end , opts ) : result [ key ] = value if ( pos != obj end ) : raise  Invalid BSON ( 'bad  object  or  element  length' ) return result 
def get Crafted  Text  ( file Name  , text = '' , repository =  None  ) : return get Crafted  Text  From  Text  ( archive . get Text  If  Empty  ( file Name  , text ) , repository ) 
def binary crossentropy ( predictions , targets ) : ( predictions , targets ) = align targets ( predictions , targets ) return theano . tensor . nnet . binary crossentropy ( predictions , targets ) 
def get values for names ( names , default value =  None  ) : result = { } kvp dbs =  Key  Value  Pair  . get by names ( names = names ) name to kvp db map = { } for kvp db in kvp dbs : name to kvp db map [ kvp db . name ] = kvp db . value for name in names : result [ name ] = name to kvp db map . get ( name , default value ) return result 
def maybe unroll group ( g ) : try : size = len ( g . tasks ) except  Type  Error  : try : size = g . tasks .   length hint   ( ) except (  Attribute  Error  ,  Type  Error  ) : return g else : return ( list ( g . tasks ) [ 0 ] if ( size == 1 ) else g ) else : return ( g . tasks [ 0 ] if ( size == 1 ) else g ) 
def pid by open file ( path ) : if is available ( 'lsof' ) : results = call ( ( GET PID BY FILE LSOF % path ) , [ ] ) if ( len ( results ) == 1 ) : pid = results [ 0 ] . strip ( ) if pid . isdigit ( ) : return int ( pid ) return  None  
@ deprecated ( u'2.1' ) def allequal ( seq ) : if ( len ( seq ) < 2 ) : return  True  val = seq [ 0 ] for i in xrange ( 1 , len ( seq ) ) : thisval = seq [ i ] if ( thisval != val ) : return  False  return  True  
def rankdata ( a , method = 'average' ) : if ( method not in ( 'average' , 'min' , 'max' , 'dense' , 'ordinal' ) ) : raise  Value  Error  ( 'unknown  method  "{0}"' . format ( method ) ) arr = np . ravel ( np . asarray ( a ) ) algo = ( 'mergesort' if ( method == 'ordinal' ) else 'quicksort' ) sorter = np . argsort ( arr , kind = algo ) inv = np . empty ( sorter . size , dtype = np . intp ) inv [ sorter ] = np . arange ( sorter . size , dtype = np . intp ) if ( method == 'ordinal' ) : return ( inv + 1 ) arr = arr [ sorter ] obs = np . r  [ (  True  , ( arr [ 1 : ] != arr [ : ( - 1 ) ] ) ) ] dense = obs . cumsum ( ) [ inv ] if ( method == 'dense' ) : return dense count = np . r  [ ( np . nonzero ( obs ) [ 0 ] , len ( obs ) ) ] if ( method == 'max' ) : return count [ dense ] if ( method == 'min' ) : return ( count [ ( dense - 1 ) ] + 1 ) return ( 0.5 * ( ( count [ dense ] + count [ ( dense - 1 ) ] ) + 1 ) ) 
def repack ( repo ) : with open repo closing ( repo ) as r : r . object store . pack loose objects ( ) 
def get auth from url ( url ) : if url : url = unquote ( url ) parsed = urlparse ( url ) return ( parsed . username , parsed . password ) else : return ( '' , '' ) 
def  unixdll getnode ( ) :  uuid generate time (  buffer ) return UUID ( bytes =  buffer . raw ) . node 
def lt ( name , value ) : ret = { 'name' : name , 'result' :  False  , 'comment' : '' , 'changes' : { } } if ( name not in   reg   ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Value   {0}  not  in  register' . format ( name ) return ret if (   reg   [ name ] [ 'val' ] < value ) : ret [ 'result' ] =  True  return ret 
def get scene numbering ( indexer id , indexer , season , episode , fallback to xem =  True  ) : if ( ( indexer id is  None  ) or ( season is  None  ) or ( episode is  None  ) ) : return ( season , episode ) show Obj  = find Certain  Show  ( sickrage . sr Core  . SHOWLIST , int ( indexer id ) ) if ( show Obj  and ( not show Obj  . is scene ) ) : return ( season , episode ) result = find scene numbering ( int ( indexer id ) , int ( indexer ) , season , episode ) if result : return result else : if fallback to xem : xem result = find xem numbering ( int ( indexer id ) , int ( indexer ) , season , episode ) if xem result : return xem result return ( season , episode ) 
def set loader ( fxn ) : @ functools . wraps ( fxn ) def set loader wrapper ( self , * args , ** kwargs ) : warnings . warn ( ' The   import  system  now  takes  care  of  this  automatically.' ,  Deprecation  Warning  , stacklevel = 2 ) module = fxn ( self , * args , ** kwargs ) if ( getattr ( module , '  loader  ' ,  None  ) is  None  ) : module .   loader   = self return module return set loader wrapper 
def on reply save ( sender , instance , ** kwargs ) : reply = instance year = reply . created . year user = reply . user if ( not user ) : return from kitsune . customercare . tasks import maybe award badge maybe award badge . delay ( AOA BADGE , year , user ) 
@ pytest . mark . nondestructive def test there are ten most popular extensions ( base url , selenium ) : page =  Home  ( selenium , base url ) . open ( ) assert ( len ( page . most popular . extensions ) == 10 ) 
def add callers ( target , source ) : new callers = { } for ( func , caller ) in target . iteritems ( ) : new callers [ func ] = caller for ( func , caller ) in source . iteritems ( ) : if ( func in new callers ) : new callers [ func ] = tuple ( [ ( i [ 0 ] + i [ 1 ] ) for i in zip ( caller , new callers [ func ] ) ] ) else : new callers [ func ] = caller return new callers 
@ register . inclusion tag ( 'horizon/ nav list.html' , takes context =  True  ) def horizon main nav ( context ) : if ( 'request' not in context ) : return { } current dashboard = context [ 'request' ] . horizon . get ( 'dashboard' ,  None  ) dashboards = [ ] for dash in  Horizon  . get dashboards ( ) : if ( callable ( dash . nav ) and dash . nav ( context ) ) : dashboards . append ( dash ) elif dash . nav : dashboards . append ( dash ) return { 'components' : dashboards , 'user' : context [ 'request' ] . user , 'current' : current dashboard , 'request' : context [ 'request' ] } 
def get path collection extents ( * args ) : from transforms import  Bbox  if ( len ( args [ 1 ] ) == 0 ) : raise  Value  Error  ( ' No   paths  provided' ) return  Bbox  . from extents ( *  get path collection extents ( * args ) ) 
def stop ( name , kill =  False  , path =  None  , use vt =  None  ) :  ensure exists ( name , path = path ) orig state = state ( name , path = path ) if ( ( orig state == 'frozen' ) and ( not kill ) ) : unfreeze ( name , path = path ) cmd = 'lxc-stop' if kill : cmd += '  -k' ret =  change state ( cmd , name , 'stopped' , use vt = use vt , path = path ) ret [ 'state' ] [ 'old' ] = orig state return ret 
def add home page ( bootinfo , docs ) : if ( frappe . session . user == u' Guest ' ) : return home page = frappe . db . get default ( u'desktop:home page' ) if ( home page == u'setup-wizard' ) : bootinfo . setup wizard requires = frappe . get hooks ( u'setup wizard requires' ) try : page = frappe . desk . desk page . get ( home page ) except ( frappe .  Does  Not  Exist  Error  , frappe .  Permission  Error  ) : if frappe . message log : frappe . message log . pop ( ) page = frappe . desk . desk page . get ( u'desktop' ) bootinfo [ u'home page' ] = page . name docs . append ( page ) 
def get localizable attributes ( obj ) : locale = { } try : if obj . label : locale [ 'label' ] = obj . label except : pass try : if obj . description : locale [ 'description' ] = obj . description except : pass return locale 
def test iht fit sample decision tree ( ) : est = 'decision-tree' iht =  Instance  Hardness  Threshold  ( est , random state = RND SEED ) ( X resampled , y resampled ) = iht . fit sample ( X , Y ) X gt = np . array ( [ [ ( - 0.3879569 ) , 0.6894251 ] , [ ( - 0.09322739 ) , 1.28177189 ] , [ ( - 0.77740357 ) , 0.74097941 ] , [ 0.91542919 , ( - 0.65453327 ) ] , [ ( - 0.43877303 ) , 1.07366684 ] , [ ( - 0.85795321 ) , 0.82980738 ] , [ ( - 0.18430329 ) , 0.52328473 ] , [ ( - 0.65571327 ) , 0.42412021 ] , [ ( - 0.28305528 ) , 0.30284991 ] , [ 1.06446472 , ( - 1.09279772 ) ] , [ 0.30543283 , ( - 0.02589502 ) ] , [ ( - 0.00717161 ) , 0.00318087 ] ] ) y gt = np . array ( [ 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 ] ) assert array equal ( X resampled , X gt ) assert array equal ( y resampled , y gt ) 
def  pairwise callable ( X , Y , metric , ** kwds ) : ( X , Y ) = check pairwise arrays ( X , Y ) if ( X is Y ) : out = np . zeros ( ( X . shape [ 0 ] , Y . shape [ 0 ] ) , dtype = 'float' ) iterator = itertools . combinations ( range ( X . shape [ 0 ] ) , 2 ) for ( i , j ) in iterator : out [ ( i , j ) ] = metric ( X [ i ] , Y [ j ] , ** kwds ) out = ( out + out . T ) for i in range ( X . shape [ 0 ] ) : x = X [ i ] out [ ( i , i ) ] = metric ( x , x , ** kwds ) else : out = np . empty ( ( X . shape [ 0 ] , Y . shape [ 0 ] ) , dtype = 'float' ) iterator = itertools . product ( range ( X . shape [ 0 ] ) , range ( Y . shape [ 0 ] ) ) for ( i , j ) in iterator : out [ ( i , j ) ] = metric ( X [ i ] , Y [ j ] , ** kwds ) return out 
def hrm human resource filters ( resource type =  None  , module =  None  , hrm type opts =  None  ) : T = current . T s3 = current . response . s3 settings = current . deployment settings if ( not module ) : module = current . request . controller text search fields = [ 'person id$first name' , 'person id$middle name' , 'person id$last name' , 'person id$email.value' , 'organisation id' ] use code = settings . get hrm use code ( ) if ( ( ( resource type != 'volunteer' ) and use code ) or ( use code is  True  ) ) : text search fields . append ( 'code' ) filter widgets = [ S3 Text  Filter  ( text search fields , label = T ( ' Search ' ) ) ] append filter = filter widgets . append if ( ( module == 'deploy' ) and current . auth . s3 has role ( 'ADMIN' ) ) : dotable = current . s3db . deploy organisation deploying orgs = current . db ( ( dotable . deleted ==  False  ) ) . count ( ) if ( deploying orgs > 1 ) : append filter ( S3 Options  Filter  ( 'application.organisation id' , label = T ( ' Deployment    Team ' ) ) ) if ( not ( resource type in ( 'staff' , 'volunteer' ) ) ) : append filter ( S3 Options  Filter  ( 'type' , label = T ( ' Type ' ) , options = hrm type opts , cols = 2 , hidden =  True  ) ) if settings . get org regions ( ) : if settings . get org regions hierarchical ( ) : if ( module == 'deploy' ) : hidden =  False  else : hidden =  True  append filter ( S3 Hierarchy  Filter  ( 'organisation id$region id' , label = T ( ' Region ' ) , hidden = hidden ) ) else : append filter ( S3 Options  Filter  ( 'organisation id$region id' , label = T ( ' Region ' ) , hidden =  True  ) ) if settings . get hrm multiple orgs ( ) : if settings . get org branches ( ) : append filter ( S3 Hierarchy  Filter  ( 'organisation id' , leafonly =  False  ) ) else : append filter ( S3 Options  Filter  ( 'organisation id' , filter =  True  , header = '' ) ) append filter ( S3 Location  Filter  ( 'location id' , label = T ( ' Location ' ) , hidden =  True  ) ) if ( ( module == 'vol' ) or ( resource type == 'volunteer' ) ) : vol active = settings . get hrm vol active ( ) if vol active : append filter ( S3 Options  Filter  ( 'details.active' , label = T ( ' Active ?' ) , cols = 2 , options = {  True  : T ( ' Yes ' ) ,  False  : T ( ' No ' ) } , hidden =  True  ) ) vol experience = settings . get hrm vol experience ( ) if ( vol experience in ( 'programme' , 'both' ) ) : append filter ( S3 Options  Filter  ( 'person id$hours.programme id' , label = T ( ' Program ' ) , hidden =  True  ) ) elif ( vol experience == 'activity' ) : append filter ( S3 Options  Filter  ( 'person id$activity hours.activity hours activity type.activity type id' , label = T ( ' Activity    Types ' ) , hidden =  True  ) ) else : filter widgets . append ( S3 Options  Filter  ( 'site id' , hidden =  True  ) ) if ( module == 'deploy' ) : append filter ( S3 Options  Filter  ( 'credential.job title id' , label = T ( ' Sector ' ) , hidden =  True  ) ) append filter ( S3 Date  Filter  ( 'human resource id:deploy assignment.start date' , label = T ( ' Deployed ' ) , hide time =  True  , hidden =  True  ) ) append filter ( S3 Date  Filter  ( 'human resource id:deploy response.created on' , label = T ( ' Responded ' ) , hide time =  True  , hidden =  True  ) ) if settings . get hrm use certificates ( ) : append filter ( S3 Options  Filter  ( 'certification.certificate id' , hidden =  True  ) ) if settings . get hrm use skills ( ) : append filter ( S3 Options  Filter  ( 'competency.skill id' , hidden = ( module != 'req' ) ) ) if settings . get hrm use trainings ( ) : if settings . get hrm training filter and ( ) : append filter ( S3 Options  Filter  ( 'trainings.course id' , label = T ( ' Training ' ) , hidden =  True  , operator = 'contains' ) ) else : append filter ( S3 Options  Filter  ( 'training.course id' , label = T ( ' Training ' ) , hidden =  True  ) ) teams = settings . get hrm teams ( ) if teams : if ( teams == ' Teams ' ) : teams = ' Team ' elif ( teams == ' Groups ' ) : teams = ' Group ' append filter ( S3 Options  Filter  ( 'group membership.group id' , label = T ( teams ) , hidden =  True  ) ) return filter widgets 
def get Crafted  Text  ( file Name  , text , home Repository  =  None  ) : return get Crafted  Text  From  Text  ( archive . get Text  If  Empty  ( file Name  , text ) , home Repository  ) 
def   virtual   ( ) : if ( salt . utils . is windows ( ) and  HAS MODULE DEPENDENCIES ) : return   virtualname   return  False  
@ app . route ( '/base64/<value>' ) def decode base64 ( value ) : encoded = value . encode ( 'utf-8' ) return base64 . urlsafe b64decode ( encoded ) . decode ( 'utf-8' ) 
def random symbols ( expr ) : try : return list ( expr . atoms (  Random  Symbol  ) ) except  Attribute  Error  : return [ ] 
def equal any ( * values ) : return IMPL . equal any ( * values ) 
def send file ( filename or fp , mimetype =  None  , as attachment =  False  , attachment filename =  None  , add etags =  True  , cache timeout =  None  , conditional =  False  , last modified =  None  ) : mtime =  None  fsize =  None  if isinstance ( filename or fp , string types ) : filename = filename or fp if ( not os . path . isabs ( filename ) ) : filename = os . path . join ( current app . root path , filename ) file =  None  if ( attachment filename is  None  ) : attachment filename = os . path . basename ( filename ) else : file = filename or fp filename =  None  if ( mimetype is  None  ) : if ( attachment filename is not  None  ) : mimetype = ( mimetypes . guess type ( attachment filename ) [ 0 ] or 'application/octet-stream' ) if ( mimetype is  None  ) : raise  Value  Error  ( ' Unable   to  infer  MIME-type  because  no  filename  is  available.   Please   set  either  `attachment filename`,  pass  a  filepath  to  `filename or fp`  or  set  your  own  MIME-type  via  `mimetype`.' ) headers =  Headers  ( ) if as attachment : if ( attachment filename is  None  ) : raise  Type  Error  ( 'filename  unavailable,  required  for  sending  as  attachment' ) headers . add ( ' Content - Disposition ' , 'attachment' , filename = attachment filename ) if ( current app . use x sendfile and filename ) : if ( file is not  None  ) : file . close ( ) headers [ 'X- Sendfile ' ] = filename fsize = os . path . getsize ( filename ) headers [ ' Content - Length ' ] = fsize data =  None  else : if ( file is  None  ) : file = open ( filename , 'rb' ) mtime = os . path . getmtime ( filename ) fsize = os . path . getsize ( filename ) headers [ ' Content - Length ' ] = fsize data = wrap file ( request . environ , file ) rv = current app . response class ( data , mimetype = mimetype , headers = headers , direct passthrough =  True  ) if ( last modified is not  None  ) : rv . last modified = last modified elif ( mtime is not  None  ) : rv . last modified = mtime rv . cache control . public =  True  if ( cache timeout is  None  ) : cache timeout = current app . get send file max age ( filename ) if ( cache timeout is not  None  ) : rv . cache control . max age = cache timeout rv . expires = int ( ( time ( ) + cache timeout ) ) if ( add etags and ( filename is not  None  ) ) : from warnings import warn try : rv . set etag ( ( '%s-%s-%s' % ( os . path . getmtime ( filename ) , os . path . getsize ( filename ) , ( adler32 ( ( filename . encode ( 'utf-8' ) if isinstance ( filename , text type ) else filename ) ) & 4294967295 ) ) ) ) except OS Error  : warn ( ( ' Access   %s  failed,  maybe  it  does  not  exist,  so  ignore  etags  in  headers' % filename ) , stacklevel = 2 ) if conditional : if callable ( getattr (  Range  , 'to content range header' ,  None  ) ) : try : rv = rv . make conditional ( request , accept ranges =  True  , complete length = fsize ) except  Requested  Range  Not  Satisfiable  : file . close ( ) raise else : rv = rv . make conditional ( request ) if ( rv . status code == 304 ) : rv . headers . pop ( 'x-sendfile' ,  None  ) return rv 
def u ( s ) : return ( s if ( PY3 or ( type ( s ) is unicode ) ) else unicode ( s . encode ( 'string-escape' ) , 'unicode escape' ) ) 
def  get default context ( request ) : types =  Object  . filter by request ( request ,  Item  Type  . objects . filter ( parent  isnull =  True  ) ) statuses =  Object  . filter by request ( request ,  Item  Status  . objects ) locations =  Object  . filter by request ( request ,  Location  . objects ) massform =  Mass  Action  Form  ( request . user . profile ) context = { 'statuses' : statuses , 'types' : types , 'massform' : massform , 'locations' : locations } return context 
def  transform messages base64 ( messages , transform , key =  None  ) : for message in messages : if ( key is not  None  ) : message = message [ key ] if ( 'data' in message ) : message [ 'data' ] = transform ( message [ 'data' ] ) 
def  Lop  ( f , wrt , eval points , consider constant =  None  , disconnected inputs = 'raise' ) : if ( type ( eval points ) not in ( list , tuple ) ) : eval points = [ eval points ] using list = isinstance ( wrt , list ) using tuple = isinstance ( wrt , tuple ) if ( not isinstance ( f , ( list , tuple ) ) ) : f = [ f ] f = list ( f ) grads = list ( eval points ) if ( not isinstance ( wrt , ( list , tuple ) ) ) : wrt = [ wrt ] assert ( len ( f ) == len ( grads ) ) known =  Ordered  Dict  ( izip ( f , grads ) ) ret = grad ( cost =  None  , known grads = known , consider constant = consider constant , wrt = wrt , disconnected inputs = disconnected inputs ) return format as ( using list , using tuple , ret ) 
def walk ( top , func , arg ) : warnings . warnpy3k ( ' In   3.x,  os.path.walk  is  removed  in  favor  of  os.walk.' , stacklevel = 2 ) try : names = os . listdir ( top ) except os . error : return func ( arg , top , names ) for name in names : name = join ( top , name ) if isdir ( name ) : walk ( name , func , arg ) 
def available ( name , limit = '' ) : if ( limit == 'upstart' ) : return  service is upstart ( name ) elif ( limit == 'sysvinit' ) : return  service is sysv ( name ) else : return (  service is upstart ( name ) or  service is sysv ( name ) or  service is chkconfig ( name ) ) 
def is valid ip ( ip address ) : valid =  True  try : socket . inet aton ( ip address . strip ( ) ) except : valid =  False  return valid 
def  update dict ( dictionary , Y , code , verbose =  False  , return r2 =  False  , random state =  None  ) : n components = len ( code ) n samples = Y . shape [ 0 ] random state = check random state ( random state ) R = ( - np . dot ( dictionary , code ) ) R += Y R = np . asfortranarray ( R ) ( ger , ) = linalg . get blas funcs ( ( 'ger' , ) , ( dictionary , code ) ) for k in range ( n components ) : R = ger ( 1.0 , dictionary [ : , k ] , code [ k , : ] , a = R , overwrite a =  True  ) dictionary [ : , k ] = np . dot ( R , code [ k , : ] . T ) atom norm square = np . dot ( dictionary [ : , k ] , dictionary [ : , k ] ) if ( atom norm square < 1e-20 ) : if ( verbose == 1 ) : sys . stdout . write ( '+' ) sys . stdout . flush ( ) elif verbose : print ( ' Adding   new  random  atom' ) dictionary [ : , k ] = random state . randn ( n samples ) code [ k , : ] = 0.0 dictionary [ : , k ] /= sqrt ( np . dot ( dictionary [ : , k ] , dictionary [ : , k ] ) ) else : dictionary [ : , k ] /= sqrt ( atom norm square ) R = ger ( ( - 1.0 ) , dictionary [ : , k ] , code [ k , : ] , a = R , overwrite a =  True  ) if return r2 : R **= 2 R = as strided ( R , shape = ( R . size , ) , strides = ( R . dtype . itemsize , ) ) R = np . sum ( R ) return ( dictionary , R ) return dictionary 
def reducer ( * tokens ) : def decorator ( func ) : if ( not hasattr ( func , 'reducers' ) ) : func . reducers = [ ] func . reducers . append ( list ( tokens ) ) return func return decorator 
def filename ( ) : if ( not  state ) : raise  Runtime  Error  ( 'no  active  input()' ) return  state . filename ( ) 
def url to parts ( url ) : scheme = urlparse ( url ) . scheme schemeless = url [ ( len ( scheme ) + 3 ) : ] parts = urlparse ( ( u'http://' + schemeless ) ) path = ( parts . path or u'' ) path = ( path [ 1 : ] if ( path and ( path [ 0 ] == u'/' ) ) else path ) return urlparts ( scheme , ( unquote ( ( parts . hostname or u'' ) ) or  None  ) , parts . port , ( unquote ( ( parts . username or u'' ) ) or  None  ) , ( unquote ( ( parts . password or u'' ) ) or  None  ) , ( unquote ( ( path or u'' ) ) or  None  ) , dict ( parse qsl ( parts . query ) ) ) 
def py encode basestring ascii ( s ) : if ( isinstance ( s , str ) and ( HAS UTF8 . search ( s ) is not  None  ) ) : s = s . decode ( 'utf-8' ) def replace ( match ) : s = match . group ( 0 ) try : return ESCAPE DCT [ s ] except  Key  Error  : n = ord ( s ) if ( n < 65536 ) : return ( '\\u%04x' % ( n , ) ) else : n -= 65536 s1 = ( 55296 | ( ( n >> 10 ) & 1023 ) ) s2 = ( 56320 | ( n & 1023 ) ) return ( '\\u%04x\\u%04x' % ( s1 , s2 ) ) return ( ( '"' + str ( ESCAPE ASCII . sub ( replace , s ) ) ) + '"' ) 
def  Thing 2 Literal  ( o , d ) : return string literal ( o , d ) 
def compute hash ( localfn ) : with open ( localfn , u'rb' ) as f : h = hashlib . md5 ( ) block = f . read ( conf . compute hash block size ) while block : h . update ( block ) block = f . read ( conf . compute hash block size ) return h . hexdigest ( ) 
def resolve email domain ( domain ) : request = DNS .  Request  ( ) reply = request . req ( name = sys . argv [ 1 ] , qtype = DNS .  Type  . MX ) if reply . answers : print ( ' The   domain  %r  has  explicit  MX  records!' % ( domain , ) ) print ' Try   the  servers  in  this  order:' datalist = [ answer [ 'data' ] for answer in reply . answers ] datalist . sort ( ) for data in datalist : priority = data [ 0 ] hostname = data [ 1 ] print ' Priority :' , priority , '     Hostname :' , hostname resolve hostname ( hostname ) else : print ' Drat ,  this  domain  has  no  explicit  MX  records' print ' We   will  have  to  try  resolving  it  as  an  A,  AAAA,  or  CNAME' resolve hostname ( domain ) 
def  resolve requirements chain ( requirements ) : chain = [ ] if isinstance ( requirements , string types ) : requirements = [ requirements ] for req file in requirements : chain . append ( req file ) chain . extend (  resolve requirements chain (  find req ( req file ) ) ) return chain 
def detect ( fp , max read = 1024 ) : if ( not is filelike ( fp ) ) : return  None  for  Format  in  registry . values ( ) : if  Format  . detect ( fp . read ( max read ) ) : fp . seek ( 0 ) return  Format  fp . seek ( 0 ) return  None  
def inotify code changed ( ) : class  Event  Handler  ( pyinotify .  Process  Event  , ) : modified code =  None  def process default ( self , event ) : if event . path . endswith ( '.mo' ) :  Event  Handler  . modified code = I18N MODIFIED else :  Event  Handler  . modified code = FILE MODIFIED wm = pyinotify .  Watch  Manager  ( ) notifier = pyinotify .  Notifier  ( wm ,  Event  Handler  ( ) ) def update watch ( sender =  None  , ** kwargs ) : if ( sender and getattr ( sender , 'handles files' ,  False  ) ) : return mask = ( ( ( ( ( ( ( pyinotify . IN MODIFY | pyinotify . IN DELETE ) | pyinotify . IN ATTRIB ) | pyinotify . IN MOVED FROM ) | pyinotify . IN MOVED TO ) | pyinotify . IN CREATE ) | pyinotify . IN DELETE SELF ) | pyinotify . IN MOVE SELF ) for path in gen filenames ( only new =  True  ) : wm . add watch ( path , mask ) request finished . connect ( update watch ) update watch ( ) notifier . check events ( timeout =  None  ) notifier . read events ( ) notifier . process events ( ) notifier . stop ( ) return  Event  Handler  . modified code 
def remove ( mod , persist =  False  ) : pre mods = lsmod ( )   salt   [ 'cmd.run all' ] ( 'kldunload  {0}' . format ( mod ) , python shell =  False  ) post mods = lsmod ( ) mods =  rm mods ( pre mods , post mods ) persist mods = set ( ) if persist : persist mods =  remove persistent module ( mod ) return sorted ( list ( ( mods | persist mods ) ) ) 
def tplquad ( func , a , b , gfun , hfun , qfun , rfun , args = ( ) , epsabs = 1.49e-08 , epsrel = 1.49e-08 ) : def ranges0 ( * args ) : return [ qfun ( args [ 1 ] , args [ 0 ] ) , rfun ( args [ 1 ] , args [ 0 ] ) ] def ranges1 ( * args ) : return [ gfun ( args [ 0 ] ) , hfun ( args [ 0 ] ) ] ranges = [ ranges0 , ranges1 , [ a , b ] ] return nquad ( func , ranges , args = args ) 
@ slow test @ requires sklearn def test ica additional ( ) : import matplotlib . pyplot as plt tempdir =   Temp  Dir  ( ) stop2 = 500 raw = read raw fif ( raw fname ) . crop ( 1.5 , stop ) . load data ( ) test cov = read cov ( test cov name ) events = read events ( event name ) picks = pick types ( raw . info , meg =  True  , stim =  False  , ecg =  False  , eog =  False  , exclude = 'bads' ) epochs =  Epochs  ( raw , events [ : 4 ] , event id , tmin , tmax , picks = picks , baseline = (  None  , 0 ) , preload =  True  ) with warnings . catch warnings ( record =  True  ) : ica = ICA ( n components =  None  , max pca components =  None  , n pca components =  None  , random state = 0 ) ica . fit ( epochs , picks = picks , decim = 3 ) picks2 = pick types ( raw . info , meg =  True  , stim =  False  , ecg =  False  , eog =  True  , exclude = 'bads' ) epochs eog =  Epochs  ( raw , events [ : 4 ] , event id , tmin , tmax , picks = picks2 , baseline = (  None  , 0 ) , preload =  True  ) test cov2 = test cov . copy ( ) ica = ICA ( noise cov = test cov2 , n components = 3 , max pca components = 4 , n pca components = 4 ) assert true ( ( ica . info is  None  ) ) with warnings . catch warnings ( record =  True  ) : ica . fit ( raw , picks [ : 5 ] ) assert true ( isinstance ( ica . info ,  Info  ) ) assert true ( ( ica . n components  < 5 ) ) ica = ICA ( n components = 3 , max pca components = 4 , n pca components = 4 ) assert raises (  Runtime  Error  , ica . save , '' ) with warnings . catch warnings ( record =  True  ) : ica . fit ( raw , picks = [ 1 , 2 , 3 , 4 , 5 ] , start = start , stop = stop2 ) ica2 = ica . copy ( ) ica3 = ica . copy ( ) corrmap ( [ ica , ica2 ] , ( 0 , 0 ) , threshold = 'auto' , label = 'blinks' , plot =  True  , ch type = 'mag' ) corrmap ( [ ica , ica2 ] , ( 0 , 0 ) , threshold = 2 , plot =  False  , show =  False  ) assert true ( ( ica . labels  [ 'blinks' ] == ica2 . labels  [ 'blinks' ] ) ) assert true ( ( 0 in ica . labels  [ 'blinks' ] ) ) components = ica . get components ( ) template = components [ : , 0 ]  Evoked  Array  ( components , ica . info , tmin = 0.0 ) . plot topomap ( [ 0 ] ) corrmap ( [ ica , ica3 ] , template , threshold = 'auto' , label = 'blinks' , plot =  True  , ch type = 'mag' ) assert true ( ( ica2 . labels  [ 'blinks' ] == ica3 . labels  [ 'blinks' ] ) ) plt . close ( 'all' ) with warnings . catch warnings ( record =  True  ) as w : warnings . simplefilter ( 'always' ) ica badname = op . join ( op . dirname ( tempdir ) , 'test-bad-name.fif.gz' ) ica . save ( ica badname ) read ica ( ica badname ) assert naming ( w , 'test ica.py' , 2 ) ica = ICA ( n components = 3 , max pca components = 4 , n pca components = 4 ) raw  = raw . copy ( ) for   in range ( 3 ) : raw  . append ( raw  ) n samples = raw  .  data . shape [ 1 ] with warnings . catch warnings ( record =  True  ) : ica . fit ( raw , picks =  None  , decim = 3 ) assert true ( raw  .  data . shape [ 1 ] , n samples ) ica = ICA ( n components = 1.0 , max pca components = 4 , n pca components = 4 ) with warnings . catch warnings ( record =  True  ) : ica . fit ( raw , picks =  None  , decim = 3 ) assert true ( ( ica . n components  == 4 ) ) ica var =  ica explained variance ( ica , raw , normalize =  True  ) assert true ( np . all ( ( ica var [ : ( - 1 ) ] >= ica var [ 1 : ] ) ) ) ica . exclude = [ 0 ] ica . labels  = dict ( blink = [ 0 ] , think = [ 1 ] ) ica sorted =  sort components ( ica , [ 3 , 2 , 1 , 0 ] , copy =  True  ) assert equal ( ica sorted . exclude , [ 3 ] ) assert equal ( ica sorted . labels  , dict ( blink = [ 3 ] , think = [ 2 ] ) ) assert raises (  Runtime  Error  , ica . get sources , epochs ) test ica fname = op . join ( op . dirname ( tempdir ) , 'test-ica.fif' ) for cov in (  None  , test cov ) : ica = ICA ( noise cov = cov , n components = 2 , max pca components = 4 , n pca components = 4 ) with warnings . catch warnings ( record =  True  ) : ica . fit ( raw , picks = picks , start = start , stop = stop2 ) sources = ica . get sources ( epochs ) . get data ( ) assert true ( ( ica . mixing matrix  . shape == ( 2 , 2 ) ) ) assert true ( ( ica . unmixing matrix  . shape == ( 2 , 2 ) ) ) assert true ( ( ica . pca components  . shape == ( 4 , len ( picks ) ) ) ) assert true ( ( sources . shape [ 1 ] == ica . n components  ) ) for exclude in [ [ ] , [ 0 ] ] : ica . exclude = exclude ica . labels  = { 'foo' : [ 0 ] } ica . save ( test ica fname ) ica read = read ica ( test ica fname ) assert true ( ( ica . exclude == ica read . exclude ) ) assert equal ( ica . labels  , ica read . labels  ) ica . exclude = [ ] ica . apply ( raw , exclude = [ 1 ] ) assert true ( ( ica . exclude == [ ] ) ) ica . exclude = [ 0 , 1 ] ica . apply ( raw , exclude = [ 1 ] ) assert true ( ( ica . exclude == [ 0 , 1 ] ) ) ica raw = ica . get sources ( raw ) assert true ( ( ica . exclude == [ ica raw . ch names . index ( e ) for e in ica raw . info [ 'bads' ] ] ) ) d1 = ica raw .  data [ 0 ] . copy ( ) ica raw . filter ( 4 , 20 , l trans bandwidth = 'auto' , h trans bandwidth = 'auto' , filter length = 'auto' , phase = 'zero' , fir window = 'hamming' ) assert equal ( ica raw . info [ 'lowpass' ] , 20.0 ) assert equal ( ica raw . info [ 'highpass' ] , 4.0 ) assert true ( ( d1 != ica raw .  data [ 0 ] ) . any ( ) ) d1 = ica raw .  data [ 0 ] . copy ( ) ica raw . notch filter ( [ 10 ] , filter length = 'auto' , trans bandwidth = 10 , phase = 'zero' , fir window = 'hamming' ) assert true ( ( d1 != ica raw .  data [ 0 ] ) . any ( ) ) ica . n pca components = 2 ica . method = 'fake' ica . save ( test ica fname ) ica read = read ica ( test ica fname ) assert true ( ( ica . n pca components == ica read . n pca components ) ) assert equal ( ica . method , ica read . method ) assert equal ( ica . labels  , ica read . labels  ) attrs = 'mixing matrix   unmixing matrix   pca components   pca explained variance    pre whitener' def f ( x , y ) : return getattr ( x , y ) . dtype for attr in attrs . split ( ) : assert equal ( f ( ica read , attr ) , f ( ica , attr ) ) ica . n pca components = 4 ica read . n pca components = 4 ica . exclude = [ ] ica . save ( test ica fname ) ica read = read ica ( test ica fname ) for attr in [ 'mixing matrix ' , 'unmixing matrix ' , 'pca components ' , 'pca mean ' , 'pca explained variance ' , ' pre whitener' ] : assert array almost equal ( getattr ( ica , attr ) , getattr ( ica read , attr ) ) assert true ( ( ica . ch names == ica read . ch names ) ) assert true ( isinstance ( ica read . info ,  Info  ) ) sources = ica . get sources ( raw ) [ : , : ] [ 0 ] sources2 = ica read . get sources ( raw ) [ : , : ] [ 0 ] assert array almost equal ( sources , sources2 )  raw1 = ica . apply ( raw , exclude = [ 1 ] )  raw2 = ica read . apply ( raw , exclude = [ 1 ] ) assert array almost equal (  raw1 [ : , : ] [ 0 ] ,  raw2 [ : , : ] [ 0 ] ) os . remove ( test ica fname ) for ( name , func ) in get score funcs ( ) . items ( ) : if ( name in score funcs unsuited ) : continue scores = ica . score sources ( raw , target = 'EOG  061' , score func = func , start = 0 , stop = 10 ) assert true ( ( ica . n components  == len ( scores ) ) ) scores = ica . score sources ( raw , score func = stats . skew ) assert raises (  Value  Error  , ica . score sources , raw , target = np . arange ( 1 ) ) params = [ ] params += [ (  None  , ( - 1 ) , slice ( 2 ) , [ 0 , 1 ] ) ] params += [ (  None  , 'MEG  1531' ) ] for ( idx , ch name ) in product ( * params ) : ica . detect artifacts ( raw , start find = 0 , stop find = 50 , ecg ch = ch name , eog ch = ch name , skew criterion = idx , var criterion = idx , kurt criterion = idx ) evoked = epochs . average ( ) evoked data = evoked . data . copy ( ) raw data = raw [ : ] [ 0 ] . copy ( ) epochs data = epochs . get data ( ) . copy ( ) with warnings . catch warnings ( record =  True  ) : ( idx , scores ) = ica . find bads ecg ( raw , method = 'ctps' ) assert equal ( len ( scores ) , ica . n components  ) ( idx , scores ) = ica . find bads ecg ( raw , method = 'correlation' ) assert equal ( len ( scores ) , ica . n components  ) ( idx , scores ) = ica . find bads eog ( raw ) assert equal ( len ( scores ) , ica . n components  ) ( idx , scores ) = ica . find bads ecg ( epochs , method = 'ctps' ) assert equal ( len ( scores ) , ica . n components  ) assert raises (  Value  Error  , ica . find bads ecg , epochs . average ( ) , method = 'ctps' ) assert raises (  Value  Error  , ica . find bads ecg , raw , method = 'crazy-coupling' ) raw . info [ 'chs' ] [ ( raw . ch names . index ( 'EOG  061' ) - 1 ) ] [ 'kind' ] = 202 ( idx , scores ) = ica . find bads eog ( raw ) assert true ( isinstance ( scores , list ) ) assert equal ( len ( scores [ 0 ] ) , ica . n components  ) ( idx , scores ) = ica . find bads eog ( evoked , ch name = 'MEG  1441' ) assert equal ( len ( scores ) , ica . n components  ) ( idx , scores ) = ica . find bads ecg ( evoked , method = 'correlation' ) assert equal ( len ( scores ) , ica . n components  ) assert array equal ( raw data , raw [ : ] [ 0 ] ) assert array equal ( epochs data , epochs . get data ( ) ) assert array equal ( evoked data , evoked . data ) for ( name , func ) in get score funcs ( ) . items ( ) : if ( name in score funcs unsuited ) : continue scores = ica . score sources ( epochs eog , target = 'EOG  061' , score func = func ) assert true ( ( ica . n components  == len ( scores ) ) ) scores = ica . score sources ( epochs , score func = stats . skew ) assert raises (  Value  Error  , ica . score sources , epochs , target = np . arange ( 1 ) ) ecg scores = ica . score sources ( raw , target = 'MEG  1531' , score func = 'pearsonr' ) with warnings . catch warnings ( record =  True  ) : ecg events = ica find ecg events ( raw , sources [ np . abs ( ecg scores ) . argmax ( ) ] ) assert true ( ( ecg events . ndim == 2 ) ) eog scores = ica . score sources ( raw , target = 'EOG  061' , score func = 'pearsonr' ) with warnings . catch warnings ( record =  True  ) : eog events = ica find eog events ( raw , sources [ np . abs ( eog scores ) . argmax ( ) ] ) assert true ( ( eog events . ndim == 2 ) ) ica raw = ica . get sources ( raw , start = 0 , stop = 100 ) assert true ( ( ( ica raw . last samp - ica raw . first samp ) == 100 ) ) assert equal ( len ( ica raw .  filenames ) , 1 ) ica chans = [ ch for ch in ica raw . ch names if ( 'ICA' in ch ) ] assert true ( ( ica . n components  == len ( ica chans ) ) ) test ica fname = op . join ( op . abspath ( op . curdir ) , 'test-ica raw.fif' ) ica . n components = np . int32 ( ica . n components ) ica raw . save ( test ica fname , overwrite =  True  ) ica raw2 = read raw fif ( test ica fname , preload =  True  ) assert allclose ( ica raw .  data , ica raw2 .  data , rtol = 1e-05 , atol = 0.0001 ) ica raw2 . close ( ) os . remove ( test ica fname ) ica epochs = ica . get sources ( epochs ) assert true ( ( ica epochs . events . shape == epochs . events . shape ) ) ica chans = [ ch for ch in ica epochs . ch names if ( 'ICA' in ch ) ] assert true ( ( ica . n components  == len ( ica chans ) ) ) assert true ( ( ica . n components  == ica epochs . get data ( ) . shape [ 1 ] ) ) assert true ( ( ica epochs .  raw is  None  ) ) assert true ( ( ica epochs . preload is  True  ) ) ica . pca explained variance  = np . array ( ( [ 0.2 ] * 5 ) ) ica . n components  = 0 for ( ncomps , expected ) in [ [ 0.3 , 1 ] , [ 0.9 , 4 ] , [ 1 , 1 ] ] : ncomps  = ica .  check n pca components ( ncomps ) assert true ( ( ncomps  == expected ) ) ica = ICA ( ) ica . fit ( raw , picks = picks [ : 5 ] ) ica . find bads ecg ( raw ) ica . find bads eog ( epochs , ch name = 'MEG  0121' ) assert array equal ( raw data , raw [ : ] [ 0 ] ) raw . drop channels ( [ 'MEG  0122' ] ) assert raises (  Runtime  Error  , ica . find bads eog , raw ) assert raises (  Runtime  Error  , ica . find bads ecg , raw ) 
def get params ( section div , params class ) : parameters = [ ] params row = section div . find ( 'tr' , attrs = { 'class' : params class } ) if ( not params row ) : return params cells = params row . find ( 'td' , { 'class' : 'field-body' } ) for tag in params cells . children : try : t = tag . text t . strip ( ) if t . startswith ( '\n' ) : t = t . lstrip ( ) t = ( ( '  ' * 4 ) + t ) t = t . replace ( '\n' , '  ' ) t . rstrip ( ) t = ( t + '\n' ) parameters . append ( t ) except  Attribute  Error  : pass return '' . join ( parameters ) . rstrip ( ) 
def job title ( ) : mode = session . s3 . hrm . mode def prep ( r ) : if ( mode is not  None  ) : auth . permission . fail ( ) elif ( r . representation == 'xls' ) : current . messages [ 'NONE' ] = '' table = s3db . hrm job title table . organisation id . represent = s3db . org  Organisation  Represent  ( acronym =  False  , parent =  False  ) table . organisation id . label =  None  table . type . label =  None  table . comments . label =  None  table . comments . represent = ( lambda v : ( v or '' ) ) return  True  s3 . prep = prep s3 . filter = FS ( 'human resource.type' ) . belongs ( ( 2 , 3 ) ) if ( not auth . s3 has role ( ADMIN ) ) : s3 . filter &= auth . filter by root org ( s3db . hrm job title ) return s3 rest controller ( 'hrm' , resourcename , csv stylesheet = ( 'hrm' , 'job title.xsl' ) , csv template = ( 'hrm' , 'job title' ) ) 
def  termination condition ( t , k , g , n , s , alpha , delta ) : diff = ( k - solow steady state ( g , n , s , alpha , delta ) ) return diff 
def all pairs dijkstra path ( G , cutoff =  None  , weight = 'weight' ) : path = single source dijkstra path return { n : path ( G , n , cutoff = cutoff , weight = weight ) for n in G } 
def cpu percent ( interval =  None  , percpu =  False  ) : global  last cpu times global  last per cpu times blocking = ( ( interval is not  None  ) and ( interval > 0.0 ) ) def calculate ( t1 , t2 ) : t1 all = sum ( t1 ) t1 busy = ( t1 all - t1 . idle ) t2 all = sum ( t2 ) t2 busy = ( t2 all - t2 . idle ) if ( t2 busy <= t1 busy ) : return 0.0 busy delta = ( t2 busy - t1 busy ) all delta = ( t2 all - t1 all ) busy perc = ( ( busy delta / all delta ) * 100 ) return round ( busy perc , 1 ) if ( not percpu ) : if blocking : t1 = cpu times ( ) time . sleep ( interval ) else : t1 =  last cpu times if ( t1 is  None  ) : t1 = cpu times ( )  last cpu times = cpu times ( ) return calculate ( t1 ,  last cpu times ) else : ret = [ ] if blocking : tot1 = cpu times ( percpu =  True  ) time . sleep ( interval ) else : tot1 =  last per cpu times if ( tot1 is  None  ) : tot1 = cpu times ( percpu =  True  )  last per cpu times = cpu times ( percpu =  True  ) for ( t1 , t2 ) in zip ( tot1 ,  last per cpu times ) : ret . append ( calculate ( t1 , t2 ) ) return ret 
def  feed stream ( feeder , in stream , out stream , block size = BLOCK SIZE ) : while  True  : chunk = in stream . read ( block size ) if ( not chunk ) : break converted = feeder . feed ( chunk ) out stream . write ( converted ) converted = feeder . feed ( ) out stream . write ( converted ) 
def extract module locals ( depth = 0 ) : f = sys .  getframe ( ( depth + 1 ) ) global ns = f . f globals module = sys . modules [ global ns [ '  name  ' ] ] return ( module , f . f locals ) 
def prepare student data ( dataset , nb teachers , save =  False  ) : assert input . create dir if needed ( FLAGS . train dir ) if ( dataset == 'svhn' ) : ( test data , test labels ) = input . ld svhn ( test only =  True  ) elif ( dataset == 'cifar10' ) : ( test data , test labels ) = input . ld cifar10 ( test only =  True  ) elif ( dataset == 'mnist' ) : ( test data , test labels ) = input . ld mnist ( test only =  True  ) else : print ( ' Check   value  of  dataset  flag' ) return  False  assert ( FLAGS . stdnt share < len ( test data ) ) stdnt data = test data [ : FLAGS . stdnt share ] teachers preds = ensemble preds ( dataset , nb teachers , stdnt data ) if ( not save ) : stdnt labels = aggregation . noisy max ( teachers preds , FLAGS . lap scale ) else : ( stdnt labels , clean votes , labels for dump ) = aggregation . noisy max ( teachers preds , FLAGS . lap scale , return clean votes =  True  ) filepath = ( ( ( ( ( ( ( FLAGS . data dir + '/' ) + str ( dataset ) ) + ' ' ) + str ( nb teachers ) ) + ' student clean votes lap ' ) + str ( FLAGS . lap scale ) ) + '.npy' ) filepath labels = ( ( ( ( ( ( ( FLAGS . data dir + '/' ) + str ( dataset ) ) + ' ' ) + str ( nb teachers ) ) + ' teachers labels lap ' ) + str ( FLAGS . lap scale ) ) + '.npy' ) with tf . gfile .  Open  ( filepath , mode = 'w' ) as file obj : np . save ( file obj , clean votes ) with tf . gfile .  Open  ( filepath labels , mode = 'w' ) as file obj : np . save ( file obj , labels for dump ) ac ag labels = metrics . accuracy ( stdnt labels , test labels [ : FLAGS . stdnt share ] ) print ( ( ' Accuracy   of  the  aggregated  labels:  ' + str ( ac ag labels ) ) ) stdnt test data = test data [ FLAGS . stdnt share : ] stdnt test labels = test labels [ FLAGS . stdnt share : ] if save : filepath = ( ( ( ( ( ( ( FLAGS . data dir + '/' ) + str ( dataset ) ) + ' ' ) + str ( nb teachers ) ) + ' student labels lap ' ) + str ( FLAGS . lap scale ) ) + '.npy' ) with tf . gfile .  Open  ( filepath , mode = 'w' ) as file obj : np . save ( file obj , stdnt labels ) return ( stdnt data , stdnt labels , stdnt test data , stdnt test labels ) 
def staff ( ) : return s3 rest controller ( ) 
def req job reset ( r , ** attr ) : if r . interactive : if ( r . component and ( r . component . alias == 'job' ) ) : job id = r . component id if job id : S3 Task  . reset ( job id ) current . session . confirmation = current . T ( ' Job   reactivated' ) r . component id =  None  redirect ( r . url ( method = '' ) ) 
@ verbose def spatio temporal cluster 1samp test ( X , threshold =  None  , n permutations = 1024 , tail = 0 , stat fun = ttest 1samp no p , connectivity =  None  , verbose =  None  , n jobs = 1 , seed =  None  , max step = 1 , spatial exclude =  None  , step down p = 0 , t power = 1 , out type = 'indices' , check disjoint =  False  , buffer size = 1000 ) : ( n samples , n times , n vertices ) = X . shape if ( spatial exclude is not  None  ) : exclude =  st mask from s inds ( n times , n vertices , spatial exclude ,  True  ) else : exclude =  None  out = permutation cluster 1samp test ( X , threshold = threshold , stat fun = stat fun , tail = tail , n permutations = n permutations , connectivity = connectivity , n jobs = n jobs , seed = seed , max step = max step , exclude = exclude , step down p = step down p , t power = t power , out type = out type , check disjoint = check disjoint , buffer size = buffer size ) return out 
def  parse igd profile ( profile xml ) : try : dom = parse String  ( profile xml ) except  Expat  Error  as e : raise IGD Error  ( ' Unable   to  parse  IGD  reply:  {0}  \n\n\n  {1}' . format ( profile xml , e ) ) service types = dom . get Elements  By  Tag  Name  ( 'service Type ' ) for service in service types : if ( (  get first child data ( service ) . find ( 'WANIP Connection ' ) > 0 ) or (  get first child data ( service ) . find ( 'WANPPP Connection ' ) > 0 ) ) : try : control url =  get first child data ( service . parent Node  . get Elements  By  Tag  Name  ( 'controlURL' ) [ 0 ] ) upnp schema =  get first child data ( service ) . split ( ':' ) [ ( - 2 ) ] return ( control url , upnp schema ) except  Index  Error  : pass raise IGD Error  ( ' Could   not  find  a  control  url  or  UPNP  schema  in  IGD  response.' ) 
def horse ( ) : with expected warnings ( [ ' Possible   precision  loss' , ' Possible   sign  loss' ] ) : return img as bool ( load ( 'horse.png' , as grey =  True  ) ) 
def  summary judment ( rec ) : if config [ 'import' ] [ 'quiet' ] : if ( rec ==  Recommendation  . strong ) : return importer . action . APPLY else : action = config [ 'import' ] [ 'quiet fallback' ] . as choice ( { 'skip' : importer . action . SKIP , 'asis' : importer . action . ASIS } ) elif ( rec ==  Recommendation  . none ) : action = config [ 'import' ] [ 'none rec action' ] . as choice ( { 'skip' : importer . action . SKIP , 'asis' : importer . action . ASIS , 'ask' :  None  } ) else : return  None  if ( action == importer . action . SKIP ) : print  ( ' Skipping .' ) elif ( action == importer . action . ASIS ) : print  ( ' Importing   as-is.' ) return action 
@ handle response format @ treeio login required @ module admin required ( ) def page edit ( request , page id , response format = 'html' ) : page = get object or 404 (  Page  , pk = page id ) if request . POST : form =  Page  Form  ( request . POST , instance = page ) if form . is valid ( ) : page = form . save ( ) return  Http  Response  Redirect  ( reverse ( 'core admin page view' , args = [ page . id ] ) ) else : form =  Page  Form  ( instance = page ) return render to response ( 'core/administration/page edit' , { 'page' : page , 'form' : form } , context instance =  Request  Context  ( request ) , response format = response format ) 
@ pytest . mark . parametrize ( 'lang' , settings . AMO LANGUAGES ) def test get locale from lang ( lang ) : locale = get locale from lang ( lang ) assert isinstance ( locale ,  Locale  ) assert ( locale . language == lang [ : 2 ] ) separator = filter (  None  , [ ( sep if ( sep in lang ) else  None  ) for sep in ( '-' , ' ' ) ] ) if separator : territory = lang . split ( separator [ 0 ] ) [ 1 ] assert ( locale . territory == territory ) 
def conv Nd  ( input , kernel , pad , stride = 1 , n =  None  ) : if ( n is  None  ) : n = ( input . ndim - 2 ) if ( pad not in [ 'valid' , 'same' , 'full' ] ) : pad = as tuple ( pad , n , int ) input = np . pad ( input , [ ( p , p ) for p in ( ( 0 , 0 ) + pad ) ] , mode = 'constant' ) pad = 'valid' output = np . zeros ( ( ( input . shape [ 0 ] , kernel . shape [ 0 ] ) + tuple ( ( ( ( i + k ) - 1 ) for ( i , k ) in zip ( input . shape [ 2 : ] , kernel . shape [ 2 : ] ) ) ) ) ) if ( n == 1 ) : for i in range ( kernel . shape [ 2 ] ) : f = kernel [ : , : , i : ( i + 1 ) ] c = ( input [ : , np . newaxis ] * f ) . sum ( axis = 2 ) output [ : , : , i : ( i + input . shape [ 2 ] ) ] += c elif ( n == 2 ) : for i in range ( kernel . shape [ 2 ] ) : for j in range ( kernel . shape [ 3 ] ) : f = kernel [ : , : , i : ( i + 1 ) , j : ( j + 1 ) ] c = ( input [ : , np . newaxis ] * f ) . sum ( axis = 2 ) output [ : , : , i : ( i + input . shape [ 2 ] ) , j : ( j + input . shape [ 3 ] ) ] += c elif ( n == 3 ) : for i in range ( kernel . shape [ 2 ] ) : for j in range ( kernel . shape [ 3 ] ) : for k in range ( kernel . shape [ 4 ] ) : f = kernel [ : , : , i : ( i + 1 ) , j : ( j + 1 ) , k : ( k + 1 ) ] c = ( input [ : , np . newaxis ] * f ) . sum ( axis = 2 ) output [ : , : , i : ( i + input . shape [ 2 ] ) , j : ( j + input . shape [ 3 ] ) , k : ( k + input . shape [ 4 ] ) ] += c else : raise  Not  Implemented  Error  ( 'conv Nd ()  only  supports  n  in  (1,  2,  3)' ) if ( pad == 'valid' ) : trim = tuple ( ( ( k - 1 ) for k in kernel . shape [ 2 : ] ) ) slices = [ slice (  None  ) , slice (  None  ) ] slices += [ slice ( t , ( ( - t ) or  None  ) ) for t in trim ] output = output [ slices ] elif ( pad == 'same' ) : shift = tuple ( ( ( ( k - 1 ) // 2 ) for k in kernel . shape [ 2 : ] ) ) slices = [ slice (  None  ) , slice (  None  ) ] slices += [ slice ( s , ( s + i ) ) for ( s , i ) in zip ( shift , input . shape [ 2 : ] ) ] output = output [ slices ] stride = as tuple ( stride , n , int ) if any ( ( ( s > 1 ) for s in stride ) ) : slices = [ slice (  None  ) , slice (  None  ) ] slices += [ slice (  None  ,  None  , s ) for s in stride ] output = output [ slices ] return output 
def versions from parentdir ( parentdir prefix , root , verbose ) : rootdirs = [ ] for i in range ( 3 ) : dirname = os . path . basename ( root ) if dirname . startswith ( parentdir prefix ) : return { 'version' : dirname [ len ( parentdir prefix ) : ] , 'full-revisionid' :  None  , 'dirty' :  False  , 'error' :  None  , 'date' :  None  } else : rootdirs . append ( root ) root = os . path . dirname ( root ) if verbose : print ( ( ' Tried   directories  %s  but  none  started  with  prefix  %s' % ( str ( rootdirs ) , parentdir prefix ) ) ) raise  Not  This  Method  ( "rootdir  doesn't  start  with  parentdir prefix" ) 
def  Property  Value  To  Key  Value  ( prop value ) : if ( not isinstance ( prop value , entity pb .  Property  Value  ) ) : raise datastore errors .  Bad  Argument  Error  ( ( 'prop value  arg  expected  to  be  entity pb. Property  Value   (%r)' % ( prop value , ) ) ) if prop value . has stringvalue ( ) : return ( entity pb .  Property  Value  . kstring Value  , prop value . stringvalue ( ) ) if prop value . has int64value ( ) : return ( entity pb .  Property  Value  . kint64 Value  , prop value . int64value ( ) ) if prop value . has booleanvalue ( ) : return ( entity pb .  Property  Value  . kboolean Value  , prop value . booleanvalue ( ) ) if prop value . has doublevalue ( ) : encoder = sortable pb encoder .  Encoder  ( ) encoder . put Double  ( prop value . doublevalue ( ) ) return ( entity pb .  Property  Value  . kdouble Value  , tuple ( encoder . buf ) ) if prop value . has pointvalue ( ) : return ( entity pb .  Property  Value  . k Point  Value  Group  , prop value . pointvalue ( ) . x ( ) , prop value . pointvalue ( ) . y ( ) ) if prop value . has referencevalue ( ) : return  Reference  To  Key  Value  ( prop value . referencevalue ( ) ) if prop value . has uservalue ( ) : result = [ ] uservalue = prop value . uservalue ( ) if uservalue . has email ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valueemail  , uservalue . email ( ) ) ) if uservalue . has auth domain ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valueauth  domain , uservalue . auth domain ( ) ) ) if uservalue . has nickname ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valuenickname  , uservalue . nickname ( ) ) ) if uservalue . has gaiaid ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valuegaiaid  , uservalue . gaiaid ( ) ) ) if uservalue . has obfuscated gaiaid ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valueobfuscated  gaiaid , uservalue . obfuscated gaiaid ( ) ) ) if uservalue . has federated identity ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valuefederated  identity , uservalue . federated identity ( ) ) ) if uservalue . has federated provider ( ) : result . append ( ( entity pb .  Property  Value  . k User  Valuefederated  provider , uservalue . federated provider ( ) ) ) result . sort ( ) return ( entity pb .  Property  Value  . k User  Value  Group  , tuple ( result ) ) return ( ) 
def test feature max length on scenario outline keys ( ) : feature1 =  Feature  . from string ( FEATURE8 ) feature2 =  Feature  . from string ( FEATURE9 ) assert equals ( feature1 . max length , 68 ) assert equals ( feature2 . max length , 68 ) 
def compress tokens ( tokens ) : result = [ tokens [ 0 ] ] for tok in tokens [ 1 : ] : if ( ( not result [ ( - 1 ) ] . post tags ) and ( not tok . pre tags ) and ( result [ ( - 1 ) ] . annotation == tok . annotation ) ) : compress merge back ( result , tok ) else : result . append ( tok ) return result 
def DA DE Seq 2 ( input path , out path , mapping fp , mapping category , subcategory 1 , subcategory 2 , DE Seq 2 diagnostic plots ) : tmp bt = load table ( input path ) ( tmp pmf ,   ) = parse mapping file to dict ( mapping fp ) check mapping file category ( tmp bt , mapping fp , mapping category , subcategory 1 , subcategory 2 ) tmp bt . add metadata ( tmp pmf , 'sample' ) ( base fname , ext ) = splitext ( out path ) outfile diagnostic = join ( ( base fname + ' diagnostic plots.pdf' ) ) with tempfile .  Named  Temporary  File  ( dir = get qiime temp dir ( ) , prefix = 'QIIME-differential-abundance-temp-table-' , suffix = '.biom' ) as temp fh : temp fh . write ( tmp bt . to json ( 'forR' ) ) temp fh . flush ( ) run DE Seq 2 ( temp fh . name , out path , mapping category , subcategory 1 , subcategory 2 , DE Seq 2 diagnostic plots , outfile diagnostic ) 
def  get comments ( group tasks ) : comments = { } for ( status , human ) in  COMMENTS : num tasks =  get number of tasks for ( status , group tasks ) if num tasks : space = ( '        ' if ( status in  PENDING SUB STATUSES ) else '' ) comments [ status ] = '{space}*  {num tasks}  {human}:\n' . format ( space = space , num tasks = num tasks , human = human ) return comments 
def idz findrank ( eps , m , n , matveca ) : ( k , ra , ier ) =  id . idz findrank ( eps , m , n , matveca ) if ier : raise  RETCODE ERROR return k 
def  get shift anchor re ( sh anc , sh nuc , shift val , aa2re , anchor len , shift id pos ) : import re shift id = [ chr ( i ) for i in range ( 97 , 107 ) ] if ( 0 < shift val < ( ( 3 * anchor len ) - 2 ) ) : for j in range ( len ( sh anc ) ) : qcodon = '^' for ( k , aa ) in enumerate ( sh anc ) : if ( k == j ) : qcodon += ( ( ( aa2re [ aa ] + '(?P<' ) + shift id [ shift id pos ] ) + '>..*)' ) else : qcodon += aa2re [ aa ] qcodon += '$' match = re . search ( qcodon , sh nuc ) if match : qcodon = qcodon . replace ( '^' , '' ) . replace ( '$' , '' ) shift id pos += 1 return ( qcodon , shift id pos ) if ( not match ) : return ( ( - 1 ) , shift id pos ) elif ( shift val in ( ( ( 3 * anchor len ) - 1 ) , ( ( 3 * anchor len ) - 2 ) ) ) : shift val = ( ( 3 * anchor len ) - shift val ) for j in range ( 1 , len ( sh anc ) ) : qcodon = '^' for ( k , aa ) in enumerate ( sh anc ) : if ( k == ( j - 1 ) ) : pass elif ( k == j ) : qcodon +=  merge aa2re ( sh anc [ ( j - 1 ) ] , sh anc [ j ] , shift val , aa2re , shift id [ shift id pos ] . upper ( ) ) else : qcodon += aa2re [ aa ] qcodon += '$' match = re . search ( qcodon , sh nuc ) if match : qcodon = qcodon . replace ( '^' , '' ) . replace ( '$' , '' ) shift id pos += 1 return ( qcodon , shift id pos ) if ( not match ) : return ( ( - 1 ) , shift id pos ) 
def archive as tarball ( source dir , dest dir , tarball name =  None  , compression = 'bz2' , verbose =  True  ) : tarball name = get archive tarball name ( source dir , tarball name , compression ) if ( not os . path . isabs ( tarball name ) ) : tarball path = os . path . join ( dest dir , tarball name ) else : tarball path = tarball name if verbose : logging . debug ( ( ' Archiving   %s  as  %s' % ( source dir , tarball path ) ) ) os . chdir ( os . path . dirname ( source dir ) ) tarball = tarfile .  Tar  File  ( name = tarball path , mode = 'w' ) tarball = tarball . open ( name = tarball path , mode = ( 'w:%s' % compression ) ) tarball . add ( os . path . basename ( source dir ) ) tarball . close ( ) 
def maxzerodown ( x ) : x = np . asarray ( x ) cond1 = ( x [ : ( - 1 ) ] > 0 ) cond2 = ( x [ 1 : ] < 0 ) allzeros = ( np . nonzero ( ( ( cond1 & cond2 ) | ( x [ 1 : ] == 0 ) ) ) [ 0 ] + 1 ) if ( x [ ( - 1 ) ] <= 0 ) : maxz = max ( allzeros ) else : maxz =  None  return ( maxz , allzeros ) 
def  get epochs delayed ssp ( ) : raw = read raw fif ( raw fname ) events = read events ( event name ) picks =  get picks ( raw ) reject = dict ( mag = 4e-12 ) epochs delayed ssp =  Epochs  ( raw , events [ : 10 ] , event id , tmin , tmax , picks = picks , proj = 'delayed' , reject = reject ) return epochs delayed ssp 
def stats ( syslog ng sbin dir =  None  ) : try : ret =  run command in extended path ( syslog ng sbin dir , 'syslog-ng-ctl' , ( 'stats' , ) ) except  Command  Execution  Error  as err : return  format return data ( retcode = ( - 1 ) , stderr = str ( err ) ) return  format return data ( ret [ 'retcode' ] , ret . get ( 'stdout' ) , ret . get ( 'stderr' ) ) 
def send file ( filepath or fp , mimetype =  None  , as attachment =  False  , filename =  None  , mtime =  None  , add etags =  True  , cache timeout = STATIC CACHE , conditional =  True  ) : if isinstance ( filepath or fp , ( str , unicode ) ) : if ( not filename ) : filename = os . path . basename ( filepath or fp ) file = open ( filepath or fp , 'rb' ) if ( not mtime ) : mtime = os . path . getmtime ( filepath or fp ) else : file = filepath or fp if ( not filename ) : filename = getattr ( file , 'name' ,  None  ) file . seek ( 0 , 2 ) size = file . tell ( ) file . seek ( 0 ) if ( ( mimetype is  None  ) and filename ) : mimetype = mimetypes . guess type ( filename ) [ 0 ] if ( mimetype is  None  ) : mimetype = 'application/octet-stream' headers = werkzeug . datastructures .  Headers  ( ) if as attachment : if ( filename is  None  ) : raise  Type  Error  ( 'filename  unavailable,  required  for  sending  as  attachment' ) headers . add ( ' Content - Disposition ' , 'attachment' , filename = filename ) headers [ ' Content - Length ' ] = size data = wrap file ( request . httprequest . environ , file ) rv =  Response  ( data , mimetype = mimetype , headers = headers , direct passthrough =  True  ) if isinstance ( mtime , str ) : try : server format = odoo . tools . misc . DEFAULT SERVER DATETIME FORMAT mtime = datetime . datetime . strptime ( mtime . split ( '.' ) [ 0 ] , server format ) except  Exception  : mtime =  None  if ( mtime is not  None  ) : rv . last modified = mtime rv . cache control . public =  True  if cache timeout : rv . cache control . max age = cache timeout rv . expires = int ( ( time . time ( ) + cache timeout ) ) if ( add etags and filename and mtime ) : rv . set etag ( ( 'odoo-%s-%s-%s' % ( mtime , size , ( adler32 ( ( filename . encode ( 'utf-8' ) if isinstance ( filename , unicode ) else filename ) ) & 4294967295 ) ) ) ) if conditional : rv = rv . make conditional ( request . httprequest ) if ( rv . status code == 304 ) : rv . headers . pop ( 'x-sendfile' ,  None  ) return rv 
def die ( message ) : print ( message , file = sys . stderr ) sys . exit ( 1 ) 
def  Safe  Format  Message  ( event Log  Record  , log Type  =  None  ) : if ( log Type  is  None  ) : log Type  = ' Application ' try : return  Format  Message  ( event Log  Record  , log Type  ) except win32api . error : if ( event Log  Record  .  String  Inserts  is  None  ) : desc = '' else : desc = u',  ' . join ( event Log  Record  .  String  Inserts  ) return ( u'< The   description  for   Event   ID  (  %d  )  in   Source   (  %r  )  could  not  be  found.   It   contains  the  following  insertion  string(s):%r.>' % ( winerror . HRESULT CODE ( event Log  Record  .  Event ID ) , event Log  Record  .  Source  Name  , desc ) ) 
def whitesource ( registry , xml parent , data ) : whitesource = XML .  Sub  Element  ( xml parent , 'org.whitesource.jenkins. White  Source  Publisher ' ) whitesource . set ( 'plugin' , 'whitesource' ) policies = [ 'global' , 'enable' , 'disable' ] mappings = [ ( 'policies' , 'job Check  Policies ' , 'global' , policies ) , ( 'override-token' , 'job Api  Token ' , '' ) , ( 'product-token' , 'product' , '' ) , ( 'version' , 'product Version ' , '' ) , ( 'project-token' , 'project Token ' , '' ) , ( 'requester-email' , 'requester Email ' , '' ) ] helpers . convert mapping to xml ( whitesource , data , mappings , fail required =  True  ) XML .  Sub  Element  ( whitesource , 'lib Includes ' ) . text = '  ' . join ( data . get ( 'includes' , [ ] ) ) XML .  Sub  Element  ( whitesource , 'lib Excludes ' ) . text = '  ' . join ( data . get ( 'excludes' , [ ] ) ) XML .  Sub  Element  ( whitesource , 'ignore Pom  Modules ' ) . text = 'false' 
def  Drop  Privileges  ( ) : if config lib . CONFIG [ ' Server .username' ] : try : os . setuid ( pwd . getpwnam ( config lib . CONFIG [ ' Server .username' ] ) . pw uid ) except (  Key  Error  , OS Error  ) : logging . exception ( ' Unable   to  switch  to  user  %s' , config lib . CONFIG [ ' Server .username' ] ) raise 
def packbits ( myarray ) : if ( myarray . dtype . kind not in 'biu' ) : raise  Type  Error  ( ' Expected   an  input  array  of  integer  or  boolean  data  type' ) myarray = myarray . ravel ( ) packed size = ( ( myarray . size + 7 ) // 8 ) packed = cupy . zeros ( ( packed size , ) , dtype = cupy . uint8 ) cupy .  Elementwise  Kernel  ( 'raw  T  myarray,  raw  int32  myarray size' , 'uint8  packed' , 'for  (int  j  =  0;  j  <  8;  ++j)  {\n                        int  k  =  i  *  8  +  j;\n                        int  bit  =  k  <  myarray size  &&  myarray[k]  !=  0;\n                        packed  |=  bit  <<  (7  -  j);\n                }' , 'packbits kernel' ) ( myarray , myarray . size , packed ) return packed 
def convert  Link  Property  ( model , prop , kwargs ) : kwargs [ 'validators' ] . append ( validators . url ( ) ) return get  Text  Field  ( kwargs ) 
def test wavefront ( ) : fname mesh = load data file ( 'orig/triceratops.obj.gz' ) fname out = op . join ( temp dir , 'temp.obj' ) mesh1 = read mesh ( fname mesh ) assert raises ( IO Error  , read mesh , 'foo.obj' ) assert raises (  Value  Error  , read mesh , op . abspath (   file   ) ) assert raises (  Value  Error  , write mesh , fname out , format = 'foo' , * mesh1 ) write mesh ( fname out , mesh1 [ 0 ] , mesh1 [ 1 ] , mesh1 [ 2 ] , mesh1 [ 3 ] ) assert raises ( IO Error  , write mesh , fname out , * mesh1 ) write mesh ( fname out , overwrite =  True  , * mesh1 ) mesh2 = read mesh ( fname out ) assert equal ( len ( mesh1 ) , len ( mesh2 ) ) for ( m1 , m2 ) in zip ( mesh1 , mesh2 ) : if ( m1 is  None  ) : assert equal ( m2 ,  None  ) else : assert allclose ( m1 , m2 , rtol = 1e-05 ) assert allclose ( mesh1 [ 2 ] ,  slow calculate normals ( mesh1 [ 0 ] , mesh1 [ 1 ] ) , rtol = 1e-07 , atol = 1e-07 ) 
def parse requirements ( strs ) : lines = iter ( yield lines ( strs ) ) def scan list ( ITEM , TERMINATOR , line , p , groups , item name ) : items = [ ] while ( not TERMINATOR ( line , p ) ) : if CONTINUE ( line , p ) : try : line = next ( lines ) p = 0 except  Stop  Iteration  : msg = '\\  must  not  appear  on  the  last  nonblank  line' raise  Requirement  Parse  Error  ( msg ) match = ITEM ( line , p ) if ( not match ) : msg = ( ( ' Expected   ' + item name ) + '  in' ) raise  Requirement  Parse  Error  ( msg , line , 'at' , line [ p : ] ) items . append ( match . group ( * groups ) ) p = match . end ( ) match = COMMA ( line , p ) if match : p = match . end ( ) elif ( not TERMINATOR ( line , p ) ) : msg = " Expected   ','  or  end-of-list  in" raise  Requirement  Parse  Error  ( msg , line , 'at' , line [ p : ] ) match = TERMINATOR ( line , p ) if match : p = match . end ( ) return ( line , p , items ) for line in lines : match = DISTRO ( line ) if ( not match ) : raise  Requirement  Parse  Error  ( ' Missing   distribution  spec' , line ) project name = match . group ( 1 ) p = match . end ( ) extras = [ ] match = OBRACKET ( line , p ) if match : p = match . end ( ) ( line , p , extras ) = scan list ( DISTRO , CBRACKET , line , p , ( 1 , ) , "'extra'  name" ) ( line , p , specs ) = scan list ( VERSION , LINE END , line , p , ( 1 , 2 ) , 'version  spec' ) specs = [ ( op , val ) for ( op , val ) in specs ] ( yield  Requirement  ( project name , specs , extras ) ) 
def file open ( * args , ** kwargs ) : return file ( * args , ** kwargs ) 
def creategroup ( name , primary cluster id , replication group description , wait =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : ret = { 'name' : name , 'result' :  None  , 'comment' : '' , 'changes' : { } } is present =   salt   [ 'boto elasticache.group exists' ] ( name , region , key , keyid , profile ) if ( not is present ) : if   opts   [ 'test' ] : ret [ 'comment' ] = ' Replication   {0}  is  set  to  be  created.' . format ( name ) ret [ 'result' ] =  None  created =   salt   [ 'boto elasticache.create replication group' ] ( name , primary cluster id , replication group description , wait , region , key , keyid , profile ) if created : config =   salt   [ 'boto elasticache.describe replication group' ] ( name , region , key , keyid , profile ) ret [ 'changes' ] [ 'old' ] =  None  ret [ 'changes' ] [ 'new' ] = config ret [ 'result' ] =  True  else : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  create  {0}  replication  group.' . format ( name ) else : ret [ 'comment' ] = '{0}  replication  group  exists  .' . format ( name ) ret [ 'result' ] =  True  return ret 
def matching Dict  ( d , selection , require existence =  False  ) : for ( k , v ) in list ( selection . items ( ) ) : if ( k in d ) : if isinstance ( v , list ) : if ( d [ k ] not in v ) : return  False  elif ( d [ k ] != v ) : return  False  elif require existence : return  False  return  True  
def list route ( methods =  None  , ** kwargs ) : methods = ( [ u'get' ] if ( methods is  None  ) else methods ) def decorator ( func ) : func . bind to methods = methods func . detail =  False  func . kwargs = kwargs return func return decorator 
def generic group update db ( context , group , host , cluster name ) : group . update ( { 'host' : host , 'updated at' : timeutils . utcnow ( ) , 'cluster name' : cluster name } ) group . save ( ) return group 
def   Get  Chart  Factory  ( chart class , display class ) : def  Inner  ( * args , ** kwargs ) : chart = chart class ( * args , ** kwargs ) chart . display = display class ( chart ) return chart return  Inner  
def load image ( in image ) : img =  Image  . open ( in image ) return img 
def decorate depth ( tree ) : for node in tree . levelorder ( include self =  True  ) : if ( node .  Parent  is  None  ) : node .  Score  = 0 else : node .  Score  = ( node .  Parent  .  Score  + 1 ) return tree 
def subplots adjust ( * args , ** kwargs ) : fig = gcf ( ) fig . subplots adjust ( * args , ** kwargs ) 
def calculate sha256 ( body , as hex =  False  ) : checksum = hashlib . sha256 ( ) for chunk in iter ( ( lambda : body . read ( ( 1024 * 1024 ) ) ) , '' ) : checksum . update ( chunk ) if as hex : return checksum . hexdigest ( ) else : return checksum . digest ( ) 
def changed files ( config = 'root' , num pre =  None  , num post =  None  ) : return status ( config , num pre , num post ) . keys ( ) 
def  get index ( passed id =  None  ) : if passed id : index matcher = re . search ( '.*?/?(\\d+)$' , passed id ) if index matcher : return int ( index matcher . group ( 1 ) ) return 0 
def  get user usage data ( users , groups =  None  , period start =  None  , period end =  None  , group id =  None  ) : groups = ( groups or set ( [ user . group for user in users ] ) ) user data =  Ordered  Dict  ( ) group data =  Ordered  Dict  ( ) exercise logs =  Exercise  Log  . objects . filter ( user  in = users ) video logs =  Video  Log  . objects . filter ( user  in = users , total seconds watched  gt = 0 ) login logs =  User  Log  Summary  . objects . filter ( user  in = users ) login logs = login logs . filter ( total seconds  gte = 0 ) if period start : exercise logs = exercise logs . filter ( completion timestamp  gte = period start ) video logs = video logs . filter ( latest activity timestamp  gte = period start ) if period end : period end = dateutil . parser . parse ( period end ) period end = ( period end + dateutil . relativedelta . relativedelta ( days = ( + 1 ) , microseconds = ( - 1 ) ) ) exercise logs = exercise logs . filter ( latest activity timestamp  lte = period end ) video logs = video logs . filter ( completion timestamp  lte = period end ) if ( period start and period end ) : exercise logs = exercise logs . filter ( ( Q ( latest activity timestamp  gte = period start ) & Q ( latest activity timestamp  lte = period end ) ) ) q1 = ( ( Q ( completion timestamp  isnull =  False  ) & Q ( completion timestamp  gte = period start ) ) & Q ( completion timestamp  lte = period end ) ) video logs = video logs . filter ( q1 ) login q1 = ( ( ( Q ( start datetime  gte = period start ) & Q ( start datetime  lte = period end ) ) & Q ( end datetime  gte = period start ) ) & Q ( end datetime  lte = period end ) ) login logs = login logs . filter ( login q1 ) exercise logs = list ( exercise logs . values ( 'exercise id' , 'user  pk' , 'streak progress' , 'complete' ) ) video logs = list ( video logs . values ( 'video id' , 'user  pk' ) ) login logs = list ( login logs . values ( 'activity type' , 'total seconds' , 'user  pk' , 'count' ) ) for user in users : user data [ user . pk ] =  Ordered  Dict  ( ) user data [ user . pk ] [ 'id' ] = user . pk user data [ user . pk ] [ 'first name' ] = user . first name user data [ user . pk ] [ 'last name' ] = user . last name user data [ user . pk ] [ 'username' ] = user . username user data [ user . pk ] [ 'group' ] = user . group user data [ user . pk ] [ 'total report views' ] = 0 user data [ user . pk ] [ 'total logins' ] = 0 user data [ user . pk ] [ 'total hours' ] = 0 user data [ user . pk ] [ 'total exercises' ] = 0 user data [ user . pk ] [ 'exercises completed' ] = 0 user data [ user . pk ] [ 'pct mastery' ] = 0.0 user data [ user . pk ] [ 'exercises mastered' ] = [ ] user data [ user . pk ] [ 'total videos' ] = 0 user data [ user . pk ] [ 'videos watched' ] = [ ] for elog in exercise logs : user data [ elog [ 'user  pk' ] ] [ 'pct mastery' ] += elog [ 'streak progress' ] user data [ elog [ 'user  pk' ] ] [ 'total exercises' ] += 1 if elog [ 'complete' ] : user data [ elog [ 'user  pk' ] ] [ 'exercises completed' ] += 1 user data [ elog [ 'user  pk' ] ] [ 'exercises mastered' ] . append ( elog [ 'exercise id' ] ) for vlog in video logs : user data [ vlog [ 'user  pk' ] ] [ 'total videos' ] += 1 user data [ vlog [ 'user  pk' ] ] [ 'videos watched' ] . append ( vlog [ 'video id' ] ) for llog in login logs : if ( llog [ 'activity type' ] ==  User  Log  . get activity int ( 'coachreport' ) ) : user data [ llog [ 'user  pk' ] ] [ 'total report views' ] += 1 elif ( llog [ 'activity type' ] ==  User  Log  . get activity int ( 'login' ) ) : user data [ llog [ 'user  pk' ] ] [ 'total hours' ] += ( llog [ 'total seconds' ] / 3600.0 ) user data [ llog [ 'user  pk' ] ] [ 'total logins' ] += llog [ 'count' ] for group in ( list ( groups ) + ( [  None  ] * ( ( group id ==  None  ) or ( group id == UNGROUPED ) ) ) ) : group pk = getattr ( group , 'pk' ,  None  ) group name = getattr ( group , 'name' ,   ( UNGROUPED ) ) group title = getattr ( group , 'title' ,   ( UNGROUPED ) ) group data [ group pk ] = { 'id' : group pk , 'name' : group name , 'title' : group title , 'total logins' : 0 , 'total hours' : 0 , 'total users' : 0 , 'total videos' : 0 , 'total exercises' : 0 , 'total exercises completed' : 0 , 'pct mastery' : 0 } for user in users : user data [ user . pk ] [ 'pct mastery' ] = ( user data [ user . pk ] [ 'pct mastery' ] / ( user data [ user . pk ] [ 'total exercises' ] or 1 ) ) group pk = getattr ( user . group , 'pk' ,  None  ) if ( group pk not in group data ) : logging . error ( ( ' User   %s  still  in  nonexistent  group  %s!' % ( user . id , group pk ) ) ) continue group data [ group pk ] [ 'total users' ] += 1 group data [ group pk ] [ 'total logins' ] += user data [ user . pk ] [ 'total logins' ] group data [ group pk ] [ 'total hours' ] += user data [ user . pk ] [ 'total hours' ] group data [ group pk ] [ 'total videos' ] += user data [ user . pk ] [ 'total videos' ] group data [ group pk ] [ 'total exercises' ] += user data [ user . pk ] [ 'total exercises' ] group data [ group pk ] [ 'total exercises completed' ] += user data [ user . pk ] [ 'exercises completed' ] total mastery so far = ( ( group data [ group pk ] [ 'pct mastery' ] * ( group data [ group pk ] [ 'total users' ] - 1 ) ) + user data [ user . pk ] [ 'pct mastery' ] ) group data [ group pk ] [ 'pct mastery' ] = ( total mastery so far / group data [ group pk ] [ 'total users' ] ) if ( ( len ( group data ) == 1 ) and (  None  in group data ) ) : if ( not group data [  None  ] [ 'total users' ] ) : del group data [  None  ] return ( user data , group data ) 
def test predict on toy problem ( ) : clf1 =  Logistic  Regression  ( random state = 123 ) clf2 =  Random  Forest  Classifier  ( random state = 123 ) clf3 =  Gaussian NB ( ) X = np . array ( [ [ ( - 1.1 ) , ( - 1.5 ) ] , [ ( - 1.2 ) , ( - 1.4 ) ] , [ ( - 3.4 ) , ( - 2.2 ) ] , [ 1.1 , 1.2 ] , [ 2.1 , 1.4 ] , [ 3.1 , 2.3 ] ] ) y = np . array ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) assert equal ( all ( clf1 . fit ( X , y ) . predict ( X ) ) , all ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) ) assert equal ( all ( clf2 . fit ( X , y ) . predict ( X ) ) , all ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) ) assert equal ( all ( clf3 . fit ( X , y ) . predict ( X ) ) , all ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) ) eclf =  Voting  Classifier  ( estimators = [ ( 'lr' , clf1 ) , ( 'rf' , clf2 ) , ( 'gnb' , clf3 ) ] , voting = 'hard' , weights = [ 1 , 1 , 1 ] ) assert equal ( all ( eclf . fit ( X , y ) . predict ( X ) ) , all ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) ) eclf =  Voting  Classifier  ( estimators = [ ( 'lr' , clf1 ) , ( 'rf' , clf2 ) , ( 'gnb' , clf3 ) ] , voting = 'soft' , weights = [ 1 , 1 , 1 ] ) assert equal ( all ( eclf . fit ( X , y ) . predict ( X ) ) , all ( [ 1 , 1 , 1 , 2 , 2 , 2 ] ) ) 
def load template ( name ) : full fname = os . path . join ( os . path . dirname (   file   ) , u'script templates' , name ) template file = open ( full fname ) template =  Template  ( template file . read ( ) ) template file . close ( ) return template 
def instance type extra specs get ( context , flavor id ) : return IMPL . instance type extra specs get ( context , flavor id ) 
def main ( ) : argument spec = dict ( src = dict ( ) , force = dict ( default =  False  , type = 'bool' ) , include defaults = dict ( default =  True  , type = 'bool' ) , backup = dict ( default =  False  , type = 'bool' ) , config = dict ( ) ) argument spec . update (  transitional argument spec ( ) ) mutually exclusive = [ ( 'config' , 'backup' ) , ( 'config' , 'force' ) ] module =  Local  Ansible  Module  ( argument spec = argument spec , mutually exclusive = mutually exclusive , supports check mode =  True  ) warnings = check args ( module ) result = dict ( changed =  False  , warnings = warnings ) candidate =  Network  Config  ( contents = module . params [ 'src' ] , indent = 1 ) result = { 'changed' :  False  } if module . params [ 'backup' ] : result [ '  backup  ' ] = get config ( module = module ) if ( not module . params [ 'force' ] ) : contents = get current config ( module ) configobj =  Network  Config  ( contents = contents , indent = 1 ) commands = candidate . difference ( configobj ) commands = dumps ( commands , 'commands' ) . split ( '\n' ) commands = [ str ( c ) . strip ( ) for c in commands if c ] else : commands = [ c . strip ( ) for c in str ( candidate ) . split ( '\n' ) ] if commands : if ( not module . check mode ) : load config ( module , commands ) result [ 'changed' ] =  True  result [ 'updates' ] = commands module . exit json ( ** result ) 
def  if modified since passes ( last modified , if modified since ) : return ( ( not last modified ) or ( last modified > if modified since ) ) 
def get input value ( page , css selector ) : page . wait for element presence ( css selector , ' Elements   matching  "{}"  selector  are  present' . format ( css selector ) ) return page . q ( css = css selector ) . attrs ( 'value' ) [ 0 ] 
def file list ( * packages ) : ret = file dict ( * packages ) files = [ ] for pkg files in six . itervalues ( ret [ 'files' ] ) : files . extend ( pkg files ) ret [ 'files' ] = files return ret 
def load ( fp , encoding =  None  , cls =  None  , object hook =  None  , parse float =  None  , parse int =  None  , parse constant =  None  , ** kw ) : return loads ( fp . read ( ) , encoding = encoding , cls = cls , object hook = object hook , parse float = parse float , parse int = parse int , parse constant = parse constant , ** kw ) 
def  track from response ( result , timeout ) : response = result [ 'response' ] status = response [ 'track' ] [ 'status' ] . lower ( ) if ( status == 'pending' ) : result =  wait for pending track ( response [ 'track' ] [ 'id' ] , timeout ) response = result [ 'response' ] status = response [ 'track' ] [ 'status' ] . lower ( ) if ( not ( status == 'complete' ) ) : track id = response [ 'track' ] [ 'id' ] if ( status == 'pending' ) : raise  Exception  ( ( "%s:  the  operation  didn't  complete  before  the  timeout  (%d  secs)" % ( track id , timeout ) ) ) else : raise  Exception  ( ( '%s:  there  was  an  error  analyzing  the  track,  status:  %s' % ( track id , status ) ) ) else : track properties = response [ 'track' ] identifier = track properties . pop ( 'id' ) md5 = track properties . pop ( 'md5' ,  None  ) track properties . update ( track properties . pop ( 'audio summary' ) ) return  Track  ( identifier , md5 , track properties ) 
def get help ( object name , path = ( ) , parent object names = ( ) , attribute =  None  ) : if ( object name in graph reference . ARRAYS ) : help string =  list help ( object name , path , parent object names ) elif attribute : help string =  dict attribute help ( object name , path , parent object names , attribute ) else : help string =  dict object help ( object name , path , parent object names ) return help string . expandtabs ( TAB SIZE ) 
def md5 key mangler ( key ) : return md5 ( key . encode ( 'ascii' ) ) . hexdigest ( ) 
def marching cubes ( volume , level =  None  , spacing = ( 1.0 , 1.0 , 1.0 ) , gradient direction = 'descent' , step size = 1 , allow degenerate =  True  , use classic =  False  ) : try : nout =  expected output args ( ) except  Exception  : nout = 0 if ( nout <= 2 ) : warn ( skimage deprecation ( '`marching cubes`  now  uses  a  better  and  faster  algorithm,  and  returns  four  instead  of  two  outputs  (see  docstring  for  details).   Backwards   compatibility  with  0.12  and  prior  is  available  with  `marching cubes classic`.' ) ) return marching cubes lewiner ( volume , level , spacing , gradient direction , step size , allow degenerate , use classic ) 
def  Valid  Headers  Rewriter  ( response ) : for ( key , value ) in response . headers . items ( ) : try : key . decode ( 'ascii' ) value . decode ( 'ascii' ) except  Unicode  Decode  Error  : del response . headers [ key ] 
def sqla listen ( * args ) : event . listen ( * args )  REGISTERED SQLA EVENTS . append ( args ) 
def get gdp contrib ( ) : rdint = vs . random ( ) request =  Request  ( ( vs . MACRO URL % ( vs . P TYPE [ 'http' ] , vs . DOMAINS [ 'sina' ] , rdint , vs . MACRO TYPE [ 0 ] , 6 , 60 , rdint ) ) ) text = urlopen ( request , timeout = 10 ) . read ( ) text = ( text . decode ( 'gbk' ) if ct . PY3 else text ) reg Sym  = re . compile ( '\\,count:(.*?)\\}' ) datastr = reg Sym  . findall ( text ) datastr = datastr [ 0 ] datastr = datastr . split ( 'data:' ) [ 1 ] datastr = datastr . replace ( '"' , '' ) . replace ( 'null' , '0' ) js = json . loads ( datastr ) df = pd .  Data  Frame  ( js , columns = vs . GDP CONTRIB COLS ) df [ ( df == 0 ) ] = np .  Na N return df 
def get default username ( check db =  True  ) : from django . contrib . auth . management . commands . createsuperuser import RE VALID USERNAME default username = get system username ( ) try : default username = unicodedata . normalize ( 'NFKD' , default username ) . encode ( 'ascii' , 'ignore' ) . replace ( '  ' , '' ) . lower ( ) except  Unicode  Decode  Error  : return '' if ( not RE VALID USERNAME . match ( default username ) ) : return '' if ( check db and default username ) : try :  User  . objects . get ( username = default username ) except  User  .  Does  Not  Exist  : pass else : return '' return default username 
def  get priceful ( request , item , quantity ) : if hasattr ( item , 'get price info' ) : if hasattr ( item , 'is variation parent' ) : if item . is variation parent ( ) : return item . get cheapest child price info ( request , quantity ) return item . get price info ( request , quantity = quantity ) if hasattr ( item , 'get total cost' ) : return item . get total cost ( request . basket ) assert isinstance ( item ,  Priceful  ) return item 
def email split ( text ) : if ( not text ) : return [ ] return [ addr [ 1 ] for addr in getaddresses ( [ text ] ) if addr [ 1 ] if ( '@' in addr [ 1 ] ) ] 
def check solr schema version ( schema file =  None  ) : if ( not is available ( ) ) : log . warn ( ' Problems   were  found  while  connecting  to  the  SOLR  server' ) return  False  if ( not schema file ) : ( solr url , solr user , solr password ) =  Solr  Settings  . get ( ) http auth =  None  if ( ( solr user is not  None  ) and ( solr password is not  None  ) ) : http auth = ( ( solr user + ':' ) + solr password ) http auth = ( ' Basic   ' + http auth . encode ( 'base64' ) . strip ( ) ) url = ( solr url . strip ( '/' ) + SOLR SCHEMA FILE OFFSET ) req = urllib2 .  Request  ( url = url ) if http auth : req . add header ( ' Authorization ' , http auth ) res = urllib2 . urlopen ( req ) else : url = ( 'file://%s' % schema file ) res = urllib2 . urlopen ( url ) tree = xml . dom . minidom . parse String  ( res . read ( ) ) version = tree . document Element  . get Attribute  ( 'version' ) if ( not len ( version ) ) : raise  Search  Error  ( ( ' Could   not  extract  version  info  from  the  SOLR  schema,  using  file:  \n%s' % url ) ) if ( not ( version in SUPPORTED SCHEMA VERSIONS ) ) : raise  Search  Error  ( ( 'SOLR  schema  version  not  supported:  %s.   Supported   versions  are  [%s]' % ( version , ',  ' . join ( SUPPORTED SCHEMA VERSIONS ) ) ) ) return  True  
def lombscargle scipy ( t , y , frequency , normalization = 'standard' , center data =  True  ) : try : from scipy import signal except  Import  Error  : raise  Import  Error  ( 'scipy  must  be  installed  to  use  lombscargle scipy' ) ( t , y ) = np . broadcast arrays ( t , y ) t = np . asarray ( t , dtype = float ) y = np . asarray ( y , dtype = float ) frequency = np . asarray ( frequency , dtype = float ) if ( t . ndim != 1 ) : raise  Value  Error  ( 't,  y,  dy  should  be  one  dimensional' ) if ( frequency . ndim != 1 ) : raise  Value  Error  ( 'frequency  should  be  one-dimensional' ) if center data : y = ( y - y . mean ( ) ) p = signal . lombscargle ( t , y , ( ( 2 * np . pi ) * frequency ) ) if ( normalization == 'psd' ) : pass elif ( normalization == 'standard' ) : p *= ( 2 / ( t . size * np . mean ( ( y ** 2 ) ) ) ) elif ( normalization == 'log' ) : p = ( - np . log ( ( 1 - ( ( 2 * p ) / ( t . size * np . mean ( ( y ** 2 ) ) ) ) ) ) ) elif ( normalization == 'model' ) : p /= ( ( ( 0.5 * t . size ) * np . mean ( ( y ** 2 ) ) ) - p ) else : raise  Value  Error  ( "normalization='{0}'  not  recognized" . format ( normalization ) ) return p 
def get Slice  Element Z ( slice Element  ) : id Value  = slice Element  . attribute Dictionary  [ 'id' ] . strip ( ) return float ( id Value  [ len ( 'z:' ) : ] . strip ( ) ) 
def  message pb from mapping ( message ) : return  Pubsub  Message  ( data =  to bytes ( message [ 'data' ] ) , attributes = message [ 'attributes' ] ) 
def compare generic iter ( make it , match ) : it = make it ( ) n = 0 for item in match : if ( not ( it [ n ] == item ) ) : raise  Assertion  Error  n += 1 try : it [ n ] except  Index  Error  : pass else : raise  Assertion  Error  ( ' Too   many  items  from    getitem  ' , it ) try : ( iter ,  Stop  Iteration  ) except  Name  Error  : pass else : it = make it ( ) if ( not ( iter ( it ) is it ) ) : raise  Assertion  Error  for item in match : if ( not ( next ( it ) == item ) ) : raise  Assertion  Error  try : next ( it ) except  Stop  Iteration  : pass else : raise  Assertion  Error  ( ' Too   many  items  from  .  next  ()' , it ) 
def ifequal ( parser , token ) : return do ifequal ( parser , token ,  False  ) 
def  parse file key certs ( certificate file , validate =  False  ) : while  True  : keycert content =  read until keywords ( 'dir-key-certification' , certificate file ) block end prefix = PGP BLOCK END . split ( '  ' , 1 ) [ 0 ] keycert content +=  read until keywords ( block end prefix , certificate file ,  True  ) if keycert content : ( yield stem . descriptor . networkstatus .  Key  Certificate  ( bytes . join ( '' , keycert content ) , validate = validate ) ) else : break 
def parse cookie ( value ) : if ( not value ) : return  None  return value 
def get gdp quarter ( ) : rdint = vs . random ( ) request =  Request  ( ( vs . MACRO URL % ( vs . P TYPE [ 'http' ] , vs . DOMAINS [ 'sina' ] , rdint , vs . MACRO TYPE [ 0 ] , 1 , 250 , rdint ) ) ) text = urlopen ( request , timeout = 10 ) . read ( ) text = ( text . decode ( 'gbk' ) if ct . PY3 else text ) reg Sym  = re . compile ( '\\,count:(.*?)\\}' ) datastr = reg Sym  . findall ( text ) datastr = datastr [ 0 ] datastr = datastr . split ( 'data:' ) [ 1 ] datastr = datastr . replace ( '"' , '' ) . replace ( 'null' , '0' ) js = json . loads ( datastr ) df = pd .  Data  Frame  ( js , columns = vs . GDP QUARTER COLS ) df [ 'quarter' ] = df [ 'quarter' ] . astype ( object ) df [ ( df == 0 ) ] = np .  Na N return df 
def post download ( project , filename , name =  None  , description = '' ) : if ( name is  None  ) : name = os . path . basename ( filename ) with open ( filename , 'rb' ) as f : filedata = f . read ( ) url = 'https://api.github.com/repos/{project}/downloads' . format ( project = project ) payload = json . dumps ( dict ( name = name , size = len ( filedata ) , description = description ) ) response = requests . post ( url , data = payload , headers = make auth header ( ) ) response . raise for status ( ) reply = json . loads ( response . content ) s3 url = reply [ 's3 url' ] fields = dict ( key = reply [ 'path' ] , acl = reply [ 'acl' ] , success action status = 201 ,  Filename  = reply [ 'name' ] , AWS Access  Key  Id  = reply [ 'accesskeyid' ] ,  Policy  = reply [ 'policy' ] ,  Signature  = reply [ 'signature' ] , file = ( reply [ 'name' ] , filedata ) ) fields [ ' Content - Type ' ] = reply [ 'mime type' ] ( data , content type ) = encode multipart formdata ( fields ) s3r = requests . post ( s3 url , data = data , headers = { ' Content - Type ' : content type } ) return s3r 
def  Create  Implicit  Matcher  ( config , module dict , root path , login url , create path adjuster =  Path  Adjuster  , create local dispatcher =  Local CGI Dispatcher  , create cgi dispatcher = CGI Dispatcher  , get blob storage = dev appserver blobstore .  Get  Blob  Storage  ) : url matcher = URL Matcher  ( ) path adjuster = create path adjuster ( root path ) def   Status  Checker  ( ) : '  A  path  for  the  application  manager  to  check  if  this  application  \n                server  is  up.\n        ' pass status dispatcher = create local dispatcher ( config , sys . modules , path adjuster ,   Status  Checker  ) url matcher .  Add URL ( '/ ah/health check' , status dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION REDIRECT ) def   Handle  Quit  ( ) : raise  Keyboard  Interrupt  quit dispatcher = create local dispatcher ( config , sys . modules , path adjuster ,   Handle  Quit  ) url matcher .  Add URL ( '/ ah/quit?' , quit dispatcher , '' ,  True  ,  True  , appinfo . AUTH FAIL ACTION REDIRECT ) login dispatcher = create local dispatcher ( config , sys . modules , path adjuster , dev appserver login . main ) url matcher .  Add URL ( login url , login dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION REDIRECT ) admin dispatcher = create cgi dispatcher ( config , module dict , root path , path adjuster ) url matcher .  Add URL ( '/ ah/admin(?:/.*)?' , admin dispatcher , DEVEL CONSOLE PATH ,  True  ,  True  , appinfo . AUTH FAIL ACTION REDIRECT ) upload dispatcher = dev appserver blobstore .  Create  Upload  Dispatcher  ( get blob storage ) url matcher .  Add URL ( dev appserver blobstore . UPLOAD URL PATTERN , upload dispatcher , '' ,  True  ,  True  , appinfo . AUTH FAIL ACTION UNAUTHORIZED ) blobimage dispatcher = dev appserver blobimage .  Create  Blob  Image  Dispatcher  ( apiproxy stub map . apiproxy .  Get  Stub  ( 'images' ) ) url matcher .  Add URL ( dev appserver blobimage . BLOBIMAGE URL PATTERN , blobimage dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION UNAUTHORIZED ) oauth dispatcher = dev appserver oauth .  Create O Auth  Dispatcher  ( ) url matcher .  Add URL ( dev appserver oauth . OAUTH URL PATTERN , oauth dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION UNAUTHORIZED ) channel dispatcher = dev appserver channel .  Create  Channel  Dispatcher  ( apiproxy stub map . apiproxy .  Get  Stub  ( 'channel' ) ) url matcher .  Add URL ( dev appserver channel . CHANNEL JSAPI PATTERN , channel dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION UNAUTHORIZED ) apiserver dispatcher = dev appserver apiserver .  Create  Apiserver  Dispatcher  ( ) url matcher .  Add URL ( dev appserver apiserver . API SERVING PATTERN , apiserver dispatcher , '' ,  False  ,  False  , appinfo . AUTH FAIL ACTION UNAUTHORIZED ) return url matcher 
def modgcd bivariate ( f , g ) : assert ( ( f . ring == g . ring ) and f . ring . domain . is ZZ ) result =  trivial gcd ( f , g ) if ( result is not  None  ) : return result ring = f . ring ( cf , f ) = f . primitive ( ) ( cg , g ) = g . primitive ( ) ch = ring . domain . gcd ( cf , cg ) ( xbound , ycontbound ) =  degree bound bivariate ( f , g ) if ( xbound == ycontbound == 0 ) : return ( ring ( ch ) , f . mul ground ( ( cf // ch ) ) , g . mul ground ( ( cg // ch ) ) ) fswap =  swap ( f , 1 ) gswap =  swap ( g , 1 ) degyf = fswap . degree ( ) degyg = gswap . degree ( ) ( ybound , xcontbound ) =  degree bound bivariate ( fswap , gswap ) if ( ybound == xcontbound == 0 ) : return ( ring ( ch ) , f . mul ground ( ( cf // ch ) ) , g . mul ground ( ( cg // ch ) ) ) gamma1 = ring . domain . gcd ( f . LC , g . LC ) gamma2 = ring . domain . gcd ( fswap . LC , gswap . LC ) badprimes = ( gamma1 * gamma2 ) m = 1 p = 1 while  True  : p = nextprime ( p ) while ( ( badprimes % p ) == 0 ) : p = nextprime ( p ) fp = f . trunc ground ( p ) gp = g . trunc ground ( p ) ( contfp , fp ) =  primitive ( fp , p ) ( contgp , gp ) =  primitive ( gp , p ) conthp =  gf gcd ( contfp , contgp , p ) degconthp = conthp . degree ( ) if ( degconthp > ycontbound ) : continue elif ( degconthp < ycontbound ) : m = 1 ycontbound = degconthp continue delta =  gf gcd (  LC ( fp ) ,  LC ( gp ) , p ) degcontfp = contfp . degree ( ) degcontgp = contgp . degree ( ) degdelta = delta . degree ( ) N = ( min ( ( degyf - degcontfp ) , ( degyg - degcontgp ) , ( ( ybound - ycontbound ) + degdelta ) ) + 1 ) if ( p < N ) : continue n = 0 evalpoints = [ ] hpeval = [ ] unlucky =  False  for a in range ( p ) : deltaa = delta . evaluate ( 0 , a ) if ( not ( deltaa % p ) ) : continue fpa = fp . evaluate ( 1 , a ) . trunc ground ( p ) gpa = gp . evaluate ( 1 , a ) . trunc ground ( p ) hpa =  gf gcd ( fpa , gpa , p ) deghpa = hpa . degree ( ) if ( deghpa > xbound ) : continue elif ( deghpa < xbound ) : m = 1 xbound = deghpa unlucky =  True  break hpa = hpa . mul ground ( deltaa ) . trunc ground ( p ) evalpoints . append ( a ) hpeval . append ( hpa ) n += 1 if ( n == N ) : break if unlucky : continue if ( n < N ) : continue hp =  interpolate multivariate ( evalpoints , hpeval , ring , 1 , p ) hp =  primitive ( hp , p ) [ 1 ] hp = ( hp * conthp . set ring ( ring ) ) degyhp = hp . degree ( 1 ) if ( degyhp > ybound ) : continue if ( degyhp < ybound ) : m = 1 ybound = degyhp continue hp = hp . mul ground ( gamma1 ) . trunc ground ( p ) if ( m == 1 ) : m = p hlastm = hp continue hm =  chinese remainder reconstruction multivariate ( hp , hlastm , p , m ) m *= p if ( not ( hm == hlastm ) ) : hlastm = hm continue h = hm . quo ground ( hm . content ( ) ) ( fquo , frem ) = f . div ( h ) ( gquo , grem ) = g . div ( h ) if ( ( not frem ) and ( not grem ) ) : if ( h . LC < 0 ) : ch = ( - ch ) h = h . mul ground ( ch ) cff = fquo . mul ground ( ( cf // ch ) ) cfg = gquo . mul ground ( ( cg // ch ) ) return ( h , cff , cfg ) 
def libvlc video get aspect ratio ( p mi ) : f = (   Cfunctions  . get ( 'libvlc video get aspect ratio' ,  None  ) or   Cfunction  ( 'libvlc video get aspect ratio' , ( ( 1 , ) , ) , string result , ctypes . c void p ,  Media  Player  ) ) return f ( p mi ) 
def from environment ( name , prefix , no builds =  False  , ignore channels =  False  ) : installed = linked ( prefix , ignore channels = ignore channels ) conda pkgs = copy ( installed ) add pip installed ( prefix , installed , json =  True  ) pip pkgs = sorted ( ( installed - conda pkgs ) ) if no builds : dependencies = [ '=' . join ( a . quad [ 0 : 3 ] ) for a in sorted ( conda pkgs ) ] else : dependencies = [ '=' . join ( a . quad [ 0 : 3 ] ) for a in sorted ( conda pkgs ) ] if ( len ( pip pkgs ) > 0 ) : dependencies . append ( { 'pip' : [ '==' . join ( a . rsplit ( '-' , 2 ) [ : 2 ] ) for a in pip pkgs ] } ) channels = list ( context . channels ) if ( not ignore channels ) : for dist in conda pkgs : if ( dist . channel not in channels ) : channels . insert ( 0 , dist . channel ) return  Environment  ( name = name , dependencies = dependencies , channels = channels , prefix = prefix ) 
def  Generate  Frozen  Resource  ( rc name , output name , h name =  None  ) : rcp =  Parse  ( rc name , h name ) in stat = os . stat ( rc name ) out = open ( output name , 'wt' ) out . write ( ( '#%s\n' % output name ) ) out . write ( ( '# This   is  a  generated  file.   Please   edit  %s  instead.\n' % rc name ) ) out . write ( ( '  version  =%r\n' %   version   ) ) out . write ( ( ' rc size =%d\n rc mtime =%d\n' % ( in stat [ stat . ST SIZE ] , in stat [ stat . ST MTIME ] ) ) ) out . write ( 'class   String  Def :\n' ) out . write ( ' DCTB def    init  (self,  id,  id Num ,  value):\n' ) out . write ( ' DCTB  DCTB self.id  =  id\n' ) out . write ( ' DCTB  DCTB self.id Num   =  id Num \n' ) out . write ( ' DCTB  DCTB self.value  =  value\n' ) out . write ( ' DCTB def    repr  (self):\n' ) out . write ( ' DCTB  DCTB return  " String  Def (%r,  %r,  %r)"  %  (self.id,  self.id Num ,  self.value)\n' ) out . write ( 'class   Fake  Parser :\n' ) for name in ( 'dialogs' , 'ids' , 'names' , 'bitmaps' , 'icons' , 'string Table ' ) : out . write ( ( ' DCTB %s  =  \\\n' % ( name , ) ) ) pprint . pprint ( getattr ( rcp , name ) , out ) out . write ( '\n' ) out . write ( 'def   Parse (s):\n' ) out . write ( ' DCTB return   Fake  Parser ()\n' ) out . close ( ) 
def reap All  Processes  ( ) : for process in list ( reap Process  Handlers  . values ( ) ) : process . reap Process  ( ) 
def avg pool1d ( input , kernel size , stride =  None  , padding = 0 , ceil mode =  False  , count include pad =  True  ) : if ( input . dim ( ) != 3 ) : raise  Value  Error  ( 'expected  3D  input  (got  {}  dimensions)' . format ( input . dim ( ) ) ) kernel size = (  single ( kernel size ) + ( 1 , ) ) stride = (  single ( stride ) + ( 1 , ) ) padding = (  single ( padding ) + ( 0 , ) ) f =  functions . thnn .  Avg  Pool 2d ( kernel size , stride , padding , ceil mode , count include pad ) return f ( input . unsqueeze ( 3 ) ) . squeeze ( 3 ) 
def  get server status code ( url ) : ( host , path , params , query ) = urlparse . urlparse ( url ) [ 1 : 5 ] try : conn = httplib . HTTP Connection  ( host ) conn . request ( 'HEAD' , ( ( path + '?' ) + query ) ) return conn . getresponse ( ) . status except  Standard  Error  : return  None  
def slugify ( string ) : string = u ( string ) ascii string = str ( unicodedata . normalize ( 'NFKD' , string ) . encode ( 'ascii' , 'ignore' ) ) no punctuation = re . sub ( '[^\\w\\s-]' , '' , ascii string ) . strip ( ) . lower ( ) slug = re . sub ( '[-\\s]+' , '-' , no punctuation ) return u ( slug ) 
def run crawler ( ) : projects = get projects ( ) for ( file path , project ) in projects . items ( ) : logging . info ( ( u' Processing   %s' % file path ) ) if ( api key is not  None  ) : try : add ohloh metadata ( project ) except : logging . warning ( ( u' Skipping    Ohloh   metadata  for  project  %s' % project [ u'name' ] ) ) save project ( project , file path ) logging . info ( u' Writing   to  tables' ) write to table ( projects ) logging . info ( u' Done !' ) 
def extract external port ( client , container identifier , internal port ) : container details = client . inspect container ( container identifier ) network settings = container details [ u' Network  Settings ' ] ports = network settings [ u' Ports ' ] details = ports [ u'{}/tcp' . format ( internal port ) ] host port = int ( details [ 0 ] [ u' Host  Port ' ] )  Message  . new ( message type = u'acceptance:extract external port' , host port = host port ) . write ( ) return host port 
def activity funding ( ) : return s3 rest controller ( ) 
def inputhook wx3 ( context ) : try : app = wx .  Get  App  ( ) if ( app is not  None  ) : assert wx .  Thread   Is  Main  ( ) if ( not callable ( signal . getsignal ( signal . SIGINT ) ) ) : signal . signal ( signal . SIGINT , signal . default int handler ) evtloop = wx .  Event  Loop  ( ) ea = wx .  Event  Loop  Activator  ( evtloop ) t = clock ( ) while ( not context . input is ready ( ) ) : while evtloop .  Pending  ( ) : t = clock ( ) evtloop .  Dispatch  ( ) app .  Process  Idle  ( ) used time = ( clock ( ) - t ) if ( used time > 10.0 ) : time . sleep ( 1.0 ) elif ( used time > 0.1 ) : time . sleep ( 0.05 ) else : time . sleep ( 0.001 ) del ea except  Keyboard  Interrupt  : pass return 0 
def convert mysql timestamp ( timestamp ) : if ( ( not PY2 ) and isinstance ( timestamp , ( bytes , bytearray ) ) ) : timestamp = timestamp . decode ( 'ascii' ) if ( timestamp [ 4 ] == '-' ) : return convert datetime ( timestamp ) timestamp += ( '0' * ( 14 - len ( timestamp ) ) ) ( year , month , day , hour , minute , second ) = ( int ( timestamp [ : 4 ] ) , int ( timestamp [ 4 : 6 ] ) , int ( timestamp [ 6 : 8 ] ) , int ( timestamp [ 8 : 10 ] ) , int ( timestamp [ 10 : 12 ] ) , int ( timestamp [ 12 : 14 ] ) ) try : return datetime . datetime ( year , month , day , hour , minute , second ) except  Value  Error  : return  None  
def error ( xml ) : try : ET . XML ( xml ) except ET .  Parse  Error  : return sys . exc value 
def gps noise rng ( radius ) : noise = gauss ( 0 , ( radius / 3.0 ) ) noise = min ( max ( ( - radius ) , noise ) , radius ) return noise 
def nonnegative int ( argument ) : value = int ( argument ) if ( value < 0 ) : raise  Value  Error  ( 'negative  value;  must  be  positive  or  zero' ) return value 
def test install from wheel gen entrypoint ( script , data ) : result = script . pip ( 'install' , 'script.wheel1a==0.1' , '--no-index' , ( '--find-links=' + data . find links ) , expect error =  False  ) if ( os . name == 'nt' ) : wrapper file = ( script . bin / 't1.exe' ) else : wrapper file = ( script . bin / 't1' ) assert ( wrapper file in result . files created ) if ( os . name != 'nt' ) : assert bool ( os . access ( ( script . base path / wrapper file ) , os . X OK ) ) 
def latest stable version ( get ) : metadata = loads ( get ( environ . get ( 'LE AUTO JSON URL' , 'https://pypi.python.org/pypi/certbot/json' ) ) ) return str ( max ( (  Loose  Version  ( r ) for r in metadata [ 'releases' ] . iterkeys ( ) if re . match ( '^[0-9.]+$' , r ) ) ) ) 
def refund seat ( course enrollment , request user ) : course key str = unicode ( course enrollment . course id ) unenrolled user = course enrollment . user try : refund ids = ecommerce api client ( ( request user or unenrolled user ) ) . refunds . post ( { u'course id' : course key str , u'username' : unenrolled user . username } ) except  Http  Client  Error  as exc : if ( ( exc . response . status code == 403 ) and ( request user != unenrolled user ) ) : log . warning ( u' User   [%s]  was  not  authorized  to  initiate  a  refund  for  user  [%s]  upon  unenrollment  from  course  [%s]' , request user . id , unenrolled user . id , course key str ) return [ ] else : raise exc if refund ids : log . info ( u' Refund   successfully  opened  for  user  [%s],  course  [%s]:  %r' , unenrolled user . id , course key str , refund ids ) if ( course enrollment . mode != u'verified' ) : log . info ( u' Skipping   refund  email  notification  for  non-verified  mode  for  user  [%s],  course  [%s],  mode:  [%s]' , course enrollment . user . id , course enrollment . course id , course enrollment . mode ) else : try : send refund notification ( course enrollment , refund ids ) except : log . warning ( u' Could   not  send  email  notification  for  refund.' , exc info =  True  ) else : log . debug ( u' No   refund  opened  for  user  [%s],  course  [%s]' , unenrolled user . id , course key str ) return refund ids 
@ lower builtin ( '**' , types .  Integer  , types .  Const  ) @ lower builtin ( '**' , types .  Float  , types .  Const  ) def static power impl ( context , builder , sig , args ) : exp = sig . args [ 1 ] . value if ( not isinstance ( exp , numbers .  Integral  ) ) : raise  Not  Implemented  Error  if ( abs ( exp ) > 65536 ) : raise  Not  Implemented  Error  invert = ( exp < 0 ) exp = abs ( exp ) tp = sig . return type is integer = isinstance ( tp , types .  Integer  ) zerodiv return =  get power zerodiv return ( context , tp ) val = context . cast ( builder , args [ 0 ] , sig . args [ 0 ] , tp ) lty = val . type def mul ( a , b ) : if is integer : return builder . mul ( a , b ) else : return builder . fmul ( a , b ) res = lty ( 1 ) a = val while ( exp != 0 ) : if ( exp & 1 ) : res = mul ( res , val ) exp >>= 1 val = mul ( val , val ) if invert : if is integer : def invert impl ( a ) : if ( a == 0 ) : if zerodiv return : return zerodiv return else : raise  Zero  Division  Error  ( '0  cannot  be  raised  to  a  negative  power' ) if ( ( a != 1 ) and ( a != ( - 1 ) ) ) : return 0 else : return a else : def invert impl ( a ) : return ( 1.0 / a ) res = context . compile internal ( builder , invert impl , typing . signature ( tp , tp ) , ( res , ) ) return res 
def setup platform ( hass , config , add devices , discovery info =  None  ) : if ( discovery info is  None  ) : return gateways = hass . data . get ( mysensors . MYSENSORS GATEWAYS ) if ( not gateways ) : return platform devices = [ ] for gateway in gateways : pres = gateway . const .  Presentation  set req = gateway . const .  Set  Req  map sv types = { pres . S DOOR : [ set req . V ARMED ] , pres . S MOTION : [ set req . V ARMED ] , pres . S SMOKE : [ set req . V ARMED ] , pres . S LIGHT : [ set req . V LIGHT ] , pres . S LOCK : [ set req . V LOCK STATUS ] , pres . S IR : [ set req . V IR SEND ] } device class map = { pres . S DOOR :  My  Sensors  Switch  , pres . S MOTION :  My  Sensors  Switch  , pres . S SMOKE :  My  Sensors  Switch  , pres . S LIGHT :  My  Sensors  Switch  , pres . S LOCK :  My  Sensors  Switch  , pres . S IR :  My  Sensors IR Switch  } if ( float ( gateway . protocol version ) >= 1.5 ) : map sv types . update ( { pres . S BINARY : [ set req . V STATUS , set req . V LIGHT ] , pres . S SPRINKLER : [ set req . V STATUS ] , pres . S WATER LEAK : [ set req . V ARMED ] , pres . S SOUND : [ set req . V ARMED ] , pres . S VIBRATION : [ set req . V ARMED ] , pres . S MOISTURE : [ set req . V ARMED ] } ) map sv types [ pres . S LIGHT ] . append ( set req . V STATUS ) device class map . update ( { pres . S BINARY :  My  Sensors  Switch  , pres . S SPRINKLER :  My  Sensors  Switch  , pres . S WATER LEAK :  My  Sensors  Switch  , pres . S SOUND :  My  Sensors  Switch  , pres . S VIBRATION :  My  Sensors  Switch  , pres . S MOISTURE :  My  Sensors  Switch  } ) if ( float ( gateway . protocol version ) >= 2.0 ) : map sv types . update ( { pres . S WATER QUALITY : [ set req . V STATUS ] } ) device class map . update ( { pres . S WATER QUALITY :  My  Sensors  Switch  } ) devices = { } gateway . platform callbacks . append ( mysensors . pf callback factory ( map sv types , devices , device class map , add devices ) ) platform devices . append ( devices ) def send ir code service ( service ) : ' Set   IR  code  as  device  state  attribute.' entity ids = service . data . get ( ATTR ENTITY ID ) ir code = service . data . get ( ATTR IR CODE ) if entity ids :  devices = [ device for gw devs in platform devices for device in gw devs . values ( ) if ( isinstance ( device ,  My  Sensors IR Switch  ) and ( device . entity id in entity ids ) ) ] else :  devices = [ device for gw devs in platform devices for device in gw devs . values ( ) if isinstance ( device ,  My  Sensors IR Switch  ) ] kwargs = { ATTR IR CODE : ir code } for device in  devices : device . turn on ( ** kwargs ) descriptions = load yaml config file ( os . path . join ( os . path . dirname (   file   ) , 'services.yaml' ) ) hass . services . register ( DOMAIN , SERVICE SEND IR CODE , send ir code service , descriptions . get ( SERVICE SEND IR CODE ) , schema = SEND IR CODE SERVICE SCHEMA ) 
def is valid boolstr ( val ) : val = str ( val ) . lower ( ) return ( ( val == 'true' ) or ( val == 'false' ) or ( val == 'yes' ) or ( val == 'no' ) or ( val == 'y' ) or ( val == 'n' ) or ( val == '1' ) or ( val == '0' ) ) 
def main ( ) : try : git root = os . path . abspath ( os . path . join ( os . path . dirname ( os . path . abspath (   file   ) ) , '..' , '..' ) ) os . chdir ( git root ) args = parse args ( ) config = args . config ( args ) display . verbosity = config . verbosity display . color = config . color try : args . func ( config ) except  Delegate  as ex : delegate ( config , ex . exclude , ex . require ) display . review warnings ( ) except  Application  Warning  as ex : display . warning ( str ( ex ) ) exit ( 0 ) except  Application  Error  as ex : display . error ( str ( ex ) ) exit ( 1 ) except  Keyboard  Interrupt  : exit ( 2 ) except IO Error  as ex : if ( ex . errno == errno . EPIPE ) : exit ( 3 ) raise 
def parse file ( arguments ) : input file = arguments [ 0 ] output file = arguments [ 1 ] row limit = arguments [ 2 ] output path = '.' with open ( input file , 'r' ) as input csv : datareader = csv . reader ( input csv ) all rows = [ ] for row in datareader : all rows . append ( row ) header = all rows . pop ( 0 ) current chunk = 1 for i in range ( 0 , len ( all rows ) , row limit ) : chunk = all rows [ i : ( i + row limit ) ] current output = os . path . join ( output path , '{}-{}.csv' . format ( output file , current chunk ) ) chunk . insert ( 0 , header ) with open ( current output , 'w' ) as output csv : writer = csv . writer ( output csv ) writer = writer . writerows ( chunk ) print '' print ' Chunk   #  {}:' . format ( current chunk ) print ' Filepath :  {}' . format ( current output ) print '#  of  rows:  {}' . format ( len ( chunk ) ) current chunk += 1 
def set role ( username , role ) : try : sendline ( 'config  terminal' ) role line = 'username  {0}  role  {1}' . format ( username , role ) ret = sendline ( role line ) sendline ( 'end' ) sendline ( 'copy  running-config  startup-config' ) return '\n' . join ( [ role line , ret ] ) except  Terminal  Exception  as e : log . error ( e ) return ' Failed   to  set  password' 
@ code version ( ( 1 , ) ) def inline reduce fixed shared ( N , buf , x , stride x , pos , count , manner fn , manner init , b = '' , stride b = '' ) : if b : init = manner init ( ( '%(x)s[%(pos)s  *  %(stride x)s]  +  %(b)s[%(pos)s  *  %(stride b)s]' % locals ( ) ) ) loop line = manner fn ( 'red' , manner init ( ( '%(x)s[i  *  %(stride x)s]  +  %(b)s[i  *  %(stride b)s]' % locals ( ) ) ) ) else : init = manner init ( ( '%(x)s[%(pos)s  *  %(stride x)s]' % locals ( ) ) ) loop line = manner fn ( 'red' , manner init ( ( '%(x)s[i  *  %(stride x)s]' % locals ( ) ) ) ) loop line2 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[i]' % buf ) ) r 16 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[%s+16]' % ( buf , pos ) ) ) r 8 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[%s+8]' % ( buf , pos ) ) ) r 4 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[%s+4]' % ( buf , pos ) ) ) r 2 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[%s+2]' % ( buf , pos ) ) ) r 1 = manner fn ( ( '%s[%s]' % ( buf , pos ) ) , ( '%s[%s+1]' % ( buf , pos ) ) ) return ( '\n        {\n                //   This   function  trashes  buf[1..n threads],\n                //  leaving  the  reduction  result  in  buf[0].\n                float  red  =  %(init)s;\n                #pragma  unroll  16\n                for  (int  i  =  %(pos)s  +  %(count)s;  i<%(N)s;  i  +=  %(count)s){\n                    red  =  %(loop line)s;\n                }\n                buf[%(pos)s]  =  red;\n                  syncthreads();\n                if  (%(pos)s  <  warp Size )\n                {\n                        for  (int  i  =  %(pos)s  +  warp Size ;  i  <  %(count)s;  i  +=  warp Size )\n                        {\n                                %(buf)s[%(pos)s]  =  %(loop line2)s;\n                        }\n                        if  (%(pos)s  <  16)\n                        {\n                                //reduce  so  that  %(pos)s  0  has  the  reduction  of  everything\n                                if(%(pos)s  +  16  <  %(N)s)\n                                        %(buf)s[%(pos)s]  =  %(r 16)s;\n                                if(%(pos)s  +  8  <  %(N)s)\n                                        %(buf)s[%(pos)s]  =  %(r 8)s;\n                                if(%(pos)s  +  4  <  %(N)s)\n                                        %(buf)s[%(pos)s]  =  %(r 4)s;\n                                if(%(pos)s  +  2  <  %(N)s)\n                                        %(buf)s[%(pos)s]  =  %(r 2)s;\n                                if(%(pos)s  +  1  <  %(N)s)\n                                        %(buf)s[%(pos)s]  =  %(r 1)s;\n                        }\n                }\n        }\n        ' % locals ( ) ) 
def  parse settings eth ( opts , iface type , enabled , iface ) : adapters = salt . utils . odict .  Ordered  Dict  ( ) adapters [ iface ] = salt . utils . odict .  Ordered  Dict  ( ) adapters [ iface ] [ 'type' ] = iface type adapters [ iface ] [ 'data' ] = salt . utils . odict .  Ordered  Dict  ( ) iface data = adapters [ iface ] [ 'data' ] iface data [ 'inet' ] = salt . utils . odict .  Ordered  Dict  ( ) iface data [ 'inet6' ] = salt . utils . odict .  Ordered  Dict  ( ) if enabled : adapters [ iface ] [ 'enabled' ] =  True  if opts . get ( 'hotplug' ,  False  ) : adapters [ iface ] [ 'hotplug' ] =  True  def addrfam = 'inet' dual stack =  False  if ( ( 'enable ipv6' in opts ) and opts [ 'enable ipv6' ] ) : iface data [ 'inet6' ] [ 'addrfam' ] = 'inet6' iface data [ 'inet6' ] [ 'netmask' ] = '64' def addrfam = 'inet6' if ( ( 'iface type' in opts ) and ( opts [ 'iface type' ] == 'vlan' ) ) : iface data [ 'inet6' ] [ 'vlan raw device' ] = re . sub ( '\\.\\d*' , '' , iface ) if ( ( 'ipaddr' in opts ) and ( 'ipv6ipaddr' in opts ) ) : iface data [ 'inet' ] [ 'addrfam' ] = 'inet' def addrfam = 'inet' dual stack =  True  else : iface data [ 'inet' ] [ 'addrfam' ] = 'inet' if ( iface type not in [ 'bridge' ] ) : tmp ethtool =  parse ethtool opts ( opts , iface ) if tmp ethtool : ethtool = { } for item in tmp ethtool : ethtool [  ETHTOOL CONFIG OPTS [ item ] ] = tmp ethtool [ item ] iface data [ def addrfam ] [ 'ethtool' ] = ethtool iface data [ def addrfam ] [ 'ethtool keys' ] = sorted ( ethtool ) if ( iface type == 'bridge' ) : bridging =  parse bridge opts ( opts , iface ) if bridging : opts . pop ( 'mode' ,  None  ) iface data [ def addrfam ] [ 'bridging' ] = bridging iface data [ def addrfam ] [ 'bridging keys' ] = sorted ( bridging ) iface data [ def addrfam ] [ 'addrfam' ] = def addrfam elif ( iface type == 'bond' ) : bonding =  parse settings bond ( opts , iface ) if bonding : opts . pop ( 'mode' ,  None  ) iface data [ def addrfam ] [ 'bonding' ] = bonding iface data [ def addrfam ] [ 'bonding' ] [ 'slaves' ] = opts [ 'slaves' ] iface data [ def addrfam ] [ 'bonding keys' ] = sorted ( bonding ) iface data [ def addrfam ] [ 'addrfam' ] = def addrfam elif ( iface type == 'slave' ) : adapters [ iface ] [ 'master' ] = opts [ 'master' ] opts [ 'proto' ] = 'manual' iface data [ def addrfam ] [ 'master' ] = adapters [ iface ] [ 'master' ] iface data [ def addrfam ] [ 'addrfam' ] = def addrfam elif ( iface type == 'vlan' ) : iface data [ def addrfam ] [ 'vlan raw device' ] = re . sub ( '\\.\\d*' , '' , iface ) iface data [ def addrfam ] [ 'addrfam' ] = def addrfam elif ( iface type == 'pppoe' ) : tmp ethtool =  parse ethtool pppoe opts ( opts , iface ) if tmp ethtool : for item in tmp ethtool : adapters [ iface ] [ 'data' ] [ def addrfam ] [  DEB CONFIG PPPOE OPTS [ item ] ] = tmp ethtool [ item ] iface data [ def addrfam ] [ 'addrfam' ] = def addrfam for opt in opts : if opt . startswith ( 'ipv6' ) : optname = opt [ 4 : ] v6only =  True  else : optname = opt v6only =  False   optname = SALT ATTR TO DEBIAN ATTR MAP . get ( optname , optname ) if  attrmaps contain attr (  optname ) : valuestr = opts [ opt ] if ( ( optname == 'proto' ) and ( valuestr == 'none' ) ) : valuestr = 'static' if v6only : ( valid , value , errmsg ) =  validate interface option (  optname , valuestr , addrfam = 'inet6' ) if ( not valid ) :  raise error iface ( iface , "'{0}'  '{1}'" . format ( opt , valuestr ) , [ errmsg ] )  optname =  optname . replace ( '-' , ' ' ) iface data [ 'inet6' ] [  optname ] = value elif dual stack : valid once =  False  errmsg =  None  for addrfam in [ 'inet' , 'inet6' ] : ( valid , value , errmsg ) =  validate interface option (  optname , valuestr , addrfam = addrfam ) if valid : valid once =  True   optname =  optname . replace ( '-' , ' ' ) if ( ( addrfam == 'inet' ) or (  optname not in iface data [ 'inet6' ] ) ) : iface data [ addrfam ] [  optname ] = value if ( not valid once ) :  raise error iface ( iface , "'{0}'  '{1}'" . format ( opt , valuestr ) , [ errmsg ] ) else : ( valid , value , errmsg ) =  validate interface option (  optname , valuestr , addrfam = def addrfam ) if ( not valid ) :  raise error iface ( iface , "'{0}'  '{1}'" . format ( opt , valuestr ) , [ errmsg ] )  optname =  optname . replace ( '-' , ' ' ) iface data [ def addrfam ] [  optname ] = value for opt in [ 'up cmds' , 'pre up cmds' , 'post up cmds' , 'down cmds' , 'pre down cmds' , 'post down cmds' ] : if ( opt in opts ) : iface data [ 'inet' ] [ opt ] = opts [ opt ] for addrfam in [ 'inet' , 'inet6' ] : if ( ( 'addrfam' in iface data [ addrfam ] ) and ( iface data [ addrfam ] [ 'addrfam' ] == addrfam ) ) : pass else : iface data . pop ( addrfam ) return adapters 
def prettify ( jsonobj , prettify =  True  ) : if ( not prettify ) : return json . dumps ( jsonobj ) json str = json . dumps ( jsonobj , indent = 2 , sort keys =  True  ) if pygments : try : lexer = get lexer for mimetype ( 'application/json' ) return pygments . highlight ( json str , lexer ,  Terminal  Formatter  ( ) ) except : pass return json str 
def rtl8187 fix ( iface ) : proc airmon =  Popen  ( [ 'airmon-ng' ] , stdout = PIPE , stderr = DN ) proc airmon . wait ( ) using rtl8187 =  False  for line in proc airmon . communicate ( ) [ 0 ] . split ( ) : line = line . upper ( ) if ( ( line . strip ( ) == '' ) or line . startswith ( 'INTERFACE' ) ) : continue if ( line . find ( iface . upper ( ) ) and ( line . find ( 'RTL8187' ) != ( - 1 ) ) ) : using rtl8187 =  True  if ( not using rtl8187 ) : print ( ( ( ( R + '  [!]' ) + O ) + '  unable  to  generate  airodump-ng  CSV  file' ) + W ) print ( ( ( ( R + '  [!]' ) + O ) + '  you  may  want  to  disconnect/reconnect  your  wifi  device' ) + W ) exit gracefully ( 1 ) print ( ( ( ( ( ( ( O + '  [!]' ) + W ) + '  attempting  ' ) + O ) + "RTL8187  ' Unknown    Error   132'" ) + W ) + '  fix...' ) original iface = iface airmon =  Popen  ( [ 'airmon-ng' , 'stop' , iface ] , stdout = PIPE , stderr = DN ) airmon . wait ( ) for line in airmon . communicate ( ) [ 0 ] . split ( '\n' ) : if ( ( line . strip ( ) == '' ) or line . startswith ( ' Interface ' ) or ( line . find ( '(removed)' ) != ( - 1 ) ) ) : continue original iface = line . split ( ) [ 0 ] print and exec ( [ 'ifconfig' , original iface , 'down' ] ) print and exec ( [ 'rmmod' , 'rtl8187' ] ) print and exec ( [ 'rfkill' , 'block' , 'all' ] ) print and exec ( [ 'rfkill' , 'unblock' , 'all' ] ) print and exec ( [ 'modprobe' , 'rtl8187' ] ) print and exec ( [ 'ifconfig' , original iface , 'up' ] ) print and exec ( [ 'airmon-ng' , 'start' , original iface ] ) print '\r                                                                                                                \r' , print ( ( ( O + '  [!]  ' ) + W ) + 'restarting  scan...\n' ) return  True  
def get Element  Node  Object  ( evaluated Link  Value  ) : if ( evaluated Link  Value  .   class   .   name   != ' Element  Node ' ) : print ' Warning ,  could  not  get   Element  Node   in  get Element  Node  Object   in  evaluate  for:' print evaluated Link  Value  .   class   .   name   print evaluated Link  Value  return  None  if ( evaluated Link  Value  . xml Object  ==  None  ) : print ' Warning ,  evaluated Link  Value .xml Object   is   None   in  get Element  Node  Object   in  evaluate  for:' print evaluated Link  Value  return  None  return evaluated Link  Value  . xml Object  
def to bytes ( obj , encoding = 'utf-8' , errors =  None  , nonstring = 'simplerepr' ) : if isinstance ( obj , binary type ) : return obj if ( errors in (  None  , 'surrogate or replace' ) ) : if HAS SURROGATEESCAPE : errors = 'surrogateescape' else : errors = 'replace' elif ( errors == 'surrogate or strict' ) : if HAS SURROGATEESCAPE : errors = 'surrogateescape' else : errors = 'strict' if isinstance ( obj , text type ) : return obj . encode ( encoding , errors ) if ( nonstring == 'simplerepr' ) : try : value = str ( obj ) except  Unicode  Error  : try : value = repr ( obj ) except  Unicode  Error  : return to bytes ( '' ) elif ( nonstring == 'passthru' ) : return obj elif ( nonstring == 'empty' ) : return to bytes ( '' ) elif ( nonstring == 'strict' ) : raise  Type  Error  ( 'obj  must  be  a  string  type' ) else : raise  Type  Error  ( ( " Invalid   value  %s  for  to bytes'  nonstring  parameter" % nonstring ) ) return to bytes ( value , encoding , errors ) 
def db exists ( name , ** connection args ) : dbc =  connect ( ** connection args ) if ( dbc is  None  ) : return  False  cur = dbc . cursor ( ) args = { 'dbname' : name } qry = 'SHOW  DATABASES  LIKE  %(dbname)s;' try :  execute ( cur , qry , args ) except  My SQ Ldb  .  Operational  Error  as exc : err = ' My SQL   Error   {0}:  {1}' . format ( * exc )   context   [ 'mysql.error' ] = err log . error ( err ) return  False  cur . fetchall ( ) return ( cur . rowcount == 1 ) 
def copy file ( src , dst , preserve mode = 1 , preserve times = 1 , update = 0 , link =  None  , verbose = 1 , dry run = 0 ) : from distutils . dep util import newer from stat import ST ATIME , ST MTIME , ST MODE , S IMODE if ( not os . path . isfile ( src ) ) : raise  Distutils  File  Error  , ( "can't  copy  '%s':  doesn't  exist  or  not  a  regular  file" % src ) if os . path . isdir ( dst ) : dir = dst dst = os . path . join ( dst , os . path . basename ( src ) ) else : dir = os . path . dirname ( dst ) if ( update and ( not newer ( src , dst ) ) ) : if ( verbose >= 1 ) : log . debug ( 'not  copying  %s  (output  up-to-date)' , src ) return ( dst , 0 ) try : action =  copy action [ link ] except  Key  Error  : raise  Value  Error  , ( "invalid  value  '%s'  for  'link'  argument" % link ) if ( verbose >= 1 ) : if ( os . path . basename ( dst ) == os . path . basename ( src ) ) : log . info ( '%s  %s  ->  %s' , action , src , dir ) else : log . info ( '%s  %s  ->  %s' , action , src , dst ) if dry run : return ( dst , 1 ) if ( os . name == 'mac' ) : import macostools try : macostools . copy ( src , dst , 0 , preserve times ) except os . error as exc : raise  Distutils  File  Error  , ( "could  not  copy  '%s'  to  '%s':  %s" % ( src , dst , exc [ ( - 1 ) ] ) ) elif ( link == 'hard' ) : if ( not ( os . path . exists ( dst ) and os . path . samefile ( src , dst ) ) ) : os . link ( src , dst ) elif ( link == 'sym' ) : if ( not ( os . path . exists ( dst ) and os . path . samefile ( src , dst ) ) ) : os . symlink ( src , dst ) else :  copy file contents ( src , dst ) if ( preserve mode or preserve times ) : st = os . stat ( src ) if preserve times : os . utime ( dst , ( st [ ST ATIME ] , st [ ST MTIME ] ) ) if preserve mode : os . chmod ( dst , S IMODE ( st [ ST MODE ] ) ) return ( dst , 1 ) 
def minorticks off ( ) : gca ( ) . minorticks off ( ) 
def beacon ( config ) : log . trace ( 'glxinfo  beacon  starting' ) ret = [ ]  validate =   validate   ( config ) if ( not  validate [ 0 ] ) : return ret retcode =   salt   [ 'cmd.retcode' ] ( 'DISPLAY=:0  glxinfo' , runas = config [ 'user' ] , python shell =  True  ) if ( ( 'screen event' in config ) and config [ 'screen event' ] ) : last value = last state . get ( 'screen available' ,  False  ) screen available = ( retcode == 0 ) if ( ( last value != screen available ) or ( 'screen available' not in last state ) ) : ret . append ( { 'tag' : 'screen event' , 'screen available' : screen available } ) last state [ 'screen available' ] = screen available return ret 
def test write noheader no pad ( ) : out =  String IO ( ) ascii . write ( dat , out ,  Writer  = ascii .  Fixed  Width  No  Header  , delimiter pad =  None  ) assert equal splitlines ( out . getvalue ( ) , '|1.2|    "hello"|1|a|\n|2.4|\'s  worlds|2|2|\n' ) 
@ pytest . mark . django db def test approve addons approve files no review type ( ) : amo . tests . user factory ( id = settings . TASK USER ID ) addon = addon factory ( status = amo . STATUS PUBLIC ) file  = addon . versions . get ( ) . files . get ( ) file  . update ( status = amo . STATUS PUBLIC ) approve addons . approve files ( [ ( file  ,  None  ) ] ) assert ( addon . reload ( ) . status == amo . STATUS PUBLIC ) assert ( file  . reload ( ) . status == amo . STATUS PUBLIC ) 
def get config ( key ) : return c [ key ] 
def quoted val ( state , text , i , formats , user data ) : quote = ( u'"' if ( state . parse is DQ VAL ) else u"'" ) add attr data ( user data , ATTR VALUE , ATTR START , i ) pos = text . find ( quote , i ) if ( pos == ( - 1 ) ) : num = ( len ( text ) - i ) is link = is class =  False  else : num = ( ( pos - i ) + 1 ) state . parse = IN OPENING TAG if ( ( state . tag being defined is not  None  ) and ( state . attribute name in ( u'lang' , u'xml:lang' ) ) ) : try : state . tag being defined . lang = parse lang code ( text [ i : pos ] ) except  Value  Error  : pass add attr data ( user data , ATTR VALUE , ATTR END , ( i + num ) ) is link = ( state . attribute name in LINK ATTRS ) is class = ( ( not is link ) and ( state . attribute name == u'class' ) ) if is link : if ( verify link ( text [ i : ( ( i + num ) - 1 ) ] , user data . doc name ) is  False  ) : return [ ( ( num - 1 ) , formats [ u'bad link' ] ) , ( 1 , formats [ u'string' ] ) ] return [ ( ( num - 1 ) , formats [ u'link' ] ) , ( 1 , formats [ u'string' ] ) ] elif is class : return [ ( ( num - 1 ) , formats [ u'class attr' ] ) , ( 1 , formats [ u'string' ] ) ] return [ ( num , formats [ u'string' ] ) ] 
def safecall ( f , name , * args , ** kwargs ) : lwork = kwargs . get ( 'lwork' ,  None  ) if ( lwork in (  None  , ( - 1 ) ) ) : kwargs [ 'lwork' ] = ( - 1 ) ret = f ( * args , ** kwargs ) kwargs [ 'lwork' ] = ret [ ( - 2 ) ] [ 0 ] . real . astype ( numpy . int ) ret = f ( * args , ** kwargs ) if ( ret [ ( - 1 ) ] < 0 ) : raise  Value  Error  ( ( 'illegal  value  in  %d-th  argument  of  internal  %s' % ( ( - ret [ ( - 1 ) ] ) , name ) ) ) return ret [ : ( - 2 ) ] 
def scalene ( x , alpha , beta ) : return ( ( alpha * pos ( x ) ) + ( beta * neg ( x ) ) ) 
def return future ( f ) : replacer =  Arg  Replacer  ( f , 'callback' ) @ functools . wraps ( f ) def wrapper ( * args , ** kwargs ) : future =  Traceback  Future  ( ) ( callback , args , kwargs ) = replacer . replace ( ( lambda value =  NO RESULT : future . set result ( value ) ) , args , kwargs ) def handle error ( typ , value , tb ) : future . set exc info ( ( typ , value , tb ) ) return  True  exc info =  None  with  Exception  Stack  Context  ( handle error ) : try : result = f ( * args , ** kwargs ) if ( result is not  None  ) : raise  Return  Value  Ignored  Error  ( '@return future  should  not  be  used  with  functions  that  return  values' ) except : exc info = sys . exc info ( ) raise if ( exc info is not  None  ) : future . result ( ) if ( callback is not  None  ) : def run callback ( future ) : result = future . result ( ) if ( result is  NO RESULT ) : callback ( ) else : callback ( future . result ( ) ) future . add done callback ( wrap ( run callback ) ) return future return wrapper 
def start Cc  (  Call  Control  Capabilities  presence = 0 ) : a =  Tp  Pd  ( pd = 3 ) b =  Message  Type  ( mes Type  = 9 ) packet = ( a / b ) if (  Call  Control  Capabilities  presence is 1 ) : c =  Call  Control  Capabilities  Hdr  ( ieiCCC = 21 , eight Bit CCC = 0 ) packet = ( packet / c ) return packet 
def virtual memory ( ) : mem = cext . virtual mem ( ) ( total , free , active , inactive , wired , cached , buffers , shared ) = mem if NETBSD : with open ( '/proc/meminfo' , 'rb' ) as f : for line in f : if line . startswith ( ' Buffers :' ) : buffers = ( int ( line . split ( ) [ 1 ] ) * 1024 ) elif line . startswith ( ' Mem  Shared :' ) : shared = ( int ( line . split ( ) [ 1 ] ) * 1024 ) avail = ( ( inactive + cached ) + free ) used = ( ( active + wired ) + cached ) percent = usage percent ( ( total - avail ) , total ,  round = 1 ) return svmem ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired ) 
def validate file ( fpath , md5 hash ) : hasher = hashlib . md5 ( ) with open ( fpath , 'rb' ) as f : buf = f . read ( ) hasher . update ( buf ) if ( str ( hasher . hexdigest ( ) ) == str ( md5 hash ) ) : return  True  else : return  False  
def translate exception ( exc info , initial skip = 0 ) : tb = exc info [ 2 ] frames = [ ] for x in xrange ( initial skip ) : if ( tb is not  None  ) : tb = tb . tb next initial tb = tb while ( tb is not  None  ) : if ( tb . tb frame . f code in internal code ) : tb = tb . tb next continue next = tb . tb next template = tb . tb frame . f globals . get ( '  jinja template  ' ) if ( template is not  None  ) : lineno = template . get corresponding lineno ( tb . tb lineno ) tb = fake exc info ( ( exc info [ : 2 ] + ( tb , ) ) , template . filename , lineno ) [ 2 ] frames . append (  Traceback  Frame  Proxy  ( tb ) ) tb = next if ( not frames ) : raise exc info [ 0 ] , exc info [ 1 ] , exc info [ 2 ] traceback =  Processed  Traceback  ( exc info [ 0 ] , exc info [ 1 ] , frames ) if ( tb set next is not  None  ) : traceback . chain frames ( ) return traceback 
def cinder todo format ( physical line ) : pos = physical line . find ( 'TODO' ) pos1 = physical line . find ( 'TODO(' ) pos2 = physical line . find ( '#' ) if ( ( pos != pos1 ) and ( pos2 >= 0 ) and ( pos2 < pos ) ) : return ( pos , 'CINDER  N101:   Use   TODO(NAME)' ) 
def extract mnist labels ( filename , num images ) : if ( not tf . gfile .  Exists  ( ( filename + '.npy' ) ) ) : with gzip . open ( filename ) as bytestream : bytestream . read ( 8 ) buf = bytestream . read ( ( 1 * num images ) ) labels = np . frombuffer ( buf , dtype = np . uint8 ) . astype ( np . int32 ) np . save ( filename , labels ) return labels else : with tf . gfile .  Open  ( ( filename + '.npy' ) , mode = 'r' ) as file obj : return np . load ( file obj ) 
def generator ( tvdb id , show name , cur data , force search ) : def do test ( ) : '\n                 Test   to  perform\n                ' global search items search items = cur data [ 'i' ] show = TV Show  ( 1 , tvdb id ) show . name = show name show . quality = cur data [ 'q' ] show . save To DB ( ) sickbeard . show List  . append ( show ) episode =  None  for ep Number  in cur data [ 'e' ] : episode = TV Episode  ( show , cur data [ 's' ] , ep Number  ) episode . status = common . WANTED episode . save To DB ( ) best result = search . search Providers  ( show , episode . episode , force search ) if ( not best result ) : assert ( cur data [ 'b' ] == best result ) assert ( cur data [ 'b' ] == best result . name ) return do test 
def get Closest  Point  On  Segment  ( segment Begin  , segment End  , point ) : segment Difference  = ( segment End  - segment Begin  ) if ( abs ( segment Difference  ) <= 0.0 ) : return segment Begin  point Minus  Segment  Begin  = ( point - segment Begin  ) begin Plane  Dot  = get Dot  Product  ( point Minus  Segment  Begin  , segment Difference  ) difference Plane  Dot  = get Dot  Product  ( segment Difference  , segment Difference  ) intercept = ( begin Plane  Dot  / difference Plane  Dot  ) intercept = max ( intercept , 0.0 ) intercept = min ( intercept , 1.0 ) return ( segment Begin  + ( segment Difference  * intercept ) ) 
@ staticmethod def  Python  Partial  ( func , * args , ** keywords ) : def newfunc ( * fargs , ** fkeywords ) : newkeywords = keywords . copy ( ) newkeywords . update ( fkeywords ) return func ( * ( args + fargs ) , ** newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc 
def run ( ) : for chunk in chunked (  Webapp  . objects . all ( ) , 50 ) : for app in chunk : for slug in SIZE SLUGS : assets =  Image  Asset  . objects . filter ( addon = app , slug = slug ) for asset in assets [ 1 : ] : asset . delete ( ) 
def test finder only installs data require ( data ) : finder =  Package  Finder  ( [ ] , [ data . index url ( 'datarequire' ) ] , session =  Pip  Session  ( ) ) links = finder . find all candidates ( 'fakepackage' ) expected = [ '1.0.0' , '9.9.9' ] if ( sys . version info < ( 2 , 7 ) ) : expected . append ( '2.6.0' ) elif ( ( 2 , 7 ) < sys . version info < ( 3 , ) ) : expected . append ( '2.7.0' ) elif ( sys . version info > ( 3 , 3 ) ) : expected . append ( '3.3.0' ) assert ( set ( [ str ( v . version ) for v in links ] ) == set ( expected ) ) 
def split policy string ( policy string ) : if ( '-' in policy string ) : ( base , policy index ) = policy string . rsplit ( '-' , 1 ) else : ( base , policy index ) = ( policy string ,  None  ) policy = POLICIES . get by index ( policy index ) if ( get policy string ( base , policy ) != policy string ) : raise  Policy  Error  ( ' Unknown   policy' , index = policy index ) return ( base , policy ) 
def directory ( arg ) : if ( ( not isinstance ( arg , string types ) ) and os . path . isdir ( arg ) ) : raise argparse .  Argument  Type  Error  ( '{0}  is  not  a  directory  or  does  not  exist  (the  directory  must  be  created  first)' . format ( arg ) ) return os . path . abspath ( arg ) 
def main ( ) : module =  Ansible  Module  ( argument spec = dict ( pn cliusername = dict ( required =  True  , type = 'str' ) , pn clipassword = dict ( required =  True  , type = 'str' , no log =  True  ) , pn cliswitch = dict ( required =  False  , type = 'str' ) , pn command = dict ( required =  True  , type = 'str' ) , pn parameters = dict ( default = 'all' , type = 'str' ) , pn options = dict ( type = 'str' ) ) ) command = module . params [ 'pn command' ] parameters = module . params [ 'pn parameters' ] options = module . params [ 'pn options' ] cli = pn cli ( module ) cli += ( '  %s  format  %s  ' % ( command , parameters ) ) if options : cli += options run cli ( module , cli ) 
@ hsa . jit ( device =  True  ) def warp scan ( tid , temp , inclusive ) : hsa . wavebarrier ( ) lane = ( tid & (  WARPSIZE - 1 ) ) if ( lane >= 1 ) : temp [ tid ] += temp [ ( tid - 1 ) ] hsa . wavebarrier ( ) if ( lane >= 2 ) : temp [ tid ] += temp [ ( tid - 2 ) ] hsa . wavebarrier ( ) if ( lane >= 4 ) : temp [ tid ] += temp [ ( tid - 4 ) ] hsa . wavebarrier ( ) if ( lane >= 8 ) : temp [ tid ] += temp [ ( tid - 8 ) ] hsa . wavebarrier ( ) if ( lane >= 16 ) : temp [ tid ] += temp [ ( tid - 16 ) ] hsa . wavebarrier ( ) if ( lane >= 32 ) : temp [ tid ] += temp [ ( tid - 32 ) ] hsa . wavebarrier ( ) if inclusive : return temp [ tid ] else : return ( temp [ ( tid - 1 ) ] if ( lane > 0 ) else 0 ) 
def get unit status ( code ) : output = check output ( ( 'heyu  onstate  ' + code ) , shell =  True  ) return int ( output . decode ( 'utf-8' ) [ 0 ] ) 
def  get queryset ( klass ) : return klass . query 
def  gem ( command , ruby =  None  , runas =  None  , gem bin =  None  ) : cmdline = ( [ ( gem bin or 'gem' ) ] + command ) if ( gem bin is  None  ) : if   salt   [ 'rvm.is installed' ] ( runas = runas ) : return   salt   [ 'rvm.do' ] ( ruby , cmdline , runas = runas ) if ( ( not salt . utils . is windows ( ) ) and   salt   [ 'rbenv.is installed' ] ( runas = runas ) ) : if ( ruby is  None  ) : return   salt   [ 'rbenv.do' ] ( cmdline , runas = runas ) else : return   salt   [ 'rbenv.do with ruby' ] ( ruby , cmdline , runas = runas ) ret =   salt   [ 'cmd.run all' ] ( cmdline , runas = runas , python shell =  False  ) if ( ret [ 'retcode' ] == 0 ) : return ret [ 'stdout' ] else : raise  Command  Execution  Error  ( ret [ 'stderr' ] ) 
@ register . simple tag ( takes context =  True  ) def avatar url ( context , user , size , resolution = u'1x' , service id =  None  ) : if ( resolution not in ( u'1x' , u'2x' ) ) : raise  Value  Error  ( ( u'resolution  should  be  "1x"  or  "2x",  not  %r.' % resolution ) ) service = avatar services . for user ( user , service id ) if ( service is  None  ) : logging . error ( u' Could   not  get  a  suitable  avatar  service  for  user  %s.' , user ) return mark safe ( u'' ) urls = service . get avatar urls ( request = context [ u'request' ] , user = user , size = size ) return urls [ resolution ] 
def get steps pkg ( distribution , package source =  Package  Source  ( ) ) : if ( distribution not in PACKAGED CLIENT DISTRIBUTIONS ) : raise  Usage  Error  ( ( ' Distribution   %r  not  supported.   Available   distributions:  %s' % ( distribution , ',  ' . join ( PACKAGED CLIENT DISTRIBUTIONS ) ) ) ) package manager = DOCKER IMAGES [ distribution ] . package manager steps = [ ensure minimal setup ( package manager ) , task cli pkg install ( distribution , package source ) ] return steps 
def dirichlet logpdf vec ( x , alpha ) : if ( len ( x . shape ) == 1 ) : return stats . dirichlet . logpdf ( x , alpha ) else : size = x . shape [ 0 ] return np . array ( [ stats . dirichlet . logpdf ( x [ i , : ] , alpha ) for i in range ( size ) ] ) 
def dead code elimination ( graph , du , ud ) : for node in graph . rpo : for ( i , ins ) in node . get loc with ins ( ) [ : ] : reg = ins . get lhs ( ) if ( reg is not  None  ) : if ( ( reg , i ) not in du ) : if ins . is call ( ) : ins . remove defined var ( ) elif ins . has side effect ( ) : continue else : update chain ( graph , i , du , ud ) graph . remove ins ( i ) 
def print rcode ( expr , ** settings ) : print ( rcode ( expr , ** settings ) ) 
def create nic ( module , profitbricks ) : datacenter = module . params . get ( 'datacenter' ) server = module . params . get ( 'server' ) lan = module . params . get ( 'lan' ) name = module . params . get ( 'name' ) wait = module . params . get ( 'wait' ) wait timeout = module . params . get ( 'wait timeout' ) if ( not uuid match . match ( datacenter ) ) : datacenter list = profitbricks . list datacenters ( ) for d in datacenter list [ 'items' ] : dc = profitbricks . get datacenter ( d [ 'id' ] ) if ( datacenter == dc [ 'properties' ] [ 'name' ] ) : datacenter = d [ 'id' ] break if ( not uuid match . match ( server ) ) : server list = profitbricks . list servers ( datacenter ) for s in server list [ 'items' ] : if ( server == s [ 'properties' ] [ 'name' ] ) : server = s [ 'id' ] break try : n = NIC ( name = name , lan = lan ) nic response = profitbricks . create nic ( datacenter , server , n ) if wait :  wait for completion ( profitbricks , nic response , wait timeout , 'create nic' ) return nic response except  Exception  as e : module . fail json ( msg = ( 'failed  to  create  the  NIC:  %s' % str ( e ) ) ) 
def  pr compile ( regex , cleanup =  None  ) : return (  re compile ( regex ) , cleanup ) 
def  calc array sizeof ( ndim ) : ctx = cpu target . target context return ctx . calc array sizeof ( ndim ) 
@ task def destroy instance ( nodetype , instance id ) : env . nodetype = nodetype instance = ec2 utils .  Get  Instance  ( env . region , instance id ) assert instance , ( ' Instance   %s  not  found' % instance id ) with settings ( host string = instance . public dns name ) : if ( instance . state == 'running' ) : check min healthy instances ( 3 ) drain ( ) stop ( ) fprint ( ( ' Terminating   instance  %s' % instance id ) ) ec2 utils .  Terminate  Instance  ( env . region , instance id ) 
def main ( ) : module =  Ansible  Module  ( argument spec = dict ( pn cliusername = dict ( required =  False  , type = 'str' ) , pn clipassword = dict ( required =  False  , type = 'str' , no log =  True  ) , pn cliswitch = dict ( required =  False  , type = 'str' , default = 'local' ) , state = dict ( required =  True  , type = 'str' , choices = [ 'present' , 'absent' , 'update' ] ) , pn name = dict ( required =  True  , type = 'str' ) , pn port = dict ( type = 'str' ) , pn peer port = dict ( type = 'str' ) , pn mode = dict ( type = 'str' , choices = [ 'active-standby' , 'active-active' ] ) , pn peer switch = dict ( type = 'str' ) , pn failover action = dict ( type = 'str' , choices = [ 'move' , 'ignore' ] ) , pn lacp mode = dict ( type = 'str' , choices = [ 'off' , 'passive' , 'active' ] ) , pn lacp timeout = dict ( type = 'str' , choices = [ 'slow' , 'fast' ] ) , pn lacp fallback = dict ( type = 'str' , choices = [ 'individual' , 'bundled' ] ) , pn lacp fallback timeout = dict ( type = 'str' ) ) , required if = ( [ 'state' , 'present' , [ 'pn name' , 'pn port' , 'pn peer port' , 'pn peer switch' ] ] , [ 'state' , 'absent' , [ 'pn name' ] ] , [ 'state' , 'update' , [ 'pn name' ] ] ) ) state = module . params [ 'state' ] name = module . params [ 'pn name' ] port = module . params [ 'pn port' ] peer port = module . params [ 'pn peer port' ] mode = module . params [ 'pn mode' ] peer switch = module . params [ 'pn peer switch' ] failover action = module . params [ 'pn failover action' ] lacp mode = module . params [ 'pn lacp mode' ] lacp timeout = module . params [ 'pn lacp timeout' ] lacp fallback = module . params [ 'pn lacp fallback' ] lacp fallback timeout = module . params [ 'pn lacp fallback timeout' ] command = get command from state ( state ) cli = pn cli ( module ) if ( command == 'vlag-delete' ) : check cli ( module , cli ) if ( VLAG EXISTS is  False  ) : module . exit json ( skipped =  True  , msg = ( 'VLAG  with  name  %s  does  not  exist' % name ) ) cli += ( '  %s  name  %s  ' % ( command , name ) ) else : if ( command == 'vlag-create' ) : check cli ( module , cli ) if ( VLAG EXISTS is  True  ) : module . exit json ( skipped =  True  , msg = ( 'VLAG  with  name  %s  already  exists' % name ) ) cli += ( '  %s  name  %s  ' % ( command , name ) ) if port : cli += ( '  port  %s  peer-port  %s  ' % ( port , peer port ) ) if mode : cli += ( '  mode  ' + mode ) if peer switch : cli += ( '  peer-switch  ' + peer switch ) if failover action : cli += ( ( '  failover-' + failover action ) + '-L2  ' ) if lacp mode : cli += ( '  lacp-mode  ' + lacp mode ) if lacp timeout : cli += ( '  lacp-timeout  ' + lacp timeout ) if lacp fallback : cli += ( '  lacp-fallback  ' + lacp fallback ) if lacp fallback timeout : cli += ( '  lacp-fallback-timeout  ' + lacp fallback timeout ) run cli ( module , cli ) 
def  write file iface ( iface , data , folder , pattern ) : filename = os . path . join ( folder , pattern . format ( iface ) ) if ( not os . path . exists ( folder ) ) : msg = '{0}  cannot  be  written.  {1}  does  not  exist' msg = msg . format ( filename , folder ) log . error ( msg ) raise  Attribute  Error  ( msg ) with salt . utils . fopen ( filename , 'w' ) as fp  : fp  . write ( data ) 
def  Stemming  Analyzer  ( expression = default pattern , stoplist = STOP WORDS , minsize = 2 , maxsize =  None  , gaps =  False  , stemfn = stem , ignore =  None  , cachesize = 50000 ) : ret =  Regex  Tokenizer  ( expression = expression , gaps = gaps ) chain = ( ret |  Lowercase  Filter  ( ) ) if ( stoplist is not  None  ) : chain = ( chain |  Stop  Filter  ( stoplist = stoplist , minsize = minsize , maxsize = maxsize ) ) return ( chain |  Stem  Filter  ( stemfn = stemfn , ignore = ignore , cachesize = cachesize ) ) 
def get Paths  By  Lists  ( vertex Lists  ) : vector3 Lists  = get Vector 3 Lists  Recursively  ( vertex Lists  ) paths = [ ] add To  Paths  Recursively  ( paths , vector3 Lists  ) return paths 
def get theme sass dirs ( system , theme dir ) : if ( system not in ( 'lms' , 'cms' ) ) : raise  Value  Error  ( '"system"  must  either  be  "lms"  or  "cms"' ) dirs = [ ] system sass dir = ( ( path ( system ) / 'static' ) / 'sass' ) sass dir = ( ( ( theme dir / system ) / 'static' ) / 'sass' ) css dir = ( ( ( theme dir / system ) / 'static' ) / 'css' ) dependencies = SASS LOOKUP DEPENDENCIES . get ( system , [ ] ) if sass dir . isdir ( ) : css dir . mkdir p ( ) dirs . append ( { 'sass source dir' : system sass dir , 'css destination dir' : css dir , 'lookup paths' : ( dependencies + [ ( sass dir / 'partials' ) , ( system sass dir / 'partials' ) , system sass dir ] ) } ) dirs . append ( { 'sass source dir' : sass dir , 'css destination dir' : css dir , 'lookup paths' : ( dependencies + [ ( sass dir / 'partials' ) , ( system sass dir / 'partials' ) , system sass dir ] ) } ) return dirs 
def transfer get ( context , transfer id ) : return IMPL . transfer get ( context , transfer id ) 
def  setup ( options , lib =  None  ) : mb . configure ( ) config =  configure ( options ) plugins =  load plugins ( config ) from beets . ui . commands import default commands subcommands = list ( default commands ) subcommands . extend ( plugins . commands ( ) ) if ( lib is  None  ) : lib =  open library ( config ) plugins . send ( 'library opened' , lib = lib ) library .  Item  .  types . update ( plugins . types ( library .  Item  ) ) library .  Album  .  types . update ( plugins . types ( library .  Album  ) ) return ( subcommands , plugins , lib ) 
def find up ( l node , f node ) : if isinstance ( l node , gof .  Apply  ) : l outs = l node . outputs else : l outs = l node l ins = gof . graph . inputs ( l outs ) nodes = gof . graph . io toposort ( l ins , l outs ) return ( f node in nodes ) 
def task cli pkg install ( distribution , package source =  Package  Source  ( ) ) : commands = task package install ( 'clusterhq-flocker-cli' , distribution , package source ) return sequence ( [ (  Effect  (  Sudo  ( command = e . intent . command , log command filter = e . intent . log command filter ) ) if isinstance ( e . intent ,  Run  ) else e ) for e in commands . intent . effects ] ) 
def set Up  Module  ( ) : global ENGINE global SESSION ENGINE = create engine ( 'sqlite://' )  Base  . metadata . create all ( ENGINE ) session factory = sessionmaker ( bind = ENGINE ) SESSION = scoped session ( session factory ) 
def decode unicode obj ( obj ) : if isinstance ( obj , dict ) : r = { } for ( k , v ) in iteritems ( obj ) : r [ decode unicode string ( k ) ] = decode unicode obj ( v ) return r elif isinstance ( obj , six . string types ) : return decode unicode string ( obj ) elif isinstance ( obj , ( list , tuple ) ) : return [ decode unicode obj ( x ) for x in obj ] else : return obj 
def aslinearoperator ( A ) : if isinstance ( A ,  Linear  Operator  ) : return A elif ( isinstance ( A , np . ndarray ) or isinstance ( A , np . matrix ) ) : if ( A . ndim > 2 ) : raise  Value  Error  ( 'array  must  have  ndim  <=  2' ) A = np . atleast 2d ( np . asarray ( A ) ) return  Matrix  Linear  Operator  ( A ) elif isspmatrix ( A ) : return  Matrix  Linear  Operator  ( A ) elif ( hasattr ( A , 'shape' ) and hasattr ( A , 'matvec' ) ) : rmatvec =  None  dtype =  None  if hasattr ( A , 'rmatvec' ) : rmatvec = A . rmatvec if hasattr ( A , 'dtype' ) : dtype = A . dtype return  Linear  Operator  ( A . shape , A . matvec , rmatvec = rmatvec , dtype = dtype ) else : raise  Type  Error  ( 'type  not  understood' ) 
def  process text args ( override , fontdict =  None  , ** kwargs ) : if ( fontdict is not  None  ) : override . update ( fontdict ) override . update ( kwargs ) return override 
def help stream ( ) : s = ( '  ' * 2 ) usage = '\n' usage += ( s + grey ( ( u'\u266a' + '   Switching   streams  \n' ) ) ) usage += ( ( ( ( ( s * 2 ) + light green ( 'switch  public  #AKB' ) ) + '  will  switch  to  public  stream  and  follow  "' ) + light yellow ( 'AKB' ) ) + '"  keyword.\n' ) usage += ( ( ( s * 2 ) + light green ( 'switch  mine' ) ) + '  will  switch  to  your  personal  stream.\n' ) usage += ( ( ( s * 2 ) + light green ( 'switch  mine  -f  ' ) ) + '  will  prompt  to  enter  the  filter.\n' ) usage += ( ( ( s * 3 ) + light yellow ( ' Only   nicks' ) ) + '  filter  will  decide  nicks  will  be  INCLUDE  ONLY.\n' ) usage += ( ( ( s * 3 ) + light yellow ( ' Ignore   nicks' ) ) + '  filter  will  decide  nicks  will  be  EXCLUDE.\n' ) usage += ( ( ( s * 2 ) + light green ( 'switch  list' ) ) + "  will  switch  to  a   Twitter   list's  stream.   You   will  be  asked  for  list  name\n" ) print Nicely  ( usage ) 
def  get test cluster ( reactor ) : control node = environ . get ( 'FLOCKER ACCEPTANCE CONTROL NODE' ) if ( control node is  None  ) : raise  Skip  Test  ( ( ' Set   acceptance  testing  control  node  IP  address  using  the  ' + 'FLOCKER ACCEPTANCE CONTROL NODE  environment  variable.' ) ) agent nodes env var = environ . get ( 'FLOCKER ACCEPTANCE NUM AGENT NODES' ) if ( agent nodes env var is  None  ) : raise  Skip  Test  ( ' Set   the  number  of  configured  acceptance  testing  nodes  using  the  FLOCKER ACCEPTANCE NUM AGENT NODES  environment  variable.' ) num agent nodes = int ( agent nodes env var ) certificates path =  File  Path  ( environ [ 'FLOCKER ACCEPTANCE API CERTIFICATES PATH' ] ) hostname to public address env var = environ . get ( 'FLOCKER ACCEPTANCE HOSTNAME TO PUBLIC ADDRESS' , '{}' ) hostname to public address = json . loads ( hostname to public address env var ) return connected cluster ( reactor , control node , certificates path , num agent nodes , hostname to public address ) 
def add edited exploration id ( user id , exploration id ) : user contributions = get user contributions ( user id , strict =  False  ) if ( not user contributions ) : create user contributions ( user id , [ ] , [ exploration id ] ) elif ( exploration id not in user contributions . edited exploration ids ) : user contributions . edited exploration ids . append ( exploration id ) user contributions . edited exploration ids . sort ( )  save user contributions ( user contributions ) 
def get key ( key , host =  None  , port =  None  , db =  None  , password =  None  ) : server =  connect ( host , port , db , password ) return server . get ( key ) 
def  check guts eq ( attr , old , new , last build ) : if ( old != new ) : logger . info ( ' Building   because  %s  changed' , attr ) return  True  return  False  
def clear time override ( ) : utcnow . override time =  None  
def version is compatible ( imp version , version ) : version parts = version . split ( '.' ) imp version parts = imp version . split ( '.' ) if ( int ( version parts [ 0 ] ) != int ( imp version parts [ 0 ] ) ) : return  False  if ( int ( version parts [ 1 ] ) > int ( imp version parts [ 1 ] ) ) : return  False  return  True  
@ command ( '(mv|sw)\\s*(\\d{1,4})\\s*[\\s,]\\s*(\\d{1,4})' ) def songlist mv sw ( action , a , b ) : ( i , j ) = ( ( int ( a ) - 1 ) , ( int ( b ) - 1 ) ) if ( action == 'mv' ) : g . model . songs . insert ( j , g . model . songs . pop ( i ) ) g . message = ( util . F ( 'song  move' ) % ( g . model [ j ] . title , b ) ) elif ( action == 'sw' ) : ( g . model [ i ] , g . model [ j ] ) = ( g . model [ j ] , g . model [ i ] ) g . message = ( util . F ( 'song  sw' ) % ( min ( a , b ) , max ( a , b ) ) ) g . content = content . generate songlist display ( ) 
def role list ( profile =  None  , ** connection args ) : kstone = auth ( profile , ** connection args ) ret = { } for role in kstone . roles . list ( ) : ret [ role . name ] = dict ( ( ( value , getattr ( role , value ) ) for value in dir ( role ) if ( ( not value . startswith ( ' ' ) ) and isinstance ( getattr ( role , value ) , ( six . text type , dict , bool , str ) ) ) ) ) return ret 
@ bp . route ( '/delete' , methods = [ 'GET' , 'POST' ] ) @ require login def delete ( ) : return 'not  ready' 
def get loaded rules ( rules paths ) : for path in rules paths : if ( path . name != '  init  .py' ) : rule =  Rule  . from path ( path ) if rule . is enabled : ( yield rule ) 
def server message ( response ) : message =  None  if ( response . headers . get ( 'content-type' ) and ( 'text/html' in response . headers . get ( 'content-type' ) ) ) : try : soup =  Beautiful  Soup  ( response . content , 'html5lib' ) except  Exception  : pass message = soup . find ( 'body' ) elements = ( 'header' , 'script' , 'footer' , 'nav' , 'input' , 'textarea' ) for element in elements : for tag in soup . find all ( element ) : tag . replace With  ( '' ) message = ( message . text if message else soup . text ) message = message . strip ( ) if ( message is  None  ) : message = response . content . strip ( ) if message : if ( len ( message ) > 150 ) : message = ( message [ : 150 ] + '...' ) logger . debug ( ' Server   responded  with  message:  %s' , message ) 
def batch shuffle ( index array , batch size ) : batch count = int ( ( len ( index array ) / batch size ) ) last batch = index array [ ( batch count * batch size ) : ] index array = index array [ : ( batch count * batch size ) ] index array = index array . reshape ( ( batch count , batch size ) ) np . random . shuffle ( index array ) index array = index array . flatten ( ) return np . append ( index array , last batch ) 
def  cond ( condition , then lambda , else lambda ) : try : cond fn = tf . cond except  Attribute  Error  : from tensorflow . python . ops import control flow ops cond fn = control flow ops . cond return cond fn ( condition , then lambda , else lambda ) 
def migrate off ( ) : print green ( ( '%s:   Disabling    Migrations ' % env . host ) ) with cd ( '/home/web2py/applications/eden/models/' ) : run ( "sed  -i  's/deployment settings.base.migrate  =   True /deployment settings.base.migrate  =   False /'  000 config.py" , pty =  True  ) 
def get interface language ( ) : try : locale language = locale . getdefaultlocale ( ) [ 0 ] except  Value  Error  : locale language = DEFAULT LANGUAGE language = DEFAULT LANGUAGE if ( locale language is not  None  ) : spyder languages = get available translations ( ) for lang in spyder languages : if ( locale language == lang ) : language = locale language break elif ( locale language . startswith ( lang ) or lang . startswith ( locale language ) ) : language = lang break return language 
def load host keys ( filename ) : from paramiko . hostkeys import  Host  Keys  return  Host  Keys  ( filename ) 
def addHTML Listings  ( document , dir ) : for node in domhelpers . find Elements  With  Attribute  ( document , 'class' , 'html-listing' ) : filename = node . get Attribute  ( 'href' ) val = ( '<pre  class="htmlsource">\n%s</pre>' % cgi . escape ( open ( os . path . join ( dir , filename ) ) . read ( ) ) )  replace With  Listing  ( node , val , filename , 'html-listing' ) 
def renumerate ( it ) : return zip ( xrange ( ( len ( it ) - 1 ) , ( - 1 ) , ( - 1 ) ) , reversed ( it ) ) 
def examplesquick Test  ( vm , prompt =  Prompt  ) : install Pexpect  ( vm , prompt ) vm . sendline ( 'sudo  -n  python  ~/mininet/examples/test/runner.py  -v  -quick' ) 
def get backend api ( cluster id ) : backend config filename = environ . get ( 'FLOCKER ACCEPTANCE TEST VOLUME BACKEND CONFIG' ) if ( backend config filename is  None  ) : raise  Skip  Test  ( ' This   test  requires  the  ability  to  construct  an  I Block  Device API  in  order  to  verify  construction.   Please   set  FLOCKER ACCEPTANCE TEST VOLUME BACKEND CONFIG  to  a  yaml  filepath  with  the  dataset  configuration.' ) backend name = environ . get ( 'FLOCKER ACCEPTANCE VOLUME BACKEND' ) if ( backend name is  None  ) : raise  Skip  Test  ( ( ' Set   acceptance  testing  volume  backend  using  the  ' + 'FLOCKER ACCEPTANCE VOLUME BACKEND  environment  variable.' ) ) if ( backend name in ( 'loopback' , 'zfs' ) ) : raise  Skip  Test  ( " The   loopback  backend  API  can't  be  used  remotely." ) backend config filepath =  File  Path  ( backend config filename ) full backend config = yaml . safe load ( backend config filepath . get Content  ( ) ) backend config = full backend config . get ( backend name ) if ( 'backend' in backend config ) : backend config . pop ( 'backend' ) backend = backend loader . get ( backend name ) return get api ( backend , pmap ( backend config ) , reactor , cluster id ) 
def unique ( values ) : values = com .  asarray tuplesafe ( values ) f = ( lambda htype , caster :  unique object ( values , htype , caster ) ) return  hashtable algo ( f , values ) 
def test make imbalance invalid ratio ( ) : y  = np . zeros ( ( X . shape [ 0 ] , ) ) y  [ 0 ] = 1 ratio = 0.5 assert raises (  Value  Error  , make imbalance , X , y  , ratio ) 
def  get version from pkg info ( package name ) : try : pkg info file = open ( 'PKG-INFO' , 'r' ) except ( IO Error  , OS Error  ) : return  None  try : pkg info = email . message from file ( pkg info file ) except email .  Message  Error  : return  None  if ( pkg info . get ( ' Name ' ,  None  ) != package name ) : return  None  return pkg info . get ( ' Version ' ,  None  ) 
def strftime ( fmt , t =  None  ) : if ( not fmt ) : return u'' if ( t is  None  ) : t = time . localtime ( ) if hasattr ( t , 'timetuple' ) : t = t . timetuple ( ) early year = ( t [ 0 ] < 1900 ) if early year : replacement = ( 1900 if ( ( t [ 0 ] % 4 ) == 0 ) else 1901 ) fmt = fmt . replace ( '%Y' , ' early  year  hack##' ) t = list ( t ) orig year = t [ 0 ] t [ 0 ] = replacement ans =  None  if iswindows : if isinstance ( fmt , unicode ) : fmt = fmt . encode ( 'mbcs' ) fmt = fmt . replace ( '%e' , '%#d' ) ans = plugins [ 'winutil' ] [ 0 ] . strftime ( fmt , t ) else : ans = time . strftime ( fmt , t ) . decode ( preferred encoding , 'replace' ) if early year : ans = ans . replace ( ' early  year  hack##' , str ( orig year ) ) return ans 
def deactivate Aa  Pdp  Context  Accept  ( ) : a =  Tp  Pd  ( pd = 8 ) b =  Message  Type  ( mes Type  = 84 ) packet = ( a / b ) return packet 
def match str ( filter str , dct ) : return all ( (  match one ( filter part , dct ) for filter part in filter str . split ( u'&' ) ) ) 
def service Lanes  ( stacks , duration = 1.0 ) : start = time . time ( ) while ( ( start + duration ) > time . time ( ) ) : for stack in stacks : stack . service All  ( ) if all ( [ ( not stack . tx Msgs  ) for stack in stacks ] ) : console . terse ( ' Service   stacks  done  normally\n' ) break time . sleep ( 0.05 ) for stack in stacks : console . terse ( ' Stack   {0}  remotes:  {1}\n' . format ( stack . name , stack . name Remotes  ) ) console . terse ( ' Service   stacks  exit\n' ) 
def sproot ( tck , mest = 10 ) : ( t , c , k ) = tck if ( k != 3 ) : raise  Value  Error  ( 'sproot  works  only  for  cubic  (k=3)  splines' ) try : c [ 0 ] [ 0 ] parametric =  True  except : parametric =  False  if parametric : return list ( map ( ( lambda c , t = t , k = k , mest = mest : sproot ( [ t , c , k ] , mest ) ) , c ) ) else : if ( len ( t ) < 8 ) : raise  Type  Error  ( ( ' The   number  of  knots  %d>=8' % len ( t ) ) ) ( z , ier ) =  fitpack .  sproot ( t , c , k , mest ) if ( ier == 10 ) : raise  Type  Error  ( ' Invalid   input  data.  t1<=..<=t4<t5<..<tn-3<=..<=tn  must  hold.' ) if ( ier == 0 ) : return z if ( ier == 1 ) : warnings . warn (  Runtime  Warning  ( ' The   number  of  zeros  exceeds  mest' ) ) return z raise  Type  Error  ( ' Unknown   error' ) 
def  req json rpc ( url , session id , rpcmethod , subsystem , method , ** params ) : data = json . dumps ( { 'jsonrpc' : '2.0' , 'id' : 1 , 'method' : rpcmethod , 'params' : [ session id , subsystem , method , params ] } ) try : res = requests . post ( url , data = data , timeout = 5 ) except requests . exceptions .  Timeout  : return if ( res . status code == 200 ) : response = res . json ( ) if ( rpcmethod == 'call' ) : return response [ 'result' ] [ 1 ] else : return response [ 'result' ] 
def   Replacement  write data ( writer , data , is attrib =  False  ) : data = data . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) data = data . replace ( '"' , '&quot;' ) . replace ( '>' , '&gt;' ) if is attrib : data = data . replace ( '\r' , '&#xD;' ) . replace ( '\n' , '&#xA;' ) . replace ( ' DCTB ' , '&#x9;' ) writer . write ( data ) 
def lint file ( filename , apply fix , verbose ) : errors = [ ] nodes changed = 0 filters = [  Pronoun  Filter  ,  Ternary  Filter  ,  Always  Plural  Filter  ,  Plural  Filter  ,  An  Filter  ] if ERROR AMBIGUOUS PLURALS : filters . append (  Ambiguous  Plural  Filter  ) html tree = lxml . html . html5parser . parse ( filename , parser = PARSER ) nodes =  extract nodes ( html tree , filename ) root tree = ( nodes [ 0 ] . getroottree ( ) if nodes else  None  ) bad nodes = (  REJECT NODES +  CANNOT CONTAIN NODES ) lint expr = '|' . join ( [ ( './/%s' % name ) for name in bad nodes ] ) for node in nodes : lint nodes = node . xpath ( lint expr ) for lint node in lint nodes : errors . append ( ( ' Contains   invalid  node:\n%s\n Invalid   node:\n%s' % (  get outerhtml ( node ) ,  get outerhtml ( lint node ) ) ) )  IS SINGULAR = '*[contains(@data-if,"is Singular ")]' search expr = ( '//%s[not(%s)][not(./%s)]' % (  IS SINGULAR ,  HAS TEXT ,  IS SINGULAR ) ) non bottom level issingular = html tree . xpath ( search expr ) for lint node in non bottom level issingular : errors . append ( ( "'is Singular '  nodes  must  contain  text  directly;  distribute  this  node  into  its  children:\n%s" %  get outerhtml ( lint node ) ) ) for filter class in filters : filter = filter class ( ) ( new nodes , new errors , new nodes changed ) = filter . process ( nodes ) nodes = new nodes errors += new errors nodes changed += new nodes changed text nodes = root tree . xpath ( '|' . join (  VAR NODES ) ) filter =  String  In  Var  Filter  ( ) ( new nodes , new errors , new nodes changed ) = filter . process ( text nodes ) nodes = new nodes errors += new errors nodes changed += new nodes changed text nodes = root tree . xpath ( '|' . join (  TEXT NODES ) ) filter =  Math  Jax  Text  Filter  ( ) ( new nodes , new errors , new nodes changed ) = filter . process ( text nodes ) nodes = new nodes errors += new errors nodes changed += new nodes changed if nodes changed : if apply fix : with open ( filename , 'w' ) as f : f . write ( get page html ( root tree ) ) else : errors . append ( ( '%s  node%s  need  to  be  fixed.   Re -run  with  --fix  to  automatically  fix  them.' % ( nodes changed , ( '' if ( nodes changed == 1 ) else 's' ) ) ) ) return ( errors , nodes changed ) 
def can file be synced on current platform ( path ) : can be synced =  True  fullpath = os . path . join ( os . environ [ 'HOME' ] , path ) library path = os . path . join ( os . environ [ 'HOME' ] , ' Library /' ) if ( platform . system ( ) == constants . PLATFORM LINUX ) : if fullpath . startswith ( library path ) : can be synced =  False  return can be synced 
def get restart freeze ( ) : ret = salt . utils . mac utils . execute return result ( 'systemsetup  -getrestartfreeze' ) return ( salt . utils . mac utils . validate enabled ( salt . utils . mac utils . parse return ( ret ) ) == 'on' ) 
def fix Set  Group ID ( child Path  ) : if ( platform . system ( ) == u' Windows ' ) : return parent Path  = ek ( os . path . dirname , child Path  ) parent Stat  = ek ( os . stat , parent Path  ) parent Mode  = stat . S IMODE ( parent Stat  [ stat . ST MODE ] ) child Path  = ek ( os . path . join , parent Path  , ek ( os . path . basename , child Path  ) ) if ( parent Mode  & stat . S ISGID ) : parentGID = parent Stat  [ stat . ST GID ] child Stat  = ek ( os . stat , child Path  . encode ( sickbeard . SYS ENCODING ) ) childGID = child Stat  [ stat . ST GID ] if ( childGID == parentGID ) : return child Path  owner = child Stat  . st uid user id = os . geteuid ( ) if ( ( user id != 0 ) and ( user id != child Path  owner ) ) : logger . log ( ( ( u' Not   running  as  root  or  owner  of  ' + child Path  ) + u',  not  trying  to  set  the  set-group-ID' ) , logger . DEBUG ) return try : ek ( os . chown , child Path  , ( - 1 ) , parentGID ) logger . log ( u' Respecting   the  set-group-ID  bit  on  the  parent  directory  for  {0}' . format ( child Path  ) , logger . DEBUG ) except OS Error  : logger . log ( u' Failed   to  respect  the  set-group-ID  bit  on  the  parent  directory  for  {0}  (setting  group  ID  {1})' . format ( child Path  , parentGID ) , logger . ERROR ) 
def merge grids ( grid1 , grid2 ) : ( keygen , outkeys , outdata ) = ( count ( 1 ) , [ ] , dict ( ) ) for ingrid in [ grid1 , grid2 ] : for ( index , key ) in enumerate ( ingrid [ 'keys' ] ) : if ( key not in ingrid [ 'data' ] ) : outkeys . append ( '' ) continue outkey = ( '%d' % keygen . next ( ) ) outkeys . append ( outkey ) datum = ingrid [ 'data' ] [ key ] outdata [ outkey ] = datum ( offset , outgrid ) = ( len ( grid1 [ 'keys' ] ) , [ ] ) def newchar ( char1 , char2 ) : '   Return   a  new  encoded  character  based  on  two  inputs.\n                ' ( id1 , id2 ) = ( decode char ( char1 ) , decode char ( char2 ) ) if ( grid2 [ 'keys' ] [ id2 ] == '' ) : return encode id ( id1 ) else : return encode id ( ( id2 + offset ) ) for ( row1 , row2 ) in zip ( grid1 [ 'grid' ] , grid2 [ 'grid' ] ) : outrow = [ newchar ( c1 , c2 ) for ( c1 , c2 ) in zip ( row1 , row2 ) ] outgrid . append ( '' . join ( outrow ) ) return dict ( keys = outkeys , data = outdata , grid = outgrid ) 
def normalize argv ( args , size = 0 ) : args = list ( args ) for ( idx , val ) in enumerate ( args ) : if ( to int ( val ) is not  None  ) : args [ idx ] = to int ( val ) if ( size and ( idx == size ) ) : return args [ : idx ] if ( size == 0 ) : return args for i in range ( len ( args ) , size ) : args += [  None  ] return args 
def series export word ( widget list , lang dict , title , logo ) : output =  String IO ( ) doc = pyrtf .  Document  ( default language = pyrtf .  Languages  .  English UK ) section = pyrtf .  Section  ( ) ss = doc .  Style  Sheet  ps = ss .  Paragraph  Styles  .  Normal  .  Copy  ( ) ps .  Set  Name  ( ' Normal  Grey ' ) ps .  Set  Shading  Property  Set  ( pyrtf .  Shading  Property  Set  ( pattern = 1 , background = pyrtf .  Colour  ( 'grey  light' , 224 , 224 , 224 ) ) ) ss .  Paragraph  Styles  . append ( ps ) ps = ss .  Paragraph  Styles  .  Normal  .  Copy  ( ) ps .  Set  Name  ( ' Normal  Centre ' ) ps .  Set  Paragraph  Property  Set  ( pyrtf .  Paragraph  Property  Set  ( alignment = 3 ) ) ss .  Paragraph  Styles  . append ( ps ) doc .  Sections  . append ( section ) heading = pyrtf .  Paragraph  ( ss .  Paragraph  Styles  .  Heading 1 ) if logo : image = pyrtf .  Image  ( logo ) heading . append ( image ) heading . append ( title ) section . append ( heading ) col = [ 2800 , 6500 ] table = pyrtf .  Table  ( * col )  Add  Row  = table .  Add  Row  sorted widget list = sorted ( widget list . values ( ) , key = ( lambda widget : widget . question . posn ) ) for widget in sorted widget list : line = widget . write Question  To RTF ( ss , lang dict ) try :  Add  Row  ( * line ) except : if settings . base . debug : raise pass section . append ( table ) renderer = pyrtf .  Renderer  ( ) renderer .  Write  ( doc , output ) return output 
def get blocks with unallocated ( module , cp driver , lb driver , network domain ) : total unallocated ips = 0 all blocks = list public ip blocks ( module , cp driver , network domain ) unalloc blocks = [ ] unalloc addresses = [ ] for block in all blocks : d blocks = get block allocation ( module , cp driver , lb driver , network domain , block ) i = 0 for addr in d blocks [ 'addresses' ] : if ( addr [ 'allocated' ] is  False  ) : if ( i == 0 ) : unalloc blocks . append ( d blocks ) unalloc addresses . append ( addr [ 'address' ] ) total unallocated ips += 1 i += 1 return { 'unallocated count' : total unallocated ips , 'ip blocks' : unalloc blocks , 'unallocated addresses' : unalloc addresses } 
def get cpu temp ( ) : res = os . popen ( 'vcgencmd  measure temp' ) . readline ( ) t cpu = float ( res . replace ( 'temp=' , '' ) . replace ( "'C\n" , '' ) ) return t cpu 
def get page ( url , user agent =  None  ) : if ( user agent is  None  ) : user agent = USER AGENT request =  Request  ( url ) request . add header ( ' User - Agent ' , USER AGENT ) cookie jar . add cookie header ( request ) response = urlopen ( request ) cookie jar . extract cookies ( response , request ) html = response . read ( ) response . close ( ) cookie jar . save ( ) return html 
def print col ( text , color ) : if use color : fg =  esc ( fg colors [ color . lower ( ) ] ) reset =  esc ( fg colors [ 'reset' ] ) print '' . join ( [ fg , text , reset ] ) else : print text 
def  unpack ipv4 ( ip str ) : if ( not ip str . lower ( ) . startswith ( '0000:0000:0000:0000:0000:ffff:' ) ) : return  None  hextets = ip str . split ( ':' ) return hextets [ ( - 1 ) ] 
def code analysis ( app dir , md5 , perms , typ ) : try : print '[INFO]   Static    Android    Code    Analysis    Started ' code = { key : [ ] for key in ( 'inf act' , 'inf ser' , 'inf bro' , 'log' , 'fileio' , 'rand' , 'd hcode' , 'd app tamper' , 'dex cert' , 'dex tamper' , 'd rootcheck' , 'd root' , 'd ssl pin' , 'dex root' , 'dex debug key' , 'dex debug' , 'dex debug con' , 'dex emulator' , 'd prevent screenshot' , 'd prevent tapjacking' , 'd webviewdisablessl' , 'd webviewdebug' , 'd sensitive' , 'd ssl' , 'd sqlite' , 'd con world readable' , 'd con world writable' , 'd con private' , 'd extstorage' , 'd tmpfile' , 'd jsenabled' , 'gps' , 'crypto' , 'exec' , 'server socket' , 'socket' , 'datagramp' , 'datagrams' , 'ipc' , 'msg' , 'webview addjs' , 'webview' , 'webviewget' , 'webviewpost' , 'httpcon' , 'urlcon' , 'jurl' , 'httpsurl' , 'nurl' , 'httpclient' , 'notify' , 'cellinfo' , 'cellloc' , 'subid' , 'devid' , 'softver' , 'simserial' , 'simop' , 'opname' , 'contentq' , 'refmethod' , 'obf' , 'gs' , 'bencode' , 'bdecode' , 'dex' , 'mdigest' , 'sqlc password' , 'd sql cipher' , 'd con world rw' , 'ecb' , 'rsa no pad' , 'weak iv' ) } crypto =  False  obfus =  False  reflect =  False  dynamic =  False  native =  False  email n file = '' url n file = '' url list = list ( ) domains = dict ( ) if ( typ == 'apk' ) : java src = os . path . join ( app dir , 'java source/' ) elif ( typ == 'studio' ) : java src = os . path . join ( app dir , 'app/src/main/java/' ) elif ( typ == 'eclipse' ) : java src = os . path . join ( app dir , 'src/' ) print ( '[INFO]   Code    Analysis    Started   on  -  ' + java src ) for ( dir name , sub dir , files ) in os . walk ( java src ) : for jfile in files : jfile path = os . path . join ( java src , dir name , jfile ) if ( '+' in jfile ) : p 2 = os . path . join ( java src , dir name , jfile . replace ( '+' , 'x' ) ) shutil . move ( jfile path , p 2 ) jfile path = p 2 repath = dir name . replace ( java src , '' ) if ( jfile . endswith ( '.java' ) and ( any ( ( ( cls in repath ) for cls in settings . SKIP CLASSES ) ) is  False  ) ) : dat = '' with io . open ( jfile path , mode = 'r' , encoding = 'utf8' , errors = 'ignore' ) as file pointer : dat = file pointer . read ( ) urls = [ ] emails = [ ] if ( re . findall ( 'MODE WORLD READABLE| Context \\.MODE WORLD READABLE' , dat ) or re . findall ( 'open File  Output \\(\\s*".+"\\s*,\\s*1\\s*\\)' , dat ) ) : code [ 'd con world readable' ] . append ( jfile path . replace ( java src , '' ) ) if ( re . findall ( 'MODE WORLD WRITABLE| Context \\.MODE WORLD WRITABLE' , dat ) or re . findall ( 'open File  Output \\(\\s*".+"\\s*,\\s*2\\s*\\)' , dat ) ) : code [ 'd con world writable' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'open File  Output \\(\\s*".+"\\s*,\\s*3\\s*\\)' , dat ) : code [ 'd con world rw' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'MODE PRIVATE| Context \\.MODE PRIVATE' , dat ) : code [ 'd con private' ] . append ( jfile path . replace ( java src , '' ) ) if ( any ( ( ( 'WRITE EXTERNAL STORAGE' in perm ) for perm in perms ) ) and ( ( '.get External  Storage ' in dat ) or ( '.get External  Files  Dir (' in dat ) ) ) : code [ 'd extstorage' ] . append ( jfile path . replace ( java src , '' ) ) if ( any ( ( ( 'WRITE EXTERNAL STORAGE' in perm ) for perm in perms ) ) and ( '.create Temp  File (' in dat ) ) : code [ 'd tmpfile' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'set Java  Script  Enabled (true)' in dat ) and ( '.add Javascript  Interface (' in dat ) ) : code [ 'd jsenabled' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( '.set Web  Contents  Debugging  Enabled (true)' in dat ) and ( ' Web  View ' in dat ) ) : code [ 'd webviewdebug' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'on Received  Ssl  Error ( Web  View ' in dat ) and ( '.proceed();' in dat ) ) : code [ 'd webviewdisablessl' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ( 'raw Query (' in dat ) or ( 'execSQL(' in dat ) ) and ( 'android.database.sqlite' in dat ) ) : code [ 'd sqlite' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'javax.net.ssl' in dat ) and ( ( ' Trust  All SSL Socket - Factory ' in dat ) or ( ' All  Trust SSL Socket  Factory ' in dat ) or ( ' Non  Validating SSL Socket  Factory ' in dat ) or ( 'ALLOW ALL HOSTNAME VERIFIER' in dat ) or ( '.set Default  Hostname  Verifier (' in dat ) or ( ' Null  Hostname  Verifier (' in dat ) ) ) : code [ 'd ssl' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'password  =  "' in dat . lower ( ) ) or ( 'secret  =  "' in dat . lower ( ) ) or ( 'username  =  "' in dat . lower ( ) ) or ( 'key  =  "' in dat . lower ( ) ) ) : code [ 'd sensitive' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Debug  Detector .is Debuggable ' in dat ) ) : code [ 'dex debug' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Debug  Detector .is Debugger  Connected ' in dat ) ) : code [ 'dex debug con' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Emulator  Detector .is Running  In  Emulator ' in dat ) ) : code [ 'dex emulator' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Debug  Detector .is Signed  With  Debug  Key ' in dat ) ) : code [ 'dex debug key' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Root  Detector .is Device  Rooted ' in dat ) ) : code [ 'dex root' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Tamper  Detector .check Apk ' in dat ) ) : code [ 'dex tamper' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'import  dexguard.util' in dat ) and ( ' Certificate  Checker .check Certificate ' in dat ) ) : code [ 'dex cert' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'org.thoughtcrime.ssl.pinning' in dat ) and ( ( ' Pinning  Helper .get Pinned  Https URL Connection ' in dat ) or ( ' Pinning  Helper .get Pinned  Http  Client ' in dat ) or ( ' Pinning SSL Socket  Factory (' in dat ) ) ) : code [ 'd ssl pin' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Package  Manager .GET SIGNATURES' in dat ) and ( 'get Package  Name (' in dat ) ) : code [ 'd app tamper' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'com.noshufou.android.su' in dat ) or ( 'com.thirdparty.superuser' in dat ) or ( 'eu.chainfire.supersu' in dat ) or ( 'com.koushikdutta.superuser' in dat ) or ( 'eu.chainfire.' in dat ) ) : code [ 'd root' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( '.contains("test-keys")' in dat ) or ( '/system/app/ Superuser .apk' in dat ) or ( 'is Device  Rooted ()' in dat ) or ( '/system/bin/failsafe/su' in dat ) or ( '/system/sd/xbin/su' in dat ) or ( '"/system/xbin/which",  "su"' in dat ) or ( ' Root  Tools .is Access  Given ()' in dat ) ) : code [ 'd rootcheck' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'java\\.util\\. Random ' , dat ) : code [ 'rand' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( ' Log \\.(v|d|i|w|e|f|s)| System \\.out\\.print' , dat ) : code [ 'log' ] . append ( jfile path . replace ( java src , '' ) ) if ( '.hash Code ()' in dat ) : code [ 'd hcode' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ( 'get Window ().set Flags (' in dat ) or ( 'get Window ().add Flags (' in dat ) ) and ( '.FLAG SECURE' in dat ) ) : code [ 'd prevent screenshot' ] . append ( jfile path . replace ( java src , '' ) ) if ( 'set Filter  Touches  When  Obscured (true)' in dat ) : code [ 'd prevent tapjacking' ] . append ( jfile path . replace ( java src , '' ) ) if ( 'SQ Lite  Open  Helper .get Writable  Database (' in dat ) : code [ 'sqlc password' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'SQ Lite  Database .load Libs (' in dat ) and ( 'net.sqlcipher.' in dat ) ) : code [ 'd sql cipher' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( ' Cipher \\.get Instance \\(\\s*"\\s*AES\\/ECB' , dat ) : code [ 'ecb' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'cipher\\.getinstance\\(\\s*"rsa/.+/nopadding' , dat . lower ( ) ) : code [ 'rsa no pad' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( '0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00' in dat ) or ( '0x01,0x02,0x03,0x04,0x05,0x06,0x07' in dat ) ) : code [ 'weak iv' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( ' System .load Library \\(| System .load\\(' , dat ) : native =  True  if re . findall ( 'dalvik.system. Dex  Class  Loader |java.security. Class  Loader |java.net.URL Class  Loader |java.security. Secure  Class  Loader ' , dat ) : dynamic =  True  if re . findall ( 'java.lang.reflect. Method |java.lang.reflect. Field | Class .for Name ' , dat ) : reflect =  True  if re . findall ( 'javax.crypto|kalium.crypto|bouncycastle.crypto' , dat ) : crypto =  True  code [ 'crypto' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'utils.AES Obfuscator ' in dat ) and ( 'get Obfuscator ' in dat ) ) : code [ 'obf' ] . append ( jfile path . replace ( java src , '' ) ) obfus =  True  if ( ( 'get Runtime ().exec(' in dat ) and ( 'get Runtime (' in dat ) ) : code [ 'exec' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Server  Socket ' in dat ) and ( 'net. Server  Socket ' in dat ) ) : code [ 'server socket' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Socket ' in dat ) and ( 'net. Socket ' in dat ) ) : code [ 'socket' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Datagram  Packet ' in dat ) and ( 'net. Datagram  Packet ' in dat ) ) : code [ 'datagramp' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Datagram  Socket ' in dat ) and ( 'net. Datagram  Socket ' in dat ) ) : code [ 'datagrams' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'I Remote  Service |I Remote  Service . Stub |I Binder | Intent ' , dat ) : code [ 'ipc' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ( 'send Multipart  Text  Message ' in dat ) or ( 'send Text  Message ' in dat ) or ( 'vnd.android-dir/mms-sms' in dat ) ) and ( 'telephony. Sms  Manager ' in dat ) ) : code [ 'msg' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'add Javascript  Interface ' in dat ) and ( ' Web  View ' in dat ) and ( 'android.webkit' in dat ) ) : code [ 'webview addjs' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Web  View ' in dat ) and ( 'load Data ' in dat ) and ( 'android.webkit' in dat ) ) : code [ 'webviewget' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ' Web  View ' in dat ) and ( 'post Url ' in dat ) and ( 'android.webkit' in dat ) ) : code [ 'webviewpost' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ( ' Http URL Connection ' in dat ) or ( 'org.apache.http' in dat ) ) and ( ( 'open Connection ' in dat ) or ( 'connect' in dat ) or ( ' Http  Request ' in dat ) ) ) : code [ 'httpcon' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'net.URL Connection ' in dat ) and ( ( 'connect' in dat ) or ( 'open Connection ' in dat ) or ( 'open Stream ' in dat ) ) ) : code [ 'urlcon' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'net. Jar URL Connection ' in dat ) and ( ( ' Jar URL Connection ' in dat ) or ( 'jar:' in dat ) ) ) : code [ 'jurl' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'javax.net.ssl. Https URL Connection ' in dat ) and ( ( ' Https URL Connection ' in dat ) or ( 'connect' in dat ) ) ) : code [ 'httpsurl' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'net.URL' and ( 'open Connection ' or 'open Stream ' ) ) in dat ) : code [ 'nurl' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'http.client. Http  Client |net.http. Android  Http  Client |http.impl.client. Abstract  Http  Client ' , dat ) : code [ 'httpclient' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'app. Notification  Manager ' in dat ) and ( 'notify' in dat ) ) : code [ 'notify' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get All  Cell  Info ' in dat ) ) : code [ 'cellinfo' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Cell  Location ' in dat ) ) : code [ 'cellloc' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Subscriber  Id ' in dat ) ) : code [ 'subid' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Device  Id ' in dat ) ) : code [ 'devid' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Device  Software  Version ' in dat ) ) : code [ 'softver' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Sim  Serial  Number ' in dat ) ) : code [ 'simserial' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Sim  Operator ' in dat ) ) : code [ 'simop' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'telephony. Telephony  Manager ' in dat ) and ( 'get Sim  Operator  Name ' in dat ) ) : code [ 'opname' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'content. Content  Resolver ' in dat ) and ( 'query' in dat ) ) : code [ 'contentq' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'java.lang.reflect. Method ' in dat ) and ( 'invoke' in dat ) ) : code [ 'refmethod' ] . append ( jfile path . replace ( java src , '' ) ) if ( 'get System  Service ' in dat ) : code [ 'gs' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'android.util. Base 64' in dat ) and ( ( '.encode To  String ' in dat ) or ( '.encode' in dat ) ) ) : code [ 'bencode' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'android.util. Base 64' in dat ) and ( '.decode' in dat ) ) : code [ 'bdecode' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( ( 'dalvik.system. Path  Class  Loader ' in dat ) or ( 'dalvik.system. Dex  File ' in dat ) or ( 'dalvik.system. Dex  Path  List ' in dat ) or ( 'dalvik.system. Dex  Class  Loader ' in dat ) ) and ( ( 'load Dex ' in dat ) or ( 'load Class ' in dat ) or ( ' Dex  Class  Loader ' in dat ) or ( 'load Dex  File ' in dat ) ) ) : code [ 'dex' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'java.security. Message  Digest ' in dat ) and ( ( ' Message  Digest  Spi ' in dat ) or ( ' Message  Digest ' in dat ) ) ) : code [ 'mdigest' ] . append ( jfile path . replace ( java src , '' ) ) if ( ( 'android.location' in dat ) and ( ( 'get Last  Known  Location (' in dat ) or ( 'request Location  Updates (' in dat ) or ( 'get Latitude (' in dat ) or ( 'get Longitude (' in dat ) ) ) : code [ 'gps' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( ' Open  File  Output |get Shared  Preferences | Shared  Preferences . Editor |get Cache  Dir |get External  Storage  State |open Or  Create  Database ' , dat ) : code [ 'fileio' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'start Activity \\(|start Activity  For  Result \\(' , dat ) : code [ 'inf act' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'start Service \\(|bind Service \\(' , dat ) : code [ 'inf ser' ] . append ( jfile path . replace ( java src , '' ) ) if re . findall ( 'send Broadcast \\(|send Ordered  Broadcast \\(|send Sticky  Broadcast \\(' , dat ) : code [ 'inf bro' ] . append ( jfile path . replace ( java src , '' ) ) j file = jfile path . replace ( java src , '' ) base fl = ntpath . basename ( j file ) pattern = re . compile ( u"((?:https?://|s?ftps?://|file://|javascript:|data:|www\\d{0,3}[.])[\\w().=/;,#:@?&~*+!$%\\'{}-]+)" , re . UNICODE ) urllist = re . findall ( pattern , dat . lower ( ) ) url list . extend ( urllist ) uflag = 0 for url in urllist : if ( url not in urls ) : urls . append ( url ) uflag = 1 if ( uflag == 1 ) : url n file += ( ( ( ( ( ( ( ( ( ( '<tr><td>' + '<br>' . join ( urls ) ) + "</td><td><a  href='../ View  Source /?file=" ) + escape ( j file ) ) + '&md5=' ) + md5 ) + '&type=' ) + typ ) + "'>" ) + escape ( base fl ) ) + '</a></td></tr>' ) regex = re . compile ( '[\\w.-]+@[\\w-]+\\.[\\w.]+' ) eflag = 0 for email in regex . findall ( dat . lower ( ) ) : if ( ( email not in emails ) and ( not email . startswith ( '//' ) ) ) : emails . append ( email ) eflag = 1 if ( eflag == 1 ) : email n file += ( ( ( ( ( ( ( ( ( ( '<tr><td>' + '<br>' . join ( emails ) ) + "</td><td><a  href='../ View  Source /?file=" ) + escape ( j file ) ) + '&md5=' ) + md5 ) + '&type=' ) + typ ) + "'>" ) + escape ( base fl ) ) + '</a></td></tr>' ) print '[INFO]   Performing    Malware    Check   on  extracted   Domains ' domains =  Malware  Check  ( url list ) print '[INFO]   Finished    Code    Analysis ,   Email   and  URL   Extraction ' api desc = { 'gps' : 'GPS   Location ' , 'crypto' : ' Crypto   ' , 'exec' : ' Execute    System    Command   ' , 'server socket' : 'TCP   Server    Socket   ' , 'socket' : 'TCP   Socket   ' , 'datagramp' : 'UDP   Datagram    Packet   ' , 'datagrams' : 'UDP   Datagram    Socket   ' , 'ipc' : ' Inter    Process    Communication   ' , 'msg' : ' Send   SMS  ' , 'webview addjs' : ' Web  View    Java  Script    Interface   ' , 'webview' : ' Web  View    Load   HTML/ Java  Script   ' , 'webviewget' : ' Web  View   GET   Request   ' , 'webviewpost' : ' Web  View   POST   Request   ' , 'httpcon' : 'HTTP   Connection   ' , 'urlcon' : 'URL   Connection   to  file/http/https/ftp/jar  ' , 'jurl' : 'JAR  URL   Connection   ' , 'httpsurl' : 'HTTPS   Connection   ' , 'nurl' : 'URL   Connection   supports  file,http,https,ftp  and  jar  ' , 'httpclient' : 'HTTP   Requests ,   Connections   and   Sessions   ' , 'notify' : ' Android    Notifications   ' , 'cellinfo' : ' Get    Cell    Information   ' , 'cellloc' : ' Get    Cell    Location   ' , 'subid' : ' Get    Subscriber   ID  ' , 'devid' : ' Get    Device   ID,  IMEI,MEID/ESN  etc.  ' , 'softver' : ' Get    Software    Version ,  IMEI/SV  etc.  ' , 'simserial' : ' Get   SIM   Serial    Number   ' , 'simop' : ' Get   SIM   Provider    Details   ' , 'opname' : ' Get   SIM   Operator    Name   ' , 'contentq' : ' Query    Database   of  SMS,   Contacts   etc.  ' , 'refmethod' : ' Java    Reflection    Method    Invocation   ' , 'obf' : ' Obfuscation   ' , 'gs' : ' Get    System    Service   ' , 'bencode' : ' Base 64   Encode   ' , 'bdecode' : ' Base 64   Decode   ' , 'dex' : ' Load   and   Manipulate    Dex    Files   ' , 'mdigest' : ' Message    Digest   ' , 'fileio' : ' Local    File   I/O   Operations ' , 'inf act' : ' Starting    Activity ' , 'inf ser' : ' Starting    Service ' , 'inf bro' : ' Sending    Broadcast ' } html = '' for api key in api desc : if code [ api key ] : link = '' h d = ( ( '<tr><td>' + api desc [ api key ] ) + '</td><td>' ) for elem in code [ api key ] : link += ( ( ( ( ( ( ( ( "<a  href='../ View  Source /?file=" + escape ( elem ) ) + '&md5=' ) + md5 ) + '&type=' ) + typ ) + "'>" ) + escape ( ntpath . basename ( elem ) ) ) + '</a>  ' ) html += ( ( h d + link ) + '</td></tr>' ) desc = { 'd sensitive' : ' Files   may  contain  hardcoded  sensitive  informations  like  usernames,  passwords,  keys  etc.' , 'd ssl' : ' Insecure    Implementation   of  SSL.   Trusting   all  the  certificates  or  accepting  self  signed  certificates  is  a  critical   Security    Hole .   This   application  is  vulnerable  to  MITM  attacks' , 'd sqlite' : ' App   uses  SQ Lite    Database   and  execute  raw  SQL  query.   Untrusted   user  input  in  raw  SQL  queries  can  cause  SQL   Injection .   Also   sensitive  information  should  be  encrypted  and  written  to  the  database.' , 'd con world readable' : ' The   file  is   World    Readable .   Any    App   can  read  from  the  file' , 'd con world writable' : ' The   file  is   World    Writable .   Any    App   can  write  to  the  file' , 'd con world rw' : ' The   file  is   World    Readable   and   Writable .   Any    App   can  read/write  to  the  file' , 'd con private' : ' App   can  write  to   App    Directory .   Sensitive    Information   should  be  encrypted.' , 'd extstorage' : ' App   can  read/write  to   External    Storage .   Any    App   can  read  data  written  to   External    Storage .' , 'd tmpfile' : ' App   creates  temp  file.   Sensitive   information  should  never  be  written  into  a  temp  file.' , 'd jsenabled' : ' Insecure    Web  View    Implementation .   Execution   of  user  controlled  code  in   Web  View   is  a  critical   Security    Hole .' , 'd webviewdisablessl' : ' Insecure    Web  View    Implementation .   Web  View   ignores  SSL   Certificate   errors  and  accept  any  SSL   Certificate .   This   application  is  vulnerable  to  MITM  attacks' , 'd webviewdebug' : ' Remote    Web  View   debugging  is  enabled.' , 'dex debug' : ' Dex  Guard    Debug    Detection   code  to  detect  wheather  an   App   is  debuggable  or  not  is  identified.' , 'dex debug con' : ' Dex  Guard    Debugger    Detection   code  is  identified.' , 'dex debug key' : ' Dec  Guard   code  to  detect  wheather  the   App   is  signed  with  a  debug  key  or  not  is  identified.' , 'dex emulator' : ' Dex  Guard    Emulator    Detection   code  is  identified.' , 'dex root' : ' Dex  Guard    Root    Detection   code  is  identified.' , 'dex tamper' : ' Dex  Guard    App    Tamper    Detection   code  is  identified.' , 'dex cert' : ' Dex  Guard    Signer    Certificate    Tamper    Detection   code  is  identified.' , 'd ssl pin' : '   This    App   uses  an  SSL   Pinning    Library   (org.thoughtcrime.ssl.pinning)  to  prevent  MITM  attacks  in  secure  communication  channel.' , 'd root' : ' This    App   may  request  root  ( Super    User )  privileges.' , 'd rootcheck' : ' This    App   may  have  root  detection  capabilities.' , 'd hcode' : " This    App   uses   Java    Hash    Code .   It 's  a  weak  hash  function  and  should  never  be  used  in   Secure    Crypto    Implementation ." , 'rand' : ' The    App   uses  an  insecure   Random    Number    Generator .' , 'log' : ' The    App   logs  information.   Sensitive   information  should  never  be  logged.' , 'd app tamper' : ' The    App   may  use  package  signature  for  tamper  detection.' , 'd prevent screenshot' : ' This    App   has  capabilities  to  prevent  against   Screenshots   from   Recent    Task    History /   Now    On    Tap   etc.' , 'd prevent tapjacking' : ' This   app  has  capabilities  to  prevent  tapjacking  attacks.' , 'd sql cipher' : ' This    App   uses  SQL   Cipher .  SQL Cipher   provides  256-bit  AES  encryption  to  sqlite  database  files.' , 'sqlc password' : ' This    App   uses  SQL   Cipher .   But   the  secret  may  be  hardcoded.' , 'ecb' : ' The    App   uses  ECB  mode  in   Cryptographic   encryption  algorithm.  ECB  mode  is  known  to  be  weak  as  it  results  in  the  same  ciphertext  for  identical  blocks  of  plaintext.' , 'rsa no pad' : ' This    App   uses  RSA   Crypto   without  OAEP  padding.   The   purpose  of  the  padding  scheme  is  to  prevent  a  number  of  attacks  on  RSA  that  only  work  when  the  encryption  is  performed  without  padding.' , 'weak iv' : ' The    App   may  use  weak  I Vs   like  "0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00"  or  "0x01,0x02,0x03,0x04,0x05,0x06,0x07".   Not   using  a  random  IV  makes  the  resulting  ciphertext  much  more  predictable  and  susceptible  to  a  dictionary  attack.' } dang = '' spn dang = '<span  class="label  label-danger">high</span>' spn info = '<span  class="label  label-info">info</span>' spn sec = '<span  class="label  label-success">secure</span>' spn warn = '<span  class="label  label-warning">warning</span>' for k in desc : if code [ k ] : link = '' if re . findall ( 'd con private|log' , k ) : h d = ( ( ( ( '<tr><td>' + desc [ k ] ) + '</td><td>' ) + spn info ) + '</td><td>' ) elif re . findall ( 'd sql cipher|d prevent screenshot|d prevent tapjacking|d app tamper|d rootcheck|dex cert|dex tamper|dex debug|dex debug con|dex debug key|dex emulator|dex root|d ssl pin' , k ) : h d = ( ( ( ( '<tr><td>' + desc [ k ] ) + '</td><td>' ) + spn sec ) + '</td><td>' ) elif re . findall ( 'd jsenabled' , k ) : h d = ( ( ( ( '<tr><td>' + desc [ k ] ) + '</td><td>' ) + spn warn ) + '</td><td>' ) else : h d = ( ( ( ( '<tr><td>' + desc [ k ] ) + '</td><td>' ) + spn dang ) + '</td><td>' ) for elem in code [ k ] : link += ( ( ( ( ( ( ( ( "<a  href='../ View  Source /?file=" + escape ( elem ) ) + '&md5=' ) + md5 ) + '&type=' ) + typ ) + "'>" ) + escape ( ntpath . basename ( elem ) ) ) + '</a>  ' ) dang += ( ( h d + link ) + '</td></tr>' ) code an dic = { 'api' : html , 'dang' : dang , 'urls' : url n file , 'domains' : domains , 'emails' : email n file , 'crypto' : crypto , 'obfus' : obfus , 'reflect' : reflect , 'dynamic' : dynamic , 'native' : native } return code an dic except :  Print  Exception  ( '[ERROR]   Performing    Code    Analysis ' ) 
def  make subtarget ( targetctx , flags ) : subtargetoptions = { } if flags . boundcheck : subtargetoptions [ 'enable boundcheck' ] =  True  if flags . nrt : subtargetoptions [ 'enable nrt' ] =  True  error model = callconv . create error model ( flags . error model , targetctx ) subtargetoptions [ 'error model' ] = error model return targetctx . subtarget ( ** subtargetoptions ) 
def  restore service ( service ) :  apply service ( service ,  Sonos  Device  . restore ) 
def  Get  Registered  Help  File  ( help Desc  ) : try : return  Get  Registry  Default  Value  ( ( (  Build  Default  Python  Key  ( ) + '\\ Help \\' ) + help Desc  ) ) except win32api . error : try : return  Get  Registry  Default  Value  ( ( (  Build  Default  Python  Key  ( ) + '\\ Help \\' ) + help Desc  ) , win32con . HKEY CURRENT USER ) except win32api . error : pass return  None  
def test compare fiff ( ) : check usage ( mne compare fiff ) 
def get icon ( icon name ) : if icon name : pixmap = get pixmap ( icon name ) if ( pixmap is  None  ) : return Q Icon  ( I ( icon name ) ) else : return Q Icon  ( pixmap ) return Q Icon  ( ) 
def security group get ( context , security group id , columns to join =  None  ) : return IMPL . security group get ( context , security group id , columns to join ) 
@ docfiller def gaussian gradient magnitude ( input , sigma , output =  None  , mode = 'reflect' , cval = 0.0 , ** kwargs ) : input = numpy . asarray ( input ) def derivative ( input , axis , output , mode , cval , sigma , ** kwargs ) : order = ( [ 0 ] * input . ndim ) order [ axis ] = 1 return gaussian filter ( input , sigma , order , output , mode , cval , ** kwargs ) return generic gradient magnitude ( input , derivative , output , mode , cval , extra arguments = ( sigma , ) , extra keywords = kwargs ) 
def rollback ( name ) : ret = { 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : '' } ret [ 'changes' ] =   salt   [ 'junos.rollback' ] ( ) return ret 
def init subsystem ( subsystem type , options =  None  ) : init subsystems ( [ subsystem type ] , options ) 
@ salt . utils . decorators . which ( 'rar' ) def rar ( rarfile , sources , template =  None  , cwd =  None  , runas =  None  ) : if isinstance ( sources , string types ) : sources = [ s . strip ( ) for s in sources . split ( ',' ) ] cmd = [ 'rar' , 'a' , '-idp' , '{0}' . format ( rarfile ) ] cmd . extend ( sources ) return   salt   [ 'cmd.run' ] ( cmd , cwd = cwd , template = template , runas = runas , python shell =  False  ) . splitlines ( ) 
def urlunparse ( data ) : ( scheme , netloc , url , params , query , fragment ) = data if params : url = ( '%s;%s' % ( url , params ) ) return urlunsplit ( ( scheme , netloc , url , query , fragment ) ) 
def test sanity re match ( ) : pattern = re . compile ( '(abc){1}' ) match obj = pattern . match ( 'abcxyzabc123  and  some  other  words...' )  Are  Equal  ( match obj . expand ( '\x01\\g<1>.nt' ) , '\x01abc.nt' )  Are  Equal  ( match obj . group ( ) , 'abc' )  Are  Equal  ( match obj . group ( 1 ) , 'abc' )  Are  Equal  ( match obj . groups ( ) , ( 'abc' , ) )  Are  Equal  ( match obj . groups ( 1 ) , ( 'abc' , ) )  Are  Equal  ( match obj . groups ( 99 ) , ( 'abc' , ) )  Are  Equal  ( match obj . groupdict ( ) , { } )  Are  Equal  ( match obj . groupdict (  None  ) , { } )  Are  Equal  ( re . compile ( '(abc)' ) . match ( 'abcxyzabc123  and...' ) . groupdict ( ) , { } )  Are  Equal  ( match obj . start ( ) , 0 )  Are  Equal  ( match obj . start ( 1 ) , 0 )  Are  Equal  ( match obj . end ( ) , 3 )  Are  Equal  ( match obj . end ( 1 ) , 3 )  Are  Equal  ( match obj . span ( ) , ( 0 , 3 ) )  Are  Equal  ( match obj . span ( 1 ) , ( 0 , 3 ) )  Are  Equal  ( match obj . pos , 0 )  Are  Equal  ( match obj . endpos , 36 )  Are  Equal  ( match obj . lastindex , 1 )  Assert  ( ( match obj . re == pattern ) )  Are  Equal  ( match obj . string , 'abcxyzabc123  and  some  other  words...' ) 
def download UCLUST ( ) : status ( ' Installing   UCLUST...' ) if ( sys . platform == 'darwin' ) : URL = 'http://www.drive5.com/uclust/uclustq1.2.22 i86darwin64' elif ( sys . platform == 'linux2' ) : URL = 'http://www.drive5.com/uclust/uclustq1.2.22 i86linux64' else : status ( ( ' Platform   %r  not  supported  by  UCLUST.\n' % sys . platform ) ) return return value = download file ( URL , 'scripts/' , 'uclust' ) if ( not return value ) : chmod ( 'scripts/uclust' , ( stat ( 'scripts/uclust' ) . st mode | S IEXEC ) ) status ( 'UCLUST  installed.\n' ) else : status ( 'UCLUST  could  not  be  installed.\n' ) 
def enforce ( policy name , request ) : rule method = ( 'telemetry:' + policy name ) headers = request . headers policy dict = dict ( ) policy dict [ 'roles' ] = headers . get ( 'X- Roles ' , '' ) . split ( ',' ) policy dict [ 'user id' ] = headers . get ( 'X- User - Id ' ) policy dict [ 'project id' ] = headers . get ( 'X- Project - Id ' ) if ( (  has rule ( 'default' ) or  has rule ( rule method ) ) and ( not pecan . request . enforcer . enforce ( rule method , { } , policy dict ) ) ) : pecan . core . abort ( status code = 403 , detail = 'RBAC   Authorization    Failed ' ) 
def scan update ( item ) : path = str ( item . path ( ) ) try : ( title , desc , svg ) = preview parse ( path ) except SAX Parse  Exception  as ex : log . error ( '%r  is  malformed  (%r)' , path , ex ) item . set Enabled  (  False  ) item . set Selectable  (  False  ) return if ( not svg ) : try : svg = scheme svg thumbnail ( path ) except  Exception  : log . error ( ' Could   not  render  scheme  preview  for  %r' , title , exc info =  True  ) if ( item . name ( ) != title ) : item . set Name  ( title ) if ( item . description ( ) != desc ) : item . set Description  ( desc ) if svg : item . set Thumbnail  ( svg ) 
def modify priority ( conf , logger ) : global  libc setpriority if (  libc setpriority is  None  ) :  libc setpriority = load libc function ( 'setpriority' , errcheck =  True  ) def  setpriority ( nice priority ) : '\n                setpriority  for  this  pid\n\n                :param  nice priority:  valid  values  are  -19  to  20\n                ' try :  libc setpriority ( PRIO PROCESS , os . getpid ( ) , int ( nice priority ) ) except (  Value  Error  , OS Error  ) : print (   ( 'WARNING:   Unable   to  modify  scheduling  priority  of  process.   Keeping   unchanged!   Check   logs  for  more  info.  ' ) ) logger . exception ( ' Unable   to  modify  nice  priority' ) else : logger . debug ( ( 'set  nice  priority  to  %s' % nice priority ) ) nice priority = conf . get ( 'nice priority' ) if ( nice priority is not  None  ) :  setpriority ( nice priority ) global  posix syscall if (  posix syscall is  None  ) :  posix syscall = load libc function ( 'syscall' , errcheck =  True  ) def  ioprio set ( io class , io priority ) : '\n                ioprio set  for  this  process\n\n                :param  io class:  the  I/O  class  component,  can  be\n                                                  IOPRIO CLASS RT,  IOPRIO CLASS BE,\n                                                  or  IOPRIO CLASS IDLE\n                :param  io priority:  priority  value  in  the  I/O  class\n                ' try : io class = IO CLASS ENUM [ io class ] io priority = int ( io priority )  posix syscall ( NR ioprio set ( ) , IOPRIO WHO PROCESS , os . getpid ( ) , IOPRIO PRIO VALUE ( io class , io priority ) ) except (  Key  Error  ,  Value  Error  , OS Error  ) : print (   ( 'WARNING:   Unable   to  modify  I/O  scheduling  class  and  priority  of  process.   Keeping   unchanged!   Check   logs  for  more  info.' ) ) logger . exception ( ' Unable   to  modify  ionice  priority' ) else : logger . debug ( 'set  ionice  class  %s  priority  %s' , io class , io priority ) io class = conf . get ( 'ionice class' ) if ( io class is  None  ) : return io priority = conf . get ( 'ionice priority' , 0 )  ioprio set ( io class , io priority ) 
def test escaped task arg split ( ) : argstr = 'foo,bar\\,biz\\,baz,what  comes  after  baz?' eq  (  escape split ( ',' , argstr ) , [ 'foo' , 'bar,biz,baz' , 'what  comes  after  baz?' ] ) 
def  studio wrap xblock ( xblock , view , frag , context , display name only =  False  ) : if ( ( not context . get ( 'is pages view' ,  None  ) ) and ( view in PREVIEW VIEWS ) ) : root xblock = context . get ( 'root xblock' ) is root = ( root xblock and ( xblock . location == root xblock . location ) ) is reorderable =  is xblock reorderable ( xblock , context ) template context = { 'xblock context' : context , 'xblock' : xblock , 'show preview' : context . get ( 'show preview' ,  True  ) , 'content' : frag . content , 'is root' : is root , 'is reorderable' : is reorderable , 'can edit' : context . get ( 'can edit' ,  True  ) , 'can edit visibility' : context . get ( 'can edit visibility' ,  True  ) , 'can add' : context . get ( 'can add' ,  True  ) } html = render to string ( 'studio xblock wrapper.html' , template context ) frag = wrap fragment ( frag , html ) return frag 
def from timestamp ( timestamp , tz offset ) : utc dt = datetime . fromtimestamp ( timestamp , utc ) local dt = utc dt . astimezone ( tzoffset ( tz offset ) ) return local dt 
def  engine builder ( con ) : global  SQLALCHEMY INSTALLED if isinstance ( con , string types ) : try : import sqlalchemy except  Import  Error  :  SQLALCHEMY INSTALLED =  False  else : con = sqlalchemy . create engine ( con ) return con return con 
def  minpoly exp ( ex , x ) : ( c , a ) = ex . args [ 0 ] . as coeff  Mul  ( ) p = sympify ( c . p ) q = sympify ( c . q ) if ( a == ( I * pi ) ) : if c . is rational : if ( ( c . p == 1 ) or ( c . p == ( - 1 ) ) ) : if ( q == 3 ) : return ( ( ( x ** 2 ) - x ) + 1 ) if ( q == 4 ) : return ( ( x ** 4 ) + 1 ) if ( q == 6 ) : return ( ( ( x ** 4 ) - ( x ** 2 ) ) + 1 ) if ( q == 8 ) : return ( ( x ** 8 ) + 1 ) if ( q == 9 ) : return ( ( ( x ** 6 ) - ( x ** 3 ) ) + 1 ) if ( q == 10 ) : return ( ( ( ( ( x ** 8 ) - ( x ** 6 ) ) + ( x ** 4 ) ) - ( x ** 2 ) ) + 1 ) if q . is prime : s = 0 for i in range ( q ) : s += ( ( - x ) ** i ) return s factors = [ cyclotomic poly ( i , x ) for i in divisors ( ( 2 * q ) ) ] mp =  choose factor ( factors , x , ex ) return mp else : raise  Not  Algebraic  ( ( "%s  doesn't  seem  to  be  an  algebraic  element" % ex ) ) raise  Not  Algebraic  ( ( "%s  doesn't  seem  to  be  an  algebraic  element" % ex ) ) 
def get message date ( content , header = ' Date ' ) : message =  Mail  Parser  ( ) . parsestr ( content ,  True  ) dateheader = message . get ( header ) datetuple = email . utils . parsedate tz ( dateheader ) if ( datetuple is  None  ) : return  None  return email . utils . mktime tz ( datetuple ) 
def print event ( e ) : event dict = { 'retweet' : notify retweet , 'favorite' : notify favorite , 'unfavorite' : notify unfavorite , 'follow' : notify follow , 'list member added' : notify list member added , 'list member removed' : notify list member removed , 'list user subscribed' : notify list user subscribed , 'list user unsubscribed' : notify list user unsubscribed } event dict . get ( e [ 'event' ] , ( lambda * args :  None  ) ) ( e ) 
def assemble ( name , devices , test mode =  False  , ** kwargs ) : opts = [ ] for key in kwargs : if ( not key . startswith ( '  ' ) ) : opts . append ( '--{0}' . format ( key ) ) if ( kwargs [ key ] is not  True  ) : opts . append ( kwargs [ key ] ) if isinstance ( devices , str ) : devices = devices . split ( ',' ) cmd = ( ( [ 'mdadm' , '-A' , name , '-v' ] + opts ) + devices ) if ( test mode is  True  ) : return cmd elif ( test mode is  False  ) : return   salt   [ 'cmd.run' ] ( cmd , python shell =  False  ) 
def test reset ( ) : ip . reset ( ) nvars user ns = len ( ip . user ns ) nvars hidden = len ( ip . user ns hidden ) ip . user ns [ 'x' ] = 1 ip . user ns [ 'y' ] = 1 ip . reset ( ) nt . assert equal ( len ( ip . user ns ) , nvars user ns ) nt . assert equal ( len ( ip . user ns hidden ) , nvars hidden ) 
def peep hash ( argv ) : parser =  Option  Parser  ( usage = 'usage:  %prog  hash  file  [file  ...]' , description = ' Print   a  peep  hash  line  for  one  or  more  files:  for  example,  "#  sha256:  oz42d Zy 6 Gowxw 8 Ael  Dt O4g Rg TW x Pdoo H484k7I5EOY".' ) (   , paths ) = parser . parse args ( args = argv ) if paths : for path in paths : print ( '#  sha256:' , hash of file ( path ) ) return ITS FINE ITS FINE else : parser . print usage ( ) return COMMAND LINE ERROR 
def  atanh ( p , x , prec ) : R = p . ring one = R ( 1 ) c = [ one ] p2 = rs square ( p , x , prec ) for k in range ( 1 , prec ) : c . append ( ( one / ( ( 2 * k ) + 1 ) ) ) s = rs series from list ( p2 , c , x , prec ) s = rs mul ( s , p , x , prec ) return s 
def raising Resolver  Factory  ( * args , ** kwargs ) : raise  Resolver  Factory  Arguments  ( args , kwargs ) 
def kruskal MST ( gr ) : sorted edges = sorted ( gr . get edge weights ( ) ) uf =  Union  Find  ( ) min cost = 0 for ( w , ( u , v ) ) in sorted edges : if ( ( ( not uf . get leader ( u ) ) and ( not uf . get leader ( v ) ) ) or ( uf . get leader ( u ) != uf . get leader ( v ) ) ) : uf . insert ( u , v ) min cost += w return min cost 
def activate Aa  Pdp  Context  Reject  (  Protocol  Configuration  Options  presence = 0 ) : a =  Tp  Pd  ( pd = 8 ) b =  Message  Type  ( mes Type  = 82 ) c =  Sm  Cause  ( ) packet = ( ( a / b ) / c ) if (  Protocol  Configuration  Options  presence is 1 ) : d =  Protocol  Configuration  Options  ( ieiPCO = 39 ) packet = ( packet / d ) return packet 
def d3viz ( fct , outfile , copy deps =  True  , * args , ** kwargs ) : formatter =  Py  Dot  Formatter  ( * args , ** kwargs ) graph = formatter ( fct ) dot graph = graph . create dot ( ) if ( not six . PY2 ) : dot graph = dot graph . decode ( 'utf8' ) outdir = os . path . dirname ( outfile ) if ( ( not ( outdir == '' ) ) and ( not os . path . exists ( outdir ) ) ) : os . makedirs ( outdir ) template file = os . path . join (   path   , 'html' , 'template.html' ) with open ( template file ) as f : template = f . read ( ) src deps =   path   if copy deps : dst deps = 'd3viz' for d in [ 'js' , 'css' ] : dep = os . path . join ( outdir , dst deps , d ) if ( not os . path . exists ( dep ) ) : shutil . copytree ( os . path . join ( src deps , d ) , dep ) else : dst deps = src deps replace = { '%%  JS DIR  %%' : os . path . join ( dst deps , 'js' ) , '%%  CSS DIR  %%' : os . path . join ( dst deps , 'css' ) , '%%  DOT GRAPH  %%' : safe json ( dot graph ) } html = replace patterns ( template , replace ) with open ( outfile , 'w' ) as f : f . write ( html ) 
def image to display ( path , start =  None  , length =  None  ) : ( rows , columns ) = os . popen ( 'stty  size' , 'r' ) . read ( ) . split ( ) if ( not start ) : start = IMAGE SHIFT if ( not length ) : length = ( int ( columns ) - ( 2 * start ) ) i =  Image  . open ( path ) i = i . convert ( 'RGBA' ) ( w , h ) = i . size i . load ( ) width = min ( w , length ) height = int ( ( float ( h ) * ( float ( width ) / float ( w ) ) ) ) height //= 2 i = i . resize ( ( width , height ) ,  Image  . BICUBIC ) height = min ( height , IMAGE MAX HEIGHT ) for y in xrange ( height ) : sys . stdout . write ( ( '  ' * start ) ) for x in xrange ( width ) : p = i . getpixel ( ( x , y ) ) ( r , g , b ) = p [ : 3 ] pixel print ( rgb2short ( r , g , b ) ) sys . stdout . write ( '\n' ) 
def makeKW ( row Class  , args ) : kw = { } for i in range ( 0 , len ( args ) ) : column Name  = row Class  . db Columns  [ i ] [ 0 ] . lower ( ) for attr in row Class  . row Columns  : if ( attr . lower ( ) == column Name  ) : kw [ attr ] = args [ i ] break return kw 
def save file dialog ( default format = 'png' ) : filename =  Qt  Gui  . Q File  Dialog  . get Save  File  Name  ( ) filename =  format filename ( filename ) if ( filename is  None  ) : return  None  ( basename , ext ) = os . path . splitext ( filename ) if ( not ext ) : filename = ( '%s.%s' % ( filename , default format ) ) return filename 
def append ( name , text =  None  , makedirs =  False  , source =  None  , source hash =  None  , template = 'jinja' , sources =  None  , source hashes =  None  , defaults =  None  , context =  None  , ignore whitespace =  True  ) : ret = { 'name' : name , 'changes' : { } , 'pchanges' : { } , 'result' :  False  , 'comment' : '' } if ( not name ) : return  error ( ret , ' Must   provide  name  to  file.append' ) name = os . path . expanduser ( name ) if ( sources is  None  ) : sources = [ ] if ( source hashes is  None  ) : source hashes = [ ] ( ok  , err , sl  ) =  unify sources and hashes ( source = source , source hash = source hash , sources = sources , source hashes = source hashes ) if ( not ok  ) : return  error ( ret , err ) if ( makedirs is  True  ) : dirname = os . path . dirname ( name ) if ( not   salt   [ 'file.directory exists' ] ( dirname ) ) :   salt   [ 'file.makedirs' ] ( name ) ( check res , check msg , ret [ 'pchanges' ] ) =  check directory ( dirname ,  None  ,  None  ,  False  ,  None  ,  False  ,  False  ,  None  ) if ( not check res ) : return  error ( ret , check msg ) ( check res , check msg ) =  check file ( name ) if ( not check res ) : touch ( name , makedirs = makedirs ) ( retry res , retry msg ) =  check file ( name ) if ( not retry res ) : return  error ( ret , check msg ) if sl  : tmpret =  get template texts ( source list = sl  , template = template , defaults = defaults , context = context ) if ( not tmpret [ 'result' ] ) : return tmpret text = tmpret [ 'data' ] text =  validate str list ( text ) with salt . utils . fopen ( name , 'rb' ) as fp  : slines = fp  . read ( ) . splitlines ( ) append lines = [ ] try : for chunk in text : if ignore whitespace : if   salt   [ 'file.search' ] ( name , salt . utils . build whitespace split regex ( chunk ) , multiline =  True  ) : continue elif   salt   [ 'file.search' ] ( name , chunk , multiline =  True  ) : continue for line item in chunk . splitlines ( ) : append lines . append ( '{0}' . format ( line item ) ) except  Type  Error  : return  error ( ret , ' No   text  found  to  append.   Nothing   appended' ) if   opts   [ 'test' ] : ret [ 'comment' ] = ' File   {0}  is  set  to  be  updated' . format ( name ) ret [ 'result' ] =  None  nlines = list ( slines ) nlines . extend ( append lines ) if ( slines != nlines ) : if ( not salt . utils . istextfile ( name ) ) : ret [ 'changes' ] [ 'diff' ] = ' Replace   binary  file' else : ret [ 'changes' ] [ 'diff' ] = '\n' . join ( difflib . unified diff ( slines , nlines ) ) else : ret [ 'comment' ] = ' File   {0}  is  in  correct  state' . format ( name ) ret [ 'result' ] =  True  return ret if append lines :   salt   [ 'file.append' ] ( name , args = append lines ) ret [ 'comment' ] = ' Appended   {0}  lines' . format ( len ( append lines ) ) else : ret [ 'comment' ] = ' File   {0}  is  in  correct  state' . format ( name ) with salt . utils . fopen ( name , 'rb' ) as fp  : nlines = fp  . read ( ) . splitlines ( ) if ( slines != nlines ) : if ( not salt . utils . istextfile ( name ) ) : ret [ 'changes' ] [ 'diff' ] = ' Replace   binary  file' else : ret [ 'changes' ] [ 'diff' ] = '\n' . join ( difflib . unified diff ( slines , nlines ) ) ret [ 'result' ] =  True  return ret 
def placement init ( ) : global  ENFORCER PLACEMENT if ( not  ENFORCER PLACEMENT ) : rules = policy .  Rules  . load ( jsonutils . dumps ( { 'placement' : 'role:admin' } ) )  ENFORCER PLACEMENT = policy .  Enforcer  ( CONF , rules = rules , use conf =  False  ) 
def get movie data ( movieID , kind Dict  , from Aka  = 0 ,  table =  None  ) : if (  table is not  None  ) :  Table  =  table elif ( not from Aka  ) :  Table  =  Title  else :  Table  =  Aka  Title  try : m =  Table  . get ( movieID ) except  Exception  as e :  aux logger . warn ( ' Unable   to  fetch  information  for  movieID  %s:  %s' , movieID , e ) mdict = { } return mdict mdict = { 'title' : m . title , 'kind' : kind Dict  [ m . kindID ] , 'year' : m . production Year  , 'imdb Index ' : m . imdb Index  , 'season' : m . season Nr  , 'episode' : m . episode Nr  } if ( not from Aka  ) : if ( m . series Years  is not  None  ) : mdict [ 'series  years' ] = unicode ( m . series Years  ) if ( mdict [ 'imdb Index ' ] is  None  ) : del mdict [ 'imdb Index ' ] if ( mdict [ 'year' ] is  None  ) : del mdict [ 'year' ] else : try : mdict [ 'year' ] = int ( mdict [ 'year' ] ) except (  Type  Error  ,  Value  Error  ) : del mdict [ 'year' ] if ( mdict [ 'season' ] is  None  ) : del mdict [ 'season' ] else : try : mdict [ 'season' ] = int ( mdict [ 'season' ] ) except : pass if ( mdict [ 'episode' ] is  None  ) : del mdict [ 'episode' ] else : try : mdict [ 'episode' ] = int ( mdict [ 'episode' ] ) except : pass episode Of ID = m . episode Of ID if ( episode Of ID is not  None  ) : ser dict = get movie data ( episode Of ID , kind Dict  , from Aka  ) mdict [ 'episode  of' ] =  Movie  ( data = ser dict , movieID = episode Of ID , access System  = 'sql' ) if from Aka  : ser note =  Aka  Title  . get ( episode Of ID ) . note if ser note : mdict [ 'episode  of' ] . notes = ser note return mdict 
def impl ret untracked ( ctx , builder , retty , ret ) : return ret 
def  update exif orientation ( exif , orientation ) : exif dict = piexif . load ( exif ) if orientation : exif dict [ '0th' ] [ piexif .  Image IFD .  Orientation  ] = orientation return piexif . dump ( exif dict ) 
def usage ( ) : print ' App  Scale    Server ' print print ' Options :' print ( ( ' DCTB --type=<' + ',' . join ( dbconstants . VALID DATASTORES ) ) + '>' ) print ' DCTB --no encryption' print ' DCTB --port' 
def  get presser ( fig ) : callbacks = fig . canvas . callbacks . callbacks [ 'button press event' ] func =  None  for ( key , val ) in callbacks . items ( ) : if ( val . func .   class   .   name   == 'partial' ) : func = val . func break assert ( func is not  None  ) return func 
def lock ( tmp dir , timeout = notset , min wait =  None  , max wait =  None  , verbosity = 1 ) : if ( min wait is  None  ) : min wait = config . compile . wait if ( max wait is  None  ) : max wait = ( min wait * 2 ) if ( timeout is notset ) : timeout = config . compile . timeout base lock = os . path . dirname ( tmp dir ) if ( not os . path . isdir ( base lock ) ) : try : os . makedirs ( base lock ) except OS Error  : time . sleep ( 2 ) assert os . path . isdir ( base lock ) lock file = os . path . join ( tmp dir , 'lock' ) my pid = os . getpid ( ) no display = ( verbosity == 0 ) nb error = 0 nb wait = 0 while  True  : try : last owner = 'no owner' time start = time . time ( ) other dead =  False  while os . path . isdir ( tmp dir ) : try : with open ( lock file ) as f : read owner = f . readlines ( ) [ 0 ] . strip ( ) try : other host = read owner . split ( ' ' ) [ 2 ] except  Index  Error  : other host = ( ) if ( other host == hostname ) : try : os . kill ( int ( read owner . split ( ' ' ) [ 0 ] ) , 0 ) except OS Error  : other dead =  True  except  Attribute  Error  : pass except  Exception  : read owner = 'failure' if other dead : if ( not no display ) : msg = ( "process  '%s'" % read owner . split ( ' ' ) [ 0 ] )  logger . warning ( " Overriding   existing  lock  by  dead  %s  (I  am  process  '%s')" , msg , my pid ) get lock . unlocker . unlock ( force =  True  ) continue if ( last owner == read owner ) : if ( ( timeout is not  None  ) and ( ( time . time ( ) - time start ) >= timeout ) ) : if ( not no display ) : if ( read owner == 'failure' ) : msg = 'unknown  process' else : msg = ( "process  '%s'" % read owner . split ( ' ' ) [ 0 ] )  logger . warning ( " Overriding   existing  lock  by  %s  (I  am  process  '%s')" , msg , my pid ) get lock . unlocker . unlock ( force =  True  ) continue else : last owner = read owner time start = time . time ( ) no display = ( verbosity == 0 ) if ( ( not no display ) and ( nb wait > 0 ) ) : if ( read owner == 'failure' ) : msg = 'unknown  process' else : msg = ( "process  '%s'" % read owner . split ( ' ' ) [ 0 ] )  logger . info ( " Waiting   for  existing  lock  by  %s  (I  am  process  '%s')" , msg , my pid )  logger . info ( ' To   manually  release  the  lock,  delete  %s' , tmp dir ) if ( verbosity <= 1 ) : no display =  True  nb wait += 1 time . sleep ( random . uniform ( min wait , max wait ) ) try : os . mkdir ( tmp dir ) except OS Error  : nb error += 1 if ( nb error < 10 ) : continue else : raise assert os . path . isdir ( tmp dir ) unique id = refresh lock ( lock file ) with open ( lock file ) as f : owner = f . readlines ( ) [ 0 ] . strip ( ) if ( owner != unique id ) : continue else : return except  Exception  as e :  logger . warning ( ' Something   wrong  happened:  %s  %s' , type ( e ) , e ) nb error += 1 if ( nb error > 10 ) : raise time . sleep ( random . uniform ( min wait , max wait ) ) continue 
def get vm id ( kwargs =  None  , call =  None  ) : if ( call == 'action' ) : raise  Salt  Cloud  System  Exit  ( ' The   get vm id  function  must  be  called  with  -f  or  --function.' ) if ( kwargs is  None  ) : kwargs = { } name = kwargs . get ( 'name' ,  None  ) if ( name is  None  ) : raise  Salt  Cloud  System  Exit  ( ' The   get vm id  function  requires  a  name.' ) try : ret = list nodes ( ) [ name ] [ 'id' ] except  Key  Error  : raise  Salt  Cloud  System  Exit  ( " The   VM  '{0}'  could  not  be  found." . format ( name ) ) return ret 
def send alert ( request , message =  None  , url =  None  , code = 'soft-eol' ) : if ( url is  None  ) : url = request . registry . settings [ 'project docs' ] request . response . headers [ ' Alert ' ] = encode header ( json . dumps ( { 'code' : code , 'message' : message , 'url' : url } ) ) 
def get categories ( x , object only =  False  ) : if is categorical dtype ( x ) : return (  None  if has known categories ( x ) else x . map partitions (  get categories ) . unique ( ) . values ) return ( x . dropna ( ) . drop duplicates ( ) if ( ( not object only ) or ( x . dtype == object ) ) else  None  ) 
def load graph xml ( xml , filename , load all =  False  ) : ret = [ ] try : root = objectify . fromstring ( xml ) except  Exception  : return [ ] if ( root . tag != 'graphs' ) : return [ ] if ( not hasattr ( root , 'graph' ) ) : return [ ] for g in root . graph : name = g . attrib [ 'name' ] expressions = [ e . text for e in g . expression ] if load all : ret . append (  Graph  Definition  ( name , e , g . description . text , expressions , filename ) ) continue if have graph ( name ) : continue for e in expressions : if expression ok ( e ) : ret . append (  Graph  Definition  ( name , e , g . description . text , expressions , filename ) ) break return ret 
def show to programming language ( promo , programming language ) : if promo . programming language : return ( programming language == promo . programming language ) return  True  
def run child process ( test info ) : test info . start server ( ) sys . exit ( 0 ) 
def import no db in virt ( logical line , filename ) : if ( ( 'nova/virt' in filename ) and ( not filename . endswith ( 'fake.py' ) ) ) : if logical line . startswith ( 'from  nova  import  db' ) : ( yield ( 0 , 'N307:  nova.db  import  not  allowed  in  nova/virt/*' ) ) 
def get startup disk ( ) : ret = salt . utils . mac utils . execute return result ( 'systemsetup  -getstartupdisk' ) return salt . utils . mac utils . parse return ( ret ) 
def to address ( name ) : if name . is subdomain ( ipv4 reverse domain ) : name = name . relativize ( ipv4 reverse domain ) labels = list ( name . labels ) labels . reverse ( ) text = '.' . join ( labels ) return dns . ipv4 . inet ntoa ( dns . ipv4 . inet aton ( text ) ) elif name . is subdomain ( ipv6 reverse domain ) : name = name . relativize ( ipv6 reverse domain ) labels = list ( name . labels ) labels . reverse ( ) parts = [ ] i = 0 l = len ( labels ) while ( i < l ) : parts . append ( '' . join ( labels [ i : ( i + 4 ) ] ) ) i += 4 text = ':' . join ( parts ) return dns . ipv6 . inet ntoa ( dns . ipv6 . inet aton ( text ) ) else : raise dns . exception .  Syntax  Error  ( 'unknown  reverse-map  address  family' ) 
def  nearest tri edge ( pt tris , to pt , pqs , dist , tri geom ) : aa = tri geom [ 'a' ] [ pt tris ] bb = tri geom [ 'b' ] [ pt tris ] cc = tri geom [ 'c' ] [ pt tris ] pp = pqs [ 0 ] qq = pqs [ 1 ] p0 = np . minimum ( np . maximum ( ( pp + ( ( 0.5 * ( qq * cc ) ) / aa ) ) , 0.0 ) , 1.0 ) q0 = np . zeros like ( p0 ) t1 = ( ( 0.5 * ( ( ( ( 2.0 * aa ) - cc ) * ( 1.0 - pp ) ) + ( ( ( 2.0 * bb ) - cc ) * qq ) ) ) / ( ( aa + bb ) - cc ) ) t1 = np . minimum ( np . maximum ( t1 , 0.0 ) , 1.0 ) p1 = ( 1.0 - t1 ) q1 = t1 q2 = np . minimum ( np . maximum ( ( qq + ( ( 0.5 * ( pp * cc ) ) / bb ) ) , 0.0 ) , 1.0 ) p2 = np . zeros like ( q2 ) dist0 =  get tri dist ( pp , qq , p0 , q0 , aa , bb , cc , dist ) dist1 =  get tri dist ( pp , qq , p1 , q1 , aa , bb , cc , dist ) dist2 =  get tri dist ( pp , qq , p2 , q2 , aa , bb , cc , dist ) pp = np . r  [ ( p0 , p1 , p2 ) ] qq = np . r  [ ( q0 , q1 , q2 ) ] dists = np . r  [ ( dist0 , dist1 , dist2 ) ] ii = np . argmin ( np . abs ( dists ) ) ( p , q , pt , dist ) = ( pp [ ii ] , qq [ ii ] , pt tris [ ( ii % len ( pt tris ) ) ] , dists [ ii ] ) return ( p , q , pt , dist ) 
def superposition basis ( nqubits ) : amp = ( 1 / sqrt ( ( 2 ** nqubits ) ) ) return sum ( [ ( amp *  Int  Qubit  ( n , nqubits ) ) for n in range ( ( 2 ** nqubits ) ) ] ) 
def get network adapter type ( adapter type ) : if ( adapter type == 'vmxnet' ) : return vim . vm . device .  Virtual  Vmxnet  ( ) elif ( adapter type == 'vmxnet2' ) : return vim . vm . device .  Virtual  Vmxnet 2 ( ) elif ( adapter type == 'vmxnet3' ) : return vim . vm . device .  Virtual  Vmxnet 3 ( ) elif ( adapter type == 'e1000' ) : return vim . vm . device .  Virtual E1000 ( ) elif ( adapter type == 'e1000e' ) : return vim . vm . device .  Virtual E1000e ( ) 
def invalidate country rule cache ( sender , instance , ** kwargs ) : if isinstance ( instance ,  Restricted  Course  ) :  Restricted  Course  . invalidate cache for course ( instance . course key )  Country  Access  Rule  . invalidate cache for course ( instance . course key ) if isinstance ( instance ,  Country  Access  Rule  ) : try : restricted course = instance . restricted course except  Restricted  Course  .  Does  Not  Exist  : pass else :  Country  Access  Rule  . invalidate cache for course ( restricted course . course key ) 
@ db api . retry if session inactive ( ) @ db api . context manager . writer def set resources quota usage dirty ( context , resources , tenant id , dirty =  True  ) : query = db utils . model query ( context , quota models .  Quota  Usage  ) query = query . filter by ( tenant id = tenant id ) if resources : query = query . filter ( quota models .  Quota  Usage  . resource . in  ( resources ) ) return query . update ( { 'dirty' : dirty } , synchronize session =  False  ) 
def validate caffe files ( files ) : if ( str ( files [ 'weights file' ] . filename ) is '' ) : raise werkzeug . exceptions .  Bad  Request  ( ' Missing   weights  file' ) elif ( files [ 'weights file' ] . filename . rsplit ( '.' , 1 ) [ 1 ] != 'caffemodel' ) : raise werkzeug . exceptions .  Bad  Request  ( ' Weights   must  be  a  .caffemodel  file' ) if ( str ( files [ 'model def file' ] . filename ) is '' ) : raise werkzeug . exceptions .  Bad  Request  ( ' Missing   model  definition  file' ) elif ( files [ 'model def file' ] . filename . rsplit ( '.' , 1 ) [ 1 ] != 'prototxt' ) : raise werkzeug . exceptions .  Bad  Request  ( ' Model   definition  must  be  .prototxt  file' ) weights path = get tempfile ( flask . request . files [ 'weights file' ] , '.caffemodel' ) model def path = get tempfile ( flask . request . files [ 'model def file' ] , '.prototxt' ) return ( weights path , model def path ) 
def server ( request ) : return direct to template ( request , 'server/index.html' , { 'user url' : get View URL ( request , id Page  ) , 'server xrds url' : get View URL ( request , idp Xrds  ) } ) 
@ salt . utils . decorators . depends ( 'time' , fallback function =  fallbackfunc ) def depends will not fallback ( ) : ret = { 'ret' :  True  , 'time' : time . time ( ) } return ret 
def crc ( string ) : return ( '%08x' % ( binascii . crc32 ( string . encode ( 'utf-8' ) ) & 4294967295 ) ) 
def  set to get ( set cmd , module ) : set cmd = truncate before ( set cmd , '  option:' ) get cmd = set cmd . split ( '  ' ) ( key , value ) = get cmd [ ( - 1 ) ] . split ( '=' ) module . log ( ( 'get  commands  %s  ' % key ) ) return ( ( ( [ '--' , 'get' ] + get cmd [ : ( - 1 ) ] ) + [ key ] ) , value ) 
def copy ( x ) : cls = type ( x ) copier =  copy dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , '  copy  ' ,  None  ) if copier : return copier ( x ) reductor = dispatch table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , '  reduce ex  ' ,  None  ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , '  reduce  ' ,  None  ) if reductor : rv = reductor ( ) else : raise  Error  ( ( 'un(shallow)copyable  object  of  type  %s' % cls ) ) return  reconstruct ( x , rv , 0 ) 
def  get cibfile cksum ( cibname ) : cibfile cksum = '{0}.cksum' . format (  get cibfile ( cibname ) ) log . trace ( 'cibfile cksum:  {0}' . format ( cibfile cksum ) ) return cibfile cksum 
def imax ( * args ) : np = import module ( 'numpy' ) if ( not all ( ( isinstance ( arg , ( int , float , interval ) ) for arg in args ) ) ) : return  Not  Implemented  Error  else : new args = [ a for a in args if ( isinstance ( a , ( int , float ) ) or a . is valid ) ] if ( len ( new args ) == 0 ) : if all ( ( ( a . is valid is  False  ) for a in args ) ) : return interval ( ( - np . inf ) , np . inf , is valid =  False  ) else : return interval ( ( - np . inf ) , np . inf , is valid =  None  ) start array = [ ( a if isinstance ( a , ( int , float ) ) else a . start ) for a in new args ] end array = [ ( a if isinstance ( a , ( int , float ) ) else a . end ) for a in new args ] return interval ( max ( start array ) , max ( end array ) ) 
def find management module ( app name ) : parts = app name . split ( '.' ) parts . append ( 'management' ) parts . reverse ( ) part = parts . pop ( ) path =  None  try : ( f , path , descr ) = imp . find module ( part , path ) except  Import  Error  as e : if ( os . path . basename ( os . getcwd ( ) ) != part ) : raise e else : if f : f . close ( ) while parts : part = parts . pop ( ) ( f , path , descr ) = imp . find module ( part , ( ( path and [ path ] ) or  None  ) ) if f : f . close ( ) return path 
def process Archivable  ( archivable Class  , xml Element  ) : if ( xml Element  ==  None  ) : return get Archivable  Object  Add  To  Parent  ( archivable Class  , xml Element  ) xml Element  . getXML Processor  ( ) . process Children  ( xml Element  ) 
def  time from json ( value , field ) : if  not null ( value , field ) : return  time from iso8601 time naive ( value ) 
def get user diff ( ipa user , module user ) : result = [ ] sshpubkey =  None  if ( 'ipasshpubkey' in module user ) : module user [ 'sshpubkeyfp' ] = [ get ssh key fingerprint ( pubkey ) for pubkey in module user [ 'ipasshpubkey' ] ] sshpubkey = module user [ 'ipasshpubkey' ] del module user [ 'ipasshpubkey' ] for key in module user . keys ( ) : mod value = module user . get ( key ,  None  ) ipa value = ipa user . get ( key ,  None  ) if ( isinstance ( ipa value , list ) and ( not isinstance ( mod value , list ) ) ) : mod value = [ mod value ] if ( isinstance ( ipa value , list ) and isinstance ( mod value , list ) ) : mod value = sorted ( mod value ) ipa value = sorted ( ipa value ) if ( mod value != ipa value ) : result . append ( key ) if ( sshpubkey is not  None  ) : del module user [ 'sshpubkeyfp' ] module user [ 'ipasshpubkey' ] = sshpubkey return result 
@ pytest . mark . django db def test plugin renders absolute links ( ) : context = get jinja context ( ) page = create page ( eternal =  True  , visible in menu =  True  ) absolute link = ( '/%s' % page . url ) plugin =  Page  Links  Plugin  ( { 'show all pages' :  True  } ) assert ( absolute link in plugin . render ( context ) ) 
def  get user info ( user =  None  ) : if ( not user ) : user =   salt   [ 'config.option' ] ( 'user' ) userinfo =   salt   [ 'user.info' ] ( user ) if ( not userinfo ) : if ( user == 'salt' ) : userinfo =  get user info ( ) else : raise  Salt  Invocation  Error  ( ' User   {0}  does  not  exist' . format ( user ) ) return userinfo 
def is template file ( path ) : ext = os . path . splitext ( path ) [ 1 ] . lower ( ) return ( ext in [ u'.html' , u'.htm' , u'.xml' ] ) 
def get Random  Complex  ( begin , end ) : end Minus  Begin  = ( end - begin ) return ( begin + complex ( ( random . random ( ) * end Minus  Begin  . real ) , ( random . random ( ) * end Minus  Begin  . imag ) ) ) 
def encode utf8 ( u ) : import sys if ( sys . version info [ 0 ] == 2 ) : u = u . encode ( 'utf-8' ) return u 
def  linux distribution ( distname , version , id , supported dists , full distribution name ) : try : etc = os . listdir (  UNIXCONFDIR ) except OS Error  : return ( distname , version , id ) etc . sort ( ) for file in etc : m =  release filename . match ( file ) if ( m is not  None  ) : (  distname , dummy ) = m . groups ( ) if (  distname in supported dists ) : distname =  distname break else : return  dist try harder ( distname , version , id ) with open ( os . path . join (  UNIXCONFDIR , file ) , 'r' , encoding = 'utf-8' , errors = 'surrogateescape' ) as f : firstline = f . readline ( ) (  distname ,  version ,  id ) =  parse release file ( firstline ) if (  distname and full distribution name ) : distname =  distname if  version : version =  version if  id : id =  id return ( distname , version , id ) 
def  if unmodified since passes ( last modified , if unmodified since ) : return ( last modified and ( last modified <= if unmodified since ) ) 
def ends In  Newline  ( s ) : return ( s [ ( - len ( '\n' ) ) : ] == '\n' ) 
def remove ( cwd , targets , msg =  None  , user =  None  , username =  None  , password =  None  , * opts ) : if msg : opts += ( '-m' , msg ) if targets : opts += tuple ( salt . utils . shlex split ( targets ) ) return  run svn ( 'remove' , cwd , user , username , password , opts ) 
def approx hess cs ( x , f , epsilon =  None  , args = ( ) , kwargs = { } ) : n = len ( x ) h =  get epsilon ( x , 3 , epsilon , n ) ee = np . diag ( h ) hess = np . outer ( h , h ) n = len ( x ) for i in range ( n ) : for j in range ( i , n ) : hess [ ( i , j ) ] = ( ( ( f ( * ( ( ( ( x + ( 1j * ee [ i , : ] ) ) + ee [ j , : ] ) , ) + args ) , ** kwargs ) - f ( * ( ( ( ( x + ( 1j * ee [ i , : ] ) ) - ee [ j , : ] ) , ) + args ) , ** kwargs ) ) . imag / 2.0 ) / hess [ ( i , j ) ] ) hess [ ( j , i ) ] = hess [ ( i , j ) ] return hess 
def get analysis ( ) : analyzers = { } filters = { } snowball langs = { 'eu' : ' Basque ' , 'ca' : ' Catalan ' , 'da' : ' Danish ' , 'nl' : ' Dutch ' , 'en-US' : ' English ' , 'fi' : ' Finnish ' , 'fr' : ' French ' , 'de' : ' German ' , 'hu' : ' Hungarian ' , 'it' : ' Italian ' , 'no' : ' Norwegian ' , 'pt-BR' : ' Portuguese ' , 'ro' : ' Romanian ' , 'ru' : ' Russian ' , 'es' : ' Spanish ' , 'sv' : ' Swedish ' , 'tr' : ' Turkish ' } for ( locale , language ) in snowball langs . items ( ) : analyzer name = es analyzer for locale ( locale , synonyms =  False  ) analyzers [ analyzer name ] = { 'type' : 'snowball' , 'language' : language } if ( locale in config . ES SYNONYM LOCALES ) : analyzer name = es analyzer for locale ( locale , synonyms =  True  ) analyzers [ analyzer name ] = { 'type' : 'custom' , 'tokenizer' : 'standard' , 'filter' : [ 'standard' , 'lowercase' , ( 'synonyms-' + locale ) , 'stop' , ( 'snowball-' + locale ) ] } for locale in config . ES SYNONYM LOCALES : ( filter name , filter body ) = es get synonym filter ( locale ) filters [ filter name ] = filter body filters [ ( 'snowball-' + locale ) ] = { 'type' : 'snowball' , 'language' : snowball langs [ locale ] } return { 'analyzer' : analyzers , 'filter' : filters } 
def spec hausman ( params e , params i , cov params e , cov params i , dof =  None  ) : params diff = ( params i - params e ) cov diff = ( cov params i - cov params e ) if ( not dof ) : dof = np matrix rank ( cov diff ) cov diffpinv = np . linalg . pinv ( cov diff ) H = np . dot ( params diff , np . dot ( cov diffpinv , params diff ) ) pval = stats . chi2 . sf ( H , dof ) evals = np . linalg . eigvalsh ( cov diff ) return ( H , pval , dof , evals ) 
@ receiver ( job was rejected ) def on job was rejected ( sender , job , rejecting user , ** kwargs ) : send job review message ( job , rejecting user , 'jobs/email/job was rejected subject.txt' , 'jobs/email/job was rejected.txt' ) 
def addusersitepackages ( known paths ) : global USER BASE , USER SITE , ENABLE USER SITE env base = os . environ . get ( 'PYTHONUSERBASE' ,  None  ) def joinuser ( * args ) : return os . path . expanduser ( os . path . join ( * args ) ) if ( os . name == 'nt' ) : base = ( os . environ . get ( 'APPDATA' ) or '~' ) if env base : USER BASE = env base else : USER BASE = joinuser ( base , ' Python ' ) USER SITE = os . path . join ( USER BASE , ( ( ' Python ' + sys . version [ 0 ] ) + sys . version [ 2 ] ) , 'site-packages' ) else : if env base : USER BASE = env base else : USER BASE = joinuser ( '~' , '.local' ) USER SITE = os . path . join ( USER BASE , 'lib' , ( 'python' + sys . version [ : 3 ] ) , 'site-packages' ) if ( ENABLE USER SITE and os . path . isdir ( USER SITE ) ) : addsitedir ( USER SITE , known paths ) if ENABLE USER SITE : for dist libdir in ( 'lib' , 'local/lib' ) : user site = os . path . join ( USER BASE , dist libdir , ( 'python' + sys . version [ : 3 ] ) , 'dist-packages' ) if os . path . isdir ( user site ) : addsitedir ( user site , known paths ) return known paths 
def list user permissions ( name , runas =  None  ) : if ( ( runas is  None  ) and ( not salt . utils . is windows ( ) ) ) : runas = salt . utils . get user ( ) res =   salt   [ 'cmd.run all' ] ( [   context   [ 'rabbitmqctl' ] , 'list user permissions' , name , '-q' ] , runas = runas , python shell =  False  ) return  output to dict ( res ) 
def   call   ( self , func ) : return  Function  Maker  . create ( func , 'with   self :  return   func (%(shortsignature)s)' , dict (  self  = self ,  func  = func ) ,   wrapped   = func ) 
def eppstein matching ( G ) : left = bipartite sets ( G ) [ 0 ] G = nx .  Di  Graph  ( G . edges ( left ) ) matching = { } for u in G : for v in G [ u ] : if ( v not in matching ) : matching [ v ] = u break while  True  : preds = { } unmatched = [ ] pred = { u : unmatched for u in G } for v in matching : del pred [ matching [ v ] ] layer = list ( pred ) while ( layer and ( not unmatched ) ) : new Layer  = { } for u in layer : for v in G [ u ] : if ( v not in preds ) : new Layer  . setdefault ( v , [ ] ) . append ( u ) layer = [ ] for v in new Layer  : preds [ v ] = new Layer  [ v ] if ( v in matching ) : layer . append ( matching [ v ] ) pred [ matching [ v ] ] = v else : unmatched . append ( v ) if ( not unmatched ) : unlayered = { } for u in G : for v in G [ u ] : if ( v not in preds ) : unlayered [ v ] =  None  for key in matching . copy ( ) : matching [ matching [ key ] ] = key return matching def recurse ( v ) : if ( v in preds ) : L = preds . pop ( v ) for u in L : if ( u in pred ) : pu = pred . pop ( u ) if ( ( pu is unmatched ) or recurse ( pu ) ) : matching [ v ] = u return  True  return  False  for v in unmatched : recurse ( v ) 
def get block start ( lines , lineno , maximum indents = 80 ) : pattern = get block start patterns ( ) for i in range ( lineno , 0 , ( - 1 ) ) : match = pattern . search ( lines . get line ( i ) ) if ( ( match is not  None  ) and ( count line indents ( lines . get line ( i ) ) <= maximum indents ) ) : striped = match . string . lstrip ( ) if ( ( ( i > 1 ) and striped . startswith ( 'if' ) ) or striped . startswith ( 'for' ) ) : bracs = 0 for j in range ( i , min ( ( i + 5 ) , ( lines . length ( ) + 1 ) ) ) : for c in lines . get line ( j ) : if ( c == '#' ) : break if ( c in '[(' ) : bracs += 1 if ( c in ')]' ) : bracs -= 1 if ( bracs < 0 ) : break if ( bracs < 0 ) : break if ( bracs < 0 ) : continue return i return 1 
def require post params ( * args , ** kwargs ) : required params = [ ] required params += [ ( arg ,  None  ) for arg in args ] required params += [ ( key , kwargs [ key ] ) for key in kwargs ] def decorator ( func ) : def wrapped ( * args , ** kwargs ) : request = args [ 0 ] error response data = { 'error' : ' Missing   required  query  parameter(s)' , 'parameters' : [ ] , 'info' : { } } for ( param , extra ) in required params : default = object ( ) if ( request . POST . get ( param , default ) == default ) : error response data [ 'parameters' ] . append ( param ) error response data [ 'info' ] [ param ] = extra if ( len ( error response data [ 'parameters' ] ) > 0 ) : return  Json  Response  ( error response data , status = 400 ) else : return func ( * args , ** kwargs ) return wrapped return decorator 
def validate ( cls , instance or dict , trusted data =  None  , partial =  False  , strict =  False  , convert =  True  , context =  None  , ** kwargs ) : context = ( context or get validation context ( partial = partial , strict = strict , convert = convert ) ) errors = { } try : data = import loop ( cls , instance or dict , trusted data = trusted data , context = context , ** kwargs ) except  Data  Error  as exc : errors = exc . messages data = exc . partial data errors . update (  validate model ( cls , data , context ) ) if errors : raise  Data  Error  ( errors , data ) return data 
def imread ( fname , dtype =  None  ) : ds = gdal .  Open  ( fname ) return ds .  Read  As  Array  ( ) . astype ( dtype ) 
def   virtual   ( ) : return ( 'mysql.query' in   salt   ) 
@ log call def metadef object get by id ( context , namespace name , object id ) : namespace = metadef namespace get ( context , namespace name )  check namespace visibility ( context , namespace , namespace name ) for object in DATA [ 'metadef objects' ] : if ( ( object [ 'namespace id' ] == namespace [ 'id' ] ) and ( object [ 'id' ] == object id ) ) : return object else : msg = (   ( ' Metadata   definition  object  not  found  for  id=%s' ) % object id ) LOG . warn ( msg ) raise exception .  Metadef  Object  Not  Found  ( msg ) 
def  Check  For  Newline  At EOF ( filename , lines , error ) : if ( ( len ( lines ) < 3 ) or lines [ ( - 2 ) ] ) : error ( filename , ( len ( lines ) - 2 ) , 'whitespace/ending newline' , 5 , ' Could   not  find  a  newline  character  at  the  end  of  the  file.' ) 
def wsgifunc ( func , * middleware ) : middleware = list ( middleware ) if ( reloader in middleware ) : relr = reloader (  None  ) relrcheck = relr . check middleware . remove ( reloader ) else : relr =  None  relrcheck = ( lambda :  None  ) def wsgifunc ( env , start resp ) :  load ( env ) relrcheck ( ) try : result = func ( ) except  Stop  Iteration  : result =  None  is generator = ( result and hasattr ( result , 'next' ) ) if is generator : try : firstchunk = result . next ( ) except  Stop  Iteration  : firstchunk = '' ( status , headers , output ) = ( ctx . status , ctx . headers , ctx . output )  unload ( ) start resp ( status , headers ) if is generator : return itertools . chain ( [ firstchunk ] , result ) elif isinstance ( output , str ) : return [ output ] elif hasattr ( output , 'next' ) : return output else : raise  Exception  , ' Invalid   web.ctx.output' for mw func in middleware : wsgifunc = mw func ( wsgifunc ) if relr : relr . func = wsgifunc return wsgifunc return wsgifunc 
def create real path ( name , loc , path , umask =  False  , writable =  True  ) : if path : my dir = real path ( loc , path ) if ( not os . path . exists ( my dir ) ) : logging . info ( '%s  directory:  %s  does  not  exist,  try  to  create  it' , name , my dir ) if ( not create all dirs ( my dir , umask ) ) : logging . error ( T ( ' Cannot   create  directory  %s' ) , clip path ( my dir ) ) return (  False  , my dir ) checks = ( ( os . W OK + os . R OK ) if writable else os . R OK ) if os . access ( my dir , checks ) : return (  True  , my dir ) else : logging . error ( T ( '%s  directory:  %s  error  accessing' ) , name , clip path ( my dir ) ) return (  False  , my dir ) else : return (  False  , '' ) 
def test nearmiss init ( ) : ratio = 1.0 nm2 =  Near  Miss  ( ratio = ratio , random state = RND SEED , version = VERSION NEARMISS ) assert equal ( nm2 . version , VERSION NEARMISS ) assert equal ( nm2 . n neighbors , 3 ) assert equal ( nm2 . ratio , ratio ) assert equal ( nm2 . random state , RND SEED ) 
def teardown ( ) : db teardown ( ) 
def  process long opt ( option parser , rargs , values , dests ) : arg = rargs . pop ( 0 ) if ( '=' in arg ) : ( opt , next arg ) = arg . split ( '=' , 1 ) rargs . insert ( 0 , next arg ) else : opt = arg opt = option parser .  match long opt ( opt ) option = option parser .  long opt [ opt ] rargs before processing = [ x for x in rargs ] if option . takes value ( ) : nargs = option . nargs if ( nargs == 1 ) : value = rargs . pop ( 0 ) else : value = tuple ( rargs [ 0 : nargs ] ) del rargs [ 0 : nargs ] else : value =  None  option . process ( opt , value , values , option parser ) if ( ( dests is  None  ) or ( option . dest in dests ) ) : length difference = ( len ( rargs before processing ) - len ( rargs ) ) for item in ( [ opt ] + rargs before processing [ : length difference ] ) : ( yield ( option . dest , item ) ) 
def  Tag  Bytes  ( field number , wire type ) : return   Varint  Bytes  ( wire format .  Pack  Tag  ( field number , wire type ) ) 
def align partitions ( * dfs ) : dfs1 = [ df for df in dfs if isinstance ( df ,   Frame  ) ] if ( len ( dfs ) == 0 ) : raise  Value  Error  ( 'dfs  contains  no   Data  Frame   and   Series ' ) if ( not all ( ( df . known divisions for df in dfs1 ) ) ) : raise  Value  Error  ( " Not   all  divisions  are  known,  can't  align  partitions.   Please   use  `set index`  or  `set partition`  to  set  the  index." ) divisions = list ( unique ( merge sorted ( * [ df . divisions for df in dfs1 ] ) ) ) dfs2 = [ ( df . repartition ( divisions , force =  True  ) if isinstance ( df ,   Frame  ) else df ) for df in dfs ] result = list ( ) inds = [ 0 for df in dfs ] for d in divisions [ : ( - 1 ) ] : L = list ( ) for ( i , df ) in enumerate ( dfs2 ) : if isinstance ( df ,   Frame  ) : j = inds [ i ] divs = df . divisions if ( ( j < ( len ( divs ) - 1 ) ) and ( divs [ j ] == d ) ) : L . append ( ( df .  name , inds [ i ] ) ) inds [ i ] += 1 else : L . append (  None  ) else : L . append (  None  ) result . append ( L ) return ( dfs2 , tuple ( divisions ) , result ) 
def   virtual   ( ) : if ( salt . utils . which ( 'hg' ) is  None  ) : return (  False  , ' The   hg  execution  module  cannot  be  loaded:  hg  unavailable.' ) else : return  True  
def pick Best  Result  ( results , show ) : results = ( results if isinstance ( results , list ) else [ results ] ) sickrage . sr Core  . sr Logger  . debug ( ( u' Picking   the  best  result  out  of  ' + str ( [ x . name for x in results ] ) ) ) best Result  =  None  for cur result in results : if ( show and ( cur result . show is not show ) ) : continue if show . is anime : if ( not show . release groups . is valid ( cur result ) ) : continue sickrage . sr Core  . sr Logger  . info ( ( ( ( u' Quality   of  ' + cur result . name ) + u'  is  ' ) +  Quality  . quality Strings  [ cur result . quality ] ) ) ( any Qualities  , best Qualities  ) =  Quality  . split Quality  ( show . quality ) if ( cur result . quality not in ( any Qualities  + best Qualities  ) ) : sickrage . sr Core  . sr Logger  . debug ( ( cur result . name + u"  is  a  quality  we  know  we  don't  want,  rejecting  it" ) ) continue if ( show . rls ignore words and show names . contains At  Least  One  Word  ( cur result . name , cur result . show . rls ignore words ) ) : sickrage . sr Core  . sr Logger  . info ( ( ( ( u' Ignoring   ' + cur result . name ) + u'  based  on  ignored  words  filter:  ' ) + show . rls ignore words ) ) continue if ( show . rls require words and ( not show names . contains At  Least  One  Word  ( cur result . name , cur result . show . rls require words ) ) ) : sickrage . sr Core  . sr Logger  . info ( ( ( ( u' Ignoring   ' + cur result . name ) + u'  based  on  required  words  filter:  ' ) + show . rls require words ) ) continue if ( not show names . filter Bad  Releases  ( cur result . name , parse =  False  ) ) : sickrage . sr Core  . sr Logger  . info ( ( ( u' Ignoring   ' + cur result . name ) + u'  because  its  not  a  valid  scene  release  that  we  want,  ignoring  it' ) ) continue if hasattr ( cur result , u'size' ) : if ( sickrage . sr Core  . sr Config  . USE FAILED DOWNLOADS and  Failed  History  . has Failed  ( cur result . name , cur result . size , cur result . provider . name ) ) : sickrage . sr Core  . sr Logger  . info ( ( cur result . name + u'  has  previously  failed,  rejecting  it' ) ) continue try : for ( file , file size ) in cur result . files . items ( ) : if ( not file . endswith ( tuple ( video exts ) ) ) : continue file size = float ( ( file size / 1000000 ) ) if ( file size > sickrage . sr Core  . sr Config  . QUALITY SIZES [ cur result . quality ] ) : raise ( ( u' Ignoring   ' + cur result . name ) + u'  based  on  quality  size  filter:  {},  ignoring  it' . format ( file size ) ) except  Exception  as e : sickrage . sr Core  . sr Logger  . info ( e . message ) continue if ( not best Result  ) : best Result  = cur result elif ( ( cur result . quality in best Qualities  ) and ( ( best Result  . quality < cur result . quality ) or ( best Result  . quality not in best Qualities  ) ) ) : best Result  = cur result elif ( ( cur result . quality in any Qualities  ) and ( best Result  . quality not in best Qualities  ) and ( best Result  . quality < cur result . quality ) ) : best Result  = cur result elif ( best Result  . quality == cur result . quality ) : if ( ( u'proper' in cur result . name . lower ( ) ) or ( u'repack' in cur result . name . lower ( ) ) ) : best Result  = cur result elif ( ( u'internal' in best Result  . name . lower ( ) ) and ( u'internal' not in cur result . name . lower ( ) ) ) : best Result  = cur result elif ( ( u'xvid' in best Result  . name . lower ( ) ) and ( u'x264' in cur result . name . lower ( ) ) ) : sickrage . sr Core  . sr Logger  . info ( ( ( u' Preferring   ' + cur result . name ) + u'  (x264  over  xvid)' ) ) best Result  = cur result if best Result  : sickrage . sr Core  . sr Logger  . debug ( ( ( u' Picked   ' + best Result  . name ) + u'  as  the  best' ) ) else : sickrage . sr Core  . sr Logger  . debug ( u' No   result  picked.' ) return best Result  
@ click . command ( ) @ click . argument ( 'path' ) @ click . option ( '--apps path' , default =  None  , help = 'path  to  json  files  with  apps  to  install  after  init' ) @ click . option ( '--frappe-path' , default =  None  , help = 'path  to  frappe  repo' ) @ click . option ( '--frappe-branch' , default =  None  , help = 'path  to  frappe  repo' ) @ click . option ( '--clone-from' , default =  None  , help = 'copy  repos  from  path' ) @ click . option ( '--no-procfile' , is flag =  True  , help = ' Pull   changes  in  all  the  apps  in  bench' ) @ click . option ( '--no-backups' , is flag =  True  , help = ' Run   migrations  for  all  sites  in  the  bench' ) @ click . option ( '--no-auto-update' , is flag =  True  , help = ' Build   JS  and  CSS  artifacts  for  the  bench' ) @ click . option ( '--verbose' , is flag =  True  , help = ' Verbose   output  during  install' ) def init ( path , apps path , frappe path , frappe branch , no procfile , no backups , no auto update , clone from , verbose ) : from bench . utils import init init ( path , apps path = apps path , no procfile = no procfile , no backups = no backups , no auto update = no auto update , frappe path = frappe path , frappe branch = frappe branch , verbose = verbose , clone from = clone from ) click . echo ( ' Bench   {}  initialized' . format ( path ) ) 
def get static page by path ( path ) : if ( path == 'index 2.html' ) : return get static index page (  False  ) elif ( path == 'index.html' ) : return get static index page (  True  ) elif ( path == 'NLTK   Wordnet    Browser    Database    Info .html' ) : return ' Display   of   Wordnet    Database    Statistics   is  not  supported' elif ( path == 'upper 2.html' ) : return get static upper page (  False  ) elif ( path == 'upper.html' ) : return get static upper page (  True  ) elif ( path == 'web help.html' ) : return get static web help page ( ) elif ( path == 'wx help.html' ) : return get static wx help page ( ) else : return ( " Internal   error:   Path   for  static  page  '%s'  is  unknown" % path ) 
def to bool ( value ) : if ( value is  None  ) : return  None  if isinstance ( value , bool ) : return value elif isinstance ( value , str ) : if ( value == 'no' ) : return  False  elif ( value == 'yes' ) : return  True  
@ register . filter def get content snippet ( content , keyword , max words = 30 ) : def clean text ( content ) : u'\n                 Removes   tags,  newlines  and  spaces  from  content.\n                 Return   array  of  words.\n                ' content = striptags ( content ) content = content . replace ( u'\n' , u'  ' ) . split ( u'  ' ) return list ( filter ( ( lambda x : ( x != u'' ) ) , content ) ) max words = int ( max words ) pattern = re . compile ( ( u'(?P<before>.*)%s(?P<after>.*)' % re . escape ( keyword ) ) , ( ( re . MULTILINE | re . IGNORECASE ) | re . DOTALL ) ) match = pattern . search ( content ) if match : words = clean text ( match . group ( u'before' ) ) before words = words [ ( ( - max words ) // 2 ) : ] words = clean text ( match . group ( u'after' ) ) after = u'  ' . join ( words [ : ( max words - len ( before words ) ) ] ) before = u'  ' . join ( before words ) html = ( u'%s  %s  %s' % ( before , striptags ( keyword ) , after ) ) kw p = re . compile ( ( u'(%s)' % keyword ) , re . IGNORECASE ) html = kw p . sub ( u'<strong>\\1</strong>' , html ) return mark safe ( html ) return u'  ' . join ( clean text ( content ) [ : max words ] ) 
def gen challenge path ( challbs , preferences , combinations ) : if combinations : return  find smart path ( challbs , preferences , combinations ) else : return  find dumb path ( challbs , preferences ) 
def list blobs with prefix ( bucket name , prefix , delimiter =  None  ) : storage client = storage .  Client  ( ) bucket = storage client . get bucket ( bucket name ) blobs = bucket . list blobs ( prefix = prefix , delimiter = delimiter ) print ' Blobs :' for blob in blobs : print blob . name if delimiter : print ' Prefixes :' for prefix in blobs . prefixes : print prefix 
def  run in namespace ( f , * args ) : namespace = namespace manager . get namespace ( ) try : namespace manager . set namespace ( LOG NAMESPACE ) return f ( * args ) finally : namespace manager . set namespace ( namespace ) 
def probability of ( qty , user settings ) : if ( qty in [ 'exercise' , 'video' ] ) : return sqrt ( ( ( user settings [ 'effort level' ] * 3 ) * user settings [ 'time in program' ] ) ) if ( qty == 'completed' ) : return ( ( ( ( 0.33 * user settings [ 'effort level' ] ) + ( 0.66 * user settings [ 'speed of learning' ] ) ) * 2 ) * user settings [ 'time in program' ] ) if ( qty == 'attempts' ) : return ( ( ( ( 0.33 * user settings [ 'effort level' ] ) + ( 0.55 * user settings [ 'time in program' ] ) ) / probability of ( 'completed' , user settings ) ) / 5 ) 
def convert comments ( text ) : return re . sub ( '(?<=\\n)\\s*#[^#]' , '##' , text ) 
@ contextfunction def sales product list ( context , products , skip group =  False  ) : request = context [ 'request' ] response format = 'html' if ( 'response format' in context ) : response format = context [ 'response format' ] return  Markup  ( render to string ( 'sales/tags/product list' , { 'products' : products , 'skip group' : skip group } , context instance =  Request  Context  ( request ) , response format = response format ) ) 
def instance decorator ( decorator ) : class  Decorator  ( object , ) : def   init   ( self , func = nop , * args , ** kws ) : self .   name   = func .   name   self .   doc   = func .   doc   self .  data name = ( '%s %d decorated instance' % ( func .   name   , id ( self ) ) ) self .  func = func self .  args = args self .  kws = kws def   get   ( self , obj , cls =  None  ) : if ( obj is  None  ) : return data name = self .  data name try : return obj .   dict   [ data name ] except  Key  Error  : decorated = decorator ( obj , self .  func , * self .  args , ** self .  kws ) obj .   dict   [ data name ] = decorated return decorated return  Decorator  
@ receiver ( models . signals . post save , sender =  Credit  Provider  ) @ receiver ( models . signals . post delete , sender =  Credit  Provider  ) def invalidate provider cache ( sender , ** kwargs ) : cache . delete (  Credit  Provider  . CREDIT PROVIDERS CACHE KEY ) 
def profiled ( func ) : @ wraps ( func ) def wrapped func ( * args , ** kwargs ) : fn = ( func .   name   + '.profile' ) prof = c Profile  .  Profile  ( ) retval = prof . runcall ( func , * args , ** kwargs ) prof . dump stats ( fn ) return retval return wrapped func 
def clear sessions ( user =  None  , keep current =  False  , device =  None  ) : for sid in get sessions to clear ( user , keep current , device ) : delete session ( sid , reason = u' Logged    In    From    Another    Session ' ) 
def twinx ( ax =  None  ) : if ( ax is  None  ) : ax = gca ( ) ax1 = ax . twinx ( ) draw if interactive ( ) return ax1 
def assert shape ( x , expected shape , msg = ' Unexpected   shape.' ) : if ( expected shape is  None  ) : return x shape = x . shape tests = [ ] for i in range ( x . ndim ) : if ( expected shape [ i ] is not  None  ) : tests . append ( theano . tensor . eq ( shape [ i ] , expected shape [ i ] ) ) if tests : return  Assert  ( msg ) ( x , * tests ) else : return x 
@ mock ec2 def test describe dhcp options invalid id ( ) : conn = boto . connect vpc ( u'the key' , u'the secret' ) with assert raises ( EC2 Response  Error  ) as cm : conn . get all dhcp options ( [ u'1' ] ) cm . exception . code . should . equal ( u' Invalid  Dhcp  Option ID. Not  Found ' ) cm . exception . status . should . equal ( 400 ) cm . exception . request id . should not . be . none 
def update counts ( s , counts ) : for char in s : if ( char in counts ) : counts [ char ] += 1 
@ app . route ( '/stats' , methods = [ 'GET' ] ) def stats ( ) : stats = get engines stats ( ) return render ( 'stats.html' , stats = stats ) 
def xGC skew ( seq , window = 1000 , zoom = 100 , r = 300 , px = 100 , py = 100 ) : try : import  Tkinter  as tkinter except  Import  Error  : import tkinter yscroll = tkinter .  Scrollbar  ( orient = tkinter . VERTICAL ) xscroll = tkinter .  Scrollbar  ( orient = tkinter . HORIZONTAL ) canvas = tkinter .  Canvas  ( yscrollcommand = yscroll . set , xscrollcommand = xscroll . set , background = 'white' ) win = canvas . winfo toplevel ( ) win . geometry ( '700x700' ) yscroll . config ( command = canvas . yview ) xscroll . config ( command = canvas . xview ) yscroll . pack ( side = tkinter . RIGHT , fill = tkinter . Y ) xscroll . pack ( side = tkinter . BOTTOM , fill = tkinter . X ) canvas . pack ( fill = tkinter . BOTH , side = tkinter . LEFT , expand = 1 ) canvas . update ( ) ( X0 , Y0 ) = ( ( r + px ) , ( r + py ) ) ( x1 , x2 , y1 , y2 ) = ( ( X0 - r ) , ( X0 + r ) , ( Y0 - r ) , ( Y0 + r ) ) ty = Y0 canvas . create text ( X0 , ty , text = ( '%s...%s  (%d  nt)' % ( seq [ : 7 ] , seq [ ( - 7 ) : ] , len ( seq ) ) ) ) ty += 20 canvas . create text ( X0 , ty , text = ( 'GC  %3.2f%%' % GC ( seq ) ) ) ty += 20 canvas . create text ( X0 , ty , text = 'GC   Skew ' , fill = 'blue' ) ty += 20 canvas . create text ( X0 , ty , text = ' Accumulated   GC   Skew ' , fill = 'magenta' ) ty += 20 canvas . create oval ( x1 , y1 , x2 , y2 ) acc = 0 start = 0 for gc in GC skew ( seq , window ) : r1 = r acc += gc alpha = ( pi - ( ( ( 2 * pi ) * start ) / len ( seq ) ) ) r2 = ( r1 - ( gc * zoom ) ) x1 = ( X0 + ( r1 * sin ( alpha ) ) ) y1 = ( Y0 + ( r1 * cos ( alpha ) ) ) x2 = ( X0 + ( r2 * sin ( alpha ) ) ) y2 = ( Y0 + ( r2 * cos ( alpha ) ) ) canvas . create line ( x1 , y1 , x2 , y2 , fill = 'blue' ) r1 = ( r - 50 ) r2 = ( r1 - acc ) x1 = ( X0 + ( r1 * sin ( alpha ) ) ) y1 = ( Y0 + ( r1 * cos ( alpha ) ) ) x2 = ( X0 + ( r2 * sin ( alpha ) ) ) y2 = ( Y0 + ( r2 * cos ( alpha ) ) ) canvas . create line ( x1 , y1 , x2 , y2 , fill = 'magenta' ) canvas . update ( ) start += window canvas . configure ( scrollregion = canvas . bbox ( tkinter . ALL ) ) 
def null list ( pages , size ) : res = [ ] for page in pages : if ( size > 4096 ) : size -= 4096 else : page = page [ : size ] for s in page . split ( '\x00' ) : if ( s != '' ) : res . append ( s ) return res 
def urlencode ( query , doseq =  False  , safe = '' , encoding =  None  , errors =  None  , quote via = quote plus ) : if hasattr ( query , 'items' ) : query = query . items ( ) else : try : if ( len ( query ) and ( not isinstance ( query [ 0 ] , tuple ) ) ) : raise  Type  Error  except  Type  Error  : ( ty , va , tb ) = sys . exc info ( ) raise  Type  Error  ( 'not  a  valid  non-string  sequence  or  mapping  object' ) . with traceback ( tb ) l = [ ] if ( not doseq ) : for ( k , v ) in query : if isinstance ( k , bytes ) : k = quote via ( k , safe ) else : k = quote via ( str ( k ) , safe , encoding , errors ) if isinstance ( v , bytes ) : v = quote via ( v , safe ) else : v = quote via ( str ( v ) , safe , encoding , errors ) l . append ( ( ( k + '=' ) + v ) ) else : for ( k , v ) in query : if isinstance ( k , bytes ) : k = quote via ( k , safe ) else : k = quote via ( str ( k ) , safe , encoding , errors ) if isinstance ( v , bytes ) : v = quote via ( v , safe ) l . append ( ( ( k + '=' ) + v ) ) elif isinstance ( v , str ) : v = quote via ( v , safe , encoding , errors ) l . append ( ( ( k + '=' ) + v ) ) else : try : x = len ( v ) except  Type  Error  : v = quote via ( str ( v ) , safe , encoding , errors ) l . append ( ( ( k + '=' ) + v ) ) else : for elt in v : if isinstance ( elt , bytes ) : elt = quote via ( elt , safe ) else : elt = quote via ( str ( elt ) , safe , encoding , errors ) l . append ( ( ( k + '=' ) + elt ) ) return '&' . join ( l ) 
def  Get  Async  ( keys , ** kwargs ) : extra hook = kwargs . pop ( 'extra hook' ,  None  ) config =   Get  Config  From  Kwargs  ( kwargs ) ( keys , multiple ) =  Normalize  And  Type  Check  Keys  ( keys ) def local extra hook ( entities ) : if multiple : result = entities else : if ( ( not entities ) or ( entities [ 0 ] is  None  ) ) : raise datastore errors .  Entity  Not  Found  Error  ( ) result = entities [ 0 ] if extra hook : return extra hook ( result ) return result return   Get  Connection  ( ) . async get ( config , keys , local extra hook ) 
def process ( name ) : ret = { 'name' : name , 'result' :  False  , 'comment' : '' , 'changes' : { } , 'data' : { } } data =   salt   [ 'status.pid' ] ( name ) if ( name not in data ) : ret [ 'result' ] =  False  ret [ 'comment' ] += ' Process   signature  "{0}"  not  found  ' . format ( name ) return ret ret [ 'data' ] = data ret [ 'comment' ] += ' Process   signature  "{0}"  was  found  ' . format ( name ) ret [ 'result' ] =  True  return ret 
def run ( cmd , logstream =  RUN DEFAULT LOGSTREAM ) :   run log ( logstream , "running  '%s'" , cmd ) if ( sys . platform == 'win32' ) : import process p = process .  Process  Open  ( cmd , stdin =  None  ) p . communicate ( ) retval = p . wait ( ) else : retval = os . system ( cmd ) if hasattr ( os , 'WEXITSTATUS' ) : status = os . WEXITSTATUS ( retval ) else : status = retval if status : raise OS Error  ( ( "error  running  '%s':  %r" % ( cmd , status ) ) ) 
def get version ( path ) : cmd = ( 'bzr  revno  %s' % path ) revno = run ( cmd ) . strip ( ) return revno 
@ pytest . mark . parametrize ( u'text1,text2' , [ ( u'phantom' , u'opera' ) ] ) def test vocab lexeme hash ( en vocab , text1 , text2 ) : lex1 = en vocab [ text1 ] lex2 = en vocab [ text2 ] lexes = { lex1 : lex1 , lex2 : lex2 } assert ( lexes [ lex1 ] . orth  == text1 ) assert ( lexes [ lex2 ] . orth  == text2 ) 
def parse implicit response ( uri , state =  None  , scope =  None  ) : if ( not is secure transport ( uri ) ) : raise  Insecure  Transport  Error  ( ) fragment = urlparse . urlparse ( uri ) . fragment params = dict ( urlparse . parse qsl ( fragment , keep blank values =  True  ) ) if ( u'scope' in params ) : params [ u'scope' ] = scope to list ( params [ u'scope' ] ) if ( u'expires in' in params ) : params [ u'expires at' ] = ( time . time ( ) + int ( params [ u'expires in' ] ) ) if ( state and ( params . get ( u'state' ,  None  ) != state ) ) : raise  Value  Error  ( u' Mismatching   or  missing  state  in  params.' ) params = O Auth 2 Token  ( params , old scope = scope ) validate token parameters ( params ) return params 
def flatten ( d , parent key = '' ) : items = [ ] for ( k , v ) in d . items ( ) : new key = ( ( ( parent key + '.' ) + k ) if parent key else k ) if isinstance ( v , collections .  Mutable  Mapping  ) : items . extend ( flatten ( v , new key ) . items ( ) ) else : items . append ( ( new key , v ) ) return dict ( items ) 
def extract Error  Message  ( page ) : ret Val  =  None  if isinstance ( page , basestring ) : for regex in ERROR PARSING REGEXES : match = re . search ( regex , page , ( re . DOTALL | re . IGNORECASE ) ) if match : ret Val  = htmlunescape ( match . group ( 'result' ) ) . replace ( '<br>' , '\n' ) . strip ( ) break return ret Val  
def  pipeline objects ( pipeline objects from pillars , pipeline object overrides ) : from pillars = copy . deepcopy (   salt   [ 'pillar.get' ] ( pipeline objects from pillars ) ) from pillars . update ( pipeline object overrides ) pipeline objects =  standardize (  dict to list ids ( from pillars ) ) for pipeline object in pipeline objects : pipeline object [ 'fields' ] =  properties from dict ( pipeline object [ 'fields' ] ) return pipeline objects 
def  Verify  Code  ( code ) : try : compile ( textwrap . dedent ( code ) . encode ( 'UTF-8' ) , '<string>' , 'exec' ) except  Syntax  Error  : try : ast . parse ( textwrap . dedent ( code . lstrip ( '\n' ) ) . lstrip ( ) , '<string>' , 'exec' ) except  Syntax  Error  : try : normalized code =   Normalize  Code  ( code ) compile ( normalized code . encode ( 'UTF-8' ) , '<string>' , 'exec' ) except  Syntax  Error  : raise  Internal  Error  ( sys . exc info ( ) [ 1 ] ) 
def wavwrite ( y , fs , filename ) : x = copy . deepcopy ( y ) x *= INT16 FAC x = np . int16 ( x ) write ( filename , fs , x ) 
def redirect output ( path ) : outfile = open ( path , 'a' ) sys . stdout = outfile sys . stderr = outfile 
def generate garch ( nobs , ar , ma , mu = 1.0 , scale = 0.1 ) : eta = ( scale * np . random . randn ( nobs ) ) h = signal . lfilter ( ma , ar , ( eta ** 2 ) ) err = ( np . sqrt ( h ) * eta ) return ( err , h ) 
def encode String  ( string ) : encoded String  = '' try : for char in string : octal = ( '%o' % ord ( char ) ) encoded String  += ( ( '\\' + ( ( 3 - len ( octal ) ) * '0' ) ) + octal ) except : return ( ( - 1 ) , ' Error   encoding  string' ) return ( 0 , encoded String  ) 
def is instance ( obj , klass ) : return issubclass ( type ( obj ) , klass ) 
def servicegroup server disable ( sg name , s name , s port , ** connection args ) : ret =  True  server =  servicegroup get server ( sg name , s name , s port , ** connection args ) if ( server is  None  ) : return  False  nitro =  connect ( ** connection args ) if ( nitro is  None  ) : return  False  try : NS Service  Group  . disable server ( nitro , server ) except NS Nitro  Error  as error : log . debug ( 'netscaler  module  error  -  NS Service  Group .disable server()  failed:  {0}' . format ( error ) ) ret =  False   disconnect ( nitro ) return ret 
def get channel layer ( alias = u'default' ) : if ( django . VERSION [ 1 ] > 9 ) : django . setup ( set prefix =  False  ) else : django . setup ( ) return channel layers [ alias ] . channel layer 
def absent ( name , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : ret = { 'name' : name , 'result' :  True  , 'comment' : '' , 'changes' : { } } if ( not   salt   [ 'boto cfn.exists' ] ( name , region , key , keyid , profile ) ) : ret [ 'comment' ] = ' Stack   {0}  does  not  exist.' . format ( name ) ret [ 'changes' ] =  None  return ret if   opts   [ 'test' ] : ret [ 'comment' ] = ' Stack   {0}  is  set  to  be  deleted.' . format ( name ) ret [ 'result' ] =  None  return ret deleted =   salt   [ 'boto cfn.delete' ] ( name , region , key , keyid , profile ) if isinstance ( deleted , str ) : ( code , message ) =  get error ( deleted ) ret [ 'comment' ] = ' Stack   {0}  could  not  be  deleted.\n{1}\n{2}' . format ( name , code , message ) ret [ 'result' ] =  False  ret [ 'changes' ] =  None  return ret if deleted : ret [ 'comment' ] = ' Stack   {0}  was  deleted.' . format ( name ) ret [ 'changes' ] [ 'deleted' ] = name return ret 
def swap16 ( value ) : return ( ( ( value & 255 ) << 8 ) | ( value >> 8 ) ) 
def run stdout ( cmd , cwd =  None  , stdin =  None  , runas =  None  , shell = DEFAULT SHELL , python shell =  None  , env =  None  , clean env =  False  , template =  None  , rstrip =  True  , umask =  None  , output loglevel = 'debug' , log callback =  None  , timeout =  None  , reset system locale =  True  , ignore retcode =  False  , saltenv = 'base' , use vt =  False  , password =  None  , ** kwargs ) : python shell =  python shell default ( python shell , kwargs . get ( '  pub jid' , '' ) ) ret =  run ( cmd , runas = runas , cwd = cwd , stdin = stdin , shell = shell , python shell = python shell , env = env , clean env = clean env , template = template , rstrip = rstrip , umask = umask , output loglevel = output loglevel , log callback = log callback , timeout = timeout , reset system locale = reset system locale , ignore retcode = ignore retcode , saltenv = saltenv , use vt = use vt , password = password , ** kwargs ) log callback =  check cb ( log callback ) lvl =  check loglevel ( output loglevel ) if ( lvl is not  None  ) : if ( ( not ignore retcode ) and ( ret [ 'retcode' ] != 0 ) ) : if ( lvl < LOG LEVELS [ 'error' ] ) : lvl = LOG LEVELS [ 'error' ] msg = " Command   '{0}'  failed  with  return  code:  {1}" . format ( cmd , ret [ 'retcode' ] ) log . error ( log callback ( msg ) ) if ret [ 'stdout' ] : log . log ( lvl , 'stdout:  {0}' . format ( log callback ( ret [ 'stdout' ] ) ) ) if ret [ 'stderr' ] : log . log ( lvl , 'stderr:  {0}' . format ( log callback ( ret [ 'stderr' ] ) ) ) if ret [ 'retcode' ] : log . log ( lvl , 'retcode:  {0}' . format ( ret [ 'retcode' ] ) ) return ret [ 'stdout' ] 
def copymode ( src , dst ) : if hasattr ( os , 'chmod' ) : st = os . stat ( src ) mode = stat . S IMODE ( st . st mode ) os . chmod ( dst , mode ) 
def as int ( n ) : try : result = int ( n ) if ( result != n ) : raise  Type  Error  except  Type  Error  : raise  Value  Error  ( ( '%s  is  not  an  integer' % ( n , ) ) ) return result 
def ffmpeg resize ( video , output , size ) : cmd = [ get setting ( 'FFMPEG BINARY' ) , '-i' , video , '-vf' , ( 'scale=%d:%d' % ( res [ 0 ] , res [ 1 ] ) ) , output ] subprocess call ( cmd ) 
def parse tstamp ( ev , field name ) : try : return datetime . strptime ( ev [ field name ] , '%Y-%m-%d  %H:%M' ) except  Exception  as e : log . error ( ( " Unable   to  parse  the  '%s'  field  in  the  event  named  '%s':  %s" % ( field name , ev [ 'title' ] , e ) ) ) raise 
def sort key by numeric other ( key value ) : return tuple ( ( ( ( int ( numeric ) if numeric else  None  ) , ( INSTANCE SIZES . index ( alpha ) if ( alpha in INSTANCE SIZES ) else alpha ) , other ) for ( numeric , alpha , other ) in RE NUMERIC OTHER . findall ( key value [ 0 ] ) ) ) 
def test io cov ( ) : tempdir =   Temp  Dir  ( ) cov = read cov ( cov fname ) cov [ 'method' ] = 'empirical' cov [ 'loglik' ] = ( - np . inf ) cov . save ( op . join ( tempdir , 'test-cov.fif' ) ) cov2 = read cov ( op . join ( tempdir , 'test-cov.fif' ) ) assert array almost equal ( cov . data , cov2 . data ) assert equal ( cov [ 'method' ] , cov2 [ 'method' ] ) assert equal ( cov [ 'loglik' ] , cov2 [ 'loglik' ] ) assert true ( ( ' Covariance ' in repr ( cov ) ) ) cov2 = read cov ( cov gz fname ) assert array almost equal ( cov . data , cov2 . data ) cov2 . save ( op . join ( tempdir , 'test-cov.fif.gz' ) ) cov2 = read cov ( op . join ( tempdir , 'test-cov.fif.gz' ) ) assert array almost equal ( cov . data , cov2 . data ) cov [ 'bads' ] = [ 'EEG  039' ] cov sel = pick channels cov ( cov , exclude = cov [ 'bads' ] ) assert true ( ( cov sel [ 'dim' ] == ( len ( cov [ 'data' ] ) - len ( cov [ 'bads' ] ) ) ) ) assert true ( ( cov sel [ 'data' ] . shape == ( cov sel [ 'dim' ] , cov sel [ 'dim' ] ) ) ) cov sel . save ( op . join ( tempdir , 'test-cov.fif' ) ) cov2 = read cov ( cov gz fname ) assert array almost equal ( cov . data , cov2 . data ) cov2 . save ( op . join ( tempdir , 'test-cov.fif.gz' ) ) cov2 = read cov ( op . join ( tempdir , 'test-cov.fif.gz' ) ) assert array almost equal ( cov . data , cov2 . data ) with warnings . catch warnings ( record =  True  ) as w : warnings . simplefilter ( 'always' ) cov badname = op . join ( tempdir , 'test-bad-name.fif.gz' ) write cov ( cov badname , cov ) read cov ( cov badname ) assert naming ( w , 'test cov.py' , 2 ) 
def render to text ( * args , ** kwargs ) : return  Http  Response  ( loader . render to string ( * args , ** kwargs ) , mimetype = 'text/plain' ) 
def imul ( a , b ) : a *= b return a 
def  get cache key namespace ( cache key ) : return force str ( cache key ) . split ( str ( u':' ) , 1 ) [ 0 ] 
def reduce Dim  ( data , dim , func = 'pca' ) : try : pca Func  = globals ( ) [ func ] except  Key  Error  : raise  Value  Error  ( ' Unknown   function  to  calc  principal  components' ) pc = pca Func  ( data , dim ) return ( pc * asmatrix ( make Centered  ( data ) ) . T ) . T 
def  Login  Service  Redirect  ( dest url , endpoint , ah url , outfile ) : redirect url = ( '%s?%s=%s' % ( endpoint , CONTINUE PARAM , urllib . quote ( ( '%s?%s=%s' % ( ah url , CONTINUE PARAM , dest url ) ) ) ) ) outfile . write ( ' Status :  302   Redirecting   to  login  service  URL\r\n' ) outfile . write ( ( ' Location :  %s\r\n' % redirect url ) ) outfile . write ( '\r\n' ) 
def stubout attach disks ( stubs ) : def f ( * args ) : raise  Xen API .  Failure  ( ' Test    Exception   raised  by  fake   attach disks' ) stubs .  Set  ( vmops . VM Ops  , ' attach disks' , f ) 
def limited ( items , request , max limit = CONF . osapi max limit ) : try : offset = int ( request . GET . get ( 'offset' , 0 ) ) except  Value  Error  : msg =   ( 'offset  param  must  be  an  integer' ) raise webob . exc . HTTP Bad  Request  ( explanation = msg ) try : limit = int ( request . GET . get ( 'limit' , max limit ) ) except  Value  Error  : msg =   ( 'limit  param  must  be  an  integer' ) raise webob . exc . HTTP Bad  Request  ( explanation = msg ) if ( limit < 0 ) : msg =   ( 'limit  param  must  be  positive' ) raise webob . exc . HTTP Bad  Request  ( explanation = msg ) if ( offset < 0 ) : msg =   ( 'offset  param  must  be  positive' ) raise webob . exc . HTTP Bad  Request  ( explanation = msg ) limit = min ( max limit , ( limit or max limit ) ) range end = ( offset + limit ) return items [ offset : range end ] 
def  vhost menu ( domain , vhosts ) : free chars = ( ( ( display util . WIDTH - len ( 'HTTPS' ) ) - len ( ' Enabled ' ) ) - 9 ) if ( free chars < 2 ) : logger . debug ( ' Display   size  is  too  small  for  certbot apache.display ops. vhost menu()' ) filename size = 1 disp name size = 1 else : filename size = int ( ( free chars * 0.6 ) ) disp name size = ( free chars - filename size ) choices = [ ] for vhost in vhosts : if ( len ( vhost . get names ( ) ) == 1 ) : disp name = next ( iter ( vhost . get names ( ) ) ) elif ( len ( vhost . get names ( ) ) == 0 ) : disp name = '' else : disp name = ' Multiple    Names ' choices . append ( '{fn:{fn size}s}  |  {name:{name size}s}  |  {https:5s}  |  {active:7s}' . format ( fn = os . path . basename ( vhost . filep ) [ : filename size ] , name = disp name [ : disp name size ] , https = ( 'HTTPS' if vhost . ssl else '' ) , active = ( ' Enabled ' if vhost . enabled else '' ) , fn size = filename size , name size = disp name size ) ) try : ( code , tag ) = zope . component . get Utility  ( interfaces . I Display  ) . menu ( ' We   were  unable  to  find  a  vhost  with  a   Server  Name   or   Address   of  {0}.{1} Which   virtual  host  would  you  like  to  choose?\n(note:  conf  files  with  multiple  vhosts  are  not  yet  supported)' . format ( domain , os . linesep ) , choices , help label = ' More    Info ' , ok label = ' Select ' , force interactive =  True  ) except errors .  Missing  Commandline  Flag  : msg = ' Encountered   vhost  ambiguity  but  unable  to  ask  for  user  guidance  in  non-interactive  mode.   Currently    Certbot   needs  each  vhost  to  be  in  its  own  conf  file,  and  may  need  vhosts  to  be  explicitly  labelled  with   Server  Name   or   Server  Alias   directories.' logger . warning ( msg ) raise errors .  Missing  Commandline  Flag  ( msg ) return ( code , tag ) 
def eval sort ( sorttype , expression , name =  None  , multipart = '' ) : from sabnzbd . api import  Ttemplate  path = '' name = sanitize foldername ( name ) if ( sorttype == 'series' ) : name = ( name or ( '%s  S01E05  -  %s  [DTS]' % (  Ttemplate  ( 'show-name' ) ,  Ttemplate  ( 'ep-name' ) ) ) ) sorter = sabnzbd . tvsort .  Series  Sorter  (  None  , name , path , 'tv' ) elif ( sorttype == 'generic' ) : name = ( name or (  Ttemplate  ( 'movie-sp-name' ) + '  (2009)' ) ) sorter = sabnzbd . tvsort .  Generic  Sorter  (  None  , name , path , 'tv' ) elif ( sorttype == 'date' ) : name = ( name or (  Ttemplate  ( 'show-name' ) + '  2009-01-02' ) ) sorter = sabnzbd . tvsort .  Date  Sorter  (  None  , name , path , 'tv' ) else : return  None  sorter . sort string = expression sorter . match ( force =  True  ) path = sorter . get final path ( ) path = os . path . normpath ( os . path . join ( path , sorter . filename set ) ) fname =  Ttemplate  ( 'org Filename ' ) fpath = path if ( ( sorttype == 'generic' ) and ( '%1' in multipart ) ) : fname = ( fname + multipart . replace ( '%1' , '1' ) ) fpath = ( fpath + multipart . replace ( '%1' , '1' ) ) if ( '%fn' in path ) : path = path . replace ( '%fn' , ( fname + '.avi' ) ) elif sorter . rename or not : path = ( fpath + '.avi' ) elif sabnzbd . WIN32 : path += '\\' else : path += '/' return path 
def p enumerator list 2 ( t ) : pass 
def gte ( value , arg ) : return ( value >= int ( arg ) ) 
def forward property ( member ) : class  Descriptor  ( object , ) : def   init   ( self , func or name ) : self .  property name = ( func or name .   name   if callable ( func or name ) else func or name ) def   get   ( self , obj , cls =  None  ) : return getattr ( getattr ( obj , member ) , self .  property name ) def   set   ( self , obj , value ) : return setattr ( getattr ( obj , member ) , self .  property name , value ) return  Descriptor  
def get first id ( lines ) : result = set ( ) for line in lines : if line . startswith ( '>' ) : result . add ( line [ 1 : ] . split ( ) [ 0 ] ) return result 
def get latest snapshot ( artifactory url , repository , group id , artifact id , packaging , target dir = '/tmp' , target file =  None  , classifier =  None  , username =  None  , password =  None  ) : log . debug ( '========================  MODULE  FUNCTION:  artifactory.get latest snapshot,  artifactory url=%s,  repository=%s,  group id=%s,  artifact id=%s,  packaging=%s,  target dir=%s,  classifier=%s)' , artifactory url , repository , group id , artifact id , packaging , target dir , classifier ) headers = { } if ( username and password ) : headers [ ' Authorization ' ] = ' Basic   {0}' . format ( base64 . encodestring ( '{0}:{1}' . format ( username , password ) ) . replace ( '\n' , '' ) ) artifact metadata =  get artifact metadata ( artifactory url = artifactory url , repository = repository , group id = group id , artifact id = artifact id , headers = headers ) version = artifact metadata [ 'latest version' ] ( snapshot url , file name ) =  get snapshot url ( artifactory url = artifactory url , repository = repository , group id = group id , artifact id = artifact id , version = version , packaging = packaging , classifier = classifier , headers = headers ) target file =   resolve target file ( file name , target dir , target file ) return   save artifact ( snapshot url , target file , headers ) 
@ utils . arg ( '--hypervisor' , metavar = '<hypervisor>' , default =  None  , help =   ( ' Type   of  hypervisor.' ) ) def do agent list ( cs , args ) : result = cs . agents . list ( args . hypervisor ) columns = [ ' Agent  id' , ' Hypervisor ' , 'OS' , ' Architecture ' , ' Version ' , ' Md 5hash' , ' Url ' ] utils . print list ( result , columns ) 
def main (   ) : if ( FLAGS . model == 'small' ) : init scale = 0.1 learning rate = 1.0 max grad norm = 5 num steps = 20 hidden size = 200 max epoch = 4 max max epoch = 13 keep prob = 1.0 lr decay = 0.5 batch size = 20 vocab size = 10000 elif ( FLAGS . model == 'medium' ) : init scale = 0.05 learning rate = 1.0 max grad norm = 5 num layers = 2 num steps = 35 hidden size = 650 max epoch = 6 max max epoch = 39 keep prob = 0.5 lr decay = 0.8 batch size = 20 vocab size = 10000 elif ( FLAGS . model == 'large' ) : init scale = 0.04 learning rate = 1.0 max grad norm = 10 num layers = 2 num steps = 35 hidden size = 1500 max epoch = 14 max max epoch = 55 keep prob = 0.35 lr decay = ( 1 / 1.15 ) batch size = 20 vocab size = 10000 else : raise  Value  Error  ( ' Invalid   model:  %s' , FLAGS . model ) ( train data , valid data , test data , vocab size ) = tl . files . load ptb dataset ( ) print 'len(train data)  {}' . format ( len ( train data ) ) print 'len(valid data)  {}' . format ( len ( valid data ) ) print 'len(test data)    {}' . format ( len ( test data ) ) print 'vocab size            {}' . format ( vocab size ) sess = tf .  Interactive  Session  ( ) input data = tf . placeholder ( tf . int32 , [ batch size , num steps ] ) targets = tf . placeholder ( tf . int32 , [ batch size , num steps ] ) input data test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) targets test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) def inference ( x , is training , num steps , reuse =  None  ) : ' If   reuse  is   True ,  the  inferences  use  the  existing  parameters,\n                then  different  inferences  share  the  same  parameters.\n\n                 Note   :\n                -   For    Dynamic RNN Layer ,  you  can  set  dropout  and  the  number  of  RNN  layer  internally.\n                ' print ( '\nnum steps  :  %d,  is training  :  %s,  reuse  :  %s' % ( num steps , is training , reuse ) ) initializer = tf . random uniform initializer ( init scale , init scale ) with tf . variable scope ( 'model' , reuse = reuse ) : tl . layers . set name reuse ( reuse ) network = tl . layers .  Embedding  Inputlayer  ( inputs = x , vocabulary size = vocab size , embedding size = hidden size , E init = tf . random uniform initializer ( ( - init scale ) , init scale ) , name = 'embedding layer' ) if is training : network = tl . layers .  Dropout  Layer  ( network , keep = keep prob , name = 'drop1' ) network = tl . layers . RNN Layer  ( network , cell fn = tf . nn . rnn cell .  Basic LSTM Cell  , cell init args = { 'forget bias' : 0.0 , 'state is tuple' :  True  } , n hidden = hidden size , initializer = tf . random uniform initializer ( ( - init scale ) , init scale ) , n steps = num steps , return last =  False  , name = 'basic lstm layer1' ) lstm1 = network if is training : network = tl . layers .  Dropout  Layer  ( network , keep = keep prob , name = 'drop2' ) network = tl . layers . RNN Layer  ( network , cell fn = tf . nn . rnn cell .  Basic LSTM Cell  , cell init args = { 'forget bias' : 0.0 , 'state is tuple' :  True  } , n hidden = hidden size , initializer = tf . random uniform initializer ( ( - init scale ) , init scale ) , n steps = num steps , return last =  False  , return seq 2d =  True  , name = 'basic lstm layer2' ) lstm2 = network if is training : network = tl . layers .  Dropout  Layer  ( network , keep = keep prob , name = 'drop3' ) network = tl . layers .  Dense  Layer  ( network , n units = vocab size , W init = tf . random uniform initializer ( ( - init scale ) , init scale ) , b init = tf . random uniform initializer ( ( - init scale ) , init scale ) , act = tf . identity , name = 'output layer' ) return ( network , lstm1 , lstm2 ) ( network , lstm1 , lstm2 ) = inference ( input data , is training =  True  , num steps = num steps , reuse =  None  ) ( network val , lstm1 val , lstm2 val ) = inference ( input data , is training =  False  , num steps = num steps , reuse =  True  ) ( network test , lstm1 test , lstm2 test ) = inference ( input data test , is training =  False  , num steps = 1 , reuse =  True  ) tl . layers . initialize global variables ( sess ) def loss fn ( outputs , targets , batch size , num steps ) : loss = tf . nn . seq2seq . sequence loss by example ( [ outputs ] , [ tf . reshape ( targets , [ ( - 1 ) ] ) ] , [ tf . ones ( [ ( batch size * num steps ) ] ) ] ) cost = ( tf . reduce sum ( loss ) / batch size ) return cost cost = loss fn ( network . outputs , targets , batch size , num steps ) cost val = loss fn ( network val . outputs , targets , batch size , num steps ) cost test = loss fn ( network test . outputs , targets test , 1 , 1 ) with tf . variable scope ( 'learning rate' ) : lr = tf .  Variable  ( 0.0 , trainable =  False  ) tvars = tf . trainable variables ( ) ( grads ,   ) = tf . clip by global norm ( tf . gradients ( cost , tvars ) , max grad norm ) optimizer = tf . train .  Gradient  Descent  Optimizer  ( lr ) train op = optimizer . apply gradients ( zip ( grads , tvars ) ) tl . layers . initialize global variables ( sess ) network . print params ( ) network . print layers ( ) tl . layers . print all variables ( ) print '\n Start   learning  a  language  model  by  using  PTB  dataset' for i in range ( max max epoch ) : new lr decay = ( lr decay ** max ( ( i - max epoch ) , 0.0 ) ) sess . run ( tf . assign ( lr , ( learning rate * new lr decay ) ) ) print ( ' Epoch :  %d/%d   Learning   rate:  %.3f' % ( ( i + 1 ) , max max epoch , sess . run ( lr ) ) ) epoch size = ( ( ( len ( train data ) // batch size ) - 1 ) // num steps ) start time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize rnn state ( lstm1 . initial state ) state2 = tl . layers . initialize rnn state ( lstm2 . initial state ) for ( step , ( x , y ) ) in enumerate ( tl . iterate . ptb iterator ( train data , batch size , num steps ) ) : feed dict = { input data : x , targets : y , lstm1 . initial state . c : state1 [ 0 ] , lstm1 . initial state . h : state1 [ 1 ] , lstm2 . initial state . c : state2 [ 0 ] , lstm2 . initial state . h : state2 [ 1 ] } feed dict . update ( network . all drop ) (  cost , state1 c , state1 h , state2 c , state2 h ,   ) = sess . run ( [ cost , lstm1 . final state . c , lstm1 . final state . h , lstm2 . final state . c , lstm2 . final state . h , train op ] , feed dict = feed dict ) state1 = ( state1 c , state1 h ) state2 = ( state2 c , state2 h ) costs +=  cost iters += num steps if ( ( step % ( epoch size // 10 ) ) == 10 ) : print ( '%.3f  perplexity:  %.3f  speed:  %.0f  wps' % ( ( ( step * 1.0 ) / epoch size ) , np . exp ( ( costs / iters ) ) , ( ( iters * batch size ) / ( time . time ( ) - start time ) ) ) ) train perplexity = np . exp ( ( costs / iters ) ) print ( ' Epoch :  %d/%d   Train    Perplexity :  %.3f' % ( ( i + 1 ) , max max epoch , train perplexity ) ) start time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize rnn state ( lstm1 val . initial state ) state2 = tl . layers . initialize rnn state ( lstm2 val . initial state ) for ( step , ( x , y ) ) in enumerate ( tl . iterate . ptb iterator ( valid data , batch size , num steps ) ) : feed dict = { input data : x , targets : y , lstm1 val . initial state . c : state1 [ 0 ] , lstm1 val . initial state . h : state1 [ 1 ] , lstm2 val . initial state . c : state2 [ 0 ] , lstm2 val . initial state . h : state2 [ 1 ] } (  cost , state1 c , state1 h , state2 c , state2 h ,   ) = sess . run ( [ cost val , lstm1 val . final state . c , lstm1 val . final state . h , lstm2 val . final state . c , lstm2 val . final state . h , tf . no op ( ) ] , feed dict = feed dict ) state1 = ( state1 c , state1 h ) state2 = ( state2 c , state2 h ) costs +=  cost iters += num steps valid perplexity = np . exp ( ( costs / iters ) ) print ( ' Epoch :  %d/%d   Valid    Perplexity :  %.3f' % ( ( i + 1 ) , max max epoch , valid perplexity ) ) print ' Evaluation ' start time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize rnn state ( lstm1 test . initial state ) state2 = tl . layers . initialize rnn state ( lstm2 test . initial state ) for ( step , ( x , y ) ) in enumerate ( tl . iterate . ptb iterator ( test data , batch size = 1 , num steps = 1 ) ) : feed dict = { input data test : x , targets test : y , lstm1 test . initial state . c : state1 [ 0 ] , lstm1 test . initial state . h : state1 [ 1 ] , lstm2 test . initial state . c : state2 [ 0 ] , lstm2 test . initial state . h : state2 [ 1 ] } (  cost , state1 c , state1 h , state2 c , state2 h ) = sess . run ( [ cost test , lstm1 test . final state . c , lstm1 test . final state . h , lstm2 test . final state . c , lstm2 test . final state . h ] , feed dict = feed dict ) state1 = ( state1 c , state1 h ) state2 = ( state2 c , state2 h ) costs +=  cost iters += 1 test perplexity = np . exp ( ( costs / iters ) ) print ( ' Test    Perplexity :  %.3f  took  %.2fs' % ( test perplexity , ( time . time ( ) - start time ) ) ) 
def  get mri header ( fname ) : import nibabel as nib img = nib . load ( fname ) try : return img . header except  Attribute  Error  : return img . get header ( ) 
def empty assets db ( ) : return tmp assets db ( equities =  None  ) 
def test one of ( ) : assert ( hug . types . one of ( ( 'bacon' , 'sausage' , 'pancakes' ) ) ( 'bacon' ) == 'bacon' ) assert ( hug . types . one of ( [ 'bacon' , 'sausage' , 'pancakes' ] ) ( 'sausage' ) == 'sausage' ) assert ( hug . types . one of ( { 'bacon' , 'sausage' , 'pancakes' } ) ( 'pancakes' ) == 'pancakes' ) assert ( 'bacon' in hug . types . one of ( { 'bacon' , 'sausage' , 'pancakes' } ) .   doc   ) with pytest . raises (  Key  Error  ) : hug . types . one of ( { 'bacon' , 'sausage' , 'pancakes' } ) ( 'syrup' ) 
def reverse bisect left ( a , x , lo = 0 , hi =  None  ) : if ( lo < 0 ) : raise  Value  Error  ( 'lo  must  be  non-negative' ) if ( hi is  None  ) : hi = len ( a ) while ( lo < hi ) : mid = ( ( lo + hi ) // 2 ) if ( x > a [ mid ] ) : hi = mid else : lo = ( mid + 1 ) return lo 
def test strict castform ( session , media root ) : with pytest . raises (  Value  Error  ) : castform = session . query ( tables .  Pokemon  Species  ) . filter by ( identifier = u'castform' ) . first ( ) rainy castform = [ f for f in castform . forms if ( f . form identifier == 'rainy' ) ] [ 0 ] rainy castform = media .  Pokemon  Form  Media  ( media root , rainy castform ) rainy castform . overworld ( 'up' , strict =  True  ) 
def debugger ( ) : rdb =  current [ 0 ] if ( ( rdb is  None  ) or ( not rdb . active ) ) : rdb =  current [ 0 ] =  Rdb  ( ) return rdb 
def tokenize regex ( input ) : p = re . compile ( u'^(\n                \\(\\?P\\<[a-zA-Z0-9 -]+\\>    |  #   Start   of  named  group.\n                \\(\\?#[^)]*\\)                          |  #   Comment \n                \\(\\?=                                        |  #   Start   of  lookahead  assertion\n                \\(\\?!                                        |  #   Start   of  negative  lookahead  assertion\n                \\(\\?<=                                      |  #   If   preceded  by.\n                \\(\\?<                                        |  #   If   not  preceded  by.\n                \\(?:                                          |  #   Start   of  group.  (non  capturing.)\n                \\(                                              |  #   Start   of  group.\n                \\(?[i Lmsux ]                            |  #   Flags .\n                \\(?P=[a-zA-Z]+\\)                  |  #   Back   reference  to  named  group\n                \\)                                              |  #   End   of  group.\n                \\{[^{}]*\\}                              |  #   Repetition \n                \\*\\?  |  \\+\\?  |  \\?\\?\\            |  #   Non   greedy  repetition.\n                \\*  |  \\+  |  \\?                          |  #   Repetition \n                \\#.*\\n                                      |  #   Comment \n                \\\\.  |\n\n                #   Character   group.\n                \\[\n                        (  [^\\]\\\\]    |    \\\\.)*\n                \\]                                    |\n\n                [^(){}]                          |\n                .\n        )' , re . VERBOSE ) tokens = [ ] while input : m = p . match ( input ) if m : ( token , input ) = ( input [ : m . end ( ) ] , input [ m . end ( ) : ] ) if ( not token . isspace ( ) ) : tokens . append ( token ) else : raise  Exception  ( u' Could   not  tokenize  input  regex.' ) return tokens 
def served by django ( url ) : r = requests . get ( url , allow redirects =  False  ) status = ( r . status code == 200 ) django = ( ( 'x-served' not in r . headers ) or ( r . headers [ 'x-served' ] == 'nginx-via-django' ) ) return all ( [ status , django ] ) 
@ pytest . mark . network def test upgrade from reqs file ( script ) : script . scratch path . join ( 'test-req.txt' ) . write ( textwrap . dedent ( '                 Py  Logo <0.4\n                #  and  something  else  to  test  out:\n                INI Tools ==0.3\n                ' ) ) install result = script . pip ( 'install' , '-r' , ( script . scratch path / 'test-req.txt' ) ) script . scratch path . join ( 'test-req.txt' ) . write ( textwrap . dedent ( '                 Py  Logo \n                #  and  something  else  to  test  out:\n                INI Tools \n                ' ) ) script . pip ( 'install' , '--upgrade' , '-r' , ( script . scratch path / 'test-req.txt' ) ) uninstall result = script . pip ( 'uninstall' , '-r' , ( script . scratch path / 'test-req.txt' ) , '-y' ) assert all changes ( install result , uninstall result , [ ( script . venv / 'build' ) , 'cache' , ( script . scratch / 'test-req.txt' ) ] ) 
def prefix filter flowgrams ( flowgrams , squeeze =  False  ) : if squeeze : seqs = imap ( ( lambda f : ( f .  Name  , squeeze seq ( str ( f . to Seq  ( truncate =  True  ) ) ) ) ) , flowgrams ) else : seqs = imap ( ( lambda f : ( f .  Name  , str ( f . to Seq  ( truncate =  True  ) ) ) ) , flowgrams ) mapping = build prefix map ( seqs ) l = len ( mapping ) orig l = ( sum ( [ len ( a ) for a in mapping . values ( ) ] ) + l ) return ( l , orig l , mapping ) 
def network create ( request , ** kwargs ) : LOG . debug ( ( 'network create():  kwargs  =  %s' % kwargs ) ) body = { 'network' : kwargs } network = quantumclient ( request ) . create network ( body = body ) . get ( 'network' ) return  Network  ( network ) 
def cityblock ( u , v ) : u =  validate vector ( u ) v =  validate vector ( v ) return abs ( ( u - v ) ) . sum ( ) 
def score from file ( json file ) : with open ( json file ) as f : results = json . load ( f ) if ( results is  None  ) : return  None  episode lengths = results [ 'episode lengths' ] episode rewards = results [ 'episode rewards' ] episode types = results [ 'episode types' ] timestamps = results [ 'timestamps' ] initial reset timestamp = results [ 'initial reset timestamp' ] spec = gym . spec ( results [ 'env id' ] ) return score from merged ( episode lengths , episode rewards , episode types , timestamps , initial reset timestamp , spec . trials , spec . reward threshold ) 
def sum outer product balanced ( x , n groups ) : xrs = x . reshape ( ( - 1 ) , n groups , order = 'F' ) return np . dot ( xrs , xrs . T ) 
def try send email with form ( func , form , field name , * args , ** kwargs ) : try : func ( * args , ** kwargs ) except SMTP Exception  as e : log . warning ( ( u' Failed   to  send  email:  %s' % e ) ) if ( 'email' not in form . errors ) : form . errors [ field name ] = [ ] form . errors [ field name ] . append ( unicode ( ERROR SEND EMAIL ) ) return form 
def target option ( s ) : return s 
@ gof . local optimizer ( [  Assert  ] ) def local remove all assert ( node ) : if ( not isinstance ( node . op ,  Assert  ) ) : return return [ node . inputs [ 0 ] ] 
def  best mime ( ) : supported = [ ] renders = { } for mime in  MIME TYPES : if app . config . get ( mime [ 'tag' ] ,  True  ) : for mime type in mime [ 'mime' ] : supported . append ( mime type ) renders [ mime type ] = mime [ 'renderer' ] if ( len ( supported ) == 0 ) : abort ( 500 , description = debug error message ( ' Configuration   error:  no  supported  mime  types' ) ) best match = ( request . accept mimetypes . best match ( supported ) or supported [ 0 ] ) return ( best match , renders [ best match ] ) 
def validate bool ( b ) : if ( type ( b ) is str ) : b = b . lower ( ) if ( b in ( 't' , 'y' , 'yes' , 'on' , 'true' , '1' , 1 ,  True  ) ) : return  True  elif ( b in ( 'f' , 'n' , 'no' , 'off' , 'false' , '0' , 0 ,  False  ) ) : return  False  else : raise  Value  Error  ( ( ' Could   not  convert  "%s"  to  boolean' % b ) ) 
def  get email config ( intent ) : require valid intent ( intent ) return config domain .  Registry  . get config property ( feconf . VALID MODERATOR ACTIONS [ intent ] [ 'email config' ] ) 
def sum squares ( expr ) : return quad over lin ( expr , 1 ) 
def get Changes  ( request , options =  None  ) : try : payload = json . load ( request . content ) except  Exception  as e : raise  Value  Error  ( ( ' Error   loading  JSON:  ' + str ( e ) ) ) user = payload [ 'user name' ] repo = payload [ 'repository' ] [ 'name' ] repo url = payload [ 'repository' ] [ 'url' ] project = request . args . get ( 'project' , [ '' ] ) [ 0 ] codebase = request . args . get ( 'codebase' ,  None  ) if codebase : codebase = codebase [ 0 ] changes =  process change ( payload , user , repo , repo url , project , codebase = codebase ) log . msg ( ( ' Received   %s  changes  from  gitlab' % len ( changes ) ) ) return ( changes , 'git' ) 
def get from identity ( session , key , passive ) : instance = session . identity map . get ( key ) if ( instance is not  None  ) : state = attributes . instance state ( instance ) if state . expired : if ( not ( passive & attributes . SQL OK ) ) : return attributes . PASSIVE NO RESULT elif ( not ( passive & attributes . RELATED OBJECT OK ) ) : return instance try : state ( state , passive ) except orm exc .  Object  Deleted  Error  : session .  remove newly deleted ( [ state ] ) return  None  return instance else : return  None  
def package absent ( m , name ) : retvals = { 'rc' : 0 , 'stdout' : '' , 'stderr' : '' } ( name install , name remove , urls ) = get want state ( m , name , remove =  True  ) if name install : m . fail json ( msg = " Can   not  combine  '+'  prefix  with  state=remove/absent." ) if urls : m . fail json ( msg = ' Can   not  remove  via  URL.' ) if ( m . params [ 'type' ] == 'patch' ) : m . fail json ( msg = ' Can   not  remove  patches.' ) prerun state = get installed state ( m , name remove ) remove version = [ ( p + name remove [ p ] ) for p in name remove if name remove [ p ] ] name remove = [ p for p in name remove if ( p in prerun state ) ] if ( ( not name remove ) and ( not remove version ) ) : return (  None  , retvals ) cmd = get cmd ( m , 'remove' ) cmd . extend ( name remove ) cmd . extend ( remove version ) retvals [ 'cmd' ] = cmd ( result , retvals [ 'rc' ] , retvals [ 'stdout' ] , retvals [ 'stderr' ] ) = parse zypper xml ( m , cmd ) return ( result , retvals ) 
def resolve stream name ( streams , stream name ) : if ( ( stream name in STREAM SYNONYMS ) and ( stream name in streams ) ) : for ( name , stream ) in streams . items ( ) : if ( ( stream is streams [ stream name ] ) and ( name not in STREAM SYNONYMS ) ) : return name return stream name 
def find all tests ( suite ) : suites = [ suite ] while suites : s = suites . pop ( ) try : suites . extend ( s ) except  Type  Error  : ( yield ( s , ( '%s.%s.%s' % ( s .   class   .   module   , s .   class   .   name   , s .  test Method  Name  ) ) ) ) 
def test sarcasm ( ) : dirty = u' Yeah   right  <sarcasm/>' clean = u' Yeah   right  &lt;sarcasm/&gt;' eq  ( clean , bleach . clean ( dirty ) ) 
def urlparse ( url , scheme = '' , allow fragments =  True  ) : tuple = urlsplit ( url , scheme , allow fragments ) ( scheme , netloc , url , query , fragment ) = tuple if ( ( scheme in uses params ) and ( ';' in url ) ) : ( url , params ) =  splitparams ( url ) else : params = '' return  Parse  Result  ( scheme , netloc , url , params , query , fragment ) 
def   converting factory ( specimen cls , original factory ) : instrumented cls =   canned instrumentation [ specimen cls ] def wrapper ( ) : collection = original factory ( ) return instrumented cls ( collection ) wrapper .   name   = ( '%s Wrapper ' % original factory .   name   ) wrapper .   doc   = original factory .   doc   return wrapper 
def get plus sign symbol ( locale = LC NUMERIC ) : return  Locale  . parse ( locale ) . number symbols . get ( 'plus Sign ' , u'+' ) 
def ports open ( name , ports , proto = 'tcp' , direction = 'in' ) : ports = list ( map ( str , ports ) ) diff =  False  ret = { 'name' : ',' . join ( ports ) , 'changes' : { } , 'result' :  True  , 'comment' : ' Ports   open.' } current ports =   salt   [ 'csf.get ports' ] ( proto = proto , direction = direction ) direction = direction . upper ( ) directions =   salt   [ 'csf.build directions' ] ( direction ) for direction in directions : print current ports [ direction ] print ports if ( current ports [ direction ] != ports ) : diff =  True  if diff : result =   salt   [ 'csf.allow ports' ] ( ports , proto = proto , direction = direction ) ret [ 'changes' ] [ ' Ports ' ] = ' Changed ' ret [ 'comment' ] = result return ret 
def paginate data ( request , data list , data type = 'cur' , per page = 25 , page = 1 ) : if ( not data list ) : paged data = [ ] page urls = { } else : paginator =  Paginator  ( data list , per page ) try : paged data = paginator . page ( page ) listed pages = pages to show ( paginator , page ) except  Page  Not  An  Integer  : paged data = paginator . page ( 1 ) listed pages = pages to show ( paginator , 1 ) except  Empty  Page  : paged data = paginator . page ( paginator . num pages ) listed pages = pages to show ( paginator , paginator . num pages ) if paged data : if paged data . has previous ( ) : prevGET Param  = request . GET . copy ( ) prevGET Param  [ ( data type + ' page' ) ] = paged data . previous page number ( ) previous page url = ( '?' + prevGET Param  . urlencode ( ) ) else : previous page url = '' if paged data . has next ( ) : nextGET Param  = request . GET . copy ( ) nextGET Param  [ ( data type + ' page' ) ] = paged data . next page number ( ) next page url = ( '?' + nextGET Param  . urlencode ( ) ) else : next page url = '' page urls = { 'next page' : next page url , 'prev page' : previous page url } if listed pages : for listed page in listed pages : if ( listed page != ( - 1 ) ) : GET Param  = request . GET . copy ( ) GET Param  [ ( data type + ' page' ) ] = listed page page urls . update ( { listed page : ( '?' + GET Param  . urlencode ( ) ) } ) paged data . listed pages = listed pages paged data . num listed pages = len ( listed pages ) return ( paged data , page urls ) 
def create resource ( options ) : deserializer = wsgi . JSON Request  Deserializer  ( ) serializer = serializers . JSON Response  Serializer  ( ) return wsgi .  Resource  (  Action  Controller  ( options ) , deserializer , serializer ) 
def assert identical ( a , b ) : assert equal ( a , b ) if ( type ( b ) is str ) : assert equal ( type ( a ) , type ( b ) ) else : assert equal ( np . asarray ( a ) . dtype . type , np . asarray ( b ) . dtype . type ) 
def dispatch ( c , id , methodname , args = ( ) , kwds = { } ) : c . send ( ( id , methodname , args , kwds ) ) ( kind , result ) = c . recv ( ) if ( kind == '#RETURN' ) : return result raise convert to error ( kind , result ) 
def  sanitize params ( prefix , suffix , dir ) : output type =  infer return type ( prefix , suffix , dir ) if ( suffix is  None  ) : suffix = output type ( ) if ( prefix is  None  ) : if ( output type is str ) : prefix = template else : prefix =  os . fsencode ( template ) if ( dir is  None  ) : if ( output type is str ) : dir = gettempdir ( ) else : dir = gettempdirb ( ) return ( prefix , suffix , dir , output type ) 
def  tconfint generic ( mean , std mean , dof , alpha , alternative ) : if ( alternative in [ 'two-sided' , '2-sided' , '2s' ] ) : tcrit = stats . t . ppf ( ( 1 - ( alpha / 2.0 ) ) , dof ) lower = ( mean - ( tcrit * std mean ) ) upper = ( mean + ( tcrit * std mean ) ) elif ( alternative in [ 'larger' , 'l' ] ) : tcrit = stats . t . ppf ( alpha , dof ) lower = ( mean + ( tcrit * std mean ) ) upper = np . inf elif ( alternative in [ 'smaller' , 's' ] ) : tcrit = stats . t . ppf ( ( 1 - alpha ) , dof ) lower = ( - np . inf ) upper = ( mean + ( tcrit * std mean ) ) else : raise  Value  Error  ( 'invalid  alternative' ) return ( lower , upper ) 
def processXML Element  ( xml Element  ) : lineation . processXML Element  By  Geometry  ( get Geometry  Output  (  None  , xml Element  ) , xml Element  ) 
def test pypi xml transformation ( ) : pypi hits = [ { 'name' : 'foo' , 'summary' : 'foo  summary' , 'version' : '1.0' } , { 'name' : 'foo' , 'summary' : 'foo  summary  v2' , 'version' : '2.0' } , { ' pypi ordering' : 50 , 'name' : 'bar' , 'summary' : 'bar  summary' , 'version' : '1.0' } ] expected = [ { 'versions' : [ '1.0' , '2.0' ] , 'name' : 'foo' , 'summary' : 'foo  summary  v2' } , { 'versions' : [ '1.0' ] , 'name' : 'bar' , 'summary' : 'bar  summary' } ] assert ( transform hits ( pypi hits ) == expected ) 
def get role assignments for user ( user db ) : result =  User  Role  Assignment  . query ( user = user db . name ) return result 
def user access decorator ( redirect func , redirect url func , deny func =  None  , redirect field = REDIRECT FIELD NAME ) : def decorator ( view fn ) : def  wrapped view ( request , * args , ** kwargs ) : redirect = redirect func ( request . user ) if ( redirect and ( not request . is ajax ( ) ) ) : redirect url = ( redirect url func ( ) or reverse ( 'users.login' ) ) if redirect field : path = urlquote ( request . get full path ( ) ) redirect url = ( '%s?%s=%s' % ( redirect url , redirect field , path ) ) return  Http  Response  Redirect  ( redirect url ) elif ( ( redirect and request . is ajax ( ) ) or ( deny func and deny func ( request . user ) ) ) : return  Http  Response  Forbidden  ( ) return view fn ( request , * args , ** kwargs ) return wraps ( view fn , assigned = available attrs ( view fn ) ) (  wrapped view ) return decorator 
def check ambigous ( flowgram , max allowed = 4 ) : if ( max allowed < 3 ) : raise  Value  Error  ( ' Error   in  calling  check ambigous.  max allowed  should  be  at  least  3' ) count = 0 for signal in flowgram . flowgram : if ( signal < 0.5 ) : count += 1 if ( count > max allowed ) : return  True  else : count = 0 return  False  
def parse startup ( pkt ) : values = [ ] plen = endian int ( pkt [ 0 : 4 ] ) pkt = pkt [ 8 : ] tmp = '' for i in xrange ( ( plen - 8 ) ) : tmp += pkt [ i ] . decode ( 'hex' ) if ( pkt [ i ] == '00' ) : values . append ( tmp ) tmp = '' return values 
def mask Matrix  ( matrix , shape = 'circle' , radius = 1.0 , center = ( 0.0 , 0.0 ) ) : alpha Mask  = make Mask  ( matrix . shape [ 0 ] , shape , radius , center = ( 0.0 , 0.0 ) , range = [ 0 , 1 ] ) return ( matrix * alpha Mask  ) 
def reorder coords ( coords , sample ids , order ) : try : result = array ( [ coords [ sample ids . index ( sample id ) ] for sample id in order ] ) except  Value  Error  : raise  Value  Error  ( ( ' Unknown   sample  ID:  %s' % sample id ) ) return result 
def register type ( ** kw ) :  Parse  Matcher  . custom types . update ( kw ) 
def pts to midstep ( x , * args ) : steps = np . zeros ( ( ( 1 + len ( args ) ) , ( 2 * len ( x ) ) ) ) x = np . asanyarray ( x ) steps [ 0 , 1 : ( - 1 ) : 2 ] = steps [ 0 , 2 : : 2 ] = ( ( x [ : ( - 1 ) ] + x [ 1 : ] ) / 2 ) ( steps [ ( 0 , 0 ) ] , steps [ ( 0 , ( - 1 ) ) ] ) = ( x [ 0 ] , x [ ( - 1 ) ] ) steps [ 1 : , 0 : : 2 ] = args steps [ 1 : , 1 : : 2 ] = steps [ 1 : , 0 : : 2 ] return steps 
def get Doubled  Round Z ( overhanging Segment  , segment Round Z ) : endpoint = overhanging Segment  [ 0 ] roundZ = ( endpoint . point - endpoint . other Endpoint  . point ) roundZ *= segment Round Z if ( abs ( roundZ ) == 0.0 ) : return complex ( ) if ( roundZ . real < 0.0 ) : roundZ *= ( - 1.0 ) roundZ Length  = abs ( roundZ ) return ( ( roundZ * roundZ ) / roundZ Length  ) 
def loads ( value ) : result =  None  try : result = pickle . loads ( base64 . b64decode ( value ) ) except pickle .  Pickle  Error  : pass return result 
def in6 getscope ( addr ) : if ( in6 isgladdr ( addr ) or in6 isuladdr ( addr ) ) : scope = IPV6 ADDR GLOBAL elif in6 islladdr ( addr ) : scope = IPV6 ADDR LINKLOCAL elif in6 issladdr ( addr ) : scope = IPV6 ADDR SITELOCAL elif in6 ismaddr ( addr ) : if in6 ismgladdr ( addr ) : scope = IPV6 ADDR GLOBAL elif in6 ismlladdr ( addr ) : scope = IPV6 ADDR LINKLOCAL elif in6 ismsladdr ( addr ) : scope = IPV6 ADDR SITELOCAL elif in6 ismnladdr ( addr ) : scope = IPV6 ADDR LOOPBACK else : scope = ( - 1 ) elif ( addr == '::1' ) : scope = IPV6 ADDR LOOPBACK else : scope = ( - 1 ) return scope 
@ util . positional ( 3 ) def run flow ( flow , storage , flags , http =  None  ) : logging . get Logger  ( ) . set Level  ( getattr ( logging , flags . logging level ) ) if ( not flags . noauth local webserver ) : success =  False  port number = 0 for port in flags . auth host port : port number = port try : httpd =  Client  Redirect  Server  ( ( flags . auth host name , port ) ,  Client  Redirect  Handler  ) except socket . error : pass else : success =  True  break flags . noauth local webserver = ( not success ) if ( not success ) : print ( ' Failed   to  start  a  local  webserver  listening  on  either  port  8080' ) print ( 'or  port  9090.   Please   check  your  firewall  settings  and  locally' ) print ( 'running  programs  that  may  be  blocking  or  using  those  ports.' ) print ( ) print ( ' Falling   back  to  --noauth local webserver  and  continuing  with' ) print ( 'authorization.' ) print ( ) if ( not flags . noauth local webserver ) : oauth callback = ( 'http://%s:%s/' % ( flags . auth host name , port number ) ) else : oauth callback = client . OOB CALLBACK URN flow . redirect uri = oauth callback authorize url = flow . step1 get authorize url ( ) if ( not flags . noauth local webserver ) : import webbrowser webbrowser . open ( authorize url , new = 1 , autoraise =  True  ) print ( ' Your   browser  has  been  opened  to  visit:' ) print ( ) print ( ( '        ' + authorize url ) ) print ( ) print ( ' If   your  browser  is  on  a  different  machine  then  exit  and  re-run  this' ) print ( 'application  with  the  command-line  parameter  ' ) print ( ) print ( '    --noauth local webserver' ) print ( ) else : print ( ' Go   to  the  following  link  in  your  browser:' ) print ( ) print ( ( '        ' + authorize url ) ) print ( ) code =  None  if ( not flags . noauth local webserver ) : httpd . handle request ( ) if ( 'error' in httpd . query params ) : sys . exit ( ' Authentication   request  was  rejected.' ) if ( 'code' in httpd . query params ) : code = httpd . query params [ 'code' ] else : print ( ' Failed   to  find  "code"  in  the  query  parameters  of  the  redirect.' ) sys . exit ( ' Try   running  with  --noauth local webserver.' ) else : code = input ( ' Enter   verification  code:  ' ) . strip ( ) try : credential = flow . step2 exchange ( code , http = http ) except client .  Flow  Exchange  Error  as e : sys . exit ( ( ' Authentication   has  failed:  %s' % e ) ) storage . put ( credential ) credential . set store ( storage ) print ( ' Authentication   successful.' ) return credential 
def test constant folding ( ) : x = tensor . dvector ( ) mode = theano . compile . get mode ( 'FAST COMPILE' ) . excluding ( 'fusion' ) f = theano . function ( [ x ] , [ ( x * 2 ) , ( x + x ) ] , mode = mode ) topo = f . maker . fgraph . toposort ( ) assert ( len ( topo ) == 2 ) x = tensor . constant ( 3 ) assert ( x . ndim == 0 ) mode = theano . compile . get mode ( 'FAST COMPILE' ) . excluding ( 'fusion' ) f = theano . function ( [ ] , [ ( x * 2 ) , ( x + x ) ] , mode = mode ) topo = f . maker . fgraph . toposort ( ) assert ( len ( topo ) == 2 ) assert all ( [ isinstance ( n . op ,  Deep  Copy  Op  ) for n in topo ] ) 
def bigaddrspacetest ( f ) : def wrapper ( self ) : if ( max memuse < MAX  Py  ssize t ) : if verbose : sys . stderr . write ( ( ' Skipping   %s  because  of  memory  constraint\n' % ( f .   name   , ) ) ) else : return f ( self ) return wrapper 
def calc circumcenters ( tetrahedrons ) : num = tetrahedrons . shape [ 0 ] a = np . concatenate ( ( tetrahedrons , np . ones ( ( num , 4 , 1 ) ) ) , axis = 2 ) sums = np . sum ( ( tetrahedrons ** 2 ) , axis = 2 ) d = np . concatenate ( ( sums [ : , : , np . newaxis ] , a ) , axis = 2 ) dx = np . delete ( d , 1 , axis = 2 ) dy = np . delete ( d , 2 , axis = 2 ) dz = np . delete ( d , 3 , axis = 2 ) dx = np . linalg . det ( dx ) dy = ( - np . linalg . det ( dy ) ) dz = np . linalg . det ( dz ) a = np . linalg . det ( a ) nominator = np . vstack ( ( dx , dy , dz ) ) denominator = ( 2 * a ) return ( nominator / denominator ) . T 
def  handle Model  Runner  Exception  ( jobID , modelID , jobsDAO , experiment Dir  , logger , e ) : msg =  String IO .  String IO ( ) print >> msg , ( ' Exception   occurred  while  running  model  %s:  %r  (%s)' % ( modelID , e , type ( e ) ) ) traceback . print exc (  None  , msg ) completion Reason  = jobsDAO . CMPL REASON ERROR completion Msg  = msg . getvalue ( ) logger . error ( completion Msg  ) if ( type ( e ) is not  Invalid  Connection  Exception  ) : jobsDAO . model Update  Results  ( modelID , results =  None  , num Records  = 0 ) if ( type ( e ) ==  Job  Fail  Exception  ) : worker Cmp  Reason  = jobsDAO . job Get  Fields  ( jobID , [ 'worker Completion  Reason ' ] ) [ 0 ] if ( worker Cmp  Reason  ==  Client  Jobs DAO . CMPL REASON SUCCESS ) : jobsDAO . job Set  Fields  ( jobID , fields = dict ( cancel =  True  , worker Completion  Reason  =  Client  Jobs DAO . CMPL REASON ERROR , worker Completion  Msg  = ':  ' . join ( ( str ( i ) for i in e . args ) ) ) , use Connection ID =  False  , ignore Unchanged  =  True  ) return ( completion Reason  , completion Msg  ) 
def rws ( t ) : for c in [ ' DCTB ' , '\n' , '  ' ] : t = t . replace ( c , '' ) return t 
def   virtual   ( ) : log . debug ( 'rest sample  proxy    virtual  ()  called...' ) return  True  
def paste deploy app ( paste config file , app name , conf ) : setup paste factories ( conf ) try : return deploy . loadapp ( ( 'config:%s' % paste config file ) , name = app name ) finally : teardown paste factories ( ) 
def generate password hash ( password , method = 'sha1' , length = 22 , pepper =  None  ) : salt = ( ( ( method != 'plain' ) and generate random string ( length ) ) or '' ) hashval = hash password ( password , method , salt , pepper ) if ( hashval is  None  ) : raise  Type  Error  ( ( ' Invalid   method  %r.' % method ) ) return ( '%s$%s$%s' % ( hashval , method , salt ) ) 
def db sync ( version =  None  ) : return IMPL . db sync ( version = version ) 
def  url as string ( url ) : if isinstance ( url , urllib2 .  Request  ) : return url . get full url ( ) elif isinstance ( url , basestring ) : return url else : raise  Type  Error  ( ( ' Expected   type  %r  or  %r' % ( basestring , urllib2 .  Request  ) ) ) 
def year to days ( builder , year val ) : ret = cgutils . alloca once ( builder , TIMEDELTA64 ) days = scale by constant ( builder , year val , 365 ) with builder . if else ( cgutils . is neg int ( builder , year val ) ) as ( if neg , if pos ) : with if pos : from 1968 = add constant ( builder , year val , 1 ) p days = builder . add ( days , unscale by constant ( builder , from 1968 , 4 ) ) from 1900 = add constant ( builder , from 1968 , 68 ) p days = builder . sub ( p days , unscale by constant ( builder , from 1900 , 100 ) ) from 1600 = add constant ( builder , from 1900 , 300 ) p days = builder . add ( p days , unscale by constant ( builder , from 1600 , 400 ) ) builder . store ( p days , ret ) with if neg : from 1972 = add constant ( builder , year val , ( - 2 ) ) n days = builder . add ( days , unscale by constant ( builder , from 1972 , 4 ) ) from 2000 = add constant ( builder , from 1972 , ( - 28 ) ) n days = builder . sub ( n days , unscale by constant ( builder , from 2000 , 100 ) ) n days = builder . add ( n days , unscale by constant ( builder , from 2000 , 400 ) ) builder . store ( n days , ret ) return builder . load ( ret ) 
def serialize ( model , callbacks =  None  , datasets =  None  , dump weights =  True  , keep states =  True  ) : pdict = model . serialize ( fn =  None  , keep states = keep states ) if ( callbacks is not  None  ) : pdict [ 'callbacks' ] = callbacks . serialize ( ) if ( datasets is not  None  ) : pdict [ 'datasets' ] = datasets . serialize ( ) return pdict 
def poller ( check run complete f , process run results f , clean up f , check run complete file , process run results file , clean up file , seconds to sleep ) : number of loops = 0 while ( not check run complete f ( check run complete file ) ) : sleep ( seconds to sleep ) number of loops += 1 process run results f ( process run results file ) clean up f ( clean up file ) est per proc run time = ( number of loops * seconds to sleep ) return est per proc run time 
def fix headers ( soup , data ) : header rows = soup . find all ( 'th' ) for t in header rows : if ( ' Version :' in t . text ) : if ( t . next sibling . text == '$ Revision $' ) : t . parent . extract ( ) if ( t . next sibling . text == '' ) : t . parent . extract ( ) if ( ' Last - Modified :' in t . text ) : if ( '$ Date $' in t . next sibling . text ) : t . parent . extract ( ) if ( t . next sibling . text == '' ) : t . parent . extract ( ) if ( t . text == ' Title :' ) : data [ 'title' ] = t . next sibling . text if ( t . text == ' Content - Type :' ) : t . parent . extract ( ) if ( ( ' Version :' in t . text ) and ( 'N/A' in t . next sibling . text ) ) : t . parent . extract ( ) return ( soup , data ) 
def delete  ( * keyname ) : mdata =  check mdata delete ( ) valid keynames = list  ( ) ret = { } for k in keyname : if ( mdata and ( k in valid keynames ) ) : cmd = '{0}  {1}' . format ( mdata , k ) ret [ k ] = (   salt   [ 'cmd.run all' ] ( cmd ) [ 'retcode' ] == 0 ) else : ret [ k ] =  True  return ret 
def  brick get connector properties ( multipath =  False  , enforce multipath =  False  ) : return DEFAULT CONNECTOR 
def succeed with changes ( name ) : ret = { 'name' : name , 'changes' : { } , 'result' :  True  , 'comment' : ' Success !' } ret [ 'changes' ] = { 'testing' : { 'old' : ' Unchanged ' , 'new' : ' Something   pretended  to  change' } } if   opts   [ 'test' ] : ret [ 'result' ] =  None  ret [ 'comment' ] = " If   we  weren't  testing,  this  would  be  successful  with  changes" return ret 
def  update  doc   ( data class , readwrite ) : FORMATS TEXT = u' The   available  built-in  formats  are:' class readwrite func = getattr ( data class , readwrite ) if ( not isinstance ( class readwrite func .   doc   , six . string types ) ) : return lines = class readwrite func .   doc   . splitlines ( ) sep indices = [ ii for ( ii , line ) in enumerate ( lines ) if ( FORMATS TEXT in line ) ] if sep indices : chop index = sep indices [ 0 ] lines = lines [ : chop index ] matches = [ re . search ( u'(\\S)' , line ) for line in lines [ 1 : ] ] left indent = ( u'  ' * min ( ( match . start ( ) for match in matches if match ) ) ) format table = get formats ( data class , readwrite . capitalize ( ) ) format table . remove column ( u' Data   class' ) new lines = format table . pformat ( max lines = ( - 1 ) , max width = 80 ) table rst sep = re . sub ( u'-' , u'=' , new lines [ 1 ] ) new lines [ 1 ] = table rst sep new lines . insert ( 0 , table rst sep ) new lines . append ( table rst sep ) if ( u' Deprecated ' in format table . colnames ) : new lines . extend ( [ u'' , u' Deprecated   format  names  like  ``aastex``  will  be  removed  in  a  future  version.   Use   the  full  ' , u'name  (e.g.  ``ascii.aastex``)  instead.' ] ) new lines = ( [ FORMATS TEXT , u'' ] + new lines ) lines . extend ( [ ( left indent + line ) for line in new lines ] ) try : class readwrite func .   doc   = u'\n' . join ( lines ) except  Attribute  Error  : class readwrite func .   func   .   doc   = u'\n' . join ( lines ) 
def get flowgram ali exe ( ) : return ' Flowgram  Ali  4frame' 
def   virtual   ( ) : if ( not HAS BOTO ) : return  False  boto version =  Strict  Version  ( boto .   version   ) required boto version =  Strict  Version  ( '2.8.0' ) if ( boto version < required boto version ) : log . error ( "%s:  installed  boto  version  %s  <  %s,  can't  retrieve  instance  data" ,   name   , boto version , required boto version ) return  False  return  True  
def  has Substring  ( key , text ) : escaped Key  = re . escape ( key ) return bool ( re . search ( ( ( '\\W' + escaped Key  ) + '\\W' ) , text ) ) 
def  Instance 2 Str  ( o , d ) : if d . has key ( o .   class   ) : return d [ o .   class   ] ( o , d ) cl = filter ( ( lambda x , o = o : ( ( type ( x ) is types .  Class  Type  ) and isinstance ( o , x ) ) ) , d . keys ( ) ) if ( ( not cl ) and hasattr ( types , ' Object  Type ' ) ) : cl = filter ( ( lambda x , o = o : ( ( type ( x ) is types .  Type  Type  ) and isinstance ( o , x ) and ( d [ x ] is not  Instance 2 Str  ) ) ) , d . keys ( ) ) if ( not cl ) : return d [ types .  String  Type  ] ( o , d ) d [ o .   class   ] = d [ cl [ 0 ] ] return d [ cl [ 0 ] ] ( o , d ) 
def main ( ) : fn root = FLAGS . fn root img fn list = os . listdir ( fn root ) img fn list = [ img fn for img fn in img fn list if img fn . endswith ( '.webp' ) ] num examples = len ( img fn list ) n examples per file = 10000 for ( example idx , img fn ) in enumerate ( img fn list ) : if ( ( example idx % n examples per file ) == 0 ) : file out = '%s %05d.tfrecords' file out = ( file out % ( FLAGS . file out , ( example idx // n examples per file ) ) ) print ' Writing   on:' , file out writer = tf . python io . TF Record  Writer  ( file out ) if ( ( example idx % 1000 ) == 0 ) : print example idx , '/' , num examples image raw = numpy . array (  Image  . open ( os . path . join ( fn root , img fn ) ) ) rows = image raw . shape [ 0 ] cols = image raw . shape [ 1 ] depth = image raw . shape [ 2 ] downscale = min ( ( rows / 96.0 ) , ( cols / 96.0 ) ) image raw = skimage . transform . pyramid reduce ( image raw , downscale ) image raw *= 255.0 image raw = image raw . astype ( 'uint8' ) rows = image raw . shape [ 0 ] cols = image raw . shape [ 1 ] depth = image raw . shape [ 2 ] image raw = image raw . tostring ( ) example = tf . train .  Example  ( features = tf . train .  Features  ( feature = { 'height' :  int64 feature ( rows ) , 'width' :  int64 feature ( cols ) , 'depth' :  int64 feature ( depth ) , 'image raw' :  bytes feature ( image raw ) } ) ) writer . write ( example .  Serialize  To  String  ( ) ) if ( ( example idx % n examples per file ) == ( n examples per file - 1 ) ) : writer . close ( ) writer . close ( ) 
def fetch lfw pairs ( subset = 'train' , data home =  None  , funneled =  True  , resize = 0.5 , color =  False  , slice  = ( slice ( 70 , 195 ) , slice ( 78 , 172 ) ) , download if missing =  True  ) : ( lfw home , data folder path ) = check fetch lfw ( data home = data home , funneled = funneled , download if missing = download if missing ) logger . info ( ' Loading   %s  LFW  pairs  from  %s' , subset , lfw home ) m =  Memory  ( cachedir = lfw home , compress = 6 , verbose = 0 ) load func = m . cache (  fetch lfw pairs ) label filenames = { 'train' : 'pairs Dev  Train .txt' , 'test' : 'pairs Dev  Test .txt' , '10 folds' : 'pairs.txt' } if ( subset not in label filenames ) : raise  Value  Error  ( ( "subset='%s'  is  invalid:  should  be  one  of  %r" % ( subset , list ( sorted ( label filenames . keys ( ) ) ) ) ) ) index file path = join ( lfw home , label filenames [ subset ] ) ( pairs , target , target names ) = load func ( index file path , data folder path , resize = resize , color = color , slice  = slice  ) return  Bunch  ( data = pairs . reshape ( len ( pairs ) , ( - 1 ) ) , pairs = pairs , target = target , target names = target names , DESCR = ( "'%s'  segment  of  the  LFW  pairs  dataset" % subset ) ) 
def  gen npfuncs ( k , L1 wt , alpha , loglike kwds , score kwds , hess kwds ) : def nploglike ( params , model ) : nobs = model . nobs pen llf = ( ( ( alpha [ k ] * ( 1 - L1 wt ) ) * np . sum ( ( params ** 2 ) ) ) / 2 ) llf = model . loglike ( np . r  [ params ] , ** loglike kwds ) return ( ( ( - llf ) / nobs ) + pen llf ) def npscore ( params , model ) : nobs = model . nobs pen grad = ( ( alpha [ k ] * ( 1 - L1 wt ) ) * params ) gr = ( ( - model . score ( np . r  [ params ] , ** score kwds ) [ 0 ] ) / nobs ) return ( gr + pen grad ) def nphess ( params , model ) : nobs = model . nobs pen hess = ( alpha [ k ] * ( 1 - L1 wt ) ) h = ( ( ( - model . hessian ( np . r  [ params ] , ** hess kwds ) [ ( 0 , 0 ) ] ) / nobs ) + pen hess ) return h return ( nploglike , npscore , nphess ) 
def export ( local =  False  , path = '/tmp' , format = 'qcow2' ) : if ( getpass . getuser ( ) != 'root' ) : raise  Command  Execution  Error  ( ' In   order  to  export  system,  the  minion  should  run  as  "root".' ) try : description =   ( 'query' ) .  Query  ( 'all' , cachedir =   opts   [ 'cachedir' ] ) ( ) return   ( 'collector' ) .  Inspector  ( ) . reuse snapshot ( ) . export ( description , local = local , path = path , format = format ) except  Inspector  Kiwi  Processor  Exception  as ex : raise  Command  Execution  Error  ( ex ) except  Exception  as ex : log . error (  get error message ( ex ) ) raise  Exception  ( ex ) 
@ task def browse ( ctx ) : page html = ( (  Path  ( ctx . config . sphinx . destdir ) / 'html' ) / 'index.html' ) if ( not page html . exists ( ) ) : build ( ctx , builder = 'html' ) assert page html . exists ( ) open cmd = 'open' if sys . platform . startswith ( 'win' ) : open cmd = 'start' ctx . run ( '{open}  {page html}' . format ( open = open cmd , page html = page html ) ) 
def convert Element  Node  Rename  By  Paths  ( element Node  , geometry Output  ) : create Link  Path  ( element Node  ) for geometry Output  Child  in geometry Output  : path Element  = xml simple reader .  Element  Node  ( ) path Element  . set Parent  Add  To  Child  Nodes  ( element Node  ) convert Element  Node  By  Path  ( path Element  , geometry Output  Child  ) 
def  Parse  ( text , message ) :   Parse  Or  Merge  ( text , message ,  False  ) 
def trg redirect ( uid , res type , res id , new rid , cr ) : assert isinstance ( new rid , ( long , int ) ) return  Workflow  Service  . new ( cr , uid , res type , res id ) . redirect ( new rid ) 
def get pack file abs path ( pack ref , file path ) : pack base path = get pack base path ( pack name = pack ref ) path components = [ ] path components . append ( pack base path ) normalized file path = os . path . normpath ( ( '/' + file path ) ) . lstrip ( '/' ) if ( normalized file path != file path ) : raise  Value  Error  ( ( ' Invalid   file  path:  %s' % file path ) ) path components . append ( normalized file path ) result = os . path . join ( * path components ) assert ( normalized file path in result ) common prefix = os . path . commonprefix ( [ pack base path , result ] ) if ( common prefix != pack base path ) : raise  Value  Error  ( ( ' Invalid   file path:  %s' % file path ) ) return result 
def untar file ( filename , location ) : ensure dir ( location ) if ( filename . lower ( ) . endswith ( '.gz' ) or filename . lower ( ) . endswith ( '.tgz' ) ) : mode = 'r:gz' elif filename . lower ( ) . endswith ( BZ2 EXTENSIONS ) : mode = 'r:bz2' elif filename . lower ( ) . endswith ( XZ EXTENSIONS ) : mode = 'r:xz' elif filename . lower ( ) . endswith ( '.tar' ) : mode = 'r' else : logger . warning ( ' Cannot   determine  compression  type  for  file  %s' , filename ) mode = 'r:*' tar = tarfile . open ( filename , mode ) try : leading = has leading dir ( [ member . name for member in tar . getmembers ( ) if ( member . name != 'pax global header' ) ] ) for member in tar . getmembers ( ) : fn = member . name if ( fn == 'pax global header' ) : continue if leading : fn = split leading dir ( fn ) [ 1 ] path = os . path . join ( location , fn ) if member . isdir ( ) : ensure dir ( path ) elif member . issym ( ) : try : tar .  extract member ( member , path ) except  Exception  as exc : logger . warning ( ' In   the  tar  file  %s  the  member  %s  is  invalid:  %s' , filename , member . name , exc ) continue else : try : fp = tar . extractfile ( member ) except (  Key  Error  ,  Attribute  Error  ) as exc : logger . warning ( ' In   the  tar  file  %s  the  member  %s  is  invalid:  %s' , filename , member . name , exc ) continue ensure dir ( os . path . dirname ( path ) ) with open ( path , 'wb' ) as destfp : shutil . copyfileobj ( fp , destfp ) fp . close ( ) tar . utime ( member , path ) if ( member . mode & 73 ) : os . chmod ( path , ( ( 511 - current umask ( ) ) | 73 ) ) finally : tar . close ( ) 
@ pytest . mark . django db def test campaign creation ( rf , admin user ) : with override settings ( LANGUAGES = [ ( u'en' , u'en' ) ] ) : view =  Catalog  Campaign  Edit  View  . as view ( ) data = { u'base-name' : u' Test    Campaign ' , u'base-public name  en' : u' Test    Campaign ' , u'base-shop' : get default shop ( ) . id , u'base-active' :  True  , u'base-basket line text' : u' Test   campaign  activated!' } campaigns before =  Catalog  Campaign  . objects . count ( ) request = apply request middleware ( rf . post ( u'/' , data = data ) , user = admin user ) response = view ( request , pk =  None  ) assert ( response . status code in [ 200 , 302 ] ) assert (  Catalog  Campaign  . objects . count ( ) == ( campaigns before + 1 ) ) 
def action peek xml ( body ) : dom = xmlutil . safe minidom parse string ( body ) action node = dom . child Nodes  [ 0 ] return action node . tag Name  
def waterbutler url for ( route , provider , path , node , user =  None  ,  internal =  False  , ** kwargs ) : url = furl . furl ( ( website settings . WATERBUTLER INTERNAL URL if  internal else website settings . WATERBUTLER URL ) ) url . path . segments . append ( waterbutler action map [ route ] ) url . args . update ( { 'path' : path , 'nid' : node .  id , 'provider' : provider } ) if user : url . args [ 'cookie' ] = user . get or create cookie ( ) elif ( website settings . COOKIE NAME in request . cookies ) : url . args [ 'cookie' ] = request . cookies [ website settings . COOKIE NAME ] view only =  False  if ( 'view only' in kwargs ) : view only = kwargs . get ( 'view only' ) else : view only = request . args . get ( 'view only' ) url . args [ 'view only' ] = view only url . args . update ( kwargs ) return url . url 
def test timeout start ( qtbot ) : t = usertypes .  Timer  ( ) with qtbot . wait Signal  ( t . timeout , timeout = 3000 ) : t . start ( 200 ) 
def get ssh certificate tokens ( module , ssh cert path ) : ( rc , stdout , stderr ) = module . run command ( [ 'openssl' , 'x509' , '-in' , ssh cert path , '-fingerprint' , '-noout' ] ) if ( rc != 0 ) : module . fail json ( msg = ( 'failed  to  generate  the  key  fingerprint,  error  was:  %s' % stderr ) ) fingerprint = stdout . strip ( ) [ 17 : ] . replace ( ':' , '' ) ( rc , stdout , stderr ) = module . run command ( [ 'openssl' , 'pkcs12' , '-export' , '-in' , ssh cert path , '-nokeys' , '-password' , 'pass:' ] ) if ( rc != 0 ) : module . fail json ( msg = ( 'failed  to  generate  the  pkcs12  signature  from  the  certificate,  error  was:  %s' % stderr ) ) pkcs12 base64 = base64 . b64encode ( stdout . strip ( ) ) return ( fingerprint , pkcs12 base64 ) 
def match filter ( filter list , userargs ) : found filter =  None  for f in filter list : if f . match ( userargs ) : if isinstance ( f , filters .  Exec  Command  Filter  ) : leaf filters = [ fltr for fltr in filter list if ( not isinstance ( fltr , filters .  Exec  Command  Filter  ) ) ] args = f . exec args ( userargs ) if ( ( not args ) or ( not match filter ( leaf filters , args ) ) ) : continue if ( not os . access ( f . exec path , os . X OK ) ) : if ( not found filter ) : found filter = f continue return f return found filter 
def  Query  Value  Ex  ( key , value name ) : regqueryvalueex = advapi32 [ ' Reg  Query  Value  Ex W' ] regqueryvalueex . restype = ctypes . c long regqueryvalueex . argtypes = [ ctypes . c void p , ctypes . c wchar p , LPDWORD , LPDWORD , LPBYTE , LPDWORD ] size = 256 data type = ctypes . wintypes . DWORD ( ) while  True  : tmp size = ctypes . wintypes . DWORD ( size ) buf = ctypes . create string buffer ( size ) rc = regqueryvalueex ( key . handle , value name , LPDWORD ( ) , ctypes . byref ( data type ) , ctypes . cast ( buf , LPBYTE ) , ctypes . byref ( tmp size ) ) if ( rc != ERROR MORE DATA ) : break if ( size > ( ( 10 * 1024 ) * 1024 ) ) : raise exceptions .  Windows  Error  ( ' Value   too  big  to  be  read  by  GRR.' ) size *= 2 if ( rc != ERROR SUCCESS ) : raise ctypes .  Win  Error  ( 2 ) return (  Reg 2 Py  ( buf , tmp size . value , data type . value ) , data type . value ) 
def simple Open ID Transformer  ( endpoint ) : if ( 'http://openid.net/signon/1.0' not in endpoint . type uris ) : return  None  delegates = list ( endpoint . service element . findall ( '{http://openid.net/xmlns/1.0} Delegate ' ) ) assert ( len ( delegates ) == 1 ) delegate = delegates [ 0 ] . text return ( endpoint . uri , delegate ) 
def log Snatch  ( search Result  ) : log Date  = datetime . datetime . today ( ) . strftime (  History  . date format ) release = prepare Failed  Name  ( search Result  . name ) provider Class  = search Result  . provider if ( provider Class  is not  None  ) : provider = provider Class  . name else : provider = 'unknown' show obj = search Result  . episodes [ 0 ] . show failed db con = db . DB Connection  ( 'failed.db' ) for episode in search Result  . episodes : failed db con . action ( 'INSERT  INTO  history  (date,  size,  release,  provider,  showid,  season,  episode,  old status)VALUES  (?,  ?,  ?,  ?,  ?,  ?,  ?,  ?)' , [ log Date  , search Result  . size , release , provider , show obj . indexerid , episode . season , episode . episode , episode . status ] ) 
def open repo closing ( path or repo ) : if isinstance ( path or repo ,  Base  Repo  ) : return  noop context manager ( path or repo ) return closing (  Repo  ( path or repo ) ) 
def  weight func ( dist ) : with np . errstate ( divide = 'ignore' ) : retval = ( 1.0 / dist ) return ( retval ** 2 ) 
def test control character rendering ( ) : super Console  .  Send  Keys  ( 'output Redirect  Start {(}{)}{ENTER}' ) test Regex  = '' super Console  .  Send  Keys  ( 'print  "^(d)^(d){LEFT}{DELETE}"{ENTER}' ) test Regex  += chr ( 4 ) super Console  .  Send  Keys  ( 'while   True :  pass{ENTER}{ENTER}' ) super Console  .  Send  Keys  ( '^(c)' ) print ' Code  Plex    Work    Item   12401' errors = [ ' Traceback   (most  recent  call  last):' , '     File ' , '     File ' , ' Keyboard  Interrupt ' , '' ] super Console  .  Send  Keys  ( 'def  foo{(}{)}:{ENTER}try:{ENTER}while   True :{ENTER}pass{ENTER}' ) super Console  .  Send  Keys  ( '{BACKSPACE}{BACKSPACE}except   Keyboard  Interrupt :{ENTER}print  "caught"{ENTER}{BACKSPACE}{ENTER}' ) super Console  .  Send  Keys  ( 'print  "after"{ENTER}{BACKSPACE}{ENTER}foo{(}{)}{ENTER}' ) sleep ( 2 ) super Console  .  Send  Keys  ( '^(c)' ) test Regex  += 'caughtafter' super Console  .  Send  Keys  ( 'output Redirect  Stop {(}{)}{ENTER}' ) verify Results  ( get Test  Output  ( ) [ 0 ] , test Regex  ) errlines = get Test  Output  ( ) [ 1 ]  Assert  ( ( ( ' Keyboard  Interrupt ' + newline ) in errlines ) , ( ' Keyboard  Interrupt   not  found  in:' + str ( errlines ) ) ) 
@ pytest . fixture def issue 2401 po ( po directory , settings , afrikaans tutorial ) : return  require store ( afrikaans tutorial , settings . POOTLE TRANSLATION DIRECTORY , 'issue 2401.po' ) 
def get num escape turns ( x , y ) : c = complex ( x , y ) z = complex ( x , y ) num iterations = 0 while ( ( MIN MAGNITUDE < np . absolute ( z ) < ESCAPE MAGNITUDE ) and ( num iterations < MAX ITERATIONS ) ) : z = ( ( z ** 2 ) + c ) num iterations += 1 return ( float ( num iterations ) / float ( MAX ITERATIONS ) ) 
def nonsingular ( vmin , vmax , expander = 0.001 , tiny = 1e-15 , increasing =  True  ) : if ( ( not np . isfinite ( vmin ) ) or ( not np . isfinite ( vmax ) ) ) : return ( ( - expander ) , expander ) swapped =  False  if ( vmax < vmin ) : ( vmin , vmax ) = ( vmax , vmin ) swapped =  True  if ( ( vmax - vmin ) <= ( max ( abs ( vmin ) , abs ( vmax ) ) * tiny ) ) : if ( vmin == 0.0 ) : vmin = ( - expander ) vmax = expander else : vmin -= ( expander * abs ( vmin ) ) vmax += ( expander * abs ( vmax ) ) if ( swapped and ( not increasing ) ) : ( vmin , vmax ) = ( vmax , vmin ) return ( vmin , vmax ) 
def add Geometry  List  ( element Node  , vertexes ) : for vertex in vertexes : vertex Element  = get Unbound  Vertex  Element  ( vertex ) vertex Element  . parent Node  = element Node  element Node  . child Nodes  . append ( vertex Element  ) 
def init ( image , root =  None  ) : nbd = connect ( image ) if ( not nbd ) : return '' return mount ( nbd , root ) 
def render ( template file , saltenv = 'base' , sls = '' , argline = '' , context =  None  , ** kws ) : tmp data = salt . utils . templates . WEMPY ( template file , to str =  True  , salt =   salt   , grains =   grains   , opts =   opts   , pillar =   pillar   , saltenv = saltenv , sls = sls , context = context , ** kws ) if ( not tmp data . get ( 'result' ,  False  ) ) : raise  Salt  Render  Error  ( tmp data . get ( 'data' , ' Unknown   render  error  in  the  wempy  renderer' ) ) return six . moves .  String IO ( tmp data [ 'data' ] ) 
def test arraytransforms ( ) : ra = ( np . ones ( ( 4 , ) , dtype = float ) * u . deg ) dec = ( ( 2 * np . ones ( ( 4 , ) , dtype = float ) ) * u . deg ) distance = ( np . ones ( ( 4 , ) , dtype = float ) * u . au ) test icrs = ICRS ( ra = ra , dec = dec , distance = distance ) test gcrs = GCRS ( test icrs . data ) bary arr = test icrs . transform to (  Barycentric  True  Ecliptic  ) assert ( bary arr . shape == ra . shape ) helio arr = test icrs . transform to (  Heliocentric  True  Ecliptic  ) assert ( helio arr . shape == ra . shape ) geo arr = test gcrs . transform to (  Geocentric  True  Ecliptic  ) assert ( geo arr . shape == ra . shape ) bary icrs = bary arr . transform to ( ICRS ) assert ( bary icrs . shape == test icrs . shape ) helio icrs = helio arr . transform to ( ICRS ) assert ( helio icrs . shape == test icrs . shape ) geo gcrs = geo arr . transform to ( GCRS ) assert ( geo gcrs . shape == test gcrs . shape ) 
def salt config to yaml ( configuration , line break = '\n' ) : return yaml . dump ( configuration , line break = line break , default flow style =  False  ,  Dumper  =  Safe  Ordered  Dumper  ) 
def  get daily jobs ( date =  None  ) : if ( not date ) : date = ( datetime . date . today ( ) - datetime . timedelta ( days = 1 ) ) if isinstance ( date , datetime . datetime ) : raise  Value  Error  ( ' This   requires  a  valid  date,  not  a  datetime' ) next date = ( date + datetime . timedelta ( days = 1 ) ) date str = date . strftime ( '%Y-%m-%d' ) extra = dict ( where = [ 'DATE(created)=%s' ] , params = [ date str ] ) stats = { 'addon total downloads' : ( lambda :  Download  Count  . objects . filter ( date  lt = next date ) . aggregate ( sum =  Sum  ( 'count' ) ) [ 'sum' ] ) , 'addon downloads new' : ( lambda :  Download  Count  . objects . filter ( date = date ) . aggregate ( sum =  Sum  ( 'count' ) ) [ 'sum' ] ) , 'addon count new' :  Addon  . objects . valid ( ) . extra ( ** extra ) . count , 'version count new' :  Version  . objects . filter ( channel = amo . RELEASE CHANNEL LISTED ) . extra ( ** extra ) . count , 'user count total' :  User  Profile  . objects . filter ( created  lt = next date ) . count , 'user count new' :  User  Profile  . objects . extra ( ** extra ) . count , 'review count total' :  Review  . objects . filter ( created  lte = date , editorreview = 0 ) . count , 'review count new' :  Review  . objects . filter ( editorreview = 0 ) . extra ( where = [ 'DATE(reviews.created)=%s' ] , params = [ date str ] ) . count , 'collection count total' :  Collection  . objects . filter ( created  lt = next date ) . count , 'collection count new' :  Collection  . objects . extra ( ** extra ) . count , 'collection addon downloads' : ( lambda :  Addon  Collection  Count  . objects . filter ( date  lte = date ) . aggregate ( sum =  Sum  ( 'count' ) ) [ 'sum' ] ) } if ( date == ( datetime . date . today ( ) - datetime . timedelta ( days = 1 ) ) ) : stats . update ( { 'addon count nominated' :  Addon  . objects . filter ( created  lte = date , status = amo . STATUS NOMINATED , disabled by user = 0 ) . count , 'addon count public' :  Addon  . objects . filter ( created  lte = date , status = amo . STATUS PUBLIC , disabled by user = 0 ) . count , 'addon count pending' :  Version  . objects . filter ( created  lte = date , files  status = amo . STATUS PENDING ) . count , 'collection count private' :  Collection  . objects . filter ( created  lte = date , listed = 0 ) . count , 'collection count public' :  Collection  . objects . filter ( created  lte = date , listed = 1 ) . count , 'collection count editorspicks' :  Collection  . objects . filter ( created  lte = date , type = amo . COLLECTION FEATURED ) . count , 'collection count normal' :  Collection  . objects . filter ( created  lte = date , type = amo . COLLECTION NORMAL ) . count } ) return stats 
def  State  Handler  ( auth required =  True  ) : def  Decorator  ( f ) : ' Initialised    Decorator .' @ functools . wraps ( f ) def  Decorated  ( self , responses =  None  , request =  None  , direct response =  None  ) : 'A  decorator  that  defines  allowed  follow  up  states  for  a  method.\n\n             Args :\n                self:   The   self  of  the  wrapped  function.\n\n                responses:   The   responses  for  this  state.\n\n                request:   The   request  sent  out  originally.\n\n                direct response:  A  final  responses  object  that  does  not  need  wrapping\n                                                  again.   If   given,  neither  request  nor  responses  is  used.\n\n             Returns :\n                 This   calls  the  state  and  returns  the  obtained  result.\n            ' if ( 'r' in self . mode ) : pending termination = self .  Get  ( self .  Schema  . PENDING TERMINATION ) if pending termination : self .  Error  ( pending termination . reason ) return runner = self .  Get  Runner  ( ) if ( direct response is not  None  ) : return f ( self , direct response ) if ( not isinstance ( responses ,  Responses  ) ) : responses =  Responses  ( request = request , responses = responses , auth required = auth required ) if responses . status : runner .  Save  Resource  Usage  ( request , responses ) stats . STATS .  Increment  Counter  ( 'grr worker states run' ) if ( f .   name   == ' Start ' ) : stats . STATS .  Increment  Counter  ( 'flow starts' , fields = [ self .  Name  ( ) ] ) args = [ self , responses ] res = f ( * args [ : f . func code . co argcount ] ) return res return  Decorated  return  Decorator  
def  diff ( old pipeline definition , new pipeline definition ) : old pipeline definition . pop ( ' Response  Metadata ' ,  None  ) new pipeline definition . pop ( ' Response  Metadata ' ,  None  ) diff = difflib . unified diff ( json . dumps ( old pipeline definition , indent = 4 ) . splitlines ( 1 ) , json . dumps ( new pipeline definition , indent = 4 ) . splitlines ( 1 ) ) return '' . join ( diff ) 
def test show fiff ( ) : check usage ( mne show fiff ) with  Argv  Setter  ( ( raw fname , ) ) : mne show fiff . run ( ) 
def vary on headers ( * headers ) : def decorator ( func ) : def inner func ( * args , ** kwargs ) : response = func ( * args , ** kwargs ) patch vary headers ( response , headers ) return response return inner func return decorator 
@ cmd def coverage ( ) : install ( ) sh ( ( '%s  -m  coverage  run  %s' % ( PYTHON , TSCRIPT ) ) ) sh ( ( '%s  -m  coverage  report' % PYTHON ) ) sh ( ( '%s  -m  coverage  html' % PYTHON ) ) sh ( ( '%s  -m  webbrowser  -t  htmlcov/index.html' % PYTHON ) ) 
def get accumulator dir ( cachedir ) : fn  = os . path . join ( cachedir , 'accumulator' ) if ( not os . path . isdir ( fn  ) ) : os . makedirs ( fn  ) return fn  
def incr ratelimit ( user , domain = 'all' ) : ( list key , set key ,   ) = redis key ( user , domain ) now = time . time ( ) if ( len ( rules ) == 0 ) : return with client . pipeline ( ) as pipe : count = 0 while  True  : try : pipe . watch ( list key ) last val = pipe . lindex ( list key , ( max api calls ( user ) - 1 ) ) pipe . multi ( ) pipe . lpush ( list key , now ) pipe . ltrim ( list key , 0 , ( max api calls ( user ) - 1 ) ) pipe . zadd ( set key , now , now ) if ( last val is not  None  ) : pipe . zrem ( set key , last val ) api window = max api window ( user ) pipe . expire ( list key , api window ) pipe . expire ( set key , api window ) pipe . execute ( ) break except redis .  Watch  Error  : if ( count > 10 ) : logging . error ( ' Failed   to  complete  incr ratelimit  transaction  without  interference  10  times  in  a  row!   Aborting   rate-limit  increment' ) break count += 1 continue 
def urlencode ( query ) : if isinstance ( query , dict ) : query = query . items ( ) return u'&' . join ( [ u'=' . join ( [ escape ( k ) , escape ( v ) ] ) for ( k , v ) in query ] ) 
def get common complete suffix ( document , completions ) : def doesnt change before cursor ( completion ) : end = completion . text [ : ( - completion . start position ) ] return document . text before cursor . endswith ( end ) completions2 = [ c for c in completions if doesnt change before cursor ( c ) ] if ( len ( completions2 ) != len ( completions ) ) : return u'' def get suffix ( completion ) : return completion . text [ ( - completion . start position ) : ] return  commonprefix ( [ get suffix ( c ) for c in completions2 ] ) 
def debug ssh ( function ) : def wrapper ( self , * args , ** kwargs ) : try : return function ( self , * args , ** kwargs ) except tempest . lib . exceptions . SSH Timeout  : try : original exception = sys . exc info ( ) caller = ( test utils . find test caller ( ) or 'not  found' ) if self . server : msg = ' Caller :  %s.   Timeout   trying  to  ssh  to  server  %s' LOG . debug ( msg , caller , self . server ) if ( self . log console and self . servers client ) : try : msg = ' Console   log  for  server  %s:  %s' console log = self . servers client . get console output ( self . server [ 'id' ] ) [ 'output' ] LOG . debug ( msg , self . server [ 'id' ] , console log ) except  Exception  : msg = ' Could   not  get  console log  for  server  %s' LOG . debug ( msg , self . server [ 'id' ] ) six . reraise ( * original exception ) finally : (   ,   , trace ) = original exception del trace return wrapper 