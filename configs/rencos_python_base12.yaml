# Classical Transformer  base12
# use relative position information
# no initialize model
data:
    train_data_path: "data/rencos_python/train"
    dev_data_path: "data/rencos_python/valid"
    test_data_path: "data/rencos_python/test_sample"
    dataset_type: "plain"
    src:
        language: "code"
        level: "word" # word bpe
        tokenizer_type: "sentencepiece" # subword-nmt sentencepiece
        vocab_min_freq: 1
        vocab_max_size: 50000
        lowercase: True
        normalize: True # tokernizer pre-process sentence.strip()
        max_length: 400
        min_length: 1
        filter_or_truncate: "truncate"
        vocab_file: "models/rencos_python/transformer_base12_continue/src_vocab.txt"
    trg: 
        language: "summary"
        level: "word"
        tokenizer_type: "sentencepiece"
        vocab_min_freq: 1
        vocab_max_size: 30000
        lowercase: True
        normalize: True 
        max_length: 30
        min_length: 1
        filter_or_truncate: "truncate"
        vocab_file: "models/rencos_python/transformer_base12_continue/trg_vocab.txt"

training:
    model_dir: "models/rencos_python/transformer_base12"
    overwrite: True
    random_seed: 3407
    use_cuda: True
    num_workers: 4
    optimizer: "adam"
    adam_betas: [0.9, 0.999]
    normalization: "batch"  # batch,tokens | batch means loss divide sentences in the batch; token means loss divide tokens in the batch.
    loss: "CrossEntropy"
    label_smoothing: 0.0
    scheduling: "StepLR"    # "ReduceLROnPlateau", "StepLR", "ExponentialLR"
    mode: "max"             # for schedule="ReduceLROnPlateau"
    factor: 0.5             # for schedule="ReduceLROnPlateau"
    patience: 5             # for scheduling = 'ReduceLROnPlatea'
    step_size: 1            # for scheduling = "StepLR" 
    gamma: 0.99             # for scheduling = "StepLR" or "ExponentialLR"
    learning_rate: 0.01
    weight_decay: 0
    learning_rate_min: 0.0000001
    keep_best_ckpts: 3
    logging_freq: 1
    validation_freq: 1      # after how many epochs
    log_valid_sentences: [0,1,2,3,4]
    early_stopping_metric: "bleu"
    shuffle: True
    epochs: 200
    max_updates: 800000 
    batch_size: 32
    batch_type: "sentence"
    # load_model: None
    reset_best_ckpt: False
    reset_scheduler: False
    reset_optimizer: False
    reset_iter_state: False
    # clip_grad_val: 1
    clip_grad_norm: 5.0

testing:
    batch_size: 64
    batch_type: "sentence"
    max_output_length: 30
    min_output_length: 1
    eval_metrics: ['bleu','rouge-l']
    n_best: 1
    beam_size: 4
    beam_alpha: -1
    return_attention: False
    return_prob: "hypotheses"   # hypotheses, references, none
    generate_unk: True
    repetition_penalty: -1       # >1, -1 means no repetition penalty. # no implemented
    no_repeat_ngram_size: -1    # no implemented


model:
    initializer: "xavier_uniform"     # xavier_uniform xavier_normal uniform normal
    embed_initializer: "xavier_uniform"
    tied_softmax: False
    tied_embeddings: False
    encoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "pre"
        freeze: False
        dropout: 0.2
        embeddings:
            embedding_dim: 512
            scale: False
            freeze: False
            dropout: 0.2
            # load_pretrained
        src_pos_emb: "relative"         # encoder "absolute", "learnable", "relative"
        max_src_len: 0                  # for learnable. Keep same with data segment
        max_relative_position: 32       # only for relative position, else must be set to 0
        use_negative_distance: True     # for relative position
    decoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "pre"
        freeze: False
        dropout: 0.2
        embeddings:
            embedding_dim: 512
            scale: False
            freeze: False
            dropout: 0.2
            # load_pretrained
        trg_pos_emb: "learnable"        # encoder "absolute", "learnable","relative"
        max_trg_len: 30                 # for learnable. keep same with data segment
        max_relative_position: 0        # only for relative position, else must be set to 0
        use_negative_distance: False    # for relative position


retriever:
    retriever_model_dir: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/inner_attention_encode/"
    # code_semantic_dir: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/code_semantic/"
    # code_semantic_path: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/code_semantic/embedding"
    pre_trained_model_path: "datastore/datastore_rencos_python/transformer_base12/401683.ckpt"
    embedding_path: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/inner_attention_encode/embedding"   # for static_retriever, dynamic_retriever
    token_map_path: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/inner_attention_encode/token_map"   # for static_retriever, dynamic_retriever
    index_path: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/inner_attention_encode/index"           # for static_retriever, dynamic_retriever
    # code_index_path: "datastore/datastore_rencos_python/transformer_base12/datastore_401683/code_semantic/code_index"

    type: "static_retriever"                                    # [no_retriever, static_retriever, dynamic_retriever]
    index_type: "INNER"                                         # ["L2", "INNER"]
    use_code_representation: True
    kernel: "Gaussian"                                          # [Gaussian, Laplacian]
    top_k: 16                                                    # for static_retriever, dynamic_retriever
    mixing_weight: 0.5                                          # for static_retriever
    bandwidth: 20                                               # for static_retriever
    in_memory: True                                             # for dynamic_retriever
