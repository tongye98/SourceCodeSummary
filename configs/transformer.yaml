data:



training:
    model_dir: "models/test"
    overwrite: True
    random_seed: 980820
    use_cuda: True
    num_workers: 4
    normalization: "batch" # batch tokens
    loss: "CrossEntropy"
    label_smoothing: 0.0
    learning_rate_min: 1.0e-8
    keep_best_ckpts: 5
    logging_freq: 100
    validation_freq: 1000
    log_valid_sentences: [0,1,2]
    early_stopping_metric: "bleu"
    shuffle: True
    epochs: 100
    max_updates: "np.inf"
    batch_size: 32
    batch_type: "sentence"
    load_model: None
    reset_best_ckpt: False
    reset_scheduler: False
    reset_optimizer: False
    reset_iter_state: False
    # clip_grad_val: 
    # clip_grad_norm:

testing:
    

model:
    initializer: "xavier_uniform"     # xavier_uniform xavier_normal uniform normal
    init_gain: 1.0 # for xavier
    init_weight: 0.01 # for uniform normal
    embed_initializer: "normal"
    embed_init_gain: 1.0 # for xavier
    embed_init_weight: 0.01
    bias_initializer: "zeros"
    bias_init_weight: 0.01
    tied_softmax: False
    tied_embeddings: False
    encoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "post"
        freeze: False
        dropout: 0.1
        embeddings:
            embedding_dim: 512
            scale: True
            freeze: False
            dropout: 0.1
            # load_pretrained
    decoder:
        model_dim: 512
        ff_dim: 2048
        num_layers: 6
        head_count: 8
        layer_norm_position: "post"
        freeze: False
        dropout: 0.1
        embeddings:
            embedding_dim: 512
            scale: True
            freeze: False
            dropout: 0.1
            # load_pretrained